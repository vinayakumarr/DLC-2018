{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c1 = [2,3]\n",
    "c2 = [13,15]\n",
    "no = 50\n",
    "Class1 = np.matlib.repmat(c1, no,1) + np.random.randn(no,len(c1))\n",
    "Class2 = np.matlib.repmat(c2, no,1) + np.random.randn(no,len(c2))\n",
    "Data = np.append(Class1,Class2,axis = 0)\n",
    "Trainlabel  = np.append(np.zeros((no,1)),np.ones((no,1)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuQnHWd7/H3dxg5CRkSsuQQTkIyGcbSZQkiHC9Rj4ce\nJEqtmnip3RJmgIBKlUcDBIJocDIzNSyLGDd6oudUpSQJmOCWlz0L1rpHQsVObQ4b0CXILV6YzCUk\nZrJRIJkEXSDf80d3T7p7+t799PP0zOdV1VUz3U8/z5cBnu/v+n3M3RERkamtKewAREQkfEoGIiKi\nZCAiIkoGIiKCkoGIiKBkICIiBJwMzOw+Mxs1s6fT3rvYzP7VzPaY2RNm9o4gYxARkeKC7hlsBj6U\n9d69QI+7XwL0AF8LOAYRESki0GTg7ruAl7LePgnMSv58FnAgyBhERKS45hCuuQr4qZl9HTDgvSHE\nICIiacKYQP4ccLO7LySRGDaFEIOIiKSxoGsTmVkr8GN3f1vy95fd/ay0z19x91l5vqvCSSIiFXB3\nK+f4evQMLPlKOWBmlwGY2QeA3xT6srtH/tXT0xN6DIpTMSpOxZl6VSLQOQMzexCIAWeb2QiJ1UOf\nBf6nmZ0G/BG4McgYRESkuECTgbtfnecj7S0QEYkQ7UCugVgsFnYIJVGctdMIMYLirLVGibMSgU8g\nV8PMPMrxiYhEkZnhEZxAFhGRiFMyEBERJQMREVEyEBERlAxERIRwCtWJiFRscHCY7u4tHDhwkvnz\nm+jvX0FbW2vYYTU8LS0VkYYxODjM0qUbGBjoA2YAx2lv72H79pWRSAhRSVSVLC1VMhCRyCh2M+3q\n6mPbttUkEkHKcTo717F1a09Nr1VJ7FFJVJUkAw0TicgEYbRwc91Md+/OvJkeOHCSzEQAMIODB0+W\nFXMp1ypXd/eWtPMl4hoY6KO7u/xEFYqwq+sVqbznIlJf+/YNeXv7bQ5jDu4w5u3tt/m+fUMVnauz\ns9djsbXe2dlb8Bydnb1p1/Txa3d29hY9ZvnyW8qKudC1yok5XSy2Nut8iVdHx9ry/mg1kLx3lnW/\nVc9ARDJU08JNb53PnHmCPXteYf/+9ZTS+i7U6k/p71/B7t09E4Zi3JsZGOgtOeZ81xoYOF5xj2H+\n/CbgONlDWPPmNcaizcaIUkTqppSbci6poZdt21YTj/fx8MO97N/fAhwZP0fiBr0l5/dP3UzTHWdw\n8Fk6Onro6uoDYPv2lXR2rqOjo4fOznVs376So0fPKDnmwcFhhoaezXmtQ4cG8iTC3DGn6+9fQXt7\nT9p5E4mqv39F0e9GgXoGIpKh0hZurh4F9APrSDzKJPFevqSSq9Xf3Px5hobuYmjoAtJb6dmt/VJj\nTiWsoaG7kjFl9jDmzFnA0FD5iRCgra2V7dtX0t29joMHTzJz5lHcm7nhhk2NsQS23HGler7QnIFI\n3VU6Z5BvzBzWThiXL3Ttzs5e7+hY64sWfcLh+ZLG9Xfu3FVSzJlzBUMOvQ53+qJFnxg/Z7F5iyD/\nhrVCBXMGQd/M7wNGgaez3l8J7AWeAe4p8P1A/lAiUlj6TTnfJGr2DXnZstU5b6TwlYpuiPmSy5Il\nt+e80e7cuatozMUmeWt1E69VUqlUJckg6GGizcAG4IHUG2YWAz4KXOTur5vZnIBjEJEytbW1Fpws\nzrU0c8GCVSxcuIaRkbvH31u4cA2XXNLE0aM9zJvXRH9/6Us38w39HDo0wNDQA2SP62/cWHyCu9hw\nUvZQT7kxp1Q67xKmoB97ucvMsv+KnyPRG3g9ecyRid8UkSjLNT+wf/96li//Cu9/f/qN9NaKx8nz\nrRyqZlw/85xHgO8wbdo+jh2bx+DgMG1trUUTYSkacmVRuV2Jcl9AK2nDRMAeoBfYDfwMeEeB79a8\n+yQi1avXmvpcw1XVDsHs2zfky5ff4tOnXx/YmH4jzhmEsZqoGZjt7kvM7J3A94Hz8x3c29s7/nMs\nFpvUzyAVaRT1avnmaqXn6zH096/MOC7fjuS2tlZaWs7i1VfvYuIS0trsFq7VcFO2fP9M8XiceDxe\nXdDlZo9yX0zsGfwEuCzt9xeAs/N8t/YpU0SqFnbLt9gEd7H4at2zqXTXcjnnX7ZstU+bdnVyQn6o\n4N+cqK0mSsTEIuCZtN9vBPqSP78FGC7w3Zr8IUWk9kpZcVTpOau9qRYbSqrlap+gE2Ou88Nt4wkh\nV8yRSwbAg8BB4E/ACHA9iWGi75JYVvqL9F5Cju/X5I8pItFX6U01VwLJ1/KfPfuvfd++oZrewINe\nRprv/Ik9Erl7M5Ukg6BXE12d56NrgryuiDSeSmoi5as+euGFRq45jZdeepn3vvfLPPbY39ZsTD/o\nZaT5zg8nqeU8jcpRiEgosidDX3jhJcq9qeZLIIsXf4Xm5ht5/fWNpJJEovzENzh0aA3d3VvYurWn\nJpPFQU+m5zs/nMw5cV4pJQMRqbtcLfqWllRhggvSjix8U83Xaj56dCbTpo0wNraORAu6iUThg1bg\nzJpu/ip1dVMtzz99+ko++MFZrF9/S83qHSkZiEjd3XLLtya06MfGNtDc/Elef/1HlHpTLdQqnzMH\nxsYmPhUNjmUkmFQPZWDgBIcOvcDcuQt485tnj1cbLfbAnKCWkRY+f0/ti96VO8lQzxeaQBaZVFIb\nvsz+KucEL3zKzzjjSl+y5PaSVhMVmgjeuXOXNzdfm7UK5zo/99yrxs+bf6XO875gwWd94cKbQls+\nWw0qmECO8N5oEZlMUkNDDz3UgvtbyfU8AfgjJ07M5sUX95ZU8jnVal62rJe5c6/lnHOuYfHiNwBY\nsOA8YrE3cfrpH8HsE5x++oe54opmHnvsb8fPm7vsdh/wffbvn5tWZynxWanPNmhEGiYSkbo4deO9\nF7iB7OcJwOeBu4ELePHF4yxdWvoziZ97zhkd/d/ADB566DhPPrkKs+mMjHxz/PwLFvSwcWPm+fKv\n1BkA/kvOz6JcbK4a6hmISF2cuvE2AXNITOheQyIpXAvcwanJ49Jb4bmL5pXWqp858wS5eyitwHM5\nP4t0sbkqTM5/KhGJnFOTvStIJIA5wMXAF4HFZK4igtQzibu6+sYfezk4ODzhvLlb90053sts1Q8O\nDrNnzytAN+mPqkzE9hnga8AXMj5buHBNxmMsBweHi8bXKDRMJCJ1kblEciVwD83NT+D+Cd544xwS\nN+XPkGiVAxzn2Wd/xe7d36PQw+lzryg6meO9zFZ9d/cW9u9fT6KU9TXARWQuQQWYReKxnSeBk1xy\nSdP4tfNteCt1aCtyyp1xrucLrSYSmVTS6xktX37LhNU6sGq85k5Ly/We77GX2efMXhFUykqgzJIV\nhUs+5CpkF/bTzAohauUoRETSpZek7urqY2QkfR/ADKCfuXOv4YorLuaFF2by+OMTh46yJ3Bzr8O/\nE6Dg2v/MHsUKJk5odwOfTL6X6BnMnDk2/v1GfJpZIUoGIhKKfDfTv/iLi9i6NTEG//jj+Yd68tX2\nT1eo3ETmsFUr8GlaWq5i8eI/55xznCeeOMShQz8C+kkliD171ow/Ea0hn2ZWSLldiXq+0DCRyKSV\nb5hl2bLV3tnZ60uWfNFbWj6aNlR0aqinVlVHC5XhXrZsdcFhoLCf6VAIFQwTWeJ70WRmHuX4RKR0\n2S35G2+8ghtu+D8ZE7ALF67B/dXkxO6pmkWLF8+kvX32eOu/q6uPbdsmlpro7KzNk8oAOjp6iMf7\ncr6/Y0dfxj/TqaGo4hvl6sHMcHcr5zsaJhKRwOVbebNp08fZuPHUuP7YWBMPPZRKBJCqWdTefuom\nPzg4zPbtvySxea2JxHh/K7Uery9lGCjXYzkblZKBiAQuX6npjRszW/IdHT0UmpRNJZXDh79LZmnq\nlcCcmo7XB12NNGoCTQZmdh/wEWDU3d+W9dltJHZ1zHH3PwQZh4iEq9SVN8Va4/lrCd1De/urNb1R\nB12NNGqC7hlsBjYAD6S/aWbnAUuBxt2uJyIlK3XlTbHWeL6kMnfuMNu399f8Rj2ZhoGKCXQNlLvv\nAl7K8dF64PYgry0i0dHfv4L29h7SSzskbvIrMo5LtcY7O9fR0dFDZ+e6jB29p5JKuuNccUX7pG2x\n10vgq4nMrBX4cWqYyMyWATF3v9XMBoH/mm+YSKuJRCaPWqy8yTUR3d7ewCUgAlLJaqK6JgMzmw78\nDFjq7seSyeAd7v77PN/1np5TXbRYLEYsFgs0XhGJtqgu5wxTPB4nHo+P/97X1xf5ZLAYeBQ4ARhw\nHnAAeJe7H87xXfUMRETKFNV9BpZ84e7PAueOf5DoGVzq7rnmFUREQlNKuYvJJOilpQ8CMeBsMxsB\netx9c9ohTjJRiIhERRTLUwednFSOQkQkSz3KXZSj3InzqA4TiYhEWnare2DgBFEqT51vB3d3d+2S\nk5KBiExpuVrdLS1XEaXy1PV4dkKDFt4WEamNXK3usbGv0tKykmKb5Ool32a7WiYn9QxEZErL3eq+\nIFk2Oxp1iepRNE/JQESmtHx1k9rbZ0emLlE9iuZpNZGITGmTscRFJMtRVEPJQETqYbKVuFAyEBGR\nipKBVhOJiIiSgYiIKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIkLAycDM7jOzUTN7Ou29e81s\nr5k9ZWY/MrOZQcYgIiLFBd0z2Ax8KOu9R4AL3f3twG+BLwccg4hMQYODw3R19dHR0UNXVx+Dg8Nh\nhxRpgVYtdfddZtaa9d6jab/uBj4ZZAwiMvVE8RnGURf2nMENwD+HHIOITDL5HxO5JcSooi205xmY\n2Z3Aa+7+YKHjent7x3+OxWLEYrFgAxORhlePx0RGSTweJx6PV3WOUJKBma0A/hK4vNix6clARKQU\n+R5YE9YzjIOW3VDu6+sr+xz1+MtY8pX4xexK4HZgmbv/qQ7XF5Eppr9/Be3tPUTlGcaNINDnGZjZ\ng0AMOBsYBXqANcDpwO+Th+129/+R5/t6noGIVGSyPbCmHHq4jYiI6OE2IiJSGSUDERFRMhARESUD\nERFByUBERFAyEBERlAxERAQlAxERQclARERQMhAREZQMREQEJQMREUHJQEREUDIQERGUDEREhICT\ngZndZ2ajZvZ02nuzzewRM/u1mf3UzGYFGYOIiBQXdM9gM/ChrPe+BDzq7m8FdgBfDjgGEREpItBk\n4O67gJey3l4O3J/8+X7gY0HGICIixYUxZ3COu48CuPsh4JwQYhARkTRRmEDWQ45FRELWHMI1R81s\nrruPmtm5wOFCB/f29o7/HIvFiMViwUYnItJg4vE48Xi8qnOYe7ANczNbBPzY3S9K/v5V4A/u/lUz\nuwOY7e5fyvNdDzo+EZHJxsxwdyvrO0HebM3sQSAGnA2MAj3APwI/ABYAw8Bfu/vLeb6vZCAiUqbI\nJYNqKRmIiJSvkmRQ0gSymS0xs5+b2ZiZ/YeZvWFmRysLU0REoqbU1UTfAq4CfgtMBz4DfDuooERE\npL5KXlrq7i8Ap7n7G+6+GbgyuLBERKSeSl1aesLMTgeeMrN7gd8RjT0KIiJSA6Xe0K9JHvsF4DiJ\nlUCfCCooERGpr1KTwcfc/Y/uftTd+9z9VuAjQQYmIiL1U2oyuC7HeytqGIeIiISo4JyBmV0FXA20\nmdnDaR+dCfwhyMBERKR+ik0gP0ZisngO8PW0948BT+f8hoiINBztQBYRmWS0A1lERCqiHcgiIqId\nyCIioh3IIiJCdTuQPxlUUCIiUl8lryYys/8M4O7/HmhEmdfUaiIRkTLVfDWRJfSa2RHg18BvzOzf\nzWxtNYEmz73KzJ41s6fNbFtyGEpEREJQbJhoFfA+4J3u/mfuPht4N/A+M1tV6UXNbB6wErjU3d9G\nYu7iU5WeT0REqlMsGVwDXOXug6k33H0f0AVcW+W1TwNmmFkzcAZwsMrziYhIhYolgze5+5HsN5Pz\nBm+q9KLufpBEeYsR4ADwsrs/Wun5RESkOsWSwX9U+FlBZnYWsBxoBeYBLWZ2daXnExGR6hTbZ3Bx\nnrITBkyr4rpXAPvc/Q8AZvYPwHuBB7MP7O3tHf85FosRi8WquKyIyOQTj8eJx+NVnSOUQnVm9i7g\nPuCdwJ+AzcDP3f3bWcdpaamISJkCK1RXa+7+BPBDYA/wSxI9jY1hxCIiIiphLSIy6TRMz0BERKJF\nyUBERJQMREREyUBERFAyEBERlAxERAQlAxERQclARERQMhAREZQMREQEJQMREUHJQEREUDIQERGU\nDEREBCUDERFByUBERAgxGZjZLDP7gZntNbPnzOzdYcUiIjLVNYd47W8CP3H3vzKzZuCMEGMREZnS\nQnnspZnNBPa4e3uR4/TYSxGRMjXSYy/bgCNmttnMnjSzjWY2PaRYRESmvLCGiZqBS4HPu/svzOwb\nwJeAnuwDe3t7x3+OxWLEYrE6hSgi0hji8TjxeLyqc4Q1TDQX+Fd3Pz/5+38D7nD3j2Ydp2EiEZEy\nNcwwkbuPAvvN7C3Jtz4APB9GLCIiElLPAMDMLga+A7wJ2Adc7+6vZB2jnoGISJkq6RmElgxKkZ0M\nhgcH2dLdzckDB2iaP58V/f20trWFGKGISPRM6mQwPDjIhqVL6RsYYAZwHOhpb2fl9u2BJgQlIBFp\nNJM6GfR1dbF62zZmpH1+HFjX2UnP1q2BXD+sBCQiUo2GmUCuxMkDBzISAcAM4OTBg4Fdc0t393gi\nSF2vb2CALd3dgV1TRCQMDZMMmubP53jWe8eBpnnzArtmGAlIRCQMDZMMVvT309PePp4QUkM2K/r7\nA7tmkAloeHCQvq4uejo66OvqYnhwsOpziohUqmHmDCBtMvfgQZrmzQt8MjeoOQPNRYhIkCb1BHJY\ngkhAYUyGi8jUUUkyCLOEdUNobWur+Q1acxEiEjUNM2cwmZyYObPuk+EiIoUoGdTZ8OAgr+zZw5ch\nYzL8i8khKBGRMGiYiPruMt7S3c2q/ftZD9xDIhufBI689FIg1xMRKcWUTwa5Vvas+Zd/oemSS5j5\nyis1Tw4nDxzg+8B6yJxAfvVV1nV3awJZREIx5ZNB+i7jYWALcObICM+NjPANYA7Qs3t3zZZ9Ns2f\nz2ugCWQRiZQpP2eQWtkzDGwAVgN3AVuTvz8FTB8YoHvJkppsDlvR38/TLS2aQBaRSJnyySC1y3gL\n0AcZdYg+DdxH4nmcDxw+zOpt29iwdGlVCaG1rY07/umfWJmWEOqxm1pEpJApv+ksNWcwbWCAu7I+\n6yPRUwhic1i9d1OLyNTRcJvOzKwJ+AXworsvq9V5y1kd1NrWxsrt27n18ss5PjSUceMPcmw/iM1s\n9aDnO4hMTmFPIN9M4tnHM2t1wpx1f3JMAGff1G69/356brgh43u/OO00jr/xxoSeQSlj+5Pxplnq\n31ZEGpC7h/ICzgO2AzHg4TzHeLl6Ozt9DNzTXmPgvZ2d48cM7dvnt7W3jx83Bn5be7vv2rnTezs7\nfW1Hh/d2dvqunTtzHje0b1/BGPKdv9j3gjC0b1/inykW897OzqpiKOVvKyLhS947y7onh9kzWA/c\nDsyq5UlLqfuT76E16zZunDB0c9727axLG9tfWaCFn+oN/HL7dr57+PDE89d5H0GtW/KqqSQyeYWS\nDMzsw8Couz9lZjEg70RHb2/v+M+xWIxYLFbw3KnVQYWGdsq5qZU6tp9+472XaOwjyJv0KkxKpfxt\nRaT+4vE48Xi8upOU25WoxQu4GxgB9gG/A8aAB3IcV3b3qJQhmiCGO9LP2Zs8X9jDKWtjsYwYUq+1\nHR0VnS9Kw18ikh+NMkzk7muANQBmdhlwm7tfW4tzp1YHFRraWdHfT8/u3RMfLlPFOv/03sYKoIdT\n+xZKPX+tJ51r3ZIv5W8rIg2q3OxR6xdwGTWcQC7V+MRqcrK42tZtdm9jCPwr4NfMnVvS+YNodasl\nLzI1UUHPYMpvOquVah9lGdTTz7S5TWTqabhNZ7VSbHilHmv+yxlCyRVPUCt1GnVzm4jUV8Mng2LL\nJ4PcKJXrpl7sxpsvHrvwQq3UEZHwlDuuVM8XJcwZFFsZFNRGqVLH47M3fd2yfHnOeG5Zvlzj+yJS\nEzTKaqJaKja8ku/zgUcfpaejo+Jho1LW8OfqBaycNo0jZPYAZgAzjx7lhhzDTAC3L1/O6O7djAGt\n73kPt6xfr3F/Eamphk8GxZZP5vu8dXSUvtHRioeNKt3pvOGPf+QeIH2RaSre7PH94cFB/i4W4+6R\nkfFk0v3QQ/zNk09y586dE+KdjPWQRKROyu1K1PNFCcNExYZrcn2+Krn0s5pho1KGn/Jt+rp62rSS\nhoPyXWNVjni1jFREUpiKw0TFVvFkf/7Mc8+x/vBhWtPOUcmqnUIb11It9Geefz5nr2TeBz/IujPP\nLLrqKF/vYxQ4OTCQ8X61pSfUqxCZ2ho+GUDx5ZPpn/d1dTFn27aMzytZtZMvCQHj8wRHgFuAc0k8\nUu4kMLpgAXd+4xsl3WiPzppFd/K7TSR2Ns8BzgeeP3Qo49hqlqaqNLWIhD4UVOhFADuQgx5OSR/a\nGQK/Ka1O0Rj4TQsXFrxWavXRze9+t193xhkThrc+mzzv7UuW5L1uucNfKk0tMrlQwTDRlHsG8niL\nvrOTno4O1nV21rQFnN5C30KiIl/60M3dIyOsWrKEvq6uCc9STrXQV2/bxuzHH+fbJ05kfLefRL3v\nOcCM9vaM767o76envb2i5yqrNLWITIphonIFuSs3ffXSSXKXsr7o8GFWb9s2YSgmfdw/33f/E7Cy\npYVP33hjxmfVFJFTaWoRmRS1iaI0+Zk+/r4OWM3Em+w6ElVN9wJfWbSIxYsW0TR/PicGBvjq7t1A\nouJpru9eC9wF3FdG3aNyYq6krpKIREsltYkaPhmUcyOrV9JIXeelgQFefvrp8eGe4yTqdt+aPG4D\nmWWur2pp4XtjY8wAhnN83gOsBFqpTRG7XDGroJ1I45uSyaDUap9Bt35zJRqAv7nsMubu388xEjf4\n04GFwOskWvjpce8FvtbSwoZkQtgL3NHSQosZbz12jBWQsSS2p6ODvh07qo5dRCaXKVm1tJTJz+HB\nQW69/HIuGBpiHYzfVGv1XOJCxefW79/PERKt/K2cauV/ASaUpbgAmLl4Meva28db6Bv6+9nS3Z0z\n4WlMX0RqpeGTQbHJz9SN+oGhoZzDLbVYMZNvw9fnjh1jBok5gtRwT+rzb0HOshSz29snJKcgnswm\nIpIulKWlZnaeme0ws+fM7Bkzu6nScxVbUpnzRk1i2Wd20ujr6koMveRY9llIvt7JmBnHyb8yaN+0\naSUtBQ16OayISFg9g9eBW939KTNrAf7NzB5x91+Ve6JiSyrz3ahfI7N8RDU7cPMWw1uyhJ5nn2X6\nwEBVZSlS/5x6SI2IBCWUnoG7H3L3p5I/j5GYK51f6flSN8q+HTvo2bo144aaulGnOw7sXbRo/Gaf\nb5hnS3d3zutl9yKuuPHG8d7JMNAN3DhtGs3ufHzTJsaWL2fl9OkZvYCVLS28NjrKy2NjnDjzTE4e\nOMCW7u6cPZJqei0iIiUpd8tyrV/AImAIaMnxWbW7sksqP5Gvuujajo6Sz7dr506/Zflyv3769JzX\nSpWZuH3JEv9oS4s/nywrsSqrXEV2bKpGKiLlotGqliaHiH4I3OyJHsIEvb294z/HYjFisVhZ12ht\na+PjmzZx7XXXMePllzl+1lncumlTzt5DsdU6BVclbdzIWS0t3PXqq3krh/Zs3UpfVxff2717fO6i\nH/IeD9VXIxWRyS8ejxOPx6s7SbnZo1YvEvMV/5dEIgisUF0pLeuKj0l7LsLajo6Sehjpx6zNcWyh\n44v1WkRE3CvrGYRZqG4T8Ly7fzPIi5QyH1DKap1SViXlm59I72GkH9OU/LzU4/MdIyJStXKzRy1e\nwPuAN4CngD3Ak8CVOY6rOkPWqmWd7zx3Zs0LlNPDGNKcgYgEgEaZM3D3/wecVo9r1aoiZ77z7F20\niL9L60UUqxyavRTWZ86k150zjh0r6fhyqpGKiJSq4WsTFVOrmkT1ruwZpUqsItJYpmShulLkq8hZ\n7g23XpU9VVJaRKqhZFCGKN9wS63EKiKSSyXJYMo99jKl3F3H9aTHUIpIvU3ZZBDlG66Wk4pIvU3Z\nZBDlG241D7cXEamE5gwiOGcAegyliFROE8hl0g1XRCYjJQMREdFqIhERqYySgYiIKBmIiIiSgYiI\noGQgIiIoGYiICCEmAzO70sx+ZWa/MbM7wopDRERCSgZm1gR8C/gQcCFwlZn9eRix1ELVD6KuE8VZ\nO40QIyjOWmuUOCsRVs/gXcBv3X3Y3V8D/h5YHlIsVWuU/0AUZ+00QoygOGutUeKsRFjJYD6wP+33\nF5PviYhICDSBLCIi4dQmMrMlQK+7X5n8/UuAu/tXs45TYSIRkQo0RKE6MzsN+DXwAeB3wBPAVe6+\nt+7BiIgIzWFc1N3fMLMvAI+QGKq6T4lARCQ8kS5hLSIi9RHJCeRG2JBmZueZ2Q4ze87MnjGzm8KO\nqRAzazKzJ83s4bBjycfMZpnZD8xsb/Lv+u6wY8rFzFaZ2bNm9rSZbTOz08OOCcDM7jOzUTN7Ou29\n2Wb2iJn92sx+amazwowxGVOuOO9N/nt/ysx+ZGYzw4wxGdOEONM+u83MTprZn4URW1YsOeM0s5XJ\nv+kzZnZPsfNELhk00Ia014Fb3f1C4D3A5yMaZ8rNwPNhB1HEN4GfuPsFwMVA5IYOzWwesBK41N3f\nRmKo9VPhRjVuM4n/b9J9CXjU3d8K7AC+XPeoJsoV5yPAhe7+duC3RDdOzOw8YCkwXPeIcpsQp5nF\ngI8CF7n7RcC6YieJXDKgQTakufshd38q+fMYiRtXJPdKJP/j/UvgO2HHkk+yJfh+d98M4O6vu/vR\nkMPK5zRghpk1A2cAB0OOBwB33wW8lPX2cuD+5M/3Ax+ra1A55IrT3R9195PJX3cD59U9sCx5/p4A\n64Hb6xxOXnni/Bxwj7u/njzmSLHzRDEZNNyGNDNbBLwdeDzcSPJK/ccb5QmiNuCImW1ODmdtNLPp\nYQeVzd3tiXXHAAACWklEQVQPAl8HRoADwMvu/mi4URV0jruPQqIBA5wTcjyluAH457CDyMXMlgH7\n3f2ZsGMp4i3Afzez3Wb2MzN7R7EvRDEZNBQzawF+CNyc7CFEipl9GBhN9mIs+YqiZuBS4Nvufilw\ngsQQR6SY2VkkWtutwDygxcyuDjeqskS5QYCZ3Qm85u4Phh1LtmTjZA3Qk/52SOEU0wzMdvclwBeB\n7xf7QhSTwQFgYdrv5yXfi5zkMMEPge+6+0Nhx5PH+4BlZrYP+B7QYWYPhBxTLi+SaHH9Ivn7D0kk\nh6i5Atjn7n9w9zeAfwDeG3JMhYya2VwAMzsXOBxyPHmZ2QoSw5lRTa7twCLgl2Y2SOLe9G9mFsXe\n1n4S/23i7j8HTprZ2YW+EMVk8HPgzWbWmlyl8SkgqitgNgHPu/s3ww4kH3df4+4L3f18En/LHe5+\nbdhxZUsOZew3s7ck3/oA0ZzwHgGWmNk0MzMScUZpoju79/cwsCL583VAVBotGXGa2ZUkhjKXufuf\nQotqovE43f1Zdz/X3c939zYSDZhL3D0KCTb73/s/ApcDJP+fepO7/77QCSKXDJKtrdSGtOeAv4/i\nhjQzex/QCVxuZnuS49xXhh1Xg7sJ2GZmT5FYTXR3yPFM4O5PkOi17AF+SeJ/wI2hBpVkZg8CjwFv\nMbMRM7seuAdYamapHf9FlxgGLU+cG4AWYHvy/6X/FWqQ5I0znROBYaI8cW4CzjezZ4AHgaINQG06\nExGR6PUMRESk/pQMREREyUBERJQMREQEJQMREUHJQEREUDIQERGUDEREBPj/44Upzz1muyUAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a2cf8c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Class1[:,0],Class1[:,1],'ro')\n",
    "plt.plot(Class2[:,0],Class2[:,1],'bo')\n",
    "plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Data.shape[0]\n",
    "X = Data.T\n",
    "y = Trainlabel.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "n_i = 2\n",
    "n_h = 4\n",
    "n_l = 1\n",
    "learningrate = 0.005\n",
    "numiter = 50000\n",
    "l = [n_i,n_h,n_l]\n",
    "\n",
    "W1 = np.random.randn(n_h,n_i)*0.01\n",
    "b1 = np.random.randn(n_h,1)*0.01\n",
    "W2 = np.random.randn(n_l,n_h)*0.01\n",
    "b2 = np.random.randn(n_l,1)                  \n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,play):\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    if (play == \"forward\"):\n",
    "        return z\n",
    "    elif (play ==\"backward\"):\n",
    "        return z*(1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x,play):\n",
    "    if (play==\"forward\"):\n",
    "        return np.maximum(x,0)\n",
    "    elif (play==\"backward\"):\n",
    "        x[x<=0] = 0     \n",
    "        x[x>0] = 1     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  0 th iteration => 0.801312592374\n",
      "Loss for  1 th iteration => 0.801076194545\n",
      "Loss for  2 th iteration => 0.800841092878\n",
      "Loss for  3 th iteration => 0.800607150272\n",
      "Loss for  4 th iteration => 0.80037442567\n",
      "Loss for  5 th iteration => 0.800142916463\n",
      "Loss for  6 th iteration => 0.799912470944\n",
      "Loss for  7 th iteration => 0.799683055136\n",
      "Loss for  8 th iteration => 0.799454548466\n",
      "Loss for  9 th iteration => 0.79922695724\n",
      "Loss for  10 th iteration => 0.799000207528\n",
      "Loss for  11 th iteration => 0.798774270171\n",
      "Loss for  12 th iteration => 0.79854901658\n",
      "Loss for  13 th iteration => 0.798324423567\n",
      "Loss for  14 th iteration => 0.798100473471\n",
      "Loss for  15 th iteration => 0.797877075183\n",
      "Loss for  16 th iteration => 0.797654240096\n",
      "Loss for  17 th iteration => 0.797431726929\n",
      "Loss for  18 th iteration => 0.79720961397\n",
      "Loss for  19 th iteration => 0.796987831732\n",
      "Loss for  20 th iteration => 0.796766290523\n",
      "Loss for  21 th iteration => 0.796544942367\n",
      "Loss for  22 th iteration => 0.796323787544\n",
      "Loss for  23 th iteration => 0.796102623131\n",
      "Loss for  24 th iteration => 0.795881468909\n",
      "Loss for  25 th iteration => 0.795660315411\n",
      "Loss for  26 th iteration => 0.795439004964\n",
      "Loss for  27 th iteration => 0.795217460336\n",
      "Loss for  28 th iteration => 0.794995703635\n",
      "Loss for  29 th iteration => 0.794773645817\n",
      "Loss for  30 th iteration => 0.794551248783\n",
      "Loss for  31 th iteration => 0.794328331988\n",
      "Loss for  32 th iteration => 0.794104861994\n",
      "Loss for  33 th iteration => 0.793880825803\n",
      "Loss for  34 th iteration => 0.793656194362\n",
      "Loss for  35 th iteration => 0.793430528905\n",
      "Loss for  36 th iteration => 0.793204022579\n",
      "Loss for  37 th iteration => 0.792976688807\n",
      "Loss for  38 th iteration => 0.792748234619\n",
      "Loss for  39 th iteration => 0.792518692138\n",
      "Loss for  40 th iteration => 0.792287470746\n",
      "Loss for  41 th iteration => 0.792055138933\n",
      "Loss for  42 th iteration => 0.791821122154\n",
      "Loss for  43 th iteration => 0.791585924408\n",
      "Loss for  44 th iteration => 0.791349278173\n",
      "Loss for  45 th iteration => 0.79111135745\n",
      "Loss for  46 th iteration => 0.790872113795\n",
      "Loss for  47 th iteration => 0.790631352942\n",
      "Loss for  48 th iteration => 0.790389586466\n",
      "Loss for  49 th iteration => 0.790146377134\n",
      "Loss for  50 th iteration => 0.789901571787\n",
      "Loss for  51 th iteration => 0.789655851962\n",
      "Loss for  52 th iteration => 0.78940899922\n",
      "Loss for  53 th iteration => 0.789160407248\n",
      "Loss for  54 th iteration => 0.788910522761\n",
      "Loss for  55 th iteration => 0.788659365884\n",
      "Loss for  56 th iteration => 0.788407034537\n",
      "Loss for  57 th iteration => 0.788154340561\n",
      "Loss for  58 th iteration => 0.787900244661\n",
      "Loss for  59 th iteration => 0.787645167122\n",
      "Loss for  60 th iteration => 0.787389741835\n",
      "Loss for  61 th iteration => 0.787135048122\n",
      "Loss for  62 th iteration => 0.786887432027\n",
      "Loss for  63 th iteration => 0.786649575773\n",
      "Loss for  64 th iteration => 0.786419846652\n",
      "Loss for  65 th iteration => 0.78619307236\n",
      "Loss for  66 th iteration => 0.785966621894\n",
      "Loss for  67 th iteration => 0.785740428377\n",
      "Loss for  68 th iteration => 0.785514664066\n",
      "Loss for  69 th iteration => 0.78528912544\n",
      "Loss for  70 th iteration => 0.785064054834\n",
      "Loss for  71 th iteration => 0.784839162211\n",
      "Loss for  72 th iteration => 0.784614805652\n",
      "Loss for  73 th iteration => 0.784390764844\n",
      "Loss for  74 th iteration => 0.784166865408\n",
      "Loss for  75 th iteration => 0.783943416234\n",
      "Loss for  76 th iteration => 0.78372032157\n",
      "Loss for  77 th iteration => 0.783497336221\n",
      "Loss for  78 th iteration => 0.783274775478\n",
      "Loss for  79 th iteration => 0.783052441903\n",
      "Loss for  80 th iteration => 0.782830549097\n",
      "Loss for  81 th iteration => 0.782608584408\n",
      "Loss for  82 th iteration => 0.782387072357\n",
      "Loss for  83 th iteration => 0.782165618387\n",
      "Loss for  84 th iteration => 0.781944603336\n",
      "Loss for  85 th iteration => 0.781723703833\n",
      "Loss for  86 th iteration => 0.781503202254\n",
      "Loss for  87 th iteration => 0.781282796025\n",
      "Loss for  88 th iteration => 0.781062773287\n",
      "Loss for  89 th iteration => 0.78084283762\n",
      "Loss for  90 th iteration => 0.78062332196\n",
      "Loss for  91 th iteration => 0.780403804796\n",
      "Loss for  92 th iteration => 0.780184830177\n",
      "Loss for  93 th iteration => 0.779965708304\n",
      "Loss for  94 th iteration => 0.779747014919\n",
      "Loss for  95 th iteration => 0.779528530271\n",
      "Loss for  96 th iteration => 0.779310214038\n",
      "Loss for  97 th iteration => 0.779091905993\n",
      "Loss for  98 th iteration => 0.778873820194\n",
      "Loss for  99 th iteration => 0.778655907117\n",
      "Loss for  100 th iteration => 0.778438072798\n",
      "Loss for  101 th iteration => 0.778220725983\n",
      "Loss for  102 th iteration => 0.778003168295\n",
      "Loss for  103 th iteration => 0.77778606838\n",
      "Loss for  104 th iteration => 0.777568884971\n",
      "Loss for  105 th iteration => 0.777352087863\n",
      "Loss for  106 th iteration => 0.777135670229\n",
      "Loss for  107 th iteration => 0.776919186145\n",
      "Loss for  108 th iteration => 0.776702574767\n",
      "Loss for  109 th iteration => 0.776486492503\n",
      "Loss for  110 th iteration => 0.776270256354\n",
      "Loss for  111 th iteration => 0.776054250009\n",
      "Loss for  112 th iteration => 0.775838180548\n",
      "Loss for  113 th iteration => 0.77562233517\n",
      "Loss for  114 th iteration => 0.775406722899\n",
      "Loss for  115 th iteration => 0.775191104541\n",
      "Loss for  116 th iteration => 0.77497572675\n",
      "Loss for  117 th iteration => 0.774760569425\n",
      "Loss for  118 th iteration => 0.774545363951\n",
      "Loss for  119 th iteration => 0.774330315577\n",
      "Loss for  120 th iteration => 0.774115612265\n",
      "Loss for  121 th iteration => 0.773900753308\n",
      "Loss for  122 th iteration => 0.773686692953\n",
      "Loss for  123 th iteration => 0.773471748198\n",
      "Loss for  124 th iteration => 0.773257462598\n",
      "Loss for  125 th iteration => 0.773043165295\n",
      "Loss for  126 th iteration => 0.772828741734\n",
      "Loss for  127 th iteration => 0.772614967841\n",
      "Loss for  128 th iteration => 0.772400523114\n",
      "Loss for  129 th iteration => 0.772186533696\n",
      "Loss for  130 th iteration => 0.771972821005\n",
      "Loss for  131 th iteration => 0.771759000499\n",
      "Loss for  132 th iteration => 0.771545311423\n",
      "Loss for  133 th iteration => 0.77133173752\n",
      "Loss for  134 th iteration => 0.771118284629\n",
      "Loss for  135 th iteration => 0.770904785553\n",
      "Loss for  136 th iteration => 0.770691683403\n",
      "Loss for  137 th iteration => 0.770478720158\n",
      "Loss for  138 th iteration => 0.770265574663\n",
      "Loss for  139 th iteration => 0.770051908025\n",
      "Loss for  140 th iteration => 0.769839480583\n",
      "Loss for  141 th iteration => 0.769625965159\n",
      "Loss for  142 th iteration => 0.769412473552\n",
      "Loss for  143 th iteration => 0.769199262677\n",
      "Loss for  144 th iteration => 0.768987137346\n",
      "Loss for  145 th iteration => 0.76877474943\n",
      "Loss for  146 th iteration => 0.7685625335\n",
      "Loss for  147 th iteration => 0.768347977845\n",
      "Loss for  148 th iteration => 0.768133802641\n",
      "Loss for  149 th iteration => 0.767920628256\n",
      "Loss for  150 th iteration => 0.767709017642\n",
      "Loss for  151 th iteration => 0.767496298376\n",
      "Loss for  152 th iteration => 0.767281069284\n",
      "Loss for  153 th iteration => 0.767067872543\n",
      "Loss for  154 th iteration => 0.766856355804\n",
      "Loss for  155 th iteration => 0.766644044218\n",
      "Loss for  156 th iteration => 0.766428340088\n",
      "Loss for  157 th iteration => 0.766215022997\n",
      "Loss for  158 th iteration => 0.766001936763\n",
      "Loss for  159 th iteration => 0.765789330872\n",
      "Loss for  160 th iteration => 0.765577796262\n",
      "Loss for  161 th iteration => 0.765364006635\n",
      "Loss for  162 th iteration => 0.76514923891\n",
      "Loss for  163 th iteration => 0.764938809311\n",
      "Loss for  164 th iteration => 0.76472582568\n",
      "Loss for  165 th iteration => 0.764508543902\n",
      "Loss for  166 th iteration => 0.764297035846\n",
      "Loss for  167 th iteration => 0.764082293\n",
      "Loss for  168 th iteration => 0.763867620029\n",
      "Loss for  169 th iteration => 0.763655037105\n",
      "Loss for  170 th iteration => 0.763438468889\n",
      "Loss for  171 th iteration => 0.76322716303\n",
      "Loss for  172 th iteration => 0.763014166814\n",
      "Loss for  173 th iteration => 0.762794295578\n",
      "Loss for  174 th iteration => 0.762581266279\n",
      "Loss for  175 th iteration => 0.762367221762\n",
      "Loss for  176 th iteration => 0.762151109398\n",
      "Loss for  177 th iteration => 0.76193738639\n",
      "Loss for  178 th iteration => 0.761720422364\n",
      "Loss for  179 th iteration => 0.761506762506\n",
      "Loss for  180 th iteration => 0.761289160977\n",
      "Loss for  181 th iteration => 0.761074878795\n",
      "Loss for  182 th iteration => 0.760857260399\n",
      "Loss for  183 th iteration => 0.760642622596\n",
      "Loss for  184 th iteration => 0.760425609585\n",
      "Loss for  185 th iteration => 0.760209021794\n",
      "Loss for  186 th iteration => 0.759993362889\n",
      "Loss for  187 th iteration => 0.759773670072\n",
      "Loss for  188 th iteration => 0.759560142915\n",
      "Loss for  189 th iteration => 0.759338680507\n",
      "Loss for  190 th iteration => 0.75912326296\n",
      "Loss for  191 th iteration => 0.758905386128\n",
      "Loss for  192 th iteration => 0.758690443762\n",
      "Loss for  193 th iteration => 0.758468008493\n",
      "Loss for  194 th iteration => 0.758252478995\n",
      "Loss for  195 th iteration => 0.758033299999\n",
      "Loss for  196 th iteration => 0.757818324329\n",
      "Loss for  197 th iteration => 0.757594886329\n",
      "Loss for  198 th iteration => 0.757376840919\n",
      "Loss for  199 th iteration => 0.757161595802\n",
      "Loss for  200 th iteration => 0.756941755157\n",
      "Loss for  201 th iteration => 0.756720212756\n",
      "Loss for  202 th iteration => 0.756503107383\n",
      "Loss for  203 th iteration => 0.756281567657\n",
      "Loss for  204 th iteration => 0.756063587574\n",
      "Loss for  205 th iteration => 0.755841203906\n",
      "Loss for  206 th iteration => 0.755623140193\n",
      "Loss for  207 th iteration => 0.755400470275\n",
      "Loss for  208 th iteration => 0.755181986681\n",
      "Loss for  209 th iteration => 0.754957232495\n",
      "Loss for  210 th iteration => 0.754740468963\n",
      "Loss for  211 th iteration => 0.754513983496\n",
      "Loss for  212 th iteration => 0.754295300629\n",
      "Loss for  213 th iteration => 0.754073057671\n",
      "Loss for  214 th iteration => 0.75385544513\n",
      "Loss for  215 th iteration => 0.753624749242\n",
      "Loss for  216 th iteration => 0.753402363562\n",
      "Loss for  217 th iteration => 0.753184165539\n",
      "Loss for  218 th iteration => 0.752958077512\n",
      "Loss for  219 th iteration => 0.752737016385\n",
      "Loss for  220 th iteration => 0.752513122825\n",
      "Loss for  221 th iteration => 0.752295171932\n",
      "Loss for  222 th iteration => 0.75206319047\n",
      "Loss for  223 th iteration => 0.751842919446\n",
      "Loss for  224 th iteration => 0.751619626846\n",
      "Loss for  225 th iteration => 0.751387110958\n",
      "Loss for  226 th iteration => 0.7511634895\n",
      "Loss for  227 th iteration => 0.750941955776\n",
      "Loss for  228 th iteration => 0.750708776761\n",
      "Loss for  229 th iteration => 0.750481579536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  230 th iteration => 0.750254369675\n",
      "Loss for  231 th iteration => 0.75003380188\n",
      "Loss for  232 th iteration => 0.749799397079\n",
      "Loss for  233 th iteration => 0.749573640431\n",
      "Loss for  234 th iteration => 0.749346803485\n",
      "Loss for  235 th iteration => 0.749121835383\n",
      "Loss for  236 th iteration => 0.748884694\n",
      "Loss for  237 th iteration => 0.748654907413\n",
      "Loss for  238 th iteration => 0.748432082574\n",
      "Loss for  239 th iteration => 0.748203764594\n",
      "Loss for  240 th iteration => 0.74796437946\n",
      "Loss for  241 th iteration => 0.747735244057\n",
      "Loss for  242 th iteration => 0.747507002733\n",
      "Loss for  243 th iteration => 0.747268420989\n",
      "Loss for  244 th iteration => 0.747040315373\n",
      "Loss for  245 th iteration => 0.74681324859\n",
      "Loss for  246 th iteration => 0.746570294881\n",
      "Loss for  247 th iteration => 0.746340882553\n",
      "Loss for  248 th iteration => 0.746101457838\n",
      "Loss for  249 th iteration => 0.745868479408\n",
      "Loss for  250 th iteration => 0.745638730484\n",
      "Loss for  251 th iteration => 0.745410395387\n",
      "Loss for  252 th iteration => 0.745169480164\n",
      "Loss for  253 th iteration => 0.744939323028\n",
      "Loss for  254 th iteration => 0.744692886835\n",
      "Loss for  255 th iteration => 0.744461852221\n",
      "Loss for  256 th iteration => 0.744218905765\n",
      "Loss for  257 th iteration => 0.743984902614\n",
      "Loss for  258 th iteration => 0.743748079285\n",
      "Loss for  259 th iteration => 0.743505492914\n",
      "Loss for  260 th iteration => 0.74326775927\n",
      "Loss for  261 th iteration => 0.743032961646\n",
      "Loss for  262 th iteration => 0.742794750708\n",
      "Loss for  263 th iteration => 0.742550743983\n",
      "Loss for  264 th iteration => 0.742315542302\n",
      "Loss for  265 th iteration => 0.742075023928\n",
      "Loss for  266 th iteration => 0.741830374527\n",
      "Loss for  267 th iteration => 0.74159337817\n",
      "Loss for  268 th iteration => 0.741352680278\n",
      "Loss for  269 th iteration => 0.741104533566\n",
      "Loss for  270 th iteration => 0.740866552779\n",
      "Loss for  271 th iteration => 0.74062558545\n",
      "Loss for  272 th iteration => 0.740377462249\n",
      "Loss for  273 th iteration => 0.74013491435\n",
      "Loss for  274 th iteration => 0.73989547259\n",
      "Loss for  275 th iteration => 0.739646449557\n",
      "Loss for  276 th iteration => 0.739399331206\n",
      "Loss for  277 th iteration => 0.739158229181\n",
      "Loss for  278 th iteration => 0.738909329046\n",
      "Loss for  279 th iteration => 0.738663553565\n",
      "Loss for  280 th iteration => 0.738419319199\n",
      "Loss for  281 th iteration => 0.738167711654\n",
      "Loss for  282 th iteration => 0.737924188551\n",
      "Loss for  283 th iteration => 0.737677023862\n",
      "Loss for  284 th iteration => 0.737425372687\n",
      "Loss for  285 th iteration => 0.7371717073\n",
      "Loss for  286 th iteration => 0.736926235218\n",
      "Loss for  287 th iteration => 0.73667792065\n",
      "Loss for  288 th iteration => 0.736424044609\n",
      "Loss for  289 th iteration => 0.736169743227\n",
      "Loss for  290 th iteration => 0.735919018753\n",
      "Loss for  291 th iteration => 0.735669336313\n",
      "Loss for  292 th iteration => 0.735413286442\n",
      "Loss for  293 th iteration => 0.735159410481\n",
      "Loss for  294 th iteration => 0.734909637316\n",
      "Loss for  295 th iteration => 0.734653597788\n",
      "Loss for  296 th iteration => 0.734396803734\n",
      "Loss for  297 th iteration => 0.734143596813\n",
      "Loss for  298 th iteration => 0.733890435531\n",
      "Loss for  299 th iteration => 0.733632757672\n",
      "Loss for  300 th iteration => 0.733372991586\n",
      "Loss for  301 th iteration => 0.733118015727\n",
      "Loss for  302 th iteration => 0.732863652204\n",
      "Loss for  303 th iteration => 0.732604291321\n",
      "Loss for  304 th iteration => 0.732343141203\n",
      "Loss for  305 th iteration => 0.732083972218\n",
      "Loss for  306 th iteration => 0.731827943828\n",
      "Loss for  307 th iteration => 0.73156608762\n",
      "Loss for  308 th iteration => 0.731300852875\n",
      "Loss for  309 th iteration => 0.731040786724\n",
      "Loss for  310 th iteration => 0.730779303602\n",
      "Loss for  311 th iteration => 0.730514801313\n",
      "Loss for  312 th iteration => 0.730247207076\n",
      "Loss for  313 th iteration => 0.729982381359\n",
      "Loss for  314 th iteration => 0.729715036944\n",
      "Loss for  315 th iteration => 0.729453217545\n",
      "Loss for  316 th iteration => 0.729186201067\n",
      "Loss for  317 th iteration => 0.728916351776\n",
      "Loss for  318 th iteration => 0.728649996471\n",
      "Loss for  319 th iteration => 0.728379052443\n",
      "Loss for  320 th iteration => 0.728113093352\n",
      "Loss for  321 th iteration => 0.727843968215\n",
      "Loss for  322 th iteration => 0.727573077002\n",
      "Loss for  323 th iteration => 0.727304102203\n",
      "Loss for  324 th iteration => 0.727030211565\n",
      "Loss for  325 th iteration => 0.726760911519\n",
      "Loss for  326 th iteration => 0.726489468892\n",
      "Loss for  327 th iteration => 0.726216679985\n",
      "Loss for  328 th iteration => 0.725944142016\n",
      "Loss for  329 th iteration => 0.72566948818\n",
      "Loss for  330 th iteration => 0.725396066495\n",
      "Loss for  331 th iteration => 0.725120210867\n",
      "Loss for  332 th iteration => 0.724847462825\n",
      "Loss for  333 th iteration => 0.724569239858\n",
      "Loss for  334 th iteration => 0.724293361591\n",
      "Loss for  335 th iteration => 0.72401660361\n",
      "Loss for  336 th iteration => 0.723737043581\n",
      "Loss for  337 th iteration => 0.723459193822\n",
      "Loss for  338 th iteration => 0.723181073871\n",
      "Loss for  339 th iteration => 0.722899212461\n",
      "Loss for  340 th iteration => 0.722620560914\n",
      "Loss for  341 th iteration => 0.722336932682\n",
      "Loss for  342 th iteration => 0.72205728783\n",
      "Loss for  343 th iteration => 0.721769756853\n",
      "Loss for  344 th iteration => 0.721485004811\n",
      "Loss for  345 th iteration => 0.721200922707\n",
      "Loss for  346 th iteration => 0.720916159528\n",
      "Loss for  347 th iteration => 0.720627591877\n",
      "Loss for  348 th iteration => 0.720342905975\n",
      "Loss for  349 th iteration => 0.720051958007\n",
      "Loss for  350 th iteration => 0.71976465723\n",
      "Loss for  351 th iteration => 0.719476207774\n",
      "Loss for  352 th iteration => 0.719186470607\n",
      "Loss for  353 th iteration => 0.71889650455\n",
      "Loss for  354 th iteration => 0.718605548081\n",
      "Loss for  355 th iteration => 0.718313119136\n",
      "Loss for  356 th iteration => 0.718022122139\n",
      "Loss for  357 th iteration => 0.717728816942\n",
      "Loss for  358 th iteration => 0.717436187491\n",
      "Loss for  359 th iteration => 0.717141024571\n",
      "Loss for  360 th iteration => 0.716847920209\n",
      "Loss for  361 th iteration => 0.716551482984\n",
      "Loss for  362 th iteration => 0.716256974698\n",
      "Loss for  363 th iteration => 0.715959823024\n",
      "Loss for  364 th iteration => 0.715663442868\n",
      "Loss for  365 th iteration => 0.715364910843\n",
      "Loss for  366 th iteration => 0.715067579174\n",
      "Loss for  367 th iteration => 0.714768553262\n",
      "Loss for  368 th iteration => 0.714469218518\n",
      "Loss for  369 th iteration => 0.714169800606\n",
      "Loss for  370 th iteration => 0.713867871612\n",
      "Loss for  371 th iteration => 0.71356845568\n",
      "Loss for  372 th iteration => 0.713264279625\n",
      "Loss for  373 th iteration => 0.712964436203\n",
      "Loss for  374 th iteration => 0.712659332016\n",
      "Loss for  375 th iteration => 0.71235441295\n",
      "Loss for  376 th iteration => 0.712050198896\n",
      "Loss for  377 th iteration => 0.711746585297\n",
      "Loss for  378 th iteration => 0.711441381898\n",
      "Loss for  379 th iteration => 0.711134033193\n",
      "Loss for  380 th iteration => 0.710825754492\n",
      "Loss for  381 th iteration => 0.710521141446\n",
      "Loss for  382 th iteration => 0.710206793706\n",
      "Loss for  383 th iteration => 0.709897963508\n",
      "Loss for  384 th iteration => 0.709587154276\n",
      "Loss for  385 th iteration => 0.709278621455\n",
      "Loss for  386 th iteration => 0.708962936254\n",
      "Loss for  387 th iteration => 0.708651640232\n",
      "Loss for  388 th iteration => 0.708336906814\n",
      "Loss for  389 th iteration => 0.708029010531\n",
      "Loss for  390 th iteration => 0.707710548638\n",
      "Loss for  391 th iteration => 0.707400698779\n",
      "Loss for  392 th iteration => 0.707080604389\n",
      "Loss for  393 th iteration => 0.706767352299\n",
      "Loss for  394 th iteration => 0.706448776819\n",
      "Loss for  395 th iteration => 0.706133190032\n",
      "Loss for  396 th iteration => 0.705811532544\n",
      "Loss for  397 th iteration => 0.705495057601\n",
      "Loss for  398 th iteration => 0.705173748493\n",
      "Loss for  399 th iteration => 0.704849422246\n",
      "Loss for  400 th iteration => 0.704528440483\n",
      "Loss for  401 th iteration => 0.704209911857\n",
      "Loss for  402 th iteration => 0.703884587394\n",
      "Loss for  403 th iteration => 0.703560927478\n",
      "Loss for  404 th iteration => 0.703237221112\n",
      "Loss for  405 th iteration => 0.702909786647\n",
      "Loss for  406 th iteration => 0.702583216088\n",
      "Loss for  407 th iteration => 0.702256961063\n",
      "Loss for  408 th iteration => 0.701926319884\n",
      "Loss for  409 th iteration => 0.701604782297\n",
      "Loss for  410 th iteration => 0.701270840357\n",
      "Loss for  411 th iteration => 0.700939155903\n",
      "Loss for  412 th iteration => 0.700607052713\n",
      "Loss for  413 th iteration => 0.70027741785\n",
      "Loss for  414 th iteration => 0.699940290219\n",
      "Loss for  415 th iteration => 0.699606697822\n",
      "Loss for  416 th iteration => 0.699267577352\n",
      "Loss for  417 th iteration => 0.69893371452\n",
      "Loss for  418 th iteration => 0.698601550578\n",
      "Loss for  419 th iteration => 0.698275724246\n",
      "Loss for  420 th iteration => 0.697937803596\n",
      "Loss for  421 th iteration => 0.697590801162\n",
      "Loss for  422 th iteration => 0.697252827421\n",
      "Loss for  423 th iteration => 0.696913843967\n",
      "Loss for  424 th iteration => 0.696581585117\n",
      "Loss for  425 th iteration => 0.69623992595\n",
      "Loss for  426 th iteration => 0.695888248551\n",
      "Loss for  427 th iteration => 0.695540823929\n",
      "Loss for  428 th iteration => 0.695209871238\n",
      "Loss for  429 th iteration => 0.694860712469\n",
      "Loss for  430 th iteration => 0.694515684473\n",
      "Loss for  431 th iteration => 0.694171275682\n",
      "Loss for  432 th iteration => 0.693811459508\n",
      "Loss for  433 th iteration => 0.693471532453\n",
      "Loss for  434 th iteration => 0.693130100668\n",
      "Loss for  435 th iteration => 0.692775827179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  436 th iteration => 0.692419511383\n",
      "Loss for  437 th iteration => 0.692063876208\n",
      "Loss for  438 th iteration => 0.691723703767\n",
      "Loss for  439 th iteration => 0.691366118084\n",
      "Loss for  440 th iteration => 0.690995838933\n",
      "Loss for  441 th iteration => 0.690637666128\n",
      "Loss for  442 th iteration => 0.690295782425\n",
      "Loss for  443 th iteration => 0.689940229336\n",
      "Loss for  444 th iteration => 0.689577024421\n",
      "Loss for  445 th iteration => 0.68921200402\n",
      "Loss for  446 th iteration => 0.688843942311\n",
      "Loss for  447 th iteration => 0.68847353286\n",
      "Loss for  448 th iteration => 0.688124359484\n",
      "Loss for  449 th iteration => 0.687743133545\n",
      "Loss for  450 th iteration => 0.687398187971\n",
      "Loss for  451 th iteration => 0.687003234326\n",
      "Loss for  452 th iteration => 0.686658372643\n",
      "Loss for  453 th iteration => 0.686248860071\n",
      "Loss for  454 th iteration => 0.685888732318\n",
      "Loss for  455 th iteration => 0.685503975322\n",
      "Loss for  456 th iteration => 0.685169176939\n",
      "Loss for  457 th iteration => 0.684779597936\n",
      "Loss for  458 th iteration => 0.684395625734\n",
      "Loss for  459 th iteration => 0.684006473034\n",
      "Loss for  460 th iteration => 0.683630089903\n",
      "Loss for  461 th iteration => 0.683241190055\n",
      "Loss for  462 th iteration => 0.682879467316\n",
      "Loss for  463 th iteration => 0.682563706241\n",
      "Loss for  464 th iteration => 0.682135555159\n",
      "Loss for  465 th iteration => 0.681830564845\n",
      "Loss for  466 th iteration => 0.681309366983\n",
      "Loss for  467 th iteration => 0.680902060538\n",
      "Loss for  468 th iteration => 0.680534882341\n",
      "Loss for  469 th iteration => 0.680255261152\n",
      "Loss for  470 th iteration => 0.679794032566\n",
      "Loss for  471 th iteration => 0.67954734101\n",
      "Loss for  472 th iteration => 0.678953612508\n",
      "Loss for  473 th iteration => 0.678638493336\n",
      "Loss for  474 th iteration => 0.67830992936\n",
      "Loss for  475 th iteration => 0.678185695451\n",
      "Loss for  476 th iteration => 0.67732675083\n",
      "Loss for  477 th iteration => 0.676976450711\n",
      "Loss for  478 th iteration => 0.676757622749\n",
      "Loss for  479 th iteration => 0.676176822227\n",
      "Loss for  480 th iteration => 0.675975354815\n",
      "Loss for  481 th iteration => 0.675460663309\n",
      "Loss for  482 th iteration => 0.675366826894\n",
      "Loss for  483 th iteration => 0.674492153046\n",
      "Loss for  484 th iteration => 0.674141906686\n",
      "Loss for  485 th iteration => 0.673862172289\n",
      "Loss for  486 th iteration => 0.673911127082\n",
      "Loss for  487 th iteration => 0.67286440957\n",
      "Loss for  488 th iteration => 0.672476672415\n",
      "Loss for  489 th iteration => 0.672242022151\n",
      "Loss for  490 th iteration => 0.671872854031\n",
      "Loss for  491 th iteration => 0.672154617732\n",
      "Loss for  492 th iteration => 0.670956557159\n",
      "Loss for  493 th iteration => 0.670819988469\n",
      "Loss for  494 th iteration => 0.671050801734\n",
      "Loss for  495 th iteration => 0.669785251306\n",
      "Loss for  496 th iteration => 0.669539135742\n",
      "Loss for  497 th iteration => 0.669882797816\n",
      "Loss for  498 th iteration => 0.668566188668\n",
      "Loss for  499 th iteration => 0.668278912862\n",
      "Loss for  500 th iteration => 0.668667698768\n",
      "Loss for  501 th iteration => 0.66737976325\n",
      "Loss for  502 th iteration => 0.666964636332\n",
      "Loss for  503 th iteration => 0.667481643574\n",
      "Loss for  504 th iteration => 0.66614340281\n",
      "Loss for  505 th iteration => 0.665803301314\n",
      "Loss for  506 th iteration => 0.666279402149\n",
      "Loss for  507 th iteration => 0.664936774567\n",
      "Loss for  508 th iteration => 0.664445379492\n",
      "Loss for  509 th iteration => 0.66506749033\n",
      "Loss for  510 th iteration => 0.663653970241\n",
      "Loss for  511 th iteration => 0.663332264601\n",
      "Loss for  512 th iteration => 0.664067028643\n",
      "Loss for  513 th iteration => 0.662696504876\n",
      "Loss for  514 th iteration => 0.66159029477\n",
      "Loss for  515 th iteration => 0.661864856862\n",
      "Loss for  516 th iteration => 0.660909134808\n",
      "Loss for  517 th iteration => 0.661609390668\n",
      "Loss for  518 th iteration => 0.660317727696\n",
      "Loss for  519 th iteration => 0.660882872771\n",
      "Loss for  520 th iteration => 0.661979921238\n",
      "Loss for  521 th iteration => 0.659376695187\n",
      "Loss for  522 th iteration => 0.658364953296\n",
      "Loss for  523 th iteration => 0.659371953369\n",
      "Loss for  524 th iteration => 0.658572787634\n",
      "Loss for  525 th iteration => 0.658075188564\n",
      "Loss for  526 th iteration => 0.658306194115\n",
      "Loss for  527 th iteration => 0.657269885065\n",
      "Loss for  528 th iteration => 0.656912409332\n",
      "Loss for  529 th iteration => 0.656681141349\n",
      "Loss for  530 th iteration => 0.655226307533\n",
      "Loss for  531 th iteration => 0.655208133638\n",
      "Loss for  532 th iteration => 0.655802044743\n",
      "Loss for  533 th iteration => 0.654532758363\n",
      "Loss for  534 th iteration => 0.653281949924\n",
      "Loss for  535 th iteration => 0.653840555427\n",
      "Loss for  536 th iteration => 0.655129611042\n",
      "Loss for  537 th iteration => 0.652728716585\n",
      "Loss for  538 th iteration => 0.651222111623\n",
      "Loss for  539 th iteration => 0.651625086435\n",
      "Loss for  540 th iteration => 0.653073768467\n",
      "Loss for  541 th iteration => 0.651171486753\n",
      "Loss for  542 th iteration => 0.649506403886\n",
      "Loss for  543 th iteration => 0.651126314615\n",
      "Loss for  544 th iteration => 0.650805372565\n",
      "Loss for  545 th iteration => 0.649042182197\n",
      "Loss for  546 th iteration => 0.64832852966\n",
      "Loss for  547 th iteration => 0.649996706501\n",
      "Loss for  548 th iteration => 0.649191344644\n",
      "Loss for  549 th iteration => 0.646557531516\n",
      "Loss for  550 th iteration => 0.647781675109\n",
      "Loss for  551 th iteration => 0.647683045693\n",
      "Loss for  552 th iteration => 0.646929039678\n",
      "Loss for  553 th iteration => 0.645306271395\n",
      "Loss for  554 th iteration => 0.646877575771\n",
      "Loss for  555 th iteration => 0.645556572277\n",
      "Loss for  556 th iteration => 0.644582758806\n",
      "Loss for  557 th iteration => 0.645177590396\n",
      "Loss for  558 th iteration => 0.644763111994\n",
      "Loss for  559 th iteration => 0.643355088836\n",
      "Loss for  560 th iteration => 0.643030440555\n",
      "Loss for  561 th iteration => 0.643472056341\n",
      "Loss for  562 th iteration => 0.64192792636\n",
      "Loss for  563 th iteration => 0.640691845361\n",
      "Loss for  564 th iteration => 0.641135629425\n",
      "Loss for  565 th iteration => 0.641455878155\n",
      "Loss for  566 th iteration => 0.64118074338\n",
      "Loss for  567 th iteration => 0.639763450975\n",
      "Loss for  568 th iteration => 0.639230928255\n",
      "Loss for  569 th iteration => 0.639526939982\n",
      "Loss for  570 th iteration => 0.637949354559\n",
      "Loss for  571 th iteration => 0.637774409436\n",
      "Loss for  572 th iteration => 0.639692761321\n",
      "Loss for  573 th iteration => 0.637327438195\n",
      "Loss for  574 th iteration => 0.635576442892\n",
      "Loss for  575 th iteration => 0.637150228558\n",
      "Loss for  576 th iteration => 0.637281405842\n",
      "Loss for  577 th iteration => 0.63500296311\n",
      "Loss for  578 th iteration => 0.633583547448\n",
      "Loss for  579 th iteration => 0.634544633739\n",
      "Loss for  580 th iteration => 0.633505319926\n",
      "Loss for  581 th iteration => 0.632788888826\n",
      "Loss for  582 th iteration => 0.63240083406\n",
      "Loss for  583 th iteration => 0.634075372144\n",
      "Loss for  584 th iteration => 0.632793645471\n",
      "Loss for  585 th iteration => 0.630334165058\n",
      "Loss for  586 th iteration => 0.629955686653\n",
      "Loss for  587 th iteration => 0.631652871009\n",
      "Loss for  588 th iteration => 0.632414853259\n",
      "Loss for  589 th iteration => 0.629605830721\n",
      "Loss for  590 th iteration => 0.62808744934\n",
      "Loss for  591 th iteration => 0.62830314279\n",
      "Loss for  592 th iteration => 0.62990288684\n",
      "Loss for  593 th iteration => 0.629163312691\n",
      "Loss for  594 th iteration => 0.627339284259\n",
      "Loss for  595 th iteration => 0.62623768448\n",
      "Loss for  596 th iteration => 0.62588744502\n",
      "Loss for  597 th iteration => 0.6263542988\n",
      "Loss for  598 th iteration => 0.627062733201\n",
      "Loss for  599 th iteration => 0.62582870524\n",
      "Loss for  600 th iteration => 0.624860115517\n",
      "Loss for  601 th iteration => 0.624490645366\n",
      "Loss for  602 th iteration => 0.625273080098\n",
      "Loss for  603 th iteration => 0.623806460318\n",
      "Loss for  604 th iteration => 0.622472834781\n",
      "Loss for  605 th iteration => 0.622037225623\n",
      "Loss for  606 th iteration => 0.623237644175\n",
      "Loss for  607 th iteration => 0.622048956393\n",
      "Loss for  608 th iteration => 0.620893004431\n",
      "Loss for  609 th iteration => 0.619393110446\n",
      "Loss for  610 th iteration => 0.61932469687\n",
      "Loss for  611 th iteration => 0.619982560893\n",
      "Loss for  612 th iteration => 0.620848619004\n",
      "Loss for  613 th iteration => 0.61870895401\n",
      "Loss for  614 th iteration => 0.617097774657\n",
      "Loss for  615 th iteration => 0.61698102793\n",
      "Loss for  616 th iteration => 0.617102589637\n",
      "Loss for  617 th iteration => 0.618424679407\n",
      "Loss for  618 th iteration => 0.616379170488\n",
      "Loss for  619 th iteration => 0.615096013129\n",
      "Loss for  620 th iteration => 0.614842567078\n",
      "Loss for  621 th iteration => 0.614942716213\n",
      "Loss for  622 th iteration => 0.616028854744\n",
      "Loss for  623 th iteration => 0.614353581508\n",
      "Loss for  624 th iteration => 0.612268427862\n",
      "Loss for  625 th iteration => 0.612231419759\n",
      "Loss for  626 th iteration => 0.611540090378\n",
      "Loss for  627 th iteration => 0.612776090377\n",
      "Loss for  628 th iteration => 0.611318921738\n",
      "Loss for  629 th iteration => 0.610939210795\n",
      "Loss for  630 th iteration => 0.612127645764\n",
      "Loss for  631 th iteration => 0.610838014831\n",
      "Loss for  632 th iteration => 0.609152014587\n",
      "Loss for  633 th iteration => 0.609070265235\n",
      "Loss for  634 th iteration => 0.610556410618\n",
      "Loss for  635 th iteration => 0.60885626577\n",
      "Loss for  636 th iteration => 0.60675915403\n",
      "Loss for  637 th iteration => 0.607634422639\n",
      "Loss for  638 th iteration => 0.60859327378\n",
      "Loss for  639 th iteration => 0.607000142567\n",
      "Loss for  640 th iteration => 0.605000567154\n",
      "Loss for  641 th iteration => 0.605310767436\n",
      "Loss for  642 th iteration => 0.606132890705\n",
      "Loss for  643 th iteration => 0.604520915016\n",
      "Loss for  644 th iteration => 0.603304340595\n",
      "Loss for  645 th iteration => 0.604252347619\n",
      "Loss for  646 th iteration => 0.602666062976\n",
      "Loss for  647 th iteration => 0.602380417566\n",
      "Loss for  648 th iteration => 0.603469555941\n",
      "Loss for  649 th iteration => 0.601958542533\n",
      "Loss for  650 th iteration => 0.600225370267\n",
      "Loss for  651 th iteration => 0.60054412494\n",
      "Loss for  652 th iteration => 0.600132370562\n",
      "Loss for  653 th iteration => 0.600591243935\n",
      "Loss for  654 th iteration => 0.598722650062\n",
      "Loss for  655 th iteration => 0.598792973393\n",
      "Loss for  656 th iteration => 0.5993309447\n",
      "Loss for  657 th iteration => 0.597518863833\n",
      "Loss for  658 th iteration => 0.596991632004\n",
      "Loss for  659 th iteration => 0.597910666471\n",
      "Loss for  660 th iteration => 0.595839463061\n",
      "Loss for  661 th iteration => 0.596023291019\n",
      "Loss for  662 th iteration => 0.596788122017\n",
      "Loss for  663 th iteration => 0.594930744629\n",
      "Loss for  664 th iteration => 0.594120873287\n",
      "Loss for  665 th iteration => 0.594704973524\n",
      "Loss for  666 th iteration => 0.592892870111\n",
      "Loss for  667 th iteration => 0.593118534221\n",
      "Loss for  668 th iteration => 0.593831062489\n",
      "Loss for  669 th iteration => 0.591770407427\n",
      "Loss for  670 th iteration => 0.591371802182\n",
      "Loss for  671 th iteration => 0.591947450675\n",
      "Loss for  672 th iteration => 0.590124557724\n",
      "Loss for  673 th iteration => 0.590062361184\n",
      "Loss for  674 th iteration => 0.590928733629\n",
      "Loss for  675 th iteration => 0.588702691614\n",
      "Loss for  676 th iteration => 0.588655261953\n",
      "Loss for  677 th iteration => 0.588981867137\n",
      "Loss for  678 th iteration => 0.587297553532\n",
      "Loss for  679 th iteration => 0.587135922106\n",
      "Loss for  680 th iteration => 0.587931059634\n",
      "Loss for  681 th iteration => 0.585640037245\n",
      "Loss for  682 th iteration => 0.585636336966\n",
      "Loss for  683 th iteration => 0.586126295574\n",
      "Loss for  684 th iteration => 0.584434291427\n",
      "Loss for  685 th iteration => 0.584296690099\n",
      "Loss for  686 th iteration => 0.585136065188\n",
      "Loss for  687 th iteration => 0.582921789908\n",
      "Loss for  688 th iteration => 0.582917442119\n",
      "Loss for  689 th iteration => 0.583153932016\n",
      "Loss for  690 th iteration => 0.581328473657\n",
      "Loss for  691 th iteration => 0.581484587769\n",
      "Loss for  692 th iteration => 0.582327513397\n",
      "Loss for  693 th iteration => 0.579911635017\n",
      "Loss for  694 th iteration => 0.579875087763\n",
      "Loss for  695 th iteration => 0.580510171968\n",
      "Loss for  696 th iteration => 0.578762553737\n",
      "Loss for  697 th iteration => 0.578335717507\n",
      "Loss for  698 th iteration => 0.579300563508\n",
      "Loss for  699 th iteration => 0.57697695986\n",
      "Loss for  700 th iteration => 0.57728979682\n",
      "Loss for  701 th iteration => 0.577393057057\n",
      "Loss for  702 th iteration => 0.575873600328\n",
      "Loss for  703 th iteration => 0.575578977449\n",
      "Loss for  704 th iteration => 0.576552695213\n",
      "Loss for  705 th iteration => 0.574367335322\n",
      "Loss for  706 th iteration => 0.574165241979\n",
      "Loss for  707 th iteration => 0.574627900243\n",
      "Loss for  708 th iteration => 0.573038893327\n",
      "Loss for  709 th iteration => 0.57271090471\n",
      "Loss for  710 th iteration => 0.573719353441\n",
      "Loss for  711 th iteration => 0.571450077948\n",
      "Loss for  712 th iteration => 0.571285968859\n",
      "Loss for  713 th iteration => 0.571792554586\n",
      "Loss for  714 th iteration => 0.570292623465\n",
      "Loss for  715 th iteration => 0.569667259295\n",
      "Loss for  716 th iteration => 0.570557782983\n",
      "Loss for  717 th iteration => 0.568754926517\n",
      "Loss for  718 th iteration => 0.568382106643\n",
      "Loss for  719 th iteration => 0.56815105845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  720 th iteration => 0.567400529667\n",
      "Loss for  721 th iteration => 0.566759229753\n",
      "Loss for  722 th iteration => 0.566417069788\n",
      "Loss for  723 th iteration => 0.566892827706\n",
      "Loss for  724 th iteration => 0.565429020923\n",
      "Loss for  725 th iteration => 0.564733957112\n",
      "Loss for  726 th iteration => 0.56590190619\n",
      "Loss for  727 th iteration => 0.563838533792\n",
      "Loss for  728 th iteration => 0.563833665803\n",
      "Loss for  729 th iteration => 0.563230417487\n",
      "Loss for  730 th iteration => 0.562232239568\n",
      "Loss for  731 th iteration => 0.56213669475\n",
      "Loss for  732 th iteration => 0.562016010172\n",
      "Loss for  733 th iteration => 0.562135580527\n",
      "Loss for  734 th iteration => 0.560331295094\n",
      "Loss for  735 th iteration => 0.560486193278\n",
      "Loss for  736 th iteration => 0.561610162902\n",
      "Loss for  737 th iteration => 0.558733352809\n",
      "Loss for  738 th iteration => 0.558701452466\n",
      "Loss for  739 th iteration => 0.559032277617\n",
      "Loss for  740 th iteration => 0.557920624527\n",
      "Loss for  741 th iteration => 0.556757595502\n",
      "Loss for  742 th iteration => 0.556730207957\n",
      "Loss for  743 th iteration => 0.557106332845\n",
      "Loss for  744 th iteration => 0.555964440333\n",
      "Loss for  745 th iteration => 0.555323619865\n",
      "Loss for  746 th iteration => 0.55504006679\n",
      "Loss for  747 th iteration => 0.555044818745\n",
      "Loss for  748 th iteration => 0.554110539455\n",
      "Loss for  749 th iteration => 0.553277647761\n",
      "Loss for  750 th iteration => 0.553048191735\n",
      "Loss for  751 th iteration => 0.55322681957\n",
      "Loss for  752 th iteration => 0.551993107452\n",
      "Loss for  753 th iteration => 0.551496398716\n",
      "Loss for  754 th iteration => 0.5514477592\n",
      "Loss for  755 th iteration => 0.551517097915\n",
      "Loss for  756 th iteration => 0.550229689385\n",
      "Loss for  757 th iteration => 0.549255479884\n",
      "Loss for  758 th iteration => 0.549676431394\n",
      "Loss for  759 th iteration => 0.548703621796\n",
      "Loss for  760 th iteration => 0.548983162359\n",
      "Loss for  761 th iteration => 0.547662382353\n",
      "Loss for  762 th iteration => 0.547908910609\n",
      "Loss for  763 th iteration => 0.546661409746\n",
      "Loss for  764 th iteration => 0.547040804523\n",
      "Loss for  765 th iteration => 0.54565213678\n",
      "Loss for  766 th iteration => 0.546020773047\n",
      "Loss for  767 th iteration => 0.544790490787\n",
      "Loss for  768 th iteration => 0.545509396519\n",
      "Loss for  769 th iteration => 0.54348547573\n",
      "Loss for  770 th iteration => 0.543949128697\n",
      "Loss for  771 th iteration => 0.543128214352\n",
      "Loss for  772 th iteration => 0.543481776527\n",
      "Loss for  773 th iteration => 0.541894859646\n",
      "Loss for  774 th iteration => 0.542369135889\n",
      "Loss for  775 th iteration => 0.540932479281\n",
      "Loss for  776 th iteration => 0.541288180344\n",
      "Loss for  777 th iteration => 0.540161840022\n",
      "Loss for  778 th iteration => 0.540713524336\n",
      "Loss for  779 th iteration => 0.539053827837\n",
      "Loss for  780 th iteration => 0.539311947288\n",
      "Loss for  781 th iteration => 0.538390406445\n",
      "Loss for  782 th iteration => 0.538780536396\n",
      "Loss for  783 th iteration => 0.537094087581\n",
      "Loss for  784 th iteration => 0.537711188776\n",
      "Loss for  785 th iteration => 0.536407188491\n",
      "Loss for  786 th iteration => 0.536743533742\n",
      "Loss for  787 th iteration => 0.535537384658\n",
      "Loss for  788 th iteration => 0.536179743005\n",
      "Loss for  789 th iteration => 0.534276179109\n",
      "Loss for  790 th iteration => 0.534952378903\n",
      "Loss for  791 th iteration => 0.533427777543\n",
      "Loss for  792 th iteration => 0.533911142758\n",
      "Loss for  793 th iteration => 0.532610029599\n",
      "Loss for  794 th iteration => 0.533197607766\n",
      "Loss for  795 th iteration => 0.531669193554\n",
      "Loss for  796 th iteration => 0.532025317314\n",
      "Loss for  797 th iteration => 0.530789525666\n",
      "Loss for  798 th iteration => 0.531372833286\n",
      "Loss for  799 th iteration => 0.529794948399\n",
      "Loss for  800 th iteration => 0.530246726216\n",
      "Loss for  801 th iteration => 0.52886704138\n",
      "Loss for  802 th iteration => 0.529635705273\n",
      "Loss for  803 th iteration => 0.52785072191\n",
      "Loss for  804 th iteration => 0.528373162177\n",
      "Loss for  805 th iteration => 0.527183265004\n",
      "Loss for  806 th iteration => 0.527803437311\n",
      "Loss for  807 th iteration => 0.526115300123\n",
      "Loss for  808 th iteration => 0.526681691088\n",
      "Loss for  809 th iteration => 0.525053010463\n",
      "Loss for  810 th iteration => 0.52542918874\n",
      "Loss for  811 th iteration => 0.524465971144\n",
      "Loss for  812 th iteration => 0.525100372118\n",
      "Loss for  813 th iteration => 0.523312660101\n",
      "Loss for  814 th iteration => 0.524042331276\n",
      "Loss for  815 th iteration => 0.522206941636\n",
      "Loss for  816 th iteration => 0.522841106813\n",
      "Loss for  817 th iteration => 0.521551511456\n",
      "Loss for  818 th iteration => 0.522210367297\n",
      "Loss for  819 th iteration => 0.520381712271\n",
      "Loss for  820 th iteration => 0.521061285696\n",
      "Loss for  821 th iteration => 0.51968326122\n",
      "Loss for  822 th iteration => 0.520313086712\n",
      "Loss for  823 th iteration => 0.518807597394\n",
      "Loss for  824 th iteration => 0.519422606594\n",
      "Loss for  825 th iteration => 0.517698518667\n",
      "Loss for  826 th iteration => 0.518363353839\n",
      "Loss for  827 th iteration => 0.516929942202\n",
      "Loss for  828 th iteration => 0.517514685905\n",
      "Loss for  829 th iteration => 0.515967320667\n",
      "Loss for  830 th iteration => 0.51671153624\n",
      "Loss for  831 th iteration => 0.51515598063\n",
      "Loss for  832 th iteration => 0.515756383005\n",
      "Loss for  833 th iteration => 0.514301927511\n",
      "Loss for  834 th iteration => 0.5148405108\n",
      "Loss for  835 th iteration => 0.513414420669\n",
      "Loss for  836 th iteration => 0.513955007571\n",
      "Loss for  837 th iteration => 0.51250090922\n",
      "Loss for  838 th iteration => 0.513092995469\n",
      "Loss for  839 th iteration => 0.51156732665\n",
      "Loss for  840 th iteration => 0.512249169216\n",
      "Loss for  841 th iteration => 0.510618363823\n",
      "Loss for  842 th iteration => 0.511419439339\n",
      "Loss for  843 th iteration => 0.509657708003\n",
      "Loss for  844 th iteration => 0.510600656233\n",
      "Loss for  845 th iteration => 0.508688363779\n",
      "Loss for  846 th iteration => 0.509602061476\n",
      "Loss for  847 th iteration => 0.507698150041\n",
      "Loss for  848 th iteration => 0.508170915576\n",
      "Loss for  849 th iteration => 0.507375790604\n",
      "Loss for  850 th iteration => 0.508167513655\n",
      "Loss for  851 th iteration => 0.505967554591\n",
      "Loss for  852 th iteration => 0.506684953552\n",
      "Loss for  853 th iteration => 0.505199882219\n",
      "Loss for  854 th iteration => 0.505879427862\n",
      "Loss for  855 th iteration => 0.504464607978\n",
      "Loss for  856 th iteration => 0.505294812812\n",
      "Loss for  857 th iteration => 0.503321650676\n",
      "Loss for  858 th iteration => 0.504087897896\n",
      "Loss for  859 th iteration => 0.5025669516\n",
      "Loss for  860 th iteration => 0.503395854491\n",
      "Loss for  861 th iteration => 0.501698398576\n",
      "Loss for  862 th iteration => 0.502416420573\n",
      "Loss for  863 th iteration => 0.500856793392\n",
      "Loss for  864 th iteration => 0.501759910044\n",
      "Loss for  865 th iteration => 0.499819118359\n",
      "Loss for  866 th iteration => 0.500520602675\n",
      "Loss for  867 th iteration => 0.499090383672\n",
      "Loss for  868 th iteration => 0.499834484293\n",
      "Loss for  869 th iteration => 0.498172705142\n",
      "Loss for  870 th iteration => 0.499246807921\n",
      "Loss for  871 th iteration => 0.497091928513\n",
      "Loss for  872 th iteration => 0.49788633962\n",
      "Loss for  873 th iteration => 0.496471640085\n",
      "Loss for  874 th iteration => 0.497292797272\n",
      "Loss for  875 th iteration => 0.495488259058\n",
      "Loss for  876 th iteration => 0.496239273058\n",
      "Loss for  877 th iteration => 0.494809282894\n",
      "Loss for  878 th iteration => 0.495544320963\n",
      "Loss for  879 th iteration => 0.493761894544\n",
      "Loss for  880 th iteration => 0.494373445445\n",
      "Loss for  881 th iteration => 0.493039493937\n",
      "Loss for  882 th iteration => 0.493721467039\n",
      "Loss for  883 th iteration => 0.492162109435\n",
      "Loss for  884 th iteration => 0.49300211837\n",
      "Loss for  885 th iteration => 0.491186361408\n",
      "Loss for  886 th iteration => 0.492152284844\n",
      "Loss for  887 th iteration => 0.490291146774\n",
      "Loss for  888 th iteration => 0.491075351813\n",
      "Loss for  889 th iteration => 0.489833465034\n",
      "Loss for  890 th iteration => 0.490907015677\n",
      "Loss for  891 th iteration => 0.488505979364\n",
      "Loss for  892 th iteration => 0.489162195052\n",
      "Loss for  893 th iteration => 0.488176859709\n",
      "Loss for  894 th iteration => 0.489428280973\n",
      "Loss for  895 th iteration => 0.486838833136\n",
      "Loss for  896 th iteration => 0.487129073332\n",
      "Loss for  897 th iteration => 0.486155284687\n",
      "Loss for  898 th iteration => 0.487089892266\n",
      "Loss for  899 th iteration => 0.485425529834\n",
      "Loss for  900 th iteration => 0.486365371207\n",
      "Loss for  901 th iteration => 0.484305954285\n",
      "Loss for  902 th iteration => 0.485258358271\n",
      "Loss for  903 th iteration => 0.483709116843\n",
      "Loss for  904 th iteration => 0.484740195912\n",
      "Loss for  905 th iteration => 0.482554923622\n",
      "Loss for  906 th iteration => 0.48351332955\n",
      "Loss for  907 th iteration => 0.482110044731\n",
      "Loss for  908 th iteration => 0.482848145278\n",
      "Loss for  909 th iteration => 0.481078864409\n",
      "Loss for  910 th iteration => 0.481769288222\n",
      "Loss for  911 th iteration => 0.480522957993\n",
      "Loss for  912 th iteration => 0.481675946003\n",
      "Loss for  913 th iteration => 0.47907581951\n",
      "Loss for  914 th iteration => 0.479537276989\n",
      "Loss for  915 th iteration => 0.478615237995\n",
      "Loss for  916 th iteration => 0.479143446703\n",
      "Loss for  917 th iteration => 0.478233113519\n",
      "Loss for  918 th iteration => 0.478003309895\n",
      "Loss for  919 th iteration => 0.477241284556\n",
      "Loss for  920 th iteration => 0.477563281891\n",
      "Loss for  921 th iteration => 0.476711918269\n",
      "Loss for  922 th iteration => 0.476458786408\n",
      "Loss for  923 th iteration => 0.47537185871\n",
      "Loss for  924 th iteration => 0.47558697664\n",
      "Loss for  925 th iteration => 0.475183663034\n",
      "Loss for  926 th iteration => 0.474879863594\n",
      "Loss for  927 th iteration => 0.473788353913\n",
      "Loss for  928 th iteration => 0.474079949118\n",
      "Loss for  929 th iteration => 0.47342541659\n",
      "Loss for  930 th iteration => 0.473234070851\n",
      "Loss for  931 th iteration => 0.472249333277\n",
      "Loss for  932 th iteration => 0.472794950071\n",
      "Loss for  933 th iteration => 0.471672127525\n",
      "Loss for  934 th iteration => 0.471214689416\n",
      "Loss for  935 th iteration => 0.470639215791\n",
      "Loss for  936 th iteration => 0.47098042668\n",
      "Loss for  937 th iteration => 0.470025930196\n",
      "Loss for  938 th iteration => 0.469753025781\n",
      "Loss for  939 th iteration => 0.469024938777\n",
      "Loss for  940 th iteration => 0.469394651998\n",
      "Loss for  941 th iteration => 0.468595682652\n",
      "Loss for  942 th iteration => 0.468343583735\n",
      "Loss for  943 th iteration => 0.467329283295\n",
      "Loss for  944 th iteration => 0.467636737585\n",
      "Loss for  945 th iteration => 0.466925425564\n",
      "Loss for  946 th iteration => 0.466492648655\n",
      "Loss for  947 th iteration => 0.465807286125\n",
      "Loss for  948 th iteration => 0.465959876482\n",
      "Loss for  949 th iteration => 0.46522247927\n",
      "Loss for  950 th iteration => 0.464858749948\n",
      "Loss for  951 th iteration => 0.463975047028\n",
      "Loss for  952 th iteration => 0.464304698804\n",
      "Loss for  953 th iteration => 0.463860616869\n",
      "Loss for  954 th iteration => 0.463567511951\n",
      "Loss for  955 th iteration => 0.462614993661\n",
      "Loss for  956 th iteration => 0.462949108572\n",
      "Loss for  957 th iteration => 0.462114815098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  958 th iteration => 0.461756646135\n",
      "Loss for  959 th iteration => 0.461031845175\n",
      "Loss for  960 th iteration => 0.461299824573\n",
      "Loss for  961 th iteration => 0.460578733719\n",
      "Loss for  962 th iteration => 0.460086492134\n",
      "Loss for  963 th iteration => 0.459630852614\n",
      "Loss for  964 th iteration => 0.460023074698\n",
      "Loss for  965 th iteration => 0.45876251134\n",
      "Loss for  966 th iteration => 0.458536750255\n",
      "Loss for  967 th iteration => 0.457974326223\n",
      "Loss for  968 th iteration => 0.458116575728\n",
      "Loss for  969 th iteration => 0.457300160629\n",
      "Loss for  970 th iteration => 0.45690827327\n",
      "Loss for  971 th iteration => 0.456487065933\n",
      "Loss for  972 th iteration => 0.456506730893\n",
      "Loss for  973 th iteration => 0.455494966313\n",
      "Loss for  974 th iteration => 0.45545470534\n",
      "Loss for  975 th iteration => 0.454829053498\n",
      "Loss for  976 th iteration => 0.455079254687\n",
      "Loss for  977 th iteration => 0.454356847223\n",
      "Loss for  978 th iteration => 0.454489967743\n",
      "Loss for  979 th iteration => 0.453005045493\n",
      "Loss for  980 th iteration => 0.453507385752\n",
      "Loss for  981 th iteration => 0.452838359285\n",
      "Loss for  982 th iteration => 0.452317727217\n",
      "Loss for  983 th iteration => 0.451833759388\n",
      "Loss for  984 th iteration => 0.451990806982\n",
      "Loss for  985 th iteration => 0.451092564501\n",
      "Loss for  986 th iteration => 0.451014943024\n",
      "Loss for  987 th iteration => 0.450137106766\n",
      "Loss for  988 th iteration => 0.450753395096\n",
      "Loss for  989 th iteration => 0.449433838044\n",
      "Loss for  990 th iteration => 0.44932996202\n",
      "Loss for  991 th iteration => 0.448699256604\n",
      "Loss for  992 th iteration => 0.448870669469\n",
      "Loss for  993 th iteration => 0.448031373592\n",
      "Loss for  994 th iteration => 0.447726074249\n",
      "Loss for  995 th iteration => 0.447279120285\n",
      "Loss for  996 th iteration => 0.447298289893\n",
      "Loss for  997 th iteration => 0.446303745607\n",
      "Loss for  998 th iteration => 0.446496571606\n",
      "Loss for  999 th iteration => 0.44625316152\n",
      "Loss for  1000 th iteration => 0.445885820262\n",
      "Loss for  1001 th iteration => 0.44472180859\n",
      "Loss for  1002 th iteration => 0.445111500251\n",
      "Loss for  1003 th iteration => 0.44429296741\n",
      "Loss for  1004 th iteration => 0.444222089115\n",
      "Loss for  1005 th iteration => 0.443515315131\n",
      "Loss for  1006 th iteration => 0.443315027298\n",
      "Loss for  1007 th iteration => 0.442652680141\n",
      "Loss for  1008 th iteration => 0.443042792343\n",
      "Loss for  1009 th iteration => 0.442236103173\n",
      "Loss for  1010 th iteration => 0.441996280845\n",
      "Loss for  1011 th iteration => 0.441361343833\n",
      "Loss for  1012 th iteration => 0.441631170875\n",
      "Loss for  1013 th iteration => 0.44070260351\n",
      "Loss for  1014 th iteration => 0.440749990305\n",
      "Loss for  1015 th iteration => 0.439779944502\n",
      "Loss for  1016 th iteration => 0.439709330855\n",
      "Loss for  1017 th iteration => 0.439071800553\n",
      "Loss for  1018 th iteration => 0.439533126536\n",
      "Loss for  1019 th iteration => 0.438425456945\n",
      "Loss for  1020 th iteration => 0.438318989154\n",
      "Loss for  1021 th iteration => 0.437827371112\n",
      "Loss for  1022 th iteration => 0.437872610618\n",
      "Loss for  1023 th iteration => 0.436727686799\n",
      "Loss for  1024 th iteration => 0.436967835863\n",
      "Loss for  1025 th iteration => 0.43627460252\n",
      "Loss for  1026 th iteration => 0.436326906002\n",
      "Loss for  1027 th iteration => 0.435340779152\n",
      "Loss for  1028 th iteration => 0.435829927993\n",
      "Loss for  1029 th iteration => 0.434861636263\n",
      "Loss for  1030 th iteration => 0.434808849843\n",
      "Loss for  1031 th iteration => 0.434114175277\n",
      "Loss for  1032 th iteration => 0.434416049604\n",
      "Loss for  1033 th iteration => 0.433311692567\n",
      "Loss for  1034 th iteration => 0.433223748494\n",
      "Loss for  1035 th iteration => 0.432660389671\n",
      "Loss for  1036 th iteration => 0.432804448351\n",
      "Loss for  1037 th iteration => 0.431824357355\n",
      "Loss for  1038 th iteration => 0.431988747616\n",
      "Loss for  1039 th iteration => 0.431082530499\n",
      "Loss for  1040 th iteration => 0.431223869842\n",
      "Loss for  1041 th iteration => 0.430717936338\n",
      "Loss for  1042 th iteration => 0.430687185184\n",
      "Loss for  1043 th iteration => 0.429561914594\n",
      "Loss for  1044 th iteration => 0.429315179891\n",
      "Loss for  1045 th iteration => 0.429496961784\n",
      "Loss for  1046 th iteration => 0.429635314673\n",
      "Loss for  1047 th iteration => 0.428205100673\n",
      "Loss for  1048 th iteration => 0.427867573155\n",
      "Loss for  1049 th iteration => 0.427810210435\n",
      "Loss for  1050 th iteration => 0.428104217363\n",
      "Loss for  1051 th iteration => 0.427055565296\n",
      "Loss for  1052 th iteration => 0.426998653577\n",
      "Loss for  1053 th iteration => 0.42616867994\n",
      "Loss for  1054 th iteration => 0.425899284324\n",
      "Loss for  1055 th iteration => 0.425525700852\n",
      "Loss for  1056 th iteration => 0.425952906274\n",
      "Loss for  1057 th iteration => 0.424865687912\n",
      "Loss for  1058 th iteration => 0.424950422134\n",
      "Loss for  1059 th iteration => 0.424194513658\n",
      "Loss for  1060 th iteration => 0.424123268572\n",
      "Loss for  1061 th iteration => 0.423455258887\n",
      "Loss for  1062 th iteration => 0.423537464376\n",
      "Loss for  1063 th iteration => 0.422794873763\n",
      "Loss for  1064 th iteration => 0.423198088462\n",
      "Loss for  1065 th iteration => 0.422274877495\n",
      "Loss for  1066 th iteration => 0.422369262718\n",
      "Loss for  1067 th iteration => 0.421282130209\n",
      "Loss for  1068 th iteration => 0.421608606439\n",
      "Loss for  1069 th iteration => 0.421017027318\n",
      "Loss for  1070 th iteration => 0.420505215405\n",
      "Loss for  1071 th iteration => 0.419771316846\n",
      "Loss for  1072 th iteration => 0.42025567179\n",
      "Loss for  1073 th iteration => 0.419434401821\n",
      "Loss for  1074 th iteration => 0.419179637274\n",
      "Loss for  1075 th iteration => 0.418495317873\n",
      "Loss for  1076 th iteration => 0.418739882702\n",
      "Loss for  1077 th iteration => 0.41838743488\n",
      "Loss for  1078 th iteration => 0.418185080806\n",
      "Loss for  1079 th iteration => 0.417344841269\n",
      "Loss for  1080 th iteration => 0.417260051293\n",
      "Loss for  1081 th iteration => 0.416543283646\n",
      "Loss for  1082 th iteration => 0.416338600564\n",
      "Loss for  1083 th iteration => 0.415992442912\n",
      "Loss for  1084 th iteration => 0.416014543448\n",
      "Loss for  1085 th iteration => 0.4154332673\n",
      "Loss for  1086 th iteration => 0.415351682134\n",
      "Loss for  1087 th iteration => 0.414925910072\n",
      "Loss for  1088 th iteration => 0.415262424111\n",
      "Loss for  1089 th iteration => 0.413927514027\n",
      "Loss for  1090 th iteration => 0.41364767675\n",
      "Loss for  1091 th iteration => 0.413365897452\n",
      "Loss for  1092 th iteration => 0.413363537069\n",
      "Loss for  1093 th iteration => 0.41247738163\n",
      "Loss for  1094 th iteration => 0.412902396753\n",
      "Loss for  1095 th iteration => 0.412230732012\n",
      "Loss for  1096 th iteration => 0.412303036833\n",
      "Loss for  1097 th iteration => 0.411435252823\n",
      "Loss for  1098 th iteration => 0.411166137729\n",
      "Loss for  1099 th iteration => 0.410796667951\n",
      "Loss for  1100 th iteration => 0.410406594039\n",
      "Loss for  1101 th iteration => 0.410375417808\n",
      "Loss for  1102 th iteration => 0.41032882989\n",
      "Loss for  1103 th iteration => 0.409393263873\n",
      "Loss for  1104 th iteration => 0.408899495354\n",
      "Loss for  1105 th iteration => 0.408670480418\n",
      "Loss for  1106 th iteration => 0.409329231055\n",
      "Loss for  1107 th iteration => 0.408352525672\n",
      "Loss for  1108 th iteration => 0.407614229316\n",
      "Loss for  1109 th iteration => 0.407058651531\n",
      "Loss for  1110 th iteration => 0.407736891062\n",
      "Loss for  1111 th iteration => 0.407165622879\n",
      "Loss for  1112 th iteration => 0.406608840632\n",
      "Loss for  1113 th iteration => 0.40580536961\n",
      "Loss for  1114 th iteration => 0.406113585213\n",
      "Loss for  1115 th iteration => 0.405073919992\n",
      "Loss for  1116 th iteration => 0.405361327139\n",
      "Loss for  1117 th iteration => 0.405123614106\n",
      "Loss for  1118 th iteration => 0.404727227261\n",
      "Loss for  1119 th iteration => 0.404214118798\n",
      "Loss for  1120 th iteration => 0.40511646698\n",
      "Loss for  1121 th iteration => 0.403875407536\n",
      "Loss for  1122 th iteration => 0.402957411271\n",
      "Loss for  1123 th iteration => 0.402861040721\n",
      "Loss for  1124 th iteration => 0.403737257116\n",
      "Loss for  1125 th iteration => 0.401940771324\n",
      "Loss for  1126 th iteration => 0.402476540425\n",
      "Loss for  1127 th iteration => 0.40174720062\n",
      "Loss for  1128 th iteration => 0.402988644688\n",
      "Loss for  1129 th iteration => 0.400516808643\n",
      "Loss for  1130 th iteration => 0.401047485473\n",
      "Loss for  1131 th iteration => 0.400473317143\n",
      "Loss for  1132 th iteration => 0.4017721954\n",
      "Loss for  1133 th iteration => 0.399250556768\n",
      "Loss for  1134 th iteration => 0.399308620134\n",
      "Loss for  1135 th iteration => 0.39902348353\n",
      "Loss for  1136 th iteration => 0.400080210631\n",
      "Loss for  1137 th iteration => 0.398284086882\n",
      "Loss for  1138 th iteration => 0.398910522102\n",
      "Loss for  1139 th iteration => 0.397872345119\n",
      "Loss for  1140 th iteration => 0.398643699481\n",
      "Loss for  1141 th iteration => 0.397124348412\n",
      "Loss for  1142 th iteration => 0.397762453338\n",
      "Loss for  1143 th iteration => 0.396262742273\n",
      "Loss for  1144 th iteration => 0.396526222728\n",
      "Loss for  1145 th iteration => 0.39660826103\n",
      "Loss for  1146 th iteration => 0.39555897336\n",
      "Loss for  1147 th iteration => 0.395443020263\n",
      "Loss for  1148 th iteration => 0.395760442116\n",
      "Loss for  1149 th iteration => 0.394748912718\n",
      "Loss for  1150 th iteration => 0.39507728426\n",
      "Loss for  1151 th iteration => 0.394139761747\n",
      "Loss for  1152 th iteration => 0.394208164755\n",
      "Loss for  1153 th iteration => 0.393982268988\n",
      "Loss for  1154 th iteration => 0.394322390023\n",
      "Loss for  1155 th iteration => 0.392877223597\n",
      "Loss for  1156 th iteration => 0.39261329688\n",
      "Loss for  1157 th iteration => 0.392670495929\n",
      "Loss for  1158 th iteration => 0.392502134223\n",
      "Loss for  1159 th iteration => 0.392859981449\n",
      "Loss for  1160 th iteration => 0.391374581666\n",
      "Loss for  1161 th iteration => 0.390907253384\n",
      "Loss for  1162 th iteration => 0.391120512613\n",
      "Loss for  1163 th iteration => 0.391876424341\n",
      "Loss for  1164 th iteration => 0.390344235974\n",
      "Loss for  1165 th iteration => 0.389654691956\n",
      "Loss for  1166 th iteration => 0.389870082651\n",
      "Loss for  1167 th iteration => 0.389758215511\n",
      "Loss for  1168 th iteration => 0.389364205218\n",
      "Loss for  1169 th iteration => 0.38874291129\n",
      "Loss for  1170 th iteration => 0.389058991365\n",
      "Loss for  1171 th iteration => 0.38884848814\n",
      "Loss for  1172 th iteration => 0.388439025389\n",
      "Loss for  1173 th iteration => 0.387268579631\n",
      "Loss for  1174 th iteration => 0.387857468589\n",
      "Loss for  1175 th iteration => 0.38702441472\n",
      "Loss for  1176 th iteration => 0.387832658253\n",
      "Loss for  1177 th iteration => 0.3862879865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1178 th iteration => 0.386897125743\n",
      "Loss for  1179 th iteration => 0.385789564806\n",
      "Loss for  1180 th iteration => 0.386591174521\n",
      "Loss for  1181 th iteration => 0.385122665296\n",
      "Loss for  1182 th iteration => 0.386060523981\n",
      "Loss for  1183 th iteration => 0.385119856993\n",
      "Loss for  1184 th iteration => 0.384349656667\n",
      "Loss for  1185 th iteration => 0.383797964227\n",
      "Loss for  1186 th iteration => 0.384154835279\n",
      "Loss for  1187 th iteration => 0.383536300174\n",
      "Loss for  1188 th iteration => 0.384553703692\n",
      "Loss for  1189 th iteration => 0.382636400009\n",
      "Loss for  1190 th iteration => 0.38300613455\n",
      "Loss for  1191 th iteration => 0.382135380191\n",
      "Loss for  1192 th iteration => 0.382799289127\n",
      "Loss for  1193 th iteration => 0.381747967359\n",
      "Loss for  1194 th iteration => 0.382541948705\n",
      "Loss for  1195 th iteration => 0.381025938182\n",
      "Loss for  1196 th iteration => 0.381642237891\n",
      "Loss for  1197 th iteration => 0.380609483567\n",
      "Loss for  1198 th iteration => 0.381418001941\n",
      "Loss for  1199 th iteration => 0.379859797465\n",
      "Loss for  1200 th iteration => 0.380544051312\n",
      "Loss for  1201 th iteration => 0.37942172005\n",
      "Loss for  1202 th iteration => 0.380343639353\n",
      "Loss for  1203 th iteration => 0.378888129569\n",
      "Loss for  1204 th iteration => 0.379973550571\n",
      "Loss for  1205 th iteration => 0.378127199645\n",
      "Loss for  1206 th iteration => 0.378585225134\n",
      "Loss for  1207 th iteration => 0.3774397807\n",
      "Loss for  1208 th iteration => 0.377677203581\n",
      "Loss for  1209 th iteration => 0.377295762193\n",
      "Loss for  1210 th iteration => 0.378416440705\n",
      "Loss for  1211 th iteration => 0.376349414773\n",
      "Loss for  1212 th iteration => 0.376551121145\n",
      "Loss for  1213 th iteration => 0.376348120331\n",
      "Loss for  1214 th iteration => 0.376403986242\n",
      "Loss for  1215 th iteration => 0.376088188455\n",
      "Loss for  1216 th iteration => 0.375860621157\n",
      "Loss for  1217 th iteration => 0.3747095173\n",
      "Loss for  1218 th iteration => 0.375104729795\n",
      "Loss for  1219 th iteration => 0.375237167853\n",
      "Loss for  1220 th iteration => 0.374646141904\n",
      "Loss for  1221 th iteration => 0.373592150874\n",
      "Loss for  1222 th iteration => 0.373973228562\n",
      "Loss for  1223 th iteration => 0.373147630345\n",
      "Loss for  1224 th iteration => 0.3737159704\n",
      "Loss for  1225 th iteration => 0.372768593178\n",
      "Loss for  1226 th iteration => 0.37253995873\n",
      "Loss for  1227 th iteration => 0.372811316091\n",
      "Loss for  1228 th iteration => 0.372636194855\n",
      "Loss for  1229 th iteration => 0.371482592807\n",
      "Loss for  1230 th iteration => 0.371898639677\n",
      "Loss for  1231 th iteration => 0.371830984533\n",
      "Loss for  1232 th iteration => 0.37143018077\n",
      "Loss for  1233 th iteration => 0.370246474339\n",
      "Loss for  1234 th iteration => 0.370306589588\n",
      "Loss for  1235 th iteration => 0.370404842737\n",
      "Loss for  1236 th iteration => 0.370276476078\n",
      "Loss for  1237 th iteration => 0.369443901867\n",
      "Loss for  1238 th iteration => 0.36939285734\n",
      "Loss for  1239 th iteration => 0.369422867341\n",
      "Loss for  1240 th iteration => 0.369154148772\n",
      "Loss for  1241 th iteration => 0.368188198602\n",
      "Loss for  1242 th iteration => 0.368305494777\n",
      "Loss for  1243 th iteration => 0.368439968244\n",
      "Loss for  1244 th iteration => 0.368497753318\n",
      "Loss for  1245 th iteration => 0.367079400496\n",
      "Loss for  1246 th iteration => 0.367275462066\n",
      "Loss for  1247 th iteration => 0.367117984125\n",
      "Loss for  1248 th iteration => 0.366738696041\n",
      "Loss for  1249 th iteration => 0.366223578813\n",
      "Loss for  1250 th iteration => 0.366736015123\n",
      "Loss for  1251 th iteration => 0.366219679326\n",
      "Loss for  1252 th iteration => 0.3658261746\n",
      "Loss for  1253 th iteration => 0.364917945879\n",
      "Loss for  1254 th iteration => 0.364919199953\n",
      "Loss for  1255 th iteration => 0.365289340634\n",
      "Loss for  1256 th iteration => 0.365525357874\n",
      "Loss for  1257 th iteration => 0.36381637921\n",
      "Loss for  1258 th iteration => 0.3638029829\n",
      "Loss for  1259 th iteration => 0.364198766783\n",
      "Loss for  1260 th iteration => 0.363648646134\n",
      "Loss for  1261 th iteration => 0.362659480862\n",
      "Loss for  1262 th iteration => 0.362439774783\n",
      "Loss for  1263 th iteration => 0.362420220951\n",
      "Loss for  1264 th iteration => 0.362909817257\n",
      "Loss for  1265 th iteration => 0.362271291393\n",
      "Loss for  1266 th iteration => 0.363919093663\n",
      "Loss for  1267 th iteration => 0.361223839383\n",
      "Loss for  1268 th iteration => 0.361457759381\n",
      "Loss for  1269 th iteration => 0.36171333471\n",
      "Loss for  1270 th iteration => 0.361163752933\n",
      "Loss for  1271 th iteration => 0.360807727003\n",
      "Loss for  1272 th iteration => 0.361835863947\n",
      "Loss for  1273 th iteration => 0.359864190049\n",
      "Loss for  1274 th iteration => 0.360396097667\n",
      "Loss for  1275 th iteration => 0.359830519931\n",
      "Loss for  1276 th iteration => 0.359636118765\n",
      "Loss for  1277 th iteration => 0.358947702438\n",
      "Loss for  1278 th iteration => 0.358532624549\n",
      "Loss for  1279 th iteration => 0.358649943082\n",
      "Loss for  1280 th iteration => 0.358613329613\n",
      "Loss for  1281 th iteration => 0.357940161619\n",
      "Loss for  1282 th iteration => 0.357746221906\n",
      "Loss for  1283 th iteration => 0.358085637585\n",
      "Loss for  1284 th iteration => 0.358155227157\n",
      "Loss for  1285 th iteration => 0.356634178406\n",
      "Loss for  1286 th iteration => 0.35687864712\n",
      "Loss for  1287 th iteration => 0.357106410576\n",
      "Loss for  1288 th iteration => 0.357061319748\n",
      "Loss for  1289 th iteration => 0.355782142707\n",
      "Loss for  1290 th iteration => 0.356630846246\n",
      "Loss for  1291 th iteration => 0.355249172738\n",
      "Loss for  1292 th iteration => 0.356109104274\n",
      "Loss for  1293 th iteration => 0.35503208923\n",
      "Loss for  1294 th iteration => 0.356020275759\n",
      "Loss for  1295 th iteration => 0.354397192815\n",
      "Loss for  1296 th iteration => 0.354788783982\n",
      "Loss for  1297 th iteration => 0.353926232301\n",
      "Loss for  1298 th iteration => 0.354090176943\n",
      "Loss for  1299 th iteration => 0.353843099609\n",
      "Loss for  1300 th iteration => 0.353966443748\n",
      "Loss for  1301 th iteration => 0.352892052429\n",
      "Loss for  1302 th iteration => 0.353260793281\n",
      "Loss for  1303 th iteration => 0.35297825889\n",
      "Loss for  1304 th iteration => 0.352769435845\n",
      "Loss for  1305 th iteration => 0.351860677767\n",
      "Loss for  1306 th iteration => 0.352262734697\n",
      "Loss for  1307 th iteration => 0.352047967343\n",
      "Loss for  1308 th iteration => 0.351899700958\n",
      "Loss for  1309 th iteration => 0.350808211782\n",
      "Loss for  1310 th iteration => 0.351846559895\n",
      "Loss for  1311 th iteration => 0.35010099497\n",
      "Loss for  1312 th iteration => 0.350033341997\n",
      "Loss for  1313 th iteration => 0.349893726595\n",
      "Loss for  1314 th iteration => 0.350277653051\n",
      "Loss for  1315 th iteration => 0.349998067666\n",
      "Loss for  1316 th iteration => 0.34923595924\n",
      "Loss for  1317 th iteration => 0.349545622639\n",
      "Loss for  1318 th iteration => 0.348688669181\n",
      "Loss for  1319 th iteration => 0.349346172291\n",
      "Loss for  1320 th iteration => 0.348815682967\n",
      "Loss for  1321 th iteration => 0.348359174067\n",
      "Loss for  1322 th iteration => 0.347654406046\n",
      "Loss for  1323 th iteration => 0.348797729374\n",
      "Loss for  1324 th iteration => 0.347112336324\n",
      "Loss for  1325 th iteration => 0.347852874206\n",
      "Loss for  1326 th iteration => 0.346525221674\n",
      "Loss for  1327 th iteration => 0.346412364991\n",
      "Loss for  1328 th iteration => 0.346958376491\n",
      "Loss for  1329 th iteration => 0.346663170513\n",
      "Loss for  1330 th iteration => 0.346300620841\n",
      "Loss for  1331 th iteration => 0.345415990856\n",
      "Loss for  1332 th iteration => 0.345731826536\n",
      "Loss for  1333 th iteration => 0.346332735206\n",
      "Loss for  1334 th iteration => 0.345404131065\n",
      "Loss for  1335 th iteration => 0.344511641911\n",
      "Loss for  1336 th iteration => 0.344979342554\n",
      "Loss for  1337 th iteration => 0.34481904841\n",
      "Loss for  1338 th iteration => 0.34368379336\n",
      "Loss for  1339 th iteration => 0.343988206632\n",
      "Loss for  1340 th iteration => 0.343524440535\n",
      "Loss for  1341 th iteration => 0.344544553288\n",
      "Loss for  1342 th iteration => 0.342919691545\n",
      "Loss for  1343 th iteration => 0.343339908871\n",
      "Loss for  1344 th iteration => 0.342609949846\n",
      "Loss for  1345 th iteration => 0.343055583659\n",
      "Loss for  1346 th iteration => 0.342480630755\n",
      "Loss for  1347 th iteration => 0.341939791193\n",
      "Loss for  1348 th iteration => 0.341343979113\n",
      "Loss for  1349 th iteration => 0.34201670362\n",
      "Loss for  1350 th iteration => 0.34122943982\n",
      "Loss for  1351 th iteration => 0.342348707259\n",
      "Loss for  1352 th iteration => 0.340689239193\n",
      "Loss for  1353 th iteration => 0.340220974287\n",
      "Loss for  1354 th iteration => 0.340843351852\n",
      "Loss for  1355 th iteration => 0.340023351734\n",
      "Loss for  1356 th iteration => 0.341247585781\n",
      "Loss for  1357 th iteration => 0.339670101651\n",
      "Loss for  1358 th iteration => 0.338922491945\n",
      "Loss for  1359 th iteration => 0.338844460917\n",
      "Loss for  1360 th iteration => 0.338619677436\n",
      "Loss for  1361 th iteration => 0.339000392165\n",
      "Loss for  1362 th iteration => 0.338174625518\n",
      "Loss for  1363 th iteration => 0.338747274868\n",
      "Loss for  1364 th iteration => 0.338243836424\n",
      "Loss for  1365 th iteration => 0.338115748833\n",
      "Loss for  1366 th iteration => 0.337433687516\n",
      "Loss for  1367 th iteration => 0.338081859362\n",
      "Loss for  1368 th iteration => 0.337448193387\n",
      "Loss for  1369 th iteration => 0.337207147387\n",
      "Loss for  1370 th iteration => 0.33617146647\n",
      "Loss for  1371 th iteration => 0.336067519171\n",
      "Loss for  1372 th iteration => 0.336054437926\n",
      "Loss for  1373 th iteration => 0.33639857365\n",
      "Loss for  1374 th iteration => 0.33556695584\n",
      "Loss for  1375 th iteration => 0.335443123266\n",
      "Loss for  1376 th iteration => 0.336344711215\n",
      "Loss for  1377 th iteration => 0.334953761708\n",
      "Loss for  1378 th iteration => 0.335619768588\n",
      "Loss for  1379 th iteration => 0.334228868353\n",
      "Loss for  1380 th iteration => 0.334405161243\n",
      "Loss for  1381 th iteration => 0.334182249388\n",
      "Loss for  1382 th iteration => 0.33528551451\n",
      "Loss for  1383 th iteration => 0.333430551711\n",
      "Loss for  1384 th iteration => 0.333915921629\n",
      "Loss for  1385 th iteration => 0.333024939438\n",
      "Loss for  1386 th iteration => 0.332878671808\n",
      "Loss for  1387 th iteration => 0.33339814891\n",
      "Loss for  1388 th iteration => 0.333100920664\n",
      "Loss for  1389 th iteration => 0.332794384414\n",
      "Loss for  1390 th iteration => 0.332284829328\n",
      "Loss for  1391 th iteration => 0.331552156428\n",
      "Loss for  1392 th iteration => 0.331650633087\n",
      "Loss for  1393 th iteration => 0.331143863246\n",
      "Loss for  1394 th iteration => 0.331361653655\n",
      "Loss for  1395 th iteration => 0.331050130718\n",
      "Loss for  1396 th iteration => 0.332432094961\n",
      "Loss for  1397 th iteration => 0.330509191911\n",
      "Loss for  1398 th iteration => 0.330971576047\n",
      "Loss for  1399 th iteration => 0.330233715229\n",
      "Loss for  1400 th iteration => 0.329835503015\n",
      "Loss for  1401 th iteration => 0.330787650177\n",
      "Loss for  1402 th iteration => 0.329215331123\n",
      "Loss for  1403 th iteration => 0.329814774554\n",
      "Loss for  1404 th iteration => 0.329282710481\n",
      "Loss for  1405 th iteration => 0.328505359877\n",
      "Loss for  1406 th iteration => 0.328458796277\n",
      "Loss for  1407 th iteration => 0.328407973514\n",
      "Loss for  1408 th iteration => 0.328737988025\n",
      "Loss for  1409 th iteration => 0.328261495913\n",
      "Loss for  1410 th iteration => 0.32745590457\n",
      "Loss for  1411 th iteration => 0.327683552362\n",
      "Loss for  1412 th iteration => 0.327887623336\n",
      "Loss for  1413 th iteration => 0.32826021217\n",
      "Loss for  1414 th iteration => 0.326757636444\n",
      "Loss for  1415 th iteration => 0.327362403149\n",
      "Loss for  1416 th iteration => 0.326951870607\n",
      "Loss for  1417 th iteration => 0.326696841534\n",
      "Loss for  1418 th iteration => 0.326271649914\n",
      "Loss for  1419 th iteration => 0.325462557165\n",
      "Loss for  1420 th iteration => 0.325585519159\n",
      "Loss for  1421 th iteration => 0.325810049467\n",
      "Loss for  1422 th iteration => 0.325885083986\n",
      "Loss for  1423 th iteration => 0.324923694835\n",
      "Loss for  1424 th iteration => 0.325738978397\n",
      "Loss for  1425 th iteration => 0.324163075806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1426 th iteration => 0.32406084056\n",
      "Loss for  1427 th iteration => 0.324432643625\n",
      "Loss for  1428 th iteration => 0.323604268591\n",
      "Loss for  1429 th iteration => 0.323546029038\n",
      "Loss for  1430 th iteration => 0.32472154242\n",
      "Loss for  1431 th iteration => 0.322969569387\n",
      "Loss for  1432 th iteration => 0.323436188668\n",
      "Loss for  1433 th iteration => 0.32266427932\n",
      "Loss for  1434 th iteration => 0.322428744576\n",
      "Loss for  1435 th iteration => 0.323202710096\n",
      "Loss for  1436 th iteration => 0.322737749482\n",
      "Loss for  1437 th iteration => 0.322384373586\n",
      "Loss for  1438 th iteration => 0.322027264574\n",
      "Loss for  1439 th iteration => 0.321404138761\n",
      "Loss for  1440 th iteration => 0.32199289254\n",
      "Loss for  1441 th iteration => 0.320964139878\n",
      "Loss for  1442 th iteration => 0.320963666523\n",
      "Loss for  1443 th iteration => 0.321941715744\n",
      "Loss for  1444 th iteration => 0.320477979704\n",
      "Loss for  1445 th iteration => 0.320236409942\n",
      "Loss for  1446 th iteration => 0.32083770453\n",
      "Loss for  1447 th iteration => 0.319573769429\n",
      "Loss for  1448 th iteration => 0.319436945437\n",
      "Loss for  1449 th iteration => 0.319765238788\n",
      "Loss for  1450 th iteration => 0.319149152616\n",
      "Loss for  1451 th iteration => 0.319570234171\n",
      "Loss for  1452 th iteration => 0.318695692677\n",
      "Loss for  1453 th iteration => 0.318447369968\n",
      "Loss for  1454 th iteration => 0.319196181808\n",
      "Loss for  1455 th iteration => 0.317912752495\n",
      "Loss for  1456 th iteration => 0.317761750963\n",
      "Loss for  1457 th iteration => 0.317803870116\n",
      "Loss for  1458 th iteration => 0.318960253687\n",
      "Loss for  1459 th iteration => 0.317183901025\n",
      "Loss for  1460 th iteration => 0.317283037522\n",
      "Loss for  1461 th iteration => 0.318149683239\n",
      "Loss for  1462 th iteration => 0.317131313873\n",
      "Loss for  1463 th iteration => 0.317119806139\n",
      "Loss for  1464 th iteration => 0.316935910983\n",
      "Loss for  1465 th iteration => 0.31622109392\n",
      "Loss for  1466 th iteration => 0.317208916082\n",
      "Loss for  1467 th iteration => 0.315524808261\n",
      "Loss for  1468 th iteration => 0.315581632719\n",
      "Loss for  1469 th iteration => 0.316715685566\n",
      "Loss for  1470 th iteration => 0.315526702356\n",
      "Loss for  1471 th iteration => 0.316353380863\n",
      "Loss for  1472 th iteration => 0.314659800881\n",
      "Loss for  1473 th iteration => 0.314766928475\n",
      "Loss for  1474 th iteration => 0.316108356909\n",
      "Loss for  1475 th iteration => 0.314235203671\n",
      "Loss for  1476 th iteration => 0.314111973414\n",
      "Loss for  1477 th iteration => 0.314344226579\n",
      "Loss for  1478 th iteration => 0.314795597492\n",
      "Loss for  1479 th iteration => 0.313130309066\n",
      "Loss for  1480 th iteration => 0.313422528777\n",
      "Loss for  1481 th iteration => 0.313624839628\n",
      "Loss for  1482 th iteration => 0.313549368609\n",
      "Loss for  1483 th iteration => 0.312546308411\n",
      "Loss for  1484 th iteration => 0.313318945444\n",
      "Loss for  1485 th iteration => 0.312267272435\n",
      "Loss for  1486 th iteration => 0.313324269918\n",
      "Loss for  1487 th iteration => 0.312636199649\n",
      "Loss for  1488 th iteration => 0.311733145968\n",
      "Loss for  1489 th iteration => 0.312732984507\n",
      "Loss for  1490 th iteration => 0.311049596896\n",
      "Loss for  1491 th iteration => 0.310885992421\n",
      "Loss for  1492 th iteration => 0.311376888113\n",
      "Loss for  1493 th iteration => 0.310673359672\n",
      "Loss for  1494 th iteration => 0.310323372103\n",
      "Loss for  1495 th iteration => 0.311490817053\n",
      "Loss for  1496 th iteration => 0.31051446774\n",
      "Loss for  1497 th iteration => 0.309689595622\n",
      "Loss for  1498 th iteration => 0.310298878806\n",
      "Loss for  1499 th iteration => 0.31045371574\n",
      "Loss for  1500 th iteration => 0.309223964428\n",
      "Loss for  1501 th iteration => 0.309788931354\n",
      "Loss for  1502 th iteration => 0.308711166042\n",
      "Loss for  1503 th iteration => 0.30872834667\n",
      "Loss for  1504 th iteration => 0.309551737444\n",
      "Loss for  1505 th iteration => 0.308089915144\n",
      "Loss for  1506 th iteration => 0.308012812387\n",
      "Loss for  1507 th iteration => 0.308734817403\n",
      "Loss for  1508 th iteration => 0.307489305498\n",
      "Loss for  1509 th iteration => 0.307236675485\n",
      "Loss for  1510 th iteration => 0.307052648499\n",
      "Loss for  1511 th iteration => 0.30713068193\n",
      "Loss for  1512 th iteration => 0.308474779272\n",
      "Loss for  1513 th iteration => 0.307371947179\n",
      "Loss for  1514 th iteration => 0.306524014134\n",
      "Loss for  1515 th iteration => 0.307877715355\n",
      "Loss for  1516 th iteration => 0.306847183518\n",
      "Loss for  1517 th iteration => 0.305920180132\n",
      "Loss for  1518 th iteration => 0.306777214073\n",
      "Loss for  1519 th iteration => 0.305869326563\n",
      "Loss for  1520 th iteration => 0.305603448153\n",
      "Loss for  1521 th iteration => 0.306185455823\n",
      "Loss for  1522 th iteration => 0.304972052294\n",
      "Loss for  1523 th iteration => 0.304868482224\n",
      "Loss for  1524 th iteration => 0.305270679654\n",
      "Loss for  1525 th iteration => 0.304353394361\n",
      "Loss for  1526 th iteration => 0.304291931051\n",
      "Loss for  1527 th iteration => 0.305294605786\n",
      "Loss for  1528 th iteration => 0.303934403977\n",
      "Loss for  1529 th iteration => 0.303557923352\n",
      "Loss for  1530 th iteration => 0.303956629276\n",
      "Loss for  1531 th iteration => 0.303285601847\n",
      "Loss for  1532 th iteration => 0.303834741037\n",
      "Loss for  1533 th iteration => 0.302921677844\n",
      "Loss for  1534 th iteration => 0.302948660847\n",
      "Loss for  1535 th iteration => 0.30239417278\n",
      "Loss for  1536 th iteration => 0.302334971985\n",
      "Loss for  1537 th iteration => 0.302682456032\n",
      "Loss for  1538 th iteration => 0.303699504938\n",
      "Loss for  1539 th iteration => 0.301833783093\n",
      "Loss for  1540 th iteration => 0.301404175016\n",
      "Loss for  1541 th iteration => 0.301395120501\n",
      "Loss for  1542 th iteration => 0.301394551063\n",
      "Loss for  1543 th iteration => 0.301789283259\n",
      "Loss for  1544 th iteration => 0.300788117084\n",
      "Loss for  1545 th iteration => 0.300714876244\n",
      "Loss for  1546 th iteration => 0.301155993804\n",
      "Loss for  1547 th iteration => 0.300447007472\n",
      "Loss for  1548 th iteration => 0.300002673229\n",
      "Loss for  1549 th iteration => 0.300325235229\n",
      "Loss for  1550 th iteration => 0.299657826469\n",
      "Loss for  1551 th iteration => 0.299860094921\n",
      "Loss for  1552 th iteration => 0.299900134016\n",
      "Loss for  1553 th iteration => 0.29917412413\n",
      "Loss for  1554 th iteration => 0.299187553858\n",
      "Loss for  1555 th iteration => 0.299838113373\n",
      "Loss for  1556 th iteration => 0.29864868921\n",
      "Loss for  1557 th iteration => 0.298414793611\n",
      "Loss for  1558 th iteration => 0.298680548477\n",
      "Loss for  1559 th iteration => 0.298300378053\n",
      "Loss for  1560 th iteration => 0.297906256939\n",
      "Loss for  1561 th iteration => 0.298546003553\n",
      "Loss for  1562 th iteration => 0.297589421858\n",
      "Loss for  1563 th iteration => 0.297566767299\n",
      "Loss for  1564 th iteration => 0.297783103284\n",
      "Loss for  1565 th iteration => 0.297080064773\n",
      "Loss for  1566 th iteration => 0.296832842693\n",
      "Loss for  1567 th iteration => 0.297450720888\n",
      "Loss for  1568 th iteration => 0.296607070308\n",
      "Loss for  1569 th iteration => 0.29741156456\n",
      "Loss for  1570 th iteration => 0.296273843916\n",
      "Loss for  1571 th iteration => 0.296697233941\n",
      "Loss for  1572 th iteration => 0.295930462821\n",
      "Loss for  1573 th iteration => 0.295594496167\n",
      "Loss for  1574 th iteration => 0.296105669218\n",
      "Loss for  1575 th iteration => 0.295168019209\n",
      "Loss for  1576 th iteration => 0.295553131346\n",
      "Loss for  1577 th iteration => 0.294982068999\n",
      "Loss for  1578 th iteration => 0.295431843729\n",
      "Loss for  1579 th iteration => 0.295630454793\n",
      "Loss for  1580 th iteration => 0.294482039225\n",
      "Loss for  1581 th iteration => 0.295398209434\n",
      "Loss for  1582 th iteration => 0.294727670383\n",
      "Loss for  1583 th iteration => 0.293751567444\n",
      "Loss for  1584 th iteration => 0.293526679445\n",
      "Loss for  1585 th iteration => 0.293498062796\n",
      "Loss for  1586 th iteration => 0.294333715624\n",
      "Loss for  1587 th iteration => 0.293414852243\n",
      "Loss for  1588 th iteration => 0.294126120473\n",
      "Loss for  1589 th iteration => 0.293649160808\n",
      "Loss for  1590 th iteration => 0.29258931623\n",
      "Loss for  1591 th iteration => 0.292547187855\n",
      "Loss for  1592 th iteration => 0.29251209077\n",
      "Loss for  1593 th iteration => 0.292739850701\n",
      "Loss for  1594 th iteration => 0.292525770841\n",
      "Loss for  1595 th iteration => 0.291686825121\n",
      "Loss for  1596 th iteration => 0.292346642099\n",
      "Loss for  1597 th iteration => 0.292008167386\n",
      "Loss for  1598 th iteration => 0.291223000683\n",
      "Loss for  1599 th iteration => 0.291643929668\n",
      "Loss for  1600 th iteration => 0.29095061814\n",
      "Loss for  1601 th iteration => 0.290735740854\n",
      "Loss for  1602 th iteration => 0.291254180259\n",
      "Loss for  1603 th iteration => 0.29037619931\n",
      "Loss for  1604 th iteration => 0.29033112312\n",
      "Loss for  1605 th iteration => 0.29069661627\n",
      "Loss for  1606 th iteration => 0.290152763105\n",
      "Loss for  1607 th iteration => 0.289696998589\n",
      "Loss for  1608 th iteration => 0.29012829316\n",
      "Loss for  1609 th iteration => 0.289350766092\n",
      "Loss for  1610 th iteration => 0.289161516721\n",
      "Loss for  1611 th iteration => 0.289597556979\n",
      "Loss for  1612 th iteration => 0.288950560339\n",
      "Loss for  1613 th iteration => 0.288591101883\n",
      "Loss for  1614 th iteration => 0.28926675893\n",
      "Loss for  1615 th iteration => 0.288426483403\n",
      "Loss for  1616 th iteration => 0.288310841598\n",
      "Loss for  1617 th iteration => 0.288655509382\n",
      "Loss for  1618 th iteration => 0.288321609774\n",
      "Loss for  1619 th iteration => 0.287655943612\n",
      "Loss for  1620 th iteration => 0.287888251868\n",
      "Loss for  1621 th iteration => 0.287846613812\n",
      "Loss for  1622 th iteration => 0.287085934056\n",
      "Loss for  1623 th iteration => 0.287595612903\n",
      "Loss for  1624 th iteration => 0.286639535356\n",
      "Loss for  1625 th iteration => 0.286747541742\n",
      "Loss for  1626 th iteration => 0.286457901014\n",
      "Loss for  1627 th iteration => 0.28641944842\n",
      "Loss for  1628 th iteration => 0.286260766014\n",
      "Loss for  1629 th iteration => 0.286463219809\n",
      "Loss for  1630 th iteration => 0.286058167417\n",
      "Loss for  1631 th iteration => 0.285438217657\n",
      "Loss for  1632 th iteration => 0.285289597128\n",
      "Loss for  1633 th iteration => 0.285143131269\n",
      "Loss for  1634 th iteration => 0.284951852572\n",
      "Loss for  1635 th iteration => 0.284810673308\n",
      "Loss for  1636 th iteration => 0.284614824734\n",
      "Loss for  1637 th iteration => 0.284478749294\n",
      "Loss for  1638 th iteration => 0.28427850748\n",
      "Loss for  1639 th iteration => 0.284147643067\n",
      "Loss for  1640 th iteration => 0.28388435824\n",
      "Loss for  1641 th iteration => 0.283882958917\n",
      "Loss for  1642 th iteration => 0.283923639162\n",
      "Loss for  1643 th iteration => 0.2838031515\n",
      "Loss for  1644 th iteration => 0.283820532294\n",
      "Loss for  1645 th iteration => 0.283650207027\n",
      "Loss for  1646 th iteration => 0.283004152552\n",
      "Loss for  1647 th iteration => 0.282803282004\n",
      "Loss for  1648 th iteration => 0.283006388711\n",
      "Loss for  1649 th iteration => 0.282469112355\n",
      "Loss for  1650 th iteration => 0.282618427464\n",
      "Loss for  1651 th iteration => 0.282243009351\n",
      "Loss for  1652 th iteration => 0.282618488164\n",
      "Loss for  1653 th iteration => 0.281969769067\n",
      "Loss for  1654 th iteration => 0.282638583667\n",
      "Loss for  1655 th iteration => 0.281566466911\n",
      "Loss for  1656 th iteration => 0.28192296802\n",
      "Loss for  1657 th iteration => 0.281264026485\n",
      "Loss for  1658 th iteration => 0.281446942041\n",
      "Loss for  1659 th iteration => 0.280927103919\n",
      "Loss for  1660 th iteration => 0.281131600479\n",
      "Loss for  1661 th iteration => 0.280567576063\n",
      "Loss for  1662 th iteration => 0.280761753888\n",
      "Loss for  1663 th iteration => 0.280211004934\n",
      "Loss for  1664 th iteration => 0.280310734194\n",
      "Loss for  1665 th iteration => 0.279851071552\n",
      "Loss for  1666 th iteration => 0.279835913838\n",
      "Loss for  1667 th iteration => 0.279557117518\n",
      "Loss for  1668 th iteration => 0.279695021235\n",
      "Loss for  1669 th iteration => 0.279292819163\n",
      "Loss for  1670 th iteration => 0.279718123694\n",
      "Loss for  1671 th iteration => 0.278932351541\n",
      "Loss for  1672 th iteration => 0.279243943437\n",
      "Loss for  1673 th iteration => 0.278735752764\n",
      "Loss for  1674 th iteration => 0.279360491682\n",
      "Loss for  1675 th iteration => 0.278520773892\n",
      "Loss for  1676 th iteration => 0.278357964591\n",
      "Loss for  1677 th iteration => 0.278166091023\n",
      "Loss for  1678 th iteration => 0.277956096313\n",
      "Loss for  1679 th iteration => 0.277825348411\n",
      "Loss for  1680 th iteration => 0.277594031166\n",
      "Loss for  1681 th iteration => 0.277488085044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1682 th iteration => 0.277233405295\n",
      "Loss for  1683 th iteration => 0.27711876634\n",
      "Loss for  1684 th iteration => 0.276784466788\n",
      "Loss for  1685 th iteration => 0.276719879233\n",
      "Loss for  1686 th iteration => 0.276805112928\n",
      "Loss for  1687 th iteration => 0.27645907625\n",
      "Loss for  1688 th iteration => 0.276229367631\n",
      "Loss for  1689 th iteration => 0.276085699438\n",
      "Loss for  1690 th iteration => 0.275922027164\n",
      "Loss for  1691 th iteration => 0.276048904558\n",
      "Loss for  1692 th iteration => 0.275540203818\n",
      "Loss for  1693 th iteration => 0.275483461694\n",
      "Loss for  1694 th iteration => 0.2751998683\n",
      "Loss for  1695 th iteration => 0.275211081632\n",
      "Loss for  1696 th iteration => 0.275193160247\n",
      "Loss for  1697 th iteration => 0.274814717185\n",
      "Loss for  1698 th iteration => 0.275070840628\n",
      "Loss for  1699 th iteration => 0.274514578639\n",
      "Loss for  1700 th iteration => 0.274456706627\n",
      "Loss for  1701 th iteration => 0.274272563864\n",
      "Loss for  1702 th iteration => 0.274725782283\n",
      "Loss for  1703 th iteration => 0.27392493541\n",
      "Loss for  1704 th iteration => 0.274042114383\n",
      "Loss for  1705 th iteration => 0.274025723966\n",
      "Loss for  1706 th iteration => 0.273499471696\n",
      "Loss for  1707 th iteration => 0.273239605017\n",
      "Loss for  1708 th iteration => 0.273255774868\n",
      "Loss for  1709 th iteration => 0.272916826913\n",
      "Loss for  1710 th iteration => 0.273038102466\n",
      "Loss for  1711 th iteration => 0.272578500446\n",
      "Loss for  1712 th iteration => 0.272712397542\n",
      "Loss for  1713 th iteration => 0.272485237231\n",
      "Loss for  1714 th iteration => 0.27309477511\n",
      "Loss for  1715 th iteration => 0.272047359555\n",
      "Loss for  1716 th iteration => 0.272013471643\n",
      "Loss for  1717 th iteration => 0.272059901847\n",
      "Loss for  1718 th iteration => 0.271545326357\n",
      "Loss for  1719 th iteration => 0.271322070353\n",
      "Loss for  1720 th iteration => 0.271218205517\n",
      "Loss for  1721 th iteration => 0.271144203295\n",
      "Loss for  1722 th iteration => 0.270967793719\n",
      "Loss for  1723 th iteration => 0.270956576938\n",
      "Loss for  1724 th iteration => 0.27098394275\n",
      "Loss for  1725 th iteration => 0.270477430101\n",
      "Loss for  1726 th iteration => 0.270260407131\n",
      "Loss for  1727 th iteration => 0.270237134048\n",
      "Loss for  1728 th iteration => 0.270383888886\n",
      "Loss for  1729 th iteration => 0.269898424541\n",
      "Loss for  1730 th iteration => 0.269852223352\n",
      "Loss for  1731 th iteration => 0.269962435224\n",
      "Loss for  1732 th iteration => 0.269488870431\n",
      "Loss for  1733 th iteration => 0.269229487413\n",
      "Loss for  1734 th iteration => 0.269217343808\n",
      "Loss for  1735 th iteration => 0.268910058196\n",
      "Loss for  1736 th iteration => 0.268946594907\n",
      "Loss for  1737 th iteration => 0.268983529819\n",
      "Loss for  1738 th iteration => 0.268763208505\n",
      "Loss for  1739 th iteration => 0.268748804891\n",
      "Loss for  1740 th iteration => 0.268309189596\n",
      "Loss for  1741 th iteration => 0.268236116937\n",
      "Loss for  1742 th iteration => 0.2683868125\n",
      "Loss for  1743 th iteration => 0.267877099288\n",
      "Loss for  1744 th iteration => 0.267676335262\n",
      "Loss for  1745 th iteration => 0.267859656866\n",
      "Loss for  1746 th iteration => 0.267374433306\n",
      "Loss for  1747 th iteration => 0.267454867336\n",
      "Loss for  1748 th iteration => 0.266960194089\n",
      "Loss for  1749 th iteration => 0.266916335293\n",
      "Loss for  1750 th iteration => 0.266676482406\n",
      "Loss for  1751 th iteration => 0.266604943789\n",
      "Loss for  1752 th iteration => 0.266470171698\n",
      "Loss for  1753 th iteration => 0.266461814288\n",
      "Loss for  1754 th iteration => 0.266102285196\n",
      "Loss for  1755 th iteration => 0.266022543208\n",
      "Loss for  1756 th iteration => 0.265921714649\n",
      "Loss for  1757 th iteration => 0.266261105274\n",
      "Loss for  1758 th iteration => 0.265483541823\n",
      "Loss for  1759 th iteration => 0.265550243154\n",
      "Loss for  1760 th iteration => 0.265563057089\n",
      "Loss for  1761 th iteration => 0.265308335245\n",
      "Loss for  1762 th iteration => 0.265122683383\n",
      "Loss for  1763 th iteration => 0.265397458962\n",
      "Loss for  1764 th iteration => 0.264702145906\n",
      "Loss for  1765 th iteration => 0.264903766705\n",
      "Loss for  1766 th iteration => 0.264429429411\n",
      "Loss for  1767 th iteration => 0.264356768951\n",
      "Loss for  1768 th iteration => 0.264134124213\n",
      "Loss for  1769 th iteration => 0.264293270814\n",
      "Loss for  1770 th iteration => 0.263794738135\n",
      "Loss for  1771 th iteration => 0.263717919708\n",
      "Loss for  1772 th iteration => 0.263478809189\n",
      "Loss for  1773 th iteration => 0.263622331831\n",
      "Loss for  1774 th iteration => 0.263252817331\n",
      "Loss for  1775 th iteration => 0.26312558954\n",
      "Loss for  1776 th iteration => 0.262875541739\n",
      "Loss for  1777 th iteration => 0.26278928124\n",
      "Loss for  1778 th iteration => 0.262644649602\n",
      "Loss for  1779 th iteration => 0.262727157454\n",
      "Loss for  1780 th iteration => 0.262346612605\n",
      "Loss for  1781 th iteration => 0.262313049204\n",
      "Loss for  1782 th iteration => 0.261987218298\n",
      "Loss for  1783 th iteration => 0.262042635665\n",
      "Loss for  1784 th iteration => 0.261820384946\n",
      "Loss for  1785 th iteration => 0.261754574022\n",
      "Loss for  1786 th iteration => 0.261526648234\n",
      "Loss for  1787 th iteration => 0.261564561607\n",
      "Loss for  1788 th iteration => 0.261328012498\n",
      "Loss for  1789 th iteration => 0.261106565953\n",
      "Loss for  1790 th iteration => 0.261017745883\n",
      "Loss for  1791 th iteration => 0.260850820125\n",
      "Loss for  1792 th iteration => 0.26069729014\n",
      "Loss for  1793 th iteration => 0.260735014021\n",
      "Loss for  1794 th iteration => 0.260366698282\n",
      "Loss for  1795 th iteration => 0.260237580052\n",
      "Loss for  1796 th iteration => 0.260080748214\n",
      "Loss for  1797 th iteration => 0.259998660944\n",
      "Loss for  1798 th iteration => 0.259949526362\n",
      "Loss for  1799 th iteration => 0.259663916211\n",
      "Loss for  1800 th iteration => 0.259753878997\n",
      "Loss for  1801 th iteration => 0.259312266192\n",
      "Loss for  1802 th iteration => 0.259276051972\n",
      "Loss for  1803 th iteration => 0.259012260208\n",
      "Loss for  1804 th iteration => 0.259041302409\n",
      "Loss for  1805 th iteration => 0.258818573021\n",
      "Loss for  1806 th iteration => 0.258673775409\n",
      "Loss for  1807 th iteration => 0.258581442463\n",
      "Loss for  1808 th iteration => 0.258538338915\n",
      "Loss for  1809 th iteration => 0.25825378332\n",
      "Loss for  1810 th iteration => 0.258349323262\n",
      "Loss for  1811 th iteration => 0.257894519959\n",
      "Loss for  1812 th iteration => 0.257844578337\n",
      "Loss for  1813 th iteration => 0.257600279202\n",
      "Loss for  1814 th iteration => 0.257564675584\n",
      "Loss for  1815 th iteration => 0.257362688832\n",
      "Loss for  1816 th iteration => 0.257271188176\n",
      "Loss for  1817 th iteration => 0.257148435292\n",
      "Loss for  1818 th iteration => 0.257068907554\n",
      "Loss for  1819 th iteration => 0.256819977637\n",
      "Loss for  1820 th iteration => 0.256909023282\n",
      "Loss for  1821 th iteration => 0.256521409853\n",
      "Loss for  1822 th iteration => 0.256697560796\n",
      "Loss for  1823 th iteration => 0.256282600191\n",
      "Loss for  1824 th iteration => 0.256114989038\n",
      "Loss for  1825 th iteration => 0.255932766439\n",
      "Loss for  1826 th iteration => 0.255867184912\n",
      "Loss for  1827 th iteration => 0.255640775642\n",
      "Loss for  1828 th iteration => 0.255566162887\n",
      "Loss for  1829 th iteration => 0.255402032875\n",
      "Loss for  1830 th iteration => 0.255307356793\n",
      "Loss for  1831 th iteration => 0.25513613865\n",
      "Loss for  1832 th iteration => 0.255066765132\n",
      "Loss for  1833 th iteration => 0.254828092739\n",
      "Loss for  1834 th iteration => 0.254777206488\n",
      "Loss for  1835 th iteration => 0.254591640728\n",
      "Loss for  1836 th iteration => 0.254463112791\n",
      "Loss for  1837 th iteration => 0.254314145293\n",
      "Loss for  1838 th iteration => 0.254203439035\n",
      "Loss for  1839 th iteration => 0.25402913173\n",
      "Loss for  1840 th iteration => 0.253903539716\n",
      "Loss for  1841 th iteration => 0.253772043053\n",
      "Loss for  1842 th iteration => 0.253675091353\n",
      "Loss for  1843 th iteration => 0.253750369521\n",
      "Loss for  1844 th iteration => 0.253364292862\n",
      "Loss for  1845 th iteration => 0.253214451161\n",
      "Loss for  1846 th iteration => 0.253085383595\n",
      "Loss for  1847 th iteration => 0.25295753262\n",
      "Loss for  1848 th iteration => 0.252856839182\n",
      "Loss for  1849 th iteration => 0.252950961616\n",
      "Loss for  1850 th iteration => 0.25257893861\n",
      "Loss for  1851 th iteration => 0.25244762894\n",
      "Loss for  1852 th iteration => 0.252560183113\n",
      "Loss for  1853 th iteration => 0.252153856359\n",
      "Loss for  1854 th iteration => 0.252002833864\n",
      "Loss for  1855 th iteration => 0.251910473951\n",
      "Loss for  1856 th iteration => 0.25201908592\n",
      "Loss for  1857 th iteration => 0.251623916665\n",
      "Loss for  1858 th iteration => 0.251524644408\n",
      "Loss for  1859 th iteration => 0.251611031907\n",
      "Loss for  1860 th iteration => 0.251205405612\n",
      "Loss for  1861 th iteration => 0.251146069533\n",
      "Loss for  1862 th iteration => 0.251005559784\n",
      "Loss for  1863 th iteration => 0.250878875672\n",
      "Loss for  1864 th iteration => 0.250738102512\n",
      "Loss for  1865 th iteration => 0.250612575503\n",
      "Loss for  1866 th iteration => 0.25047072978\n",
      "Loss for  1867 th iteration => 0.250347056102\n",
      "Loss for  1868 th iteration => 0.250203531307\n",
      "Loss for  1869 th iteration => 0.250082225829\n",
      "Loss for  1870 th iteration => 0.249936581049\n",
      "Loss for  1871 th iteration => 0.249818010385\n",
      "Loss for  1872 th iteration => 0.249669939611\n",
      "Loss for  1873 th iteration => 0.249554349624\n",
      "Loss for  1874 th iteration => 0.249403656358\n",
      "Loss for  1875 th iteration => 0.249292084756\n",
      "Loss for  1876 th iteration => 0.249079344394\n",
      "Loss for  1877 th iteration => 0.24903363329\n",
      "Loss for  1878 th iteration => 0.248810187111\n",
      "Loss for  1879 th iteration => 0.24877544597\n",
      "Loss for  1880 th iteration => 0.248541651219\n",
      "Loss for  1881 th iteration => 0.248517526096\n",
      "Loss for  1882 th iteration => 0.248275530904\n",
      "Loss for  1883 th iteration => 0.248211256686\n",
      "Loss for  1884 th iteration => 0.248010730826\n",
      "Loss for  1885 th iteration => 0.247881912039\n",
      "Loss for  1886 th iteration => 0.247755621882\n",
      "Loss for  1887 th iteration => 0.247638289307\n",
      "Loss for  1888 th iteration => 0.247603314055\n",
      "Loss for  1889 th iteration => 0.247377470582\n",
      "Loss for  1890 th iteration => 0.247309634523\n",
      "Loss for  1891 th iteration => 0.247113571479\n",
      "Loss for  1892 th iteration => 0.247021645724\n",
      "Loss for  1893 th iteration => 0.247192543272\n",
      "Loss for  1894 th iteration => 0.246700040478\n",
      "Loss for  1895 th iteration => 0.246688899521\n",
      "Loss for  1896 th iteration => 0.246449920341\n",
      "Loss for  1897 th iteration => 0.246348626904\n",
      "Loss for  1898 th iteration => 0.246192944584\n",
      "Loss for  1899 th iteration => 0.246080220845\n",
      "Loss for  1900 th iteration => 0.246047089024\n",
      "Loss for  1901 th iteration => 0.245843242602\n",
      "Loss for  1902 th iteration => 0.245789852407\n",
      "Loss for  1903 th iteration => 0.245534869845\n",
      "Loss for  1904 th iteration => 0.245487457546\n",
      "Loss for  1905 th iteration => 0.245396240079\n",
      "Loss for  1906 th iteration => 0.245174132353\n",
      "Loss for  1907 th iteration => 0.245055063081\n",
      "Loss for  1908 th iteration => 0.244930753243\n",
      "Loss for  1909 th iteration => 0.244885403404\n",
      "Loss for  1910 th iteration => 0.244667244894\n",
      "Loss for  1911 th iteration => 0.244594533548\n",
      "Loss for  1912 th iteration => 0.244419915628\n",
      "Loss for  1913 th iteration => 0.244289017977\n",
      "Loss for  1914 th iteration => 0.244325563731\n",
      "Loss for  1915 th iteration => 0.243997345752\n",
      "Loss for  1916 th iteration => 0.243913447857\n",
      "Loss for  1917 th iteration => 0.243771119872\n",
      "Loss for  1918 th iteration => 0.243686532103\n",
      "Loss for  1919 th iteration => 0.243508985865\n",
      "Loss for  1920 th iteration => 0.243394798575\n",
      "Loss for  1921 th iteration => 0.243310181406\n",
      "Loss for  1922 th iteration => 0.243180433487\n",
      "Loss for  1923 th iteration => 0.242976228987\n",
      "Loss for  1924 th iteration => 0.2429177884\n",
      "Loss for  1925 th iteration => 0.242744915114\n",
      "Loss for  1926 th iteration => 0.242647602661\n",
      "Loss for  1927 th iteration => 0.242504257195\n",
      "Loss for  1928 th iteration => 0.242431576253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1929 th iteration => 0.242296195632\n",
      "Loss for  1930 th iteration => 0.242094565765\n",
      "Loss for  1931 th iteration => 0.242059547006\n",
      "Loss for  1932 th iteration => 0.241865499988\n",
      "Loss for  1933 th iteration => 0.241765625559\n",
      "Loss for  1934 th iteration => 0.241627322431\n",
      "Loss for  1935 th iteration => 0.241506665126\n",
      "Loss for  1936 th iteration => 0.241432269589\n",
      "Loss for  1937 th iteration => 0.241249189307\n",
      "Loss for  1938 th iteration => 0.241171419778\n",
      "Loss for  1939 th iteration => 0.241004124415\n",
      "Loss for  1940 th iteration => 0.240961871884\n",
      "Loss for  1941 th iteration => 0.240742595502\n",
      "Loss for  1942 th iteration => 0.240657090116\n",
      "Loss for  1943 th iteration => 0.240531068688\n",
      "Loss for  1944 th iteration => 0.240371860921\n",
      "Loss for  1945 th iteration => 0.24027149592\n",
      "Loss for  1946 th iteration => 0.240194901183\n",
      "Loss for  1947 th iteration => 0.239993660772\n",
      "Loss for  1948 th iteration => 0.239945205053\n",
      "Loss for  1949 th iteration => 0.239750758724\n",
      "Loss for  1950 th iteration => 0.239691648838\n",
      "Loss for  1951 th iteration => 0.239506202288\n",
      "Loss for  1952 th iteration => 0.239478989986\n",
      "Loss for  1953 th iteration => 0.23925789691\n",
      "Loss for  1954 th iteration => 0.239159105223\n",
      "Loss for  1955 th iteration => 0.239067979523\n",
      "Loss for  1956 th iteration => 0.238893528825\n",
      "Loss for  1957 th iteration => 0.238864128514\n",
      "Loss for  1958 th iteration => 0.238641188375\n",
      "Loss for  1959 th iteration => 0.238547175036\n",
      "Loss for  1960 th iteration => 0.23845400575\n",
      "Loss for  1961 th iteration => 0.238277173952\n",
      "Loss for  1962 th iteration => 0.238254935529\n",
      "Loss for  1963 th iteration => 0.238028265809\n",
      "Loss for  1964 th iteration => 0.237934230826\n",
      "Loss for  1965 th iteration => 0.237846883787\n",
      "Loss for  1966 th iteration => 0.237659583191\n",
      "Loss for  1967 th iteration => 0.237650003798\n",
      "Loss for  1968 th iteration => 0.23742098606\n",
      "Loss for  1969 th iteration => 0.237299942616\n",
      "Loss for  1970 th iteration => 0.23724774599\n",
      "Loss for  1971 th iteration => 0.237046854837\n",
      "Loss for  1972 th iteration => 0.236998459488\n",
      "Loss for  1973 th iteration => 0.236822220114\n",
      "Loss for  1974 th iteration => 0.236678359598\n",
      "Loss for  1975 th iteration => 0.236617284546\n",
      "Loss for  1976 th iteration => 0.236435966726\n",
      "Loss for  1977 th iteration => 0.236369879181\n",
      "Loss for  1978 th iteration => 0.236242687032\n",
      "Loss for  1979 th iteration => 0.236106103836\n",
      "Loss for  1980 th iteration => 0.235959096729\n",
      "Loss for  1981 th iteration => 0.235886826943\n",
      "Loss for  1982 th iteration => 0.235762396707\n",
      "Loss for  1983 th iteration => 0.235622512529\n",
      "Loss for  1984 th iteration => 0.235480666857\n",
      "Loss for  1985 th iteration => 0.235406990165\n",
      "Loss for  1986 th iteration => 0.235282386425\n",
      "Loss for  1987 th iteration => 0.235143403004\n",
      "Loss for  1988 th iteration => 0.235001608701\n",
      "Loss for  1989 th iteration => 0.234929746204\n",
      "Loss for  1990 th iteration => 0.234803044221\n",
      "Loss for  1991 th iteration => 0.234667794035\n",
      "Loss for  1992 th iteration => 0.234522598203\n",
      "Loss for  1993 th iteration => 0.234454688787\n",
      "Loss for  1994 th iteration => 0.234324656744\n",
      "Loss for  1995 th iteration => 0.234195023671\n",
      "Loss for  1996 th iteration => 0.234044115473\n",
      "Loss for  1997 th iteration => 0.233981549021\n",
      "Loss for  1998 th iteration => 0.233847428867\n",
      "Loss for  1999 th iteration => 0.233724642168\n",
      "Loss for  2000 th iteration => 0.233566496437\n",
      "Loss for  2001 th iteration => 0.233510146986\n",
      "Loss for  2002 th iteration => 0.233371503513\n",
      "Loss for  2003 th iteration => 0.23325667345\n",
      "Loss for  2004 th iteration => 0.233112590246\n",
      "Loss for  2005 th iteration => 0.233025140813\n",
      "Loss for  2006 th iteration => 0.232854877652\n",
      "Loss for  2007 th iteration => 0.232798598168\n",
      "Loss for  2008 th iteration => 0.232684613494\n",
      "Loss for  2009 th iteration => 0.232549816707\n",
      "Loss for  2010 th iteration => 0.232429536599\n",
      "Loss for  2011 th iteration => 0.232267903333\n",
      "Loss for  2012 th iteration => 0.232173572977\n",
      "Loss for  2013 th iteration => 0.232077572753\n",
      "Loss for  2014 th iteration => 0.231973946063\n",
      "Loss for  2015 th iteration => 0.231866863311\n",
      "Loss for  2016 th iteration => 0.231729793042\n",
      "Loss for  2017 th iteration => 0.231611879876\n",
      "Loss for  2018 th iteration => 0.231447757166\n",
      "Loss for  2019 th iteration => 0.231356952806\n",
      "Loss for  2020 th iteration => 0.231261379767\n",
      "Loss for  2021 th iteration => 0.23111486576\n",
      "Loss for  2022 th iteration => 0.231010046299\n",
      "Loss for  2023 th iteration => 0.230909829167\n",
      "Loss for  2024 th iteration => 0.230765697998\n",
      "Loss for  2025 th iteration => 0.230663829704\n",
      "Loss for  2026 th iteration => 0.230559920348\n",
      "Loss for  2027 th iteration => 0.230459717588\n",
      "Loss for  2028 th iteration => 0.230299096424\n",
      "Loss for  2029 th iteration => 0.230187747583\n",
      "Loss for  2030 th iteration => 0.230163845651\n",
      "Loss for  2031 th iteration => 0.229940083656\n",
      "Loss for  2032 th iteration => 0.229835336583\n",
      "Loss for  2033 th iteration => 0.229758070217\n",
      "Loss for  2034 th iteration => 0.229595772888\n",
      "Loss for  2035 th iteration => 0.229490015851\n",
      "Loss for  2036 th iteration => 0.229413545265\n",
      "Loss for  2037 th iteration => 0.229252240424\n",
      "Loss for  2038 th iteration => 0.22914480522\n",
      "Loss for  2039 th iteration => 0.229070764619\n",
      "Loss for  2040 th iteration => 0.228909479084\n",
      "Loss for  2041 th iteration => 0.228799872488\n",
      "Loss for  2042 th iteration => 0.228729476446\n",
      "Loss for  2043 th iteration => 0.228567490256\n",
      "Loss for  2044 th iteration => 0.228456016053\n",
      "Loss for  2045 th iteration => 0.228352048531\n",
      "Loss for  2046 th iteration => 0.22822668449\n",
      "Loss for  2047 th iteration => 0.228112710347\n",
      "Loss for  2048 th iteration => 0.227998691561\n",
      "Loss for  2049 th iteration => 0.227885817362\n",
      "Loss for  2050 th iteration => 0.227772506387\n",
      "Loss for  2051 th iteration => 0.22765957809\n",
      "Loss for  2052 th iteration => 0.227551588991\n",
      "Loss for  2053 th iteration => 0.227479659445\n",
      "Loss for  2054 th iteration => 0.227333517807\n",
      "Loss for  2055 th iteration => 0.227209406065\n",
      "Loss for  2056 th iteration => 0.227103365031\n",
      "Loss for  2057 th iteration => 0.226983053987\n",
      "Loss for  2058 th iteration => 0.226869149013\n",
      "Loss for  2059 th iteration => 0.226754484451\n",
      "Loss for  2060 th iteration => 0.226648627684\n",
      "Loss for  2061 th iteration => 0.226578621177\n",
      "Loss for  2062 th iteration => 0.226419601406\n",
      "Loss for  2063 th iteration => 0.226307647334\n",
      "Loss for  2064 th iteration => 0.226205987492\n",
      "Loss for  2065 th iteration => 0.226111961866\n",
      "Loss for  2066 th iteration => 0.226011237817\n",
      "Loss for  2067 th iteration => 0.225856371532\n",
      "Loss for  2068 th iteration => 0.225748918388\n",
      "Loss for  2069 th iteration => 0.225637033088\n",
      "Loss for  2070 th iteration => 0.22552376116\n",
      "Loss for  2071 th iteration => 0.225414438535\n",
      "Loss for  2072 th iteration => 0.225311939318\n",
      "Loss for  2073 th iteration => 0.225205009648\n",
      "Loss for  2074 th iteration => 0.225121637082\n",
      "Loss for  2075 th iteration => 0.224965750531\n",
      "Loss for  2076 th iteration => 0.224859916999\n",
      "Loss for  2077 th iteration => 0.224756279961\n",
      "Loss for  2078 th iteration => 0.224650794579\n",
      "Loss for  2079 th iteration => 0.224564300758\n",
      "Loss for  2080 th iteration => 0.224411429383\n",
      "Loss for  2081 th iteration => 0.224305190551\n",
      "Loss for  2082 th iteration => 0.224194179054\n",
      "Loss for  2083 th iteration => 0.224083180747\n",
      "Loss for  2084 th iteration => 0.22397403329\n",
      "Loss for  2085 th iteration => 0.223859842929\n",
      "Loss for  2086 th iteration => 0.223754069516\n",
      "Loss for  2087 th iteration => 0.223643344926\n",
      "Loss for  2088 th iteration => 0.223532982214\n",
      "Loss for  2089 th iteration => 0.223424152894\n",
      "Loss for  2090 th iteration => 0.223310474391\n",
      "Loss for  2091 th iteration => 0.223204969971\n",
      "Loss for  2092 th iteration => 0.22309413951\n",
      "Loss for  2093 th iteration => 0.222986192946\n",
      "Loss for  2094 th iteration => 0.222876276678\n",
      "Loss for  2095 th iteration => 0.222763822443\n",
      "Loss for  2096 th iteration => 0.222658506748\n",
      "Loss for  2097 th iteration => 0.222545256165\n",
      "Loss for  2098 th iteration => 0.222440857555\n",
      "Loss for  2099 th iteration => 0.222330439231\n",
      "Loss for  2100 th iteration => 0.222221017532\n",
      "Loss for  2101 th iteration => 0.222113489587\n",
      "Loss for  2102 th iteration => 0.222001025004\n",
      "Loss for  2103 th iteration => 0.22189681982\n",
      "Loss for  2104 th iteration => 0.221785616471\n",
      "Loss for  2105 th iteration => 0.221680656908\n",
      "Loss for  2106 th iteration => 0.221570543787\n",
      "Loss for  2107 th iteration => 0.221460842883\n",
      "Loss for  2108 th iteration => 0.22135486841\n",
      "Loss for  2109 th iteration => 0.221243012496\n",
      "Loss for  2110 th iteration => 0.221139038282\n",
      "Loss for  2111 th iteration => 0.221028872058\n",
      "Loss for  2112 th iteration => 0.220924376513\n",
      "Loss for  2113 th iteration => 0.220814658291\n",
      "Loss for  2114 th iteration => 0.220706120186\n",
      "Loss for  2115 th iteration => 0.220600188455\n",
      "Loss for  2116 th iteration => 0.220489104638\n",
      "Loss for  2117 th iteration => 0.220386182493\n",
      "Loss for  2118 th iteration => 0.220274955188\n",
      "Loss for  2119 th iteration => 0.220173007315\n",
      "Loss for  2120 th iteration => 0.22006278854\n",
      "Loss for  2121 th iteration => 0.219956107071\n",
      "Loss for  2122 th iteration => 0.219849499939\n",
      "Loss for  2123 th iteration => 0.219740273311\n",
      "Loss for  2124 th iteration => 0.219636539813\n",
      "Loss for  2125 th iteration => 0.219526153925\n",
      "Loss for  2126 th iteration => 0.219423306614\n",
      "Loss for  2127 th iteration => 0.219313860858\n",
      "Loss for  2128 th iteration => 0.219211966552\n",
      "Loss for  2129 th iteration => 0.219102456831\n",
      "Loss for  2130 th iteration => 0.218997021697\n",
      "Loss for  2131 th iteration => 0.218890669229\n",
      "Loss for  2132 th iteration => 0.218783022865\n",
      "Loss for  2133 th iteration => 0.218679179905\n",
      "Loss for  2134 th iteration => 0.21856982145\n",
      "Loss for  2135 th iteration => 0.218467960989\n",
      "Loss for  2136 th iteration => 0.218358498009\n",
      "Loss for  2137 th iteration => 0.218256125175\n",
      "Loss for  2138 th iteration => 0.218148384561\n",
      "Loss for  2139 th iteration => 0.218046338252\n",
      "Loss for  2140 th iteration => 0.217938081915\n",
      "Loss for  2141 th iteration => 0.217833763307\n",
      "Loss for  2142 th iteration => 0.217728078564\n",
      "Loss for  2143 th iteration => 0.217621991151\n",
      "Loss for  2144 th iteration => 0.21751834501\n",
      "Loss for  2145 th iteration => 0.217410909537\n",
      "Loss for  2146 th iteration => 0.217308867911\n",
      "Loss for  2147 th iteration => 0.217200518665\n",
      "Loss for  2148 th iteration => 0.217099856193\n",
      "Loss for  2149 th iteration => 0.216991418861\n",
      "Loss for  2150 th iteration => 0.216889906114\n",
      "Loss for  2151 th iteration => 0.216782608258\n",
      "Loss for  2152 th iteration => 0.216681567019\n",
      "Loss for  2153 th iteration => 0.216574342825\n",
      "Loss for  2154 th iteration => 0.216473408563\n",
      "Loss for  2155 th iteration => 0.21636644122\n",
      "Loss for  2156 th iteration => 0.216263848615\n",
      "Loss for  2157 th iteration => 0.216158801071\n",
      "Loss for  2158 th iteration => 0.216054945608\n",
      "Loss for  2159 th iteration => 0.215951412431\n",
      "Loss for  2160 th iteration => 0.21584662299\n",
      "Loss for  2161 th iteration => 0.21574427362\n",
      "Loss for  2162 th iteration => 0.215638822755\n",
      "Loss for  2163 th iteration => 0.215537387626\n",
      "Loss for  2164 th iteration => 0.215431500526\n",
      "Loss for  2165 th iteration => 0.21533075983\n",
      "Loss for  2166 th iteration => 0.215224622042\n",
      "Loss for  2167 th iteration => 0.2151243966\n",
      "Loss for  2168 th iteration => 0.215018160607\n",
      "Loss for  2169 th iteration => 0.214918304449\n",
      "Loss for  2170 th iteration => 0.214812095241\n",
      "Loss for  2171 th iteration => 0.214712489562\n",
      "Loss for  2172 th iteration => 0.214606409321\n",
      "Loss for  2173 th iteration => 0.214506957545\n",
      "Loss for  2174 th iteration => 0.214401089569\n",
      "Loss for  2175 th iteration => 0.214301713327\n",
      "Loss for  2176 th iteration => 0.214196125305\n",
      "Loss for  2177 th iteration => 0.214096761138\n",
      "Loss for  2178 th iteration => 0.213991507871\n",
      "Loss for  2179 th iteration => 0.213892104542\n",
      "Loss for  2180 th iteration => 0.213787230203\n",
      "Loss for  2181 th iteration => 0.213687746488\n",
      "Loss for  2182 th iteration => 0.213583286498\n",
      "Loss for  2183 th iteration => 0.213483689382\n",
      "Loss for  2184 th iteration => 0.213379671955\n",
      "Loss for  2185 th iteration => 0.213279935151\n",
      "Loss for  2186 th iteration => 0.213176382572\n",
      "Loss for  2187 th iteration => 0.213076485311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  2188 th iteration => 0.212973414994\n",
      "Loss for  2189 th iteration => 0.212873341026\n",
      "Loss for  2190 th iteration => 0.212770766384\n",
      "Loss for  2191 th iteration => 0.212670503161\n",
      "Loss for  2192 th iteration => 0.212568434324\n",
      "Loss for  2193 th iteration => 0.212467972334\n",
      "Loss for  2194 th iteration => 0.212366416736\n",
      "Loss for  2195 th iteration => 0.212265748951\n",
      "Loss for  2196 th iteration => 0.212164711819\n",
      "Loss for  2197 th iteration => 0.212063833244\n",
      "Loss for  2198 th iteration => 0.211963867448\n",
      "Loss for  2199 th iteration => 0.211884065943\n",
      "Loss for  2200 th iteration => 0.21179802014\n",
      "Loss for  2201 th iteration => 0.211676595889\n",
      "Loss for  2202 th iteration => 0.21156846895\n",
      "Loss for  2203 th iteration => 0.211479791522\n",
      "Loss for  2204 th iteration => 0.211413286432\n",
      "Loss for  2205 th iteration => 0.211276958253\n",
      "Loss for  2206 th iteration => 0.211183227781\n",
      "Loss for  2207 th iteration => 0.211076053334\n",
      "Loss for  2208 th iteration => 0.21101010745\n",
      "Loss for  2209 th iteration => 0.210893019794\n",
      "Loss for  2210 th iteration => 0.210816891067\n",
      "Loss for  2211 th iteration => 0.210675714312\n",
      "Loss for  2212 th iteration => 0.210602838546\n",
      "Loss for  2213 th iteration => 0.210476497716\n",
      "Loss for  2214 th iteration => 0.210403063531\n",
      "Loss for  2215 th iteration => 0.210277782828\n",
      "Loss for  2216 th iteration => 0.210203443574\n",
      "Loss for  2217 th iteration => 0.210079522156\n",
      "Loss for  2218 th iteration => 0.210004062556\n",
      "Loss for  2219 th iteration => 0.209867729546\n",
      "Loss for  2220 th iteration => 0.209790535257\n",
      "Loss for  2221 th iteration => 0.209700347712\n",
      "Loss for  2222 th iteration => 0.209587901974\n",
      "Loss for  2223 th iteration => 0.209499402277\n",
      "Loss for  2224 th iteration => 0.209391258406\n",
      "Loss for  2225 th iteration => 0.20927649222\n",
      "Loss for  2226 th iteration => 0.209195038033\n",
      "Loss for  2227 th iteration => 0.209114136499\n",
      "Loss for  2228 th iteration => 0.208978360274\n",
      "Loss for  2229 th iteration => 0.208888400223\n",
      "Loss for  2230 th iteration => 0.208809742512\n",
      "Loss for  2231 th iteration => 0.208747961159\n",
      "Loss for  2232 th iteration => 0.208616064969\n",
      "Loss for  2233 th iteration => 0.20850131166\n",
      "Loss for  2234 th iteration => 0.20840725218\n",
      "Loss for  2235 th iteration => 0.208308593212\n",
      "Loss for  2236 th iteration => 0.208199161672\n",
      "Loss for  2237 th iteration => 0.208130675763\n",
      "Loss for  2238 th iteration => 0.208044158553\n",
      "Loss for  2239 th iteration => 0.207916125027\n",
      "Loss for  2240 th iteration => 0.207833959138\n",
      "Loss for  2241 th iteration => 0.207719305737\n",
      "Loss for  2242 th iteration => 0.207611632511\n",
      "Loss for  2243 th iteration => 0.207516702616\n",
      "Loss for  2244 th iteration => 0.207430959911\n",
      "Loss for  2245 th iteration => 0.207330486704\n",
      "Loss for  2246 th iteration => 0.207249810993\n",
      "Loss for  2247 th iteration => 0.207154232714\n",
      "Loss for  2248 th iteration => 0.207039833856\n",
      "Loss for  2249 th iteration => 0.206932396061\n",
      "Loss for  2250 th iteration => 0.206840814979\n",
      "Loss for  2251 th iteration => 0.206770491755\n",
      "Loss for  2252 th iteration => 0.206683668835\n",
      "Loss for  2253 th iteration => 0.206549670069\n",
      "Loss for  2254 th iteration => 0.206477145275\n",
      "Loss for  2255 th iteration => 0.206368364255\n",
      "Loss for  2256 th iteration => 0.206257434645\n",
      "Loss for  2257 th iteration => 0.206167705954\n",
      "Loss for  2258 th iteration => 0.20609334621\n",
      "Loss for  2259 th iteration => 0.206000639295\n",
      "Loss for  2260 th iteration => 0.205883502401\n",
      "Loss for  2261 th iteration => 0.205778168007\n",
      "Loss for  2262 th iteration => 0.205686904606\n",
      "Loss for  2263 th iteration => 0.205615291206\n",
      "Loss for  2264 th iteration => 0.205552681528\n",
      "Loss for  2265 th iteration => 0.205407118578\n",
      "Loss for  2266 th iteration => 0.205301208873\n",
      "Loss for  2267 th iteration => 0.205207262789\n",
      "Loss for  2268 th iteration => 0.205124922667\n",
      "Loss for  2269 th iteration => 0.205042420311\n",
      "Loss for  2270 th iteration => 0.204930758402\n",
      "Loss for  2271 th iteration => 0.204824844321\n",
      "Loss for  2272 th iteration => 0.204732690924\n",
      "Loss for  2273 th iteration => 0.204667288601\n",
      "Loss for  2274 th iteration => 0.204595758246\n",
      "Loss for  2275 th iteration => 0.204446257818\n",
      "Loss for  2276 th iteration => 0.204349489366\n",
      "Loss for  2277 th iteration => 0.204261792739\n",
      "Loss for  2278 th iteration => 0.204189954716\n",
      "Loss for  2279 th iteration => 0.204125486578\n",
      "Loss for  2280 th iteration => 0.20398340988\n",
      "Loss for  2281 th iteration => 0.203876970781\n",
      "Loss for  2282 th iteration => 0.203790470624\n",
      "Loss for  2283 th iteration => 0.203721386355\n",
      "Loss for  2284 th iteration => 0.203649834778\n",
      "Loss for  2285 th iteration => 0.203512654505\n",
      "Loss for  2286 th iteration => 0.203422516607\n",
      "Loss for  2287 th iteration => 0.203318918704\n",
      "Loss for  2288 th iteration => 0.203238479171\n",
      "Loss for  2289 th iteration => 0.203176435144\n",
      "Loss for  2290 th iteration => 0.20304554355\n",
      "Loss for  2291 th iteration => 0.202956271739\n",
      "Loss for  2292 th iteration => 0.202865675346\n",
      "Loss for  2293 th iteration => 0.202765463059\n",
      "Loss for  2294 th iteration => 0.202664830156\n",
      "Loss for  2295 th iteration => 0.202579650411\n",
      "Loss for  2296 th iteration => 0.20249139422\n",
      "Loss for  2297 th iteration => 0.202397640708\n",
      "Loss for  2298 th iteration => 0.202299288281\n",
      "Loss for  2299 th iteration => 0.202210749992\n",
      "Loss for  2300 th iteration => 0.202119314211\n",
      "Loss for  2301 th iteration => 0.202020686659\n",
      "Loss for  2302 th iteration => 0.201937644079\n",
      "Loss for  2303 th iteration => 0.201851492162\n",
      "Loss for  2304 th iteration => 0.201729901485\n",
      "Loss for  2305 th iteration => 0.201640537352\n",
      "Loss for  2306 th iteration => 0.201558239291\n",
      "Loss for  2307 th iteration => 0.201470833655\n",
      "Loss for  2308 th iteration => 0.201376896945\n",
      "Loss for  2309 th iteration => 0.201280610341\n",
      "Loss for  2310 th iteration => 0.201193020576\n",
      "Loss for  2311 th iteration => 0.201100732689\n",
      "Loss for  2312 th iteration => 0.201004005184\n",
      "Loss for  2313 th iteration => 0.200923653243\n",
      "Loss for  2314 th iteration => 0.200833517086\n",
      "Loss for  2315 th iteration => 0.200715428583\n",
      "Loss for  2316 th iteration => 0.200638972628\n",
      "Loss for  2317 th iteration => 0.200552120366\n",
      "Loss for  2318 th iteration => 0.200452096116\n",
      "Loss for  2319 th iteration => 0.20035455718\n",
      "Loss for  2320 th iteration => 0.200289415873\n",
      "Loss for  2321 th iteration => 0.200178678844\n",
      "Loss for  2322 th iteration => 0.200079152506\n",
      "Loss for  2323 th iteration => 0.199995170285\n",
      "Loss for  2324 th iteration => 0.19991013304\n",
      "Loss for  2325 th iteration => 0.199818423629\n",
      "Loss for  2326 th iteration => 0.199721409769\n",
      "Loss for  2327 th iteration => 0.199636283594\n",
      "Loss for  2328 th iteration => 0.199545531575\n",
      "Loss for  2329 th iteration => 0.199448431601\n",
      "Loss for  2330 th iteration => 0.199363287209\n",
      "Loss for  2331 th iteration => 0.199273124825\n",
      "Loss for  2332 th iteration => 0.199176176007\n",
      "Loss for  2333 th iteration => 0.199091070041\n",
      "Loss for  2334 th iteration => 0.199001219036\n",
      "Loss for  2335 th iteration => 0.198904603123\n",
      "Loss for  2336 th iteration => 0.198819580276\n",
      "Loss for  2337 th iteration => 0.198729830736\n",
      "Loss for  2338 th iteration => 0.198633685779\n",
      "Loss for  2339 th iteration => 0.198548780884\n",
      "Loss for  2340 th iteration => 0.198458975124\n",
      "Loss for  2341 th iteration => 0.198363405008\n",
      "Loss for  2342 th iteration => 0.198278644921\n",
      "Loss for  2343 th iteration => 0.1981886652\n",
      "Loss for  2344 th iteration => 0.198093747217\n",
      "Loss for  2345 th iteration => 0.198009152431\n",
      "Loss for  2346 th iteration => 0.197918911564\n",
      "Loss for  2347 th iteration => 0.197824702402\n",
      "Loss for  2348 th iteration => 0.197740288384\n",
      "Loss for  2349 th iteration => 0.197649722562\n",
      "Loss for  2350 th iteration => 0.197556262987\n",
      "Loss for  2351 th iteration => 0.197472041261\n",
      "Loss for  2352 th iteration => 0.197381104562\n",
      "Loss for  2353 th iteration => 0.197288423067\n",
      "Loss for  2354 th iteration => 0.197204402087\n",
      "Loss for  2355 th iteration => 0.197113062262\n",
      "Loss for  2356 th iteration => 0.197021177911\n",
      "Loss for  2357 th iteration => 0.196937363745\n",
      "Loss for  2358 th iteration => 0.196845598997\n",
      "Loss for  2359 th iteration => 0.19675452362\n",
      "Loss for  2360 th iteration => 0.196671005498\n",
      "Loss for  2361 th iteration => 0.196572578401\n",
      "Loss for  2362 th iteration => 0.196484607655\n",
      "Loss for  2363 th iteration => 0.196417074383\n",
      "Loss for  2364 th iteration => 0.196303938974\n",
      "Loss for  2365 th iteration => 0.196222339873\n",
      "Loss for  2366 th iteration => 0.196139419803\n",
      "Loss for  2367 th iteration => 0.196034834876\n",
      "Loss for  2368 th iteration => 0.195961037747\n",
      "Loss for  2369 th iteration => 0.195869763382\n",
      "Loss for  2370 th iteration => 0.195785191501\n",
      "Loss for  2371 th iteration => 0.195695553331\n",
      "Loss for  2372 th iteration => 0.195605758271\n",
      "Loss for  2373 th iteration => 0.195521738583\n",
      "Loss for  2374 th iteration => 0.195424075218\n",
      "Loss for  2375 th iteration => 0.195339468769\n",
      "Loss for  2376 th iteration => 0.195270630589\n",
      "Loss for  2377 th iteration => 0.195157403671\n",
      "Loss for  2378 th iteration => 0.195065490448\n",
      "Loss for  2379 th iteration => 0.194991482129\n",
      "Loss for  2380 th iteration => 0.194906509122\n",
      "Loss for  2381 th iteration => 0.19481867548\n",
      "Loss for  2382 th iteration => 0.194729419104\n",
      "Loss for  2383 th iteration => 0.194645641985\n",
      "Loss for  2384 th iteration => 0.194547287596\n",
      "Loss for  2385 th iteration => 0.194466914606\n",
      "Loss for  2386 th iteration => 0.194383625524\n",
      "Loss for  2387 th iteration => 0.194282116254\n",
      "Loss for  2388 th iteration => 0.194208570791\n",
      "Loss for  2389 th iteration => 0.19411977956\n",
      "Loss for  2390 th iteration => 0.194035800387\n",
      "Loss for  2391 th iteration => 0.193938696165\n",
      "Loss for  2392 th iteration => 0.193858349429\n",
      "Loss for  2393 th iteration => 0.193788261576\n",
      "Loss for  2394 th iteration => 0.193674743844\n",
      "Loss for  2395 th iteration => 0.193586186494\n",
      "Loss for  2396 th iteration => 0.193513317136\n",
      "Loss for  2397 th iteration => 0.193428973246\n",
      "Loss for  2398 th iteration => 0.193332707125\n",
      "Loss for  2399 th iteration => 0.193254085719\n",
      "Loss for  2400 th iteration => 0.193182810111\n",
      "Loss for  2401 th iteration => 0.19306964175\n",
      "Loss for  2402 th iteration => 0.192982504608\n",
      "Loss for  2403 th iteration => 0.192909982212\n",
      "Loss for  2404 th iteration => 0.192826067586\n",
      "Loss for  2405 th iteration => 0.192728198561\n",
      "Loss for  2406 th iteration => 0.192654204373\n",
      "Loss for  2407 th iteration => 0.192565748854\n",
      "Loss for  2408 th iteration => 0.192470317802\n",
      "Loss for  2409 th iteration => 0.192415510095\n",
      "Loss for  2410 th iteration => 0.192297583548\n",
      "Loss for  2411 th iteration => 0.192228537263\n",
      "Loss for  2412 th iteration => 0.192142068258\n",
      "Loss for  2413 th iteration => 0.192042295962\n",
      "Loss for  2414 th iteration => 0.191985609488\n",
      "Loss for  2415 th iteration => 0.191876268741\n",
      "Loss for  2416 th iteration => 0.191781703925\n",
      "Loss for  2417 th iteration => 0.19171694245\n",
      "Loss for  2418 th iteration => 0.191625838272\n",
      "Loss for  2419 th iteration => 0.191544564908\n",
      "Loss for  2420 th iteration => 0.191446028703\n",
      "Loss for  2421 th iteration => 0.191376637651\n",
      "Loss for  2422 th iteration => 0.191284411789\n",
      "Loss for  2423 th iteration => 0.191194859694\n",
      "Loss for  2424 th iteration => 0.191128322531\n",
      "Loss for  2425 th iteration => 0.191021957399\n",
      "Loss for  2426 th iteration => 0.190944322053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  2427 th iteration => 0.190867899331\n",
      "Loss for  2428 th iteration => 0.19077031929\n",
      "Loss for  2429 th iteration => 0.190696773775\n",
      "Loss for  2430 th iteration => 0.190608829649\n",
      "Loss for  2431 th iteration => 0.190517485629\n",
      "Loss for  2432 th iteration => 0.190452997021\n",
      "Loss for  2433 th iteration => 0.190348991207\n",
      "Loss for  2434 th iteration => 0.190287862133\n",
      "Loss for  2435 th iteration => 0.190186703751\n",
      "Loss for  2436 th iteration => 0.190090524485\n",
      "Loss for  2437 th iteration => 0.190042861159\n",
      "Loss for  2438 th iteration => 0.189933770145\n",
      "Loss for  2439 th iteration => 0.189848963878\n",
      "Loss for  2440 th iteration => 0.189794324049\n",
      "Loss for  2441 th iteration => 0.189671454417\n",
      "Loss for  2442 th iteration => 0.189617323558\n",
      "Loss for  2443 th iteration => 0.189525117858\n",
      "Loss for  2444 th iteration => 0.189422512604\n",
      "Loss for  2445 th iteration => 0.189363554702\n",
      "Loss for  2446 th iteration => 0.189263802123\n",
      "Loss for  2447 th iteration => 0.189193551121\n",
      "Loss for  2448 th iteration => 0.189106427428\n",
      "Loss for  2449 th iteration => 0.18900411049\n",
      "Loss for  2450 th iteration => 0.188954676116\n",
      "Loss for  2451 th iteration => 0.188854322463\n",
      "Loss for  2452 th iteration => 0.18876185592\n",
      "Loss for  2453 th iteration => 0.188694139941\n",
      "Loss for  2454 th iteration => 0.188598671976\n",
      "Loss for  2455 th iteration => 0.188538891756\n",
      "Loss for  2456 th iteration => 0.188432896927\n",
      "Loss for  2457 th iteration => 0.188359573118\n",
      "Loss for  2458 th iteration => 0.188284874687\n",
      "Loss for  2459 th iteration => 0.18818417411\n",
      "Loss for  2460 th iteration => 0.188121646615\n",
      "Loss for  2461 th iteration => 0.188023740486\n",
      "Loss for  2462 th iteration => 0.18792771539\n",
      "Loss for  2463 th iteration => 0.187882510318\n",
      "Loss for  2464 th iteration => 0.187777867877\n",
      "Loss for  2465 th iteration => 0.187691943072\n",
      "Loss for  2466 th iteration => 0.187639328939\n",
      "Loss for  2467 th iteration => 0.187517093102\n",
      "Loss for  2468 th iteration => 0.187469038543\n",
      "Loss for  2469 th iteration => 0.187371342274\n",
      "Loss for  2470 th iteration => 0.187277046453\n",
      "Loss for  2471 th iteration => 0.187232594248\n",
      "Loss for  2472 th iteration => 0.187108059761\n",
      "Loss for  2473 th iteration => 0.18705864696\n",
      "Loss for  2474 th iteration => 0.186964027226\n",
      "Loss for  2475 th iteration => 0.186867013555\n",
      "Loss for  2476 th iteration => 0.186825308591\n",
      "Loss for  2477 th iteration => 0.186700531772\n",
      "Loss for  2478 th iteration => 0.186650711816\n",
      "Loss for  2479 th iteration => 0.186556704162\n",
      "Loss for  2480 th iteration => 0.186460581823\n",
      "Loss for  2481 th iteration => 0.186418176206\n",
      "Loss for  2482 th iteration => 0.186294473114\n",
      "Loss for  2483 th iteration => 0.186244846286\n",
      "Loss for  2484 th iteration => 0.186149889592\n",
      "Loss for  2485 th iteration => 0.186056949991\n",
      "Loss for  2486 th iteration => 0.186011658543\n",
      "Loss for  2487 th iteration => 0.185889866583\n",
      "Loss for  2488 th iteration => 0.185840806769\n",
      "Loss for  2489 th iteration => 0.185743919241\n",
      "Loss for  2490 th iteration => 0.185655606898\n",
      "Loss for  2491 th iteration => 0.185606053799\n",
      "Loss for  2492 th iteration => 0.185486702206\n",
      "Loss for  2493 th iteration => 0.185438437797\n",
      "Loss for  2494 th iteration => 0.185339008599\n",
      "Loss for  2495 th iteration => 0.185256224864\n",
      "Loss for  2496 th iteration => 0.185201552531\n",
      "Loss for  2497 th iteration => 0.185084972644\n",
      "Loss for  2498 th iteration => 0.185037638826\n",
      "Loss for  2499 th iteration => 0.184935294398\n",
      "Loss for  2500 th iteration => 0.184858592689\n",
      "Loss for  2501 th iteration => 0.18479827515\n",
      "Loss for  2502 th iteration => 0.184684671425\n",
      "Loss for  2503 th iteration => 0.184638343841\n",
      "Loss for  2504 th iteration => 0.184532862254\n",
      "Loss for  2505 th iteration => 0.184462573321\n",
      "Loss for  2506 th iteration => 0.184396296683\n",
      "Loss for  2507 th iteration => 0.184285792294\n",
      "Loss for  2508 th iteration => 0.184240508665\n",
      "Loss for  2509 th iteration => 0.184131764791\n",
      "Loss for  2510 th iteration => 0.184068076996\n",
      "Loss for  2511 th iteration => 0.183995662932\n",
      "Loss for  2512 th iteration => 0.183888329\n",
      "Loss for  2513 th iteration => 0.183844102993\n",
      "Loss for  2514 th iteration => 0.183732033422\n",
      "Loss for  2515 th iteration => 0.183675044151\n",
      "Loss for  2516 th iteration => 0.183596400919\n",
      "Loss for  2517 th iteration => 0.183492275241\n",
      "Loss for  2518 th iteration => 0.183449105354\n",
      "Loss for  2519 th iteration => 0.183335189403\n",
      "Loss for  2520 th iteration => 0.183279570621\n",
      "Loss for  2521 th iteration => 0.183195668984\n",
      "Loss for  2522 th iteration => 0.183099231306\n",
      "Loss for  2523 th iteration => 0.18305824881\n",
      "Loss for  2524 th iteration => 0.182940662751\n",
      "Loss for  2525 th iteration => 0.182890845025\n",
      "Loss for  2526 th iteration => 0.182791818579\n",
      "Loss for  2527 th iteration => 0.182719511082\n",
      "Loss for  2528 th iteration => 0.182655783846\n",
      "Loss for  2529 th iteration => 0.182547751288\n",
      "Loss for  2530 th iteration => 0.182501990688\n",
      "Loss for  2531 th iteration => 0.182392765713\n",
      "Loss for  2532 th iteration => 0.182315358343\n",
      "Loss for  2533 th iteration => 0.182265899562\n",
      "Loss for  2534 th iteration => 0.182152743526\n",
      "Loss for  2535 th iteration => 0.182109605237\n",
      "Loss for  2536 th iteration => 0.182018375808\n",
      "Loss for  2537 th iteration => 0.181929343465\n",
      "Loss for  2538 th iteration => 0.181874313467\n",
      "Loss for  2539 th iteration => 0.18176327399\n",
      "Loss for  2540 th iteration => 0.1817198076\n",
      "Loss for  2541 th iteration => 0.18161740293\n",
      "Loss for  2542 th iteration => 0.181550768083\n",
      "Loss for  2543 th iteration => 0.181482049561\n",
      "Loss for  2544 th iteration => 0.181382180632\n",
      "Loss for  2545 th iteration => 0.18132951148\n",
      "Loss for  2546 th iteration => 0.181218604769\n",
      "Loss for  2547 th iteration => 0.181143073827\n",
      "Loss for  2548 th iteration => 0.181100302186\n",
      "Loss for  2549 th iteration => 0.180994230402\n",
      "Loss for  2550 th iteration => 0.180935170335\n",
      "Loss for  2551 th iteration => 0.180859462489\n",
      "Loss for  2552 th iteration => 0.180756637139\n",
      "Loss for  2553 th iteration => 0.180705217397\n",
      "Loss for  2554 th iteration => 0.180611279382\n",
      "Loss for  2555 th iteration => 0.18055385538\n",
      "Loss for  2556 th iteration => 0.18045693023\n",
      "Loss for  2557 th iteration => 0.180394164536\n",
      "Loss for  2558 th iteration => 0.180310531404\n",
      "Loss for  2559 th iteration => 0.180235508335\n",
      "Loss for  2560 th iteration => 0.180166762161\n",
      "Loss for  2561 th iteration => 0.18007142241\n",
      "Loss for  2562 th iteration => 0.180016186356\n",
      "Loss for  2563 th iteration => 0.179921346451\n",
      "Loss for  2564 th iteration => 0.179854737408\n",
      "Loss for  2565 th iteration => 0.179776715851\n",
      "Loss for  2566 th iteration => 0.179699056515\n",
      "Loss for  2567 th iteration => 0.179630404668\n",
      "Loss for  2568 th iteration => 0.179536398026\n",
      "Loss for  2569 th iteration => 0.17948058722\n",
      "Loss for  2570 th iteration => 0.179387260488\n",
      "Loss for  2571 th iteration => 0.179320230509\n",
      "Loss for  2572 th iteration => 0.179242771341\n",
      "Loss for  2573 th iteration => 0.179166670223\n",
      "Loss for  2574 th iteration => 0.179096352024\n",
      "Loss for  2575 th iteration => 0.179005138807\n",
      "Loss for  2576 th iteration => 0.178947286693\n",
      "Loss for  2577 th iteration => 0.178855125532\n",
      "Loss for  2578 th iteration => 0.178789578065\n",
      "Loss for  2579 th iteration => 0.178710391505\n",
      "Loss for  2580 th iteration => 0.178621411248\n",
      "Loss for  2581 th iteration => 0.178574854524\n",
      "Loss for  2582 th iteration => 0.178472033076\n",
      "Loss for  2583 th iteration => 0.178416286347\n",
      "Loss for  2584 th iteration => 0.178329901294\n",
      "Loss for  2585 th iteration => 0.178255489341\n",
      "Loss for  2586 th iteration => 0.178195130843\n",
      "Loss for  2587 th iteration => 0.178093189668\n",
      "Loss for  2588 th iteration => 0.178048689315\n",
      "Loss for  2589 th iteration => 0.177944540115\n",
      "Loss for  2590 th iteration => 0.177890808941\n",
      "Loss for  2591 th iteration => 0.177799215335\n",
      "Loss for  2592 th iteration => 0.177714458679\n",
      "Loss for  2593 th iteration => 0.177674757131\n",
      "Loss for  2594 th iteration => 0.177561434373\n",
      "Loss for  2595 th iteration => 0.177492975749\n",
      "Loss for  2596 th iteration => 0.177446470965\n",
      "Loss for  2597 th iteration => 0.177345276782\n",
      "Loss for  2598 th iteration => 0.177290389184\n",
      "Loss for  2599 th iteration => 0.177203974515\n",
      "Loss for  2600 th iteration => 0.177132612836\n",
      "Loss for  2601 th iteration => 0.177069678854\n",
      "Loss for  2602 th iteration => 0.176970022097\n",
      "Loss for  2603 th iteration => 0.176911074941\n",
      "Loss for  2604 th iteration => 0.17683347241\n",
      "Loss for  2605 th iteration => 0.176768020433\n",
      "Loss for  2606 th iteration => 0.176675941639\n",
      "Loss for  2607 th iteration => 0.176603568263\n",
      "Loss for  2608 th iteration => 0.176544508244\n",
      "Loss for  2609 th iteration => 0.17646080902\n",
      "Loss for  2610 th iteration => 0.176376481309\n",
      "Loss for  2611 th iteration => 0.176332714115\n",
      "Loss for  2612 th iteration => 0.176230338893\n",
      "Loss for  2613 th iteration => 0.176178313906\n",
      "Loss for  2614 th iteration => 0.176085606723\n",
      "Loss for  2615 th iteration => 0.1760051669\n",
      "Loss for  2616 th iteration => 0.175964750602\n",
      "Loss for  2617 th iteration => 0.175852499745\n",
      "Loss for  2618 th iteration => 0.175818730364\n",
      "Loss for  2619 th iteration => 0.175710255334\n",
      "Loss for  2620 th iteration => 0.17566633634\n",
      "Loss for  2621 th iteration => 0.175578020573\n",
      "Loss for  2622 th iteration => 0.175502126456\n",
      "Loss for  2623 th iteration => 0.175443853158\n",
      "Loss for  2624 th iteration => 0.175345545414\n",
      "Loss for  2625 th iteration => 0.175301407792\n",
      "Loss for  2626 th iteration => 0.175193916058\n",
      "Loss for  2627 th iteration => 0.175156323851\n",
      "Loss for  2628 th iteration => 0.175047840036\n",
      "Loss for  2629 th iteration => 0.174977478907\n",
      "Loss for  2630 th iteration => 0.174935105593\n",
      "Loss for  2631 th iteration => 0.174833356647\n",
      "Loss for  2632 th iteration => 0.17478400715\n",
      "Loss for  2633 th iteration => 0.174693043527\n",
      "Loss for  2634 th iteration => 0.174613666443\n",
      "Loss for  2635 th iteration => 0.174570989247\n",
      "Loss for  2636 th iteration => 0.174463879012\n",
      "Loss for  2637 th iteration => 0.174426857852\n",
      "Loss for  2638 th iteration => 0.174320891711\n",
      "Loss for  2639 th iteration => 0.174261613863\n",
      "Loss for  2640 th iteration => 0.174199087018\n",
      "Loss for  2641 th iteration => 0.17411092488\n",
      "Loss for  2642 th iteration => 0.174056424169\n",
      "Loss for  2643 th iteration => 0.173965440843\n",
      "Loss for  2644 th iteration => 0.173892648376\n",
      "Loss for  2645 th iteration => 0.173838029967\n",
      "Loss for  2646 th iteration => 0.173750633603\n",
      "Loss for  2647 th iteration => 0.173689044719\n",
      "Loss for  2648 th iteration => 0.173596868071\n",
      "Loss for  2649 th iteration => 0.173526028011\n",
      "Loss for  2650 th iteration => 0.173485155406\n",
      "Loss for  2651 th iteration => 0.173383881213\n",
      "Loss for  2652 th iteration => 0.173336882193\n",
      "Loss for  2653 th iteration => 0.173244792862\n",
      "Loss for  2654 th iteration => 0.173169150595\n",
      "Loss for  2655 th iteration => 0.173124867069\n",
      "Loss for  2656 th iteration => 0.173021942008\n",
      "Loss for  2657 th iteration => 0.172982669123\n",
      "Loss for  2658 th iteration => 0.172877735495\n",
      "Loss for  2659 th iteration => 0.172806583468\n",
      "Loss for  2660 th iteration => 0.172770002644\n",
      "Loss for  2661 th iteration => 0.172665438647\n",
      "Loss for  2662 th iteration => 0.17262275185\n",
      "Loss for  2663 th iteration => 0.172525811776\n",
      "Loss for  2664 th iteration => 0.172456594463\n",
      "Loss for  2665 th iteration => 0.172391907945\n",
      "Loss for  2666 th iteration => 0.172325385474\n",
      "Loss for  2667 th iteration => 0.172259064991\n",
      "Loss for  2668 th iteration => 0.172172874193\n",
      "Loss for  2669 th iteration => 0.172124901692\n",
      "Loss for  2670 th iteration => 0.17202426385\n",
      "Loss for  2671 th iteration => 0.171954418204\n",
      "Loss for  2672 th iteration => 0.171911089616\n",
      "Loss for  2673 th iteration => 0.171823232799\n",
      "Loss for  2674 th iteration => 0.171757741844\n",
      "Loss for  2675 th iteration => 0.171690598307\n",
      "Loss for  2676 th iteration => 0.171608743602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  2677 th iteration => 0.171535073174\n",
      "Loss for  2678 th iteration => 0.171480491892\n",
      "Loss for  2679 th iteration => 0.171403251751\n",
      "Loss for  2680 th iteration => 0.171329294634\n",
      "Loss for  2681 th iteration => 0.171270132777\n",
      "Loss for  2682 th iteration => 0.171180135372\n",
      "Loss for  2683 th iteration => 0.17113590872\n",
      "Loss for  2684 th iteration => 0.171035140312\n",
      "Loss for  2685 th iteration => 0.1709654614\n",
      "Loss for  2686 th iteration => 0.170923559398\n",
      "Loss for  2687 th iteration => 0.170834651074\n",
      "Loss for  2688 th iteration => 0.170772799465\n",
      "Loss for  2689 th iteration => 0.170702516375\n",
      "Loss for  2690 th iteration => 0.170625503343\n",
      "Loss for  2691 th iteration => 0.170548081105\n",
      "Loss for  2692 th iteration => 0.170480532806\n",
      "Loss for  2693 th iteration => 0.170429149954\n",
      "Loss for  2694 th iteration => 0.170338159324\n",
      "Loss for  2695 th iteration => 0.170290481117\n",
      "Loss for  2696 th iteration => 0.170196555156\n",
      "Loss for  2697 th iteration => 0.170151837635\n",
      "Loss for  2698 th iteration => 0.170056187282\n",
      "Loss for  2699 th iteration => 0.169983417784\n",
      "Loss for  2700 th iteration => 0.169919403796\n",
      "Loss for  2701 th iteration => 0.169873117545\n",
      "Loss for  2702 th iteration => 0.169788535516\n",
      "Loss for  2703 th iteration => 0.169725615108\n",
      "Loss for  2704 th iteration => 0.169656536931\n",
      "Loss for  2705 th iteration => 0.16958159847\n",
      "Loss for  2706 th iteration => 0.169502478334\n",
      "Loss for  2707 th iteration => 0.169438674219\n",
      "Loss for  2708 th iteration => 0.169384397112\n",
      "Loss for  2709 th iteration => 0.16929806588\n",
      "Loss for  2710 th iteration => 0.169247078153\n",
      "Loss for  2711 th iteration => 0.169158147011\n",
      "Loss for  2712 th iteration => 0.16910978503\n",
      "Loss for  2713 th iteration => 0.169018826378\n",
      "Loss for  2714 th iteration => 0.16897252462\n",
      "Loss for  2715 th iteration => 0.168880336338\n",
      "Loss for  2716 th iteration => 0.168819167539\n",
      "Loss for  2717 th iteration => 0.168743912507\n",
      "Loss for  2718 th iteration => 0.168667481874\n",
      "Loss for  2719 th iteration => 0.168624729306\n",
      "Loss for  2720 th iteration => 0.168542487327\n",
      "Loss for  2721 th iteration => 0.168488794369\n",
      "Loss for  2722 th iteration => 0.168404397314\n",
      "Loss for  2723 th iteration => 0.168334470703\n",
      "Loss for  2724 th iteration => 0.168266984458\n",
      "Loss for  2725 th iteration => 0.168215287683\n",
      "Loss for  2726 th iteration => 0.168132961777\n",
      "Loss for  2727 th iteration => 0.168075225583\n",
      "Loss for  2728 th iteration => 0.167998678096\n",
      "Loss for  2729 th iteration => 0.167936014677\n",
      "Loss for  2730 th iteration => 0.167864180834\n",
      "Loss for  2731 th iteration => 0.167797544041\n",
      "Loss for  2732 th iteration => 0.167729515572\n",
      "Loss for  2733 th iteration => 0.16765972185\n",
      "Loss for  2734 th iteration => 0.167594724952\n",
      "Loss for  2735 th iteration => 0.167522848179\n",
      "Loss for  2736 th iteration => 0.167444364689\n",
      "Loss for  2737 th iteration => 0.167373802047\n",
      "Loss for  2738 th iteration => 0.167310025594\n",
      "Loss for  2739 th iteration => 0.167253570253\n",
      "Loss for  2740 th iteration => 0.167189700851\n",
      "Loss for  2741 th iteration => 0.167115513033\n",
      "Loss for  2742 th iteration => 0.167056220303\n",
      "Loss for  2743 th iteration => 0.166978196189\n",
      "Loss for  2744 th iteration => 0.166922570459\n",
      "Loss for  2745 th iteration => 0.166842182057\n",
      "Loss for  2746 th iteration => 0.166772085059\n",
      "Loss for  2747 th iteration => 0.166705996815\n",
      "Loss for  2748 th iteration => 0.166636979575\n",
      "Loss for  2749 th iteration => 0.166588665499\n",
      "Loss for  2750 th iteration => 0.166504378459\n",
      "Loss for  2751 th iteration => 0.166451937056\n",
      "Loss for  2752 th iteration => 0.166371612099\n",
      "Loss for  2753 th iteration => 0.166315845764\n",
      "Loss for  2754 th iteration => 0.166238722052\n",
      "Loss for  2755 th iteration => 0.166180317194\n",
      "Loss for  2756 th iteration => 0.166105746467\n",
      "Loss for  2757 th iteration => 0.166045289265\n",
      "Loss for  2758 th iteration => 0.165972719864\n",
      "Loss for  2759 th iteration => 0.165910710041\n",
      "Loss for  2760 th iteration => 0.165839763845\n",
      "Loss for  2761 th iteration => 0.165766240231\n",
      "Loss for  2762 th iteration => 0.165707543819\n",
      "Loss for  2763 th iteration => 0.16563098291\n",
      "Loss for  2764 th iteration => 0.165575046439\n",
      "Loss for  2765 th iteration => 0.165510650212\n",
      "Loss for  2766 th iteration => 0.165429081787\n",
      "Loss for  2767 th iteration => 0.165374389831\n",
      "Loss for  2768 th iteration => 0.165297643197\n",
      "Loss for  2769 th iteration => 0.165239103716\n",
      "Loss for  2770 th iteration => 0.165181339807\n",
      "Loss for  2771 th iteration => 0.165105423527\n",
      "Loss for  2772 th iteration => 0.165049330932\n",
      "Loss for  2773 th iteration => 0.164972169181\n",
      "Loss for  2774 th iteration => 0.164917351197\n",
      "Loss for  2775 th iteration => 0.164839299001\n",
      "Loss for  2776 th iteration => 0.164785418383\n",
      "Loss for  2777 th iteration => 0.164706778319\n",
      "Loss for  2778 th iteration => 0.16465354888\n",
      "Loss for  2779 th iteration => 0.164574578196\n",
      "Loss for  2780 th iteration => 0.16452175757\n",
      "Loss for  2781 th iteration => 0.164442674397\n",
      "Loss for  2782 th iteration => 0.164390057804\n",
      "Loss for  2783 th iteration => 0.16431105421\n",
      "Loss for  2784 th iteration => 0.164245651669\n",
      "Loss for  2785 th iteration => 0.1641791837\n",
      "Loss for  2786 th iteration => 0.164113579822\n",
      "Loss for  2787 th iteration => 0.164047502477\n",
      "Loss for  2788 th iteration => 0.163981817527\n",
      "Loss for  2789 th iteration => 0.163916120061\n",
      "Loss for  2790 th iteration => 0.163867312091\n",
      "Loss for  2791 th iteration => 0.163785931209\n",
      "Loss for  2792 th iteration => 0.163735731473\n",
      "Loss for  2793 th iteration => 0.163655742686\n",
      "Loss for  2794 th iteration => 0.163604518116\n",
      "Loss for  2795 th iteration => 0.163525578467\n",
      "Loss for  2796 th iteration => 0.163473641022\n",
      "Loss for  2797 th iteration => 0.163395459587\n",
      "Loss for  2798 th iteration => 0.163343073997\n",
      "Loss for  2799 th iteration => 0.163265404413\n",
      "Loss for  2800 th iteration => 0.163212794865\n",
      "Loss for  2801 th iteration => 0.163135428909\n",
      "Loss for  2802 th iteration => 0.16308278482\n",
      "Loss for  2803 th iteration => 0.163005546903\n",
      "Loss for  2804 th iteration => 0.162953027889\n",
      "Loss for  2805 th iteration => 0.162875770334\n",
      "Loss for  2806 th iteration => 0.162823510488\n",
      "Loss for  2807 th iteration => 0.162746109476\n",
      "Loss for  2808 th iteration => 0.162694221045\n",
      "Loss for  2809 th iteration => 0.162616573148\n",
      "Loss for  2810 th iteration => 0.162565149696\n",
      "Loss for  2811 th iteration => 0.1624871689\n",
      "Loss for  2812 th iteration => 0.162436288018\n",
      "Loss for  2813 th iteration => 0.16235790318\n",
      "Loss for  2814 th iteration => 0.162307628813\n",
      "Loss for  2815 th iteration => 0.162228781477\n",
      "Loss for  2816 th iteration => 0.162179165921\n",
      "Loss for  2817 th iteration => 0.162099814127\n",
      "Loss for  2818 th iteration => 0.162037899047\n",
      "Loss for  2819 th iteration => 0.161984046653\n",
      "Loss for  2820 th iteration => 0.161909255643\n",
      "Loss for  2821 th iteration => 0.161855953253\n",
      "Loss for  2822 th iteration => 0.161780775646\n",
      "Loss for  2823 th iteration => 0.161728035232\n",
      "Loss for  2824 th iteration => 0.161652733134\n",
      "Loss for  2825 th iteration => 0.161587459286\n",
      "Loss for  2826 th iteration => 0.161538033457\n",
      "Loss for  2827 th iteration => 0.161459186133\n",
      "Loss for  2828 th iteration => 0.161410747423\n",
      "Loss for  2829 th iteration => 0.161331314204\n",
      "Loss for  2830 th iteration => 0.161270535052\n",
      "Loss for  2831 th iteration => 0.161216426755\n",
      "Loss for  2832 th iteration => 0.16114282135\n",
      "Loss for  2833 th iteration => 0.161089424869\n",
      "Loss for  2834 th iteration => 0.161015647613\n",
      "Loss for  2835 th iteration => 0.160949682046\n",
      "Loss for  2836 th iteration => 0.160901757223\n",
      "Loss for  2837 th iteration => 0.160822193767\n",
      "Loss for  2838 th iteration => 0.160775359626\n",
      "Loss for  2839 th iteration => 0.160695348782\n",
      "Loss for  2840 th iteration => 0.160635718477\n",
      "Loss for  2841 th iteration => 0.160581379506\n",
      "Loss for  2842 th iteration => 0.160509008439\n",
      "Loss for  2843 th iteration => 0.160442543676\n",
      "Loss for  2844 th iteration => 0.160395733594\n",
      "Loss for  2845 th iteration => 0.160316016417\n",
      "Loss for  2846 th iteration => 0.160261575907\n",
      "Loss for  2847 th iteration => 0.160198541253\n",
      "Loss for  2848 th iteration => 0.16012267074\n",
      "Loss for  2849 th iteration => 0.160078583957\n",
      "Loss for  2850 th iteration => 0.160004060683\n",
      "Loss for  2851 th iteration => 0.159933471746\n",
      "Loss for  2852 th iteration => 0.159883944037\n",
      "Loss for  2853 th iteration => 0.159814184676\n",
      "Loss for  2854 th iteration => 0.159753057633\n",
      "Loss for  2855 th iteration => 0.159682038472\n",
      "Loss for  2856 th iteration => 0.159633274618\n",
      "Loss for  2857 th iteration => 0.159563463749\n",
      "Loss for  2858 th iteration => 0.159502854206\n",
      "Loss for  2859 th iteration => 0.159431293341\n",
      "Loss for  2860 th iteration => 0.159368537841\n",
      "Loss for  2861 th iteration => 0.159320880111\n",
      "Loss for  2862 th iteration => 0.159244257456\n",
      "Loss for  2863 th iteration => 0.159198913958\n",
      "Loss for  2864 th iteration => 0.159118669413\n",
      "Loss for  2865 th iteration => 0.159056084103\n",
      "Loss for  2866 th iteration => 0.15899354315\n",
      "Loss for  2867 th iteration => 0.158931140897\n",
      "Loss for  2868 th iteration => 0.158885298525\n",
      "Loss for  2869 th iteration => 0.158807158841\n",
      "Loss for  2870 th iteration => 0.158765202316\n",
      "Loss for  2871 th iteration => 0.158682402656\n",
      "Loss for  2872 th iteration => 0.158620115831\n",
      "Loss for  2873 th iteration => 0.1585578728\n",
      "Loss for  2874 th iteration => 0.158495673409\n",
      "Loss for  2875 th iteration => 0.158433517521\n",
      "Loss for  2876 th iteration => 0.158371441072\n",
      "Loss for  2877 th iteration => 0.158328382095\n",
      "Loss for  2878 th iteration => 0.158247908424\n",
      "Loss for  2879 th iteration => 0.158196156024\n",
      "Loss for  2880 th iteration => 0.158138169329\n",
      "Loss for  2881 th iteration => 0.158062364318\n",
      "Loss for  2882 th iteration => 0.158000501723\n",
      "Loss for  2883 th iteration => 0.1579386822\n",
      "Loss for  2884 th iteration => 0.157876905631\n",
      "Loss for  2885 th iteration => 0.157815171907\n",
      "Loss for  2886 th iteration => 0.157753480927\n",
      "Loss for  2887 th iteration => 0.157691832597\n",
      "Loss for  2888 th iteration => 0.157630532714\n",
      "Loss for  2889 th iteration => 0.157590087686\n",
      "Loss for  2890 th iteration => 0.157510715307\n",
      "Loss for  2891 th iteration => 0.157463935934\n",
      "Loss for  2892 th iteration => 0.157390858009\n",
      "Loss for  2893 th iteration => 0.157338178311\n",
      "Loss for  2894 th iteration => 0.157270984999\n",
      "Loss for  2895 th iteration => 0.157212863299\n",
      "Loss for  2896 th iteration => 0.157168673466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  2897 th iteration => 0.15710419231\n",
      "Loss for  2898 th iteration => 0.15704901825\n",
      "Loss for  2899 th iteration => 0.156979147509\n",
      "Loss for  2900 th iteration => 0.15692924534\n",
      "Loss for  2901 th iteration => 0.156854762143\n",
      "Loss for  2902 th iteration => 0.156809377548\n",
      "Loss for  2903 th iteration => 0.156732124821\n",
      "Loss for  2904 th iteration => 0.156668315233\n",
      "Loss for  2905 th iteration => 0.156610902039\n",
      "Loss for  2906 th iteration => 0.15654628094\n",
      "Loss for  2907 th iteration => 0.156509865286\n",
      "Loss for  2908 th iteration => 0.156427503458\n",
      "Loss for  2909 th iteration => 0.15638570463\n",
      "Loss for  2910 th iteration => 0.156310338108\n",
      "Loss for  2911 th iteration => 0.156245854107\n",
      "Loss for  2912 th iteration => 0.15617951172\n",
      "Loss for  2913 th iteration => 0.156121428253\n",
      "Loss for  2914 th iteration => 0.156061456262\n",
      "Loss for  2915 th iteration => 0.156015256949\n",
      "Loss for  2916 th iteration => 0.155959616278\n",
      "Loss for  2917 th iteration => 0.155889434952\n",
      "Loss for  2918 th iteration => 0.155842968756\n",
      "Loss for  2919 th iteration => 0.155765048715\n",
      "Loss for  2920 th iteration => 0.155706200419\n",
      "Loss for  2921 th iteration => 0.1556436676\n",
      "Loss for  2922 th iteration => 0.155586329575\n",
      "Loss for  2923 th iteration => 0.15552283336\n",
      "Loss for  2924 th iteration => 0.155485927673\n",
      "Loss for  2925 th iteration => 0.155405537306\n",
      "Loss for  2926 th iteration => 0.155362731074\n",
      "Loss for  2927 th iteration => 0.155290359656\n",
      "Loss for  2928 th iteration => 0.155224583663\n",
      "Loss for  2929 th iteration => 0.15516116681\n",
      "Loss for  2930 th iteration => 0.155101208365\n",
      "Loss for  2931 th iteration => 0.155045580794\n",
      "Loss for  2932 th iteration => 0.15500758706\n",
      "Loss for  2933 th iteration => 0.154936908299\n",
      "Loss for  2934 th iteration => 0.154876941612\n",
      "Loss for  2935 th iteration => 0.154826905645\n",
      "Loss for  2936 th iteration => 0.154751168332\n",
      "Loss for  2937 th iteration => 0.154694290094\n",
      "Loss for  2938 th iteration => 0.15464421207\n",
      "Loss for  2939 th iteration => 0.154585430592\n",
      "Loss for  2940 th iteration => 0.15451483114\n",
      "Loss for  2941 th iteration => 0.154475331258\n",
      "Loss for  2942 th iteration => 0.154390979908\n",
      "Loss for  2943 th iteration => 0.154331552627\n",
      "Loss for  2944 th iteration => 0.154268455618\n",
      "Loss for  2945 th iteration => 0.154216590811\n",
      "Loss for  2946 th iteration => 0.154175120759\n",
      "Loss for  2947 th iteration => 0.154109256401\n",
      "Loss for  2948 th iteration => 0.154047563294\n",
      "Loss for  2949 th iteration => 0.153980165288\n",
      "Loss for  2950 th iteration => 0.153922279873\n",
      "Loss for  2951 th iteration => 0.15386893\n",
      "Loss for  2952 th iteration => 0.153814851806\n",
      "Loss for  2953 th iteration => 0.153761398679\n",
      "Loss for  2954 th iteration => 0.153686659592\n",
      "Loss for  2955 th iteration => 0.153623356839\n",
      "Loss for  2956 th iteration => 0.153564382864\n",
      "Loss for  2957 th iteration => 0.15350928456\n",
      "Loss for  2958 th iteration => 0.153442712678\n",
      "Loss for  2959 th iteration => 0.153396173229\n",
      "Loss for  2960 th iteration => 0.15335062975\n",
      "Loss for  2961 th iteration => 0.153271566402\n",
      "Loss for  2962 th iteration => 0.15322481406\n",
      "Loss for  2963 th iteration => 0.153161224879\n",
      "Loss for  2964 th iteration => 0.153117657074\n",
      "Loss for  2965 th iteration => 0.153055827992\n",
      "Loss for  2966 th iteration => 0.152988277054\n",
      "Loss for  2967 th iteration => 0.152948987163\n",
      "Loss for  2968 th iteration => 0.15286438328\n",
      "Loss for  2969 th iteration => 0.152808952504\n",
      "Loss for  2970 th iteration => 0.152743292389\n",
      "Loss for  2971 th iteration => 0.152697100605\n",
      "Loss for  2972 th iteration => 0.1526511617\n",
      "Loss for  2973 th iteration => 0.152573953917\n",
      "Loss for  2974 th iteration => 0.152525885684\n",
      "Loss for  2975 th iteration => 0.152464929322\n",
      "Loss for  2976 th iteration => 0.152418567404\n",
      "Loss for  2977 th iteration => 0.152360669719\n",
      "Loss for  2978 th iteration => 0.15228972919\n",
      "Loss for  2979 th iteration => 0.152254934582\n",
      "Loss for  2980 th iteration => 0.152168462063\n",
      "Loss for  2981 th iteration => 0.152114960585\n",
      "Loss for  2982 th iteration => 0.152048909551\n",
      "Loss for  2983 th iteration => 0.152027687048\n",
      "Loss for  2984 th iteration => 0.151938858465\n",
      "Loss for  2985 th iteration => 0.151890319309\n",
      "Loss for  2986 th iteration => 0.151830526922\n",
      "Loss for  2987 th iteration => 0.151782981136\n",
      "Loss for  2988 th iteration => 0.151727348294\n",
      "Loss for  2989 th iteration => 0.151654908173\n",
      "Loss for  2990 th iteration => 0.151594725986\n",
      "Loss for  2991 th iteration => 0.151534426272\n",
      "Loss for  2992 th iteration => 0.15148366339\n",
      "Loss for  2993 th iteration => 0.151415194746\n",
      "Loss for  2994 th iteration => 0.15139790014\n",
      "Loss for  2995 th iteration => 0.151307979629\n",
      "Loss for  2996 th iteration => 0.15126013801\n",
      "Loss for  2997 th iteration => 0.151201843141\n",
      "Loss for  2998 th iteration => 0.151151073124\n",
      "Loss for  2999 th iteration => 0.151099640463\n",
      "Loss for  3000 th iteration => 0.151023706425\n",
      "Loss for  3001 th iteration => 0.150967645476\n",
      "Loss for  3002 th iteration => 0.150903937634\n",
      "Loss for  3003 th iteration => 0.150857416868\n",
      "Loss for  3004 th iteration => 0.150788759199\n",
      "Loss for  3005 th iteration => 0.150755821598\n",
      "Loss for  3006 th iteration => 0.150682930045\n",
      "Loss for  3007 th iteration => 0.150632493071\n",
      "Loss for  3008 th iteration => 0.1505893053\n",
      "Loss for  3009 th iteration => 0.150514247357\n",
      "Loss for  3010 th iteration => 0.150477587468\n",
      "Loss for  3011 th iteration => 0.150399697977\n",
      "Loss for  3012 th iteration => 0.150337275568\n",
      "Loss for  3013 th iteration => 0.150290346173\n",
      "Loss for  3014 th iteration => 0.150222460512\n",
      "Loss for  3015 th iteration => 0.150189501653\n",
      "Loss for  3016 th iteration => 0.150117536438\n",
      "Loss for  3017 th iteration => 0.150066639535\n",
      "Loss for  3018 th iteration => 0.150024476722\n",
      "Loss for  3019 th iteration => 0.149948948416\n",
      "Loss for  3020 th iteration => 0.149913471329\n",
      "Loss for  3021 th iteration => 0.149836166187\n",
      "Loss for  3022 th iteration => 0.149773178916\n",
      "Loss for  3023 th iteration => 0.149728246132\n",
      "Loss for  3024 th iteration => 0.149684153419\n",
      "Loss for  3025 th iteration => 0.14961089542\n",
      "Loss for  3026 th iteration => 0.149560964581\n",
      "Loss for  3027 th iteration => 0.149507920896\n",
      "Loss for  3028 th iteration => 0.149451942954\n",
      "Loss for  3029 th iteration => 0.149408517325\n",
      "Loss for  3030 th iteration => 0.14932942822\n",
      "Loss for  3031 th iteration => 0.149277101716\n",
      "Loss for  3032 th iteration => 0.14921196731\n",
      "Loss for  3033 th iteration => 0.149192531724\n",
      "Loss for  3034 th iteration => 0.149108108211\n",
      "Loss for  3035 th iteration => 0.149057457812\n",
      "Loss for  3036 th iteration => 0.149005703684\n",
      "Loss for  3037 th iteration => 0.148948459916\n",
      "Loss for  3038 th iteration => 0.148907102603\n",
      "Loss for  3039 th iteration => 0.148827713157\n",
      "Loss for  3040 th iteration => 0.148776126462\n",
      "Loss for  3041 th iteration => 0.148711918506\n",
      "Loss for  3042 th iteration => 0.148677175944\n",
      "Loss for  3043 th iteration => 0.148609370919\n",
      "Loss for  3044 th iteration => 0.148554819703\n",
      "Loss for  3045 th iteration => 0.148507150014\n",
      "Loss for  3046 th iteration => 0.148447300577\n",
      "Loss for  3047 th iteration => 0.148409283013\n",
      "Loss for  3048 th iteration => 0.148329942027\n",
      "Loss for  3049 th iteration => 0.148278374839\n",
      "Loss for  3050 th iteration => 0.148234859397\n",
      "Loss for  3051 th iteration => 0.148181908374\n",
      "Loss for  3052 th iteration => 0.148113220477\n",
      "Loss for  3053 th iteration => 0.148064474278\n",
      "Loss for  3054 th iteration => 0.1480058611\n",
      "Loss for  3055 th iteration => 0.147956624888\n",
      "Loss for  3056 th iteration => 0.147888725957\n",
      "Loss for  3057 th iteration => 0.147837739244\n",
      "Loss for  3058 th iteration => 0.147784555718\n",
      "Loss for  3059 th iteration => 0.147731603833\n",
      "Loss for  3060 th iteration => 0.14767156702\n",
      "Loss for  3061 th iteration => 0.147621397769\n",
      "Loss for  3062 th iteration => 0.147561890597\n",
      "Loss for  3063 th iteration => 0.147510842553\n",
      "Loss for  3064 th iteration => 0.147449990981\n",
      "Loss for  3065 th iteration => 0.147393788958\n",
      "Loss for  3066 th iteration => 0.147346901935\n",
      "Loss for  3067 th iteration => 0.147300069867\n",
      "Loss for  3068 th iteration => 0.147235592217\n",
      "Loss for  3069 th iteration => 0.147178689456\n",
      "Loss for  3070 th iteration => 0.14713546794\n",
      "Loss for  3071 th iteration => 0.147074533806\n",
      "Loss for  3072 th iteration => 0.147013305327\n",
      "Loss for  3073 th iteration => 0.146957765493\n",
      "Loss for  3074 th iteration => 0.146910980696\n",
      "Loss for  3075 th iteration => 0.146864271544\n",
      "Loss for  3076 th iteration => 0.146800441827\n",
      "Loss for  3077 th iteration => 0.14674316804\n",
      "Loss for  3078 th iteration => 0.146701231683\n",
      "Loss for  3079 th iteration => 0.146639629662\n",
      "Loss for  3080 th iteration => 0.146579250279\n",
      "Loss for  3081 th iteration => 0.146523677808\n",
      "Loss for  3082 th iteration => 0.146486866202\n",
      "Loss for  3083 th iteration => 0.146424712517\n",
      "Loss for  3084 th iteration => 0.146365370791\n",
      "Loss for  3085 th iteration => 0.146315979128\n",
      "Loss for  3086 th iteration => 0.146258284589\n",
      "Loss for  3087 th iteration => 0.146206240281\n",
      "Loss for  3088 th iteration => 0.146147771304\n",
      "Loss for  3089 th iteration => 0.146091511376\n",
      "Loss for  3090 th iteration => 0.146055716822\n",
      "Loss for  3091 th iteration => 0.145993354822\n",
      "Loss for  3092 th iteration => 0.145933937252\n",
      "Loss for  3093 th iteration => 0.145886050146\n",
      "Loss for  3094 th iteration => 0.145838260852\n",
      "Loss for  3095 th iteration => 0.145770819345\n",
      "Loss for  3096 th iteration => 0.14571575926\n",
      "Loss for  3097 th iteration => 0.145679365269\n",
      "Loss for  3098 th iteration => 0.145617917399\n",
      "Loss for  3099 th iteration => 0.145558813004\n",
      "Loss for  3100 th iteration => 0.145510495138\n",
      "Loss for  3101 th iteration => 0.145463449853\n",
      "Loss for  3102 th iteration => 0.145396091232\n",
      "Loss for  3103 th iteration => 0.145341448177\n",
      "Loss for  3104 th iteration => 0.145296170138\n",
      "Loss for  3105 th iteration => 0.145249052977\n",
      "Loss for  3106 th iteration => 0.14518804391\n",
      "Loss for  3107 th iteration => 0.14512925547\n",
      "Loss for  3108 th iteration => 0.145100994295\n",
      "Loss for  3109 th iteration => 0.145023099829\n",
      "Loss for  3110 th iteration => 0.144968760676\n",
      "Loss for  3111 th iteration => 0.144923659082\n",
      "Loss for  3112 th iteration => 0.144876412453\n",
      "Loss for  3113 th iteration => 0.144816213154\n",
      "Loss for  3114 th iteration => 0.144756973399\n",
      "Loss for  3115 th iteration => 0.144729286898\n",
      "Loss for  3116 th iteration => 0.144651983321\n",
      "Loss for  3117 th iteration => 0.144597359237\n",
      "Loss for  3118 th iteration => 0.144561208703\n",
      "Loss for  3119 th iteration => 0.144500964872\n",
      "Loss for  3120 th iteration => 0.144442122503\n",
      "Loss for  3121 th iteration => 0.144395141275\n",
      "Loss for  3122 th iteration => 0.144347879685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  3123 th iteration => 0.144282851018\n",
      "Loss for  3124 th iteration => 0.144227425181\n",
      "Loss for  3125 th iteration => 0.144191991826\n",
      "Loss for  3126 th iteration => 0.144131838644\n",
      "Loss for  3127 th iteration => 0.144072799941\n",
      "Loss for  3128 th iteration => 0.144027086094\n",
      "Loss for  3129 th iteration => 0.143978836912\n",
      "Loss for  3130 th iteration => 0.143915348867\n",
      "Loss for  3131 th iteration => 0.143859072546\n",
      "Loss for  3132 th iteration => 0.143824387535\n",
      "Loss for  3133 th iteration => 0.14376429394\n",
      "Loss for  3134 th iteration => 0.143705055541\n",
      "Loss for  3135 th iteration => 0.143660638254\n",
      "Loss for  3136 th iteration => 0.143611376396\n",
      "Loss for  3137 th iteration => 0.143549450328\n",
      "Loss for  3138 th iteration => 0.14349230195\n",
      "Loss for  3139 th iteration => 0.143458378337\n",
      "Loss for  3140 th iteration => 0.143398324217\n",
      "Loss for  3141 th iteration => 0.143339823626\n",
      "Loss for  3142 th iteration => 0.143292331861\n",
      "Loss for  3143 th iteration => 0.143245309445\n",
      "Loss for  3144 th iteration => 0.143178993527\n",
      "Loss for  3145 th iteration => 0.143139270089\n",
      "Loss for  3146 th iteration => 0.143088782588\n",
      "Loss for  3147 th iteration => 0.143033285657\n",
      "Loss for  3148 th iteration => 0.142972857979\n",
      "Loss for  3149 th iteration => 0.142933660559\n",
      "Loss for  3150 th iteration => 0.142873631168\n",
      "Loss for  3151 th iteration => 0.142815168261\n",
      "Loss for  3152 th iteration => 0.142771024351\n",
      "Loss for  3153 th iteration => 0.142722554462\n",
      "Loss for  3154 th iteration => 0.142667096331\n",
      "Loss for  3155 th iteration => 0.142617679088\n",
      "Loss for  3156 th iteration => 0.142562283114\n",
      "Loss for  3157 th iteration => 0.142504757591\n",
      "Loss for  3158 th iteration => 0.142455058665\n",
      "Loss for  3159 th iteration => 0.142415556725\n",
      "Loss for  3160 th iteration => 0.142361494956\n",
      "Loss for  3161 th iteration => 0.142310069183\n",
      "Loss for  3162 th iteration => 0.142264467775\n",
      "Loss for  3163 th iteration => 0.142196914518\n",
      "Loss for  3164 th iteration => 0.142156925347\n",
      "Loss for  3165 th iteration => 0.14210746457\n",
      "Loss for  3166 th iteration => 0.142052913678\n",
      "Loss for  3167 th iteration => 0.142000658083\n",
      "Loss for  3168 th iteration => 0.141953769441\n",
      "Loss for  3169 th iteration => 0.141888177322\n",
      "Loss for  3170 th iteration => 0.141838362726\n",
      "Loss for  3171 th iteration => 0.141799464006\n",
      "Loss for  3172 th iteration => 0.141745610801\n",
      "Loss for  3173 th iteration => 0.141685495968\n",
      "Loss for  3174 th iteration => 0.141650216387\n",
      "Loss for  3175 th iteration => 0.141587772898\n",
      "Loss for  3176 th iteration => 0.141531973975\n",
      "Loss for  3177 th iteration => 0.141492645497\n",
      "Loss for  3178 th iteration => 0.141439670413\n",
      "Loss for  3179 th iteration => 0.141378776133\n",
      "Loss for  3180 th iteration => 0.141334347174\n",
      "Loss for  3181 th iteration => 0.141281918845\n",
      "Loss for  3182 th iteration => 0.141226523387\n",
      "Loss for  3183 th iteration => 0.141187500786\n",
      "Loss for  3184 th iteration => 0.141134661218\n",
      "Loss for  3185 th iteration => 0.141073890644\n",
      "Loss for  3186 th iteration => 0.141029516753\n",
      "Loss for  3187 th iteration => 0.140977414621\n",
      "Loss for  3188 th iteration => 0.14092256755\n",
      "Loss for  3189 th iteration => 0.140882886371\n",
      "Loss for  3190 th iteration => 0.140831104068\n",
      "Loss for  3191 th iteration => 0.140770279224\n",
      "Loss for  3192 th iteration => 0.140725485845\n",
      "Loss for  3193 th iteration => 0.140674547224\n",
      "Loss for  3194 th iteration => 0.140627895374\n",
      "Loss for  3195 th iteration => 0.140577024568\n",
      "Loss for  3196 th iteration => 0.140522265601\n",
      "Loss for  3197 th iteration => 0.14048660254\n",
      "Loss for  3198 th iteration => 0.140420846183\n",
      "Loss for  3199 th iteration => 0.140367316419\n",
      "Loss for  3200 th iteration => 0.14034039341\n",
      "Loss for  3201 th iteration => 0.140275580914\n",
      "Loss for  3202 th iteration => 0.140217217129\n",
      "Loss for  3203 th iteration => 0.140179745741\n",
      "Loss for  3204 th iteration => 0.140120912611\n",
      "Loss for  3205 th iteration => 0.14007486308\n",
      "Loss for  3206 th iteration => 0.140024418397\n",
      "Loss for  3207 th iteration => 0.139969543578\n",
      "Loss for  3208 th iteration => 0.139934408647\n",
      "Loss for  3209 th iteration => 0.139869513042\n",
      "Loss for  3210 th iteration => 0.139815824646\n",
      "Loss for  3211 th iteration => 0.139771994265\n",
      "Loss for  3212 th iteration => 0.139725493097\n",
      "Loss for  3213 th iteration => 0.139671129312\n",
      "Loss for  3214 th iteration => 0.139623428222\n",
      "Loss for  3215 th iteration => 0.139571655898\n",
      "Loss for  3216 th iteration => 0.139523887478\n",
      "Loss for  3217 th iteration => 0.139476871559\n",
      "Loss for  3218 th iteration => 0.139420639297\n",
      "Loss for  3219 th iteration => 0.139368178068\n",
      "Loss for  3220 th iteration => 0.139322527882\n",
      "Loss for  3221 th iteration => 0.139276121911\n",
      "Loss for  3222 th iteration => 0.139230886671\n",
      "Loss for  3223 th iteration => 0.13917897578\n",
      "Loss for  3224 th iteration => 0.139119473941\n",
      "Loss for  3225 th iteration => 0.139076454685\n",
      "Loss for  3226 th iteration => 0.139026726354\n",
      "Loss for  3227 th iteration => 0.138977904293\n",
      "Loss for  3228 th iteration => 0.138931587261\n",
      "Loss for  3229 th iteration => 0.138876613122\n",
      "Loss for  3230 th iteration => 0.138830802774\n",
      "Loss for  3231 th iteration => 0.138778580806\n",
      "Loss for  3232 th iteration => 0.138731659001\n",
      "Loss for  3233 th iteration => 0.138685216174\n",
      "Loss for  3234 th iteration => 0.138629405328\n",
      "Loss for  3235 th iteration => 0.138585249599\n",
      "Loss for  3236 th iteration => 0.138531814509\n",
      "Loss for  3237 th iteration => 0.138485892988\n",
      "Loss for  3238 th iteration => 0.138439508354\n",
      "Loss for  3239 th iteration => 0.138383399996\n",
      "Loss for  3240 th iteration => 0.138331972205\n",
      "Loss for  3241 th iteration => 0.138286443318\n",
      "Loss for  3242 th iteration => 0.138241709512\n",
      "Loss for  3243 th iteration => 0.138195553851\n",
      "Loss for  3244 th iteration => 0.138145612114\n",
      "Loss for  3245 th iteration => 0.138086830967\n",
      "Loss for  3246 th iteration => 0.138042577468\n",
      "Loss for  3247 th iteration => 0.137996263773\n",
      "Loss for  3248 th iteration => 0.137951907395\n",
      "Loss for  3249 th iteration => 0.137901060393\n",
      "Loss for  3250 th iteration => 0.137842618265\n",
      "Loss for  3251 th iteration => 0.137799008779\n",
      "Loss for  3252 th iteration => 0.137752194499\n",
      "Loss for  3253 th iteration => 0.137702153297\n",
      "Loss for  3254 th iteration => 0.137657261861\n",
      "Loss for  3255 th iteration => 0.137604613087\n",
      "Loss for  3256 th iteration => 0.137556247097\n",
      "Loss for  3257 th iteration => 0.137508823603\n",
      "Loss for  3258 th iteration => 0.137459360007\n",
      "Loss for  3259 th iteration => 0.137414597023\n",
      "Loss for  3260 th iteration => 0.137361883531\n",
      "Loss for  3261 th iteration => 0.137313858235\n",
      "Loss for  3262 th iteration => 0.13726651878\n",
      "Loss for  3263 th iteration => 0.137217147258\n",
      "Loss for  3264 th iteration => 0.137172610682\n",
      "Loss for  3265 th iteration => 0.137120114538\n",
      "Loss for  3266 th iteration => 0.13707194567\n",
      "Loss for  3267 th iteration => 0.137025173538\n",
      "Loss for  3268 th iteration => 0.136975556125\n",
      "Loss for  3269 th iteration => 0.136931312913\n",
      "Loss for  3270 th iteration => 0.136879231891\n",
      "Loss for  3271 th iteration => 0.136830582539\n",
      "Loss for  3272 th iteration => 0.1367847129\n",
      "Loss for  3273 th iteration => 0.136734615454\n",
      "Loss for  3274 th iteration => 0.13669071066\n",
      "Loss for  3275 th iteration => 0.136639183053\n",
      "Loss for  3276 th iteration => 0.136589820018\n",
      "Loss for  3277 th iteration => 0.136545083681\n",
      "Loss for  3278 th iteration => 0.136494345083\n",
      "Loss for  3279 th iteration => 0.136450808326\n",
      "Loss for  3280 th iteration => 0.136399930483\n",
      "Loss for  3281 th iteration => 0.136349693545\n",
      "Loss for  3282 th iteration => 0.136306414058\n",
      "Loss for  3283 th iteration => 0.136260113211\n",
      "Loss for  3284 th iteration => 0.136213347713\n",
      "Loss for  3285 th iteration => 0.136162463769\n",
      "Loss for  3286 th iteration => 0.136111901852\n",
      "Loss for  3287 th iteration => 0.136065451013\n",
      "Loss for  3288 th iteration => 0.136021394538\n",
      "Loss for  3289 th iteration => 0.135968560075\n",
      "Loss for  3290 th iteration => 0.135922300182\n",
      "Loss for  3291 th iteration => 0.13587550408\n",
      "Loss for  3292 th iteration => 0.13582657992\n",
      "Loss for  3293 th iteration => 0.135783273231\n",
      "Loss for  3294 th iteration => 0.135732155585\n",
      "Loss for  3295 th iteration => 0.135683189365\n",
      "Loss for  3296 th iteration => 0.135639526003\n",
      "Loss for  3297 th iteration => 0.135588568046\n",
      "Loss for  3298 th iteration => 0.135546004329\n",
      "Loss for  3299 th iteration => 0.135502152742\n",
      "Loss for  3300 th iteration => 0.135446883104\n",
      "Loss for  3301 th iteration => 0.135400779934\n",
      "Loss for  3302 th iteration => 0.135357434182\n",
      "Loss for  3303 th iteration => 0.135304952313\n",
      "Loss for  3304 th iteration => 0.135258658159\n",
      "Loss for  3305 th iteration => 0.135213048188\n",
      "Loss for  3306 th iteration => 0.135163584454\n",
      "Loss for  3307 th iteration => 0.135121165944\n",
      "Loss for  3308 th iteration => 0.135070929877\n",
      "Loss for  3309 th iteration => 0.135021038069\n",
      "Loss for  3310 th iteration => 0.134979452373\n",
      "Loss for  3311 th iteration => 0.134927337843\n",
      "Loss for  3312 th iteration => 0.134886338367\n",
      "Loss for  3313 th iteration => 0.134841396085\n",
      "Loss for  3314 th iteration => 0.134788662107\n",
      "Loss for  3315 th iteration => 0.13474100992\n",
      "Loss for  3316 th iteration => 0.134698729328\n",
      "Loss for  3317 th iteration => 0.134647750668\n",
      "Loss for  3318 th iteration => 0.134599505188\n",
      "Loss for  3319 th iteration => 0.134556979606\n",
      "Loss for  3320 th iteration => 0.13450566939\n",
      "Loss for  3321 th iteration => 0.134464721475\n",
      "Loss for  3322 th iteration => 0.134420010912\n",
      "Loss for  3323 th iteration => 0.13436761343\n",
      "Loss for  3324 th iteration => 0.134320157122\n",
      "Loss for  3325 th iteration => 0.134278348483\n",
      "Loss for  3326 th iteration => 0.134227571869\n",
      "Loss for  3327 th iteration => 0.134179356048\n",
      "Loss for  3328 th iteration => 0.134137514619\n",
      "Loss for  3329 th iteration => 0.134085878065\n",
      "Loss for  3330 th iteration => 0.134045641042\n",
      "Loss for  3331 th iteration => 0.134000518937\n",
      "Loss for  3332 th iteration => 0.133949369047\n",
      "Loss for  3333 th iteration => 0.133901260955\n",
      "Loss for  3334 th iteration => 0.133860086263\n",
      "Loss for  3335 th iteration => 0.133810024357\n",
      "Loss for  3336 th iteration => 0.133760980977\n",
      "Loss for  3337 th iteration => 0.133720676194\n",
      "Loss for  3338 th iteration => 0.133668126372\n",
      "Loss for  3339 th iteration => 0.133628929523\n",
      "Loss for  3340 th iteration => 0.133583077615\n",
      "Loss for  3341 th iteration => 0.133533640115\n",
      "Loss for  3342 th iteration => 0.133484445278\n",
      "Loss for  3343 th iteration => 0.133443973991\n",
      "Loss for  3344 th iteration => 0.133394905535\n",
      "Loss for  3345 th iteration => 0.133344586328\n",
      "Loss for  3346 th iteration => 0.133306254773\n",
      "Loss for  3347 th iteration => 0.133252496694\n",
      "Loss for  3348 th iteration => 0.133214492149\n",
      "Loss for  3349 th iteration => 0.133167765952\n",
      "Loss for  3350 th iteration => 0.133120265757\n",
      "Loss for  3351 th iteration => 0.133069770648\n",
      "Loss for  3352 th iteration => 0.133030023023\n",
      "Loss for  3353 th iteration => 0.132982100096\n",
      "Loss for  3354 th iteration => 0.132930344259\n",
      "Loss for  3355 th iteration => 0.132887607792\n",
      "Loss for  3356 th iteration => 0.132845194665\n",
      "Loss for  3357 th iteration => 0.132793493333\n",
      "Loss for  3358 th iteration => 0.132750698028\n",
      "Loss for  3359 th iteration => 0.132709545982\n",
      "Loss for  3360 th iteration => 0.132656646152\n",
      "Loss for  3361 th iteration => 0.132612751857\n",
      "Loss for  3362 th iteration => 0.132573461865\n",
      "Loss for  3363 th iteration => 0.132525206177\n",
      "Loss for  3364 th iteration => 0.132475995412\n",
      "Loss for  3365 th iteration => 0.13243649233\n",
      "Loss for  3366 th iteration => 0.132388490162\n",
      "Loss for  3367 th iteration => 0.132337449676\n",
      "Loss for  3368 th iteration => 0.132302735682\n",
      "Loss for  3369 th iteration => 0.132252381325\n",
      "Loss for  3370 th iteration => 0.132202731852\n",
      "Loss for  3371 th iteration => 0.132162608514\n",
      "Loss for  3372 th iteration => 0.132118646069\n",
      "Loss for  3373 th iteration => 0.132066280969\n",
      "Loss for  3374 th iteration => 0.132021855955\n",
      "Loss for  3375 th iteration => 0.131981986474\n",
      "Loss for  3376 th iteration => 0.131935897203\n",
      "Loss for  3377 th iteration => 0.131887294722\n",
      "Loss for  3378 th iteration => 0.131846488131\n",
      "Loss for  3379 th iteration => 0.131801656874\n",
      "Loss for  3380 th iteration => 0.131754514302\n",
      "Loss for  3381 th iteration => 0.131707074756\n",
      "Loss for  3382 th iteration => 0.131665765759\n",
      "Loss for  3383 th iteration => 0.131621563013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  3384 th iteration => 0.131573986054\n",
      "Loss for  3385 th iteration => 0.131524837225\n",
      "Loss for  3386 th iteration => 0.131480476696\n",
      "Loss for  3387 th iteration => 0.131436491961\n",
      "Loss for  3388 th iteration => 0.131389966221\n",
      "Loss for  3389 th iteration => 0.131344581441\n",
      "Loss for  3390 th iteration => 0.131301126767\n",
      "Loss for  3391 th iteration => 0.131262782654\n",
      "Loss for  3392 th iteration => 0.131216066321\n",
      "Loss for  3393 th iteration => 0.131172282879\n",
      "Loss for  3394 th iteration => 0.131122284586\n",
      "Loss for  3395 th iteration => 0.131083406907\n",
      "Loss for  3396 th iteration => 0.131037350813\n",
      "Loss for  3397 th iteration => 0.130992970207\n",
      "Loss for  3398 th iteration => 0.13094371173\n",
      "Loss for  3399 th iteration => 0.13090455763\n",
      "Loss for  3400 th iteration => 0.130858912507\n",
      "Loss for  3401 th iteration => 0.130814280045\n",
      "Loss for  3402 th iteration => 0.130765441468\n",
      "Loss for  3403 th iteration => 0.130726205417\n",
      "Loss for  3404 th iteration => 0.130680783216\n",
      "Loss for  3405 th iteration => 0.130636161024\n",
      "Loss for  3406 th iteration => 0.130587499667\n",
      "Loss for  3407 th iteration => 0.130548328058\n",
      "Loss for  3408 th iteration => 0.130502987514\n",
      "Loss for  3409 th iteration => 0.130458573976\n",
      "Loss for  3410 th iteration => 0.130409953743\n",
      "Loss for  3411 th iteration => 0.130371078647\n",
      "Loss for  3412 th iteration => 0.130322638938\n",
      "Loss for  3413 th iteration => 0.130281669614\n",
      "Loss for  3414 th iteration => 0.130232780975\n",
      "Loss for  3415 th iteration => 0.130193690819\n",
      "Loss for  3416 th iteration => 0.130146027734\n",
      "Loss for  3417 th iteration => 0.130099878436\n",
      "Loss for  3418 th iteration => 0.13005623114\n",
      "Loss for  3419 th iteration => 0.130011310724\n",
      "Loss for  3420 th iteration => 0.12996961021\n",
      "Loss for  3421 th iteration => 0.129923288029\n",
      "Loss for  3422 th iteration => 0.129880308837\n",
      "Loss for  3423 th iteration => 0.129840153252\n",
      "Loss for  3424 th iteration => 0.129793442343\n",
      "Loss for  3425 th iteration => 0.129752566759\n",
      "Loss for  3426 th iteration => 0.129704588796\n",
      "Loss for  3427 th iteration => 0.129659742943\n",
      "Loss for  3428 th iteration => 0.129617074928\n",
      "Loss for  3429 th iteration => 0.129572201396\n",
      "Loss for  3430 th iteration => 0.129529195176\n",
      "Loss for  3431 th iteration => 0.129484382609\n",
      "Loss for  3432 th iteration => 0.129441820102\n",
      "Loss for  3433 th iteration => 0.12939705796\n",
      "Loss for  3434 th iteration => 0.12935440996\n",
      "Loss for  3435 th iteration => 0.129314772648\n",
      "Loss for  3436 th iteration => 0.129267035355\n",
      "Loss for  3437 th iteration => 0.129228086379\n",
      "Loss for  3438 th iteration => 0.12917944625\n",
      "Loss for  3439 th iteration => 0.129135415704\n",
      "Loss for  3440 th iteration => 0.129092122887\n",
      "Loss for  3441 th iteration => 0.129048769842\n",
      "Loss for  3442 th iteration => 0.129012335113\n",
      "Loss for  3443 th iteration => 0.128961111551\n",
      "Loss for  3444 th iteration => 0.128918990683\n",
      "Loss for  3445 th iteration => 0.12887452205\n",
      "Loss for  3446 th iteration => 0.128831173134\n",
      "Loss for  3447 th iteration => 0.128787432635\n",
      "Loss for  3448 th iteration => 0.128745490802\n",
      "Loss for  3449 th iteration => 0.12870109603\n",
      "Loss for  3450 th iteration => 0.128657606826\n",
      "Loss for  3451 th iteration => 0.128620447222\n",
      "Loss for  3452 th iteration => 0.128572103937\n",
      "Loss for  3453 th iteration => 0.128528873553\n",
      "Loss for  3454 th iteration => 0.128484354641\n",
      "Loss for  3455 th iteration => 0.128441141383\n",
      "Loss for  3456 th iteration => 0.128402499285\n",
      "Loss for  3457 th iteration => 0.128361882759\n",
      "Loss for  3458 th iteration => 0.128311305171\n",
      "Loss for  3459 th iteration => 0.128270010476\n",
      "Loss for  3460 th iteration => 0.12822759952\n",
      "Loss for  3461 th iteration => 0.128197877345\n",
      "Loss for  3462 th iteration => 0.128139499363\n",
      "Loss for  3463 th iteration => 0.128100204526\n",
      "Loss for  3464 th iteration => 0.128058536563\n",
      "Loss for  3465 th iteration => 0.128011611948\n",
      "Loss for  3466 th iteration => 0.127968410572\n",
      "Loss for  3467 th iteration => 0.127926974028\n",
      "Loss for  3468 th iteration => 0.1278876327\n",
      "Loss for  3469 th iteration => 0.1278403429\n",
      "Loss for  3470 th iteration => 0.127799040195\n",
      "Loss for  3471 th iteration => 0.127759771214\n",
      "Loss for  3472 th iteration => 0.127710646708\n",
      "Loss for  3473 th iteration => 0.127672184604\n",
      "Loss for  3474 th iteration => 0.127631479343\n",
      "Loss for  3475 th iteration => 0.127587018327\n",
      "Loss for  3476 th iteration => 0.127546904892\n",
      "Loss for  3477 th iteration => 0.127498872593\n",
      "Loss for  3478 th iteration => 0.127463429223\n",
      "Loss for  3479 th iteration => 0.127413559054\n",
      "Loss for  3480 th iteration => 0.127378872298\n",
      "Loss for  3481 th iteration => 0.127328122399\n",
      "Loss for  3482 th iteration => 0.127287189485\n",
      "Loss for  3483 th iteration => 0.12724271708\n",
      "Loss for  3484 th iteration => 0.127202046943\n",
      "Loss for  3485 th iteration => 0.127164787089\n",
      "Loss for  3486 th iteration => 0.127118962075\n",
      "Loss for  3487 th iteration => 0.12708016902\n",
      "Loss for  3488 th iteration => 0.127030537092\n",
      "Loss for  3489 th iteration => 0.126992198886\n",
      "Loss for  3490 th iteration => 0.126950758825\n",
      "Loss for  3491 th iteration => 0.126904707011\n",
      "Loss for  3492 th iteration => 0.126862028859\n",
      "Loss for  3493 th iteration => 0.126821357112\n",
      "Loss for  3494 th iteration => 0.126777235402\n",
      "Loss for  3495 th iteration => 0.126736865807\n",
      "Loss for  3496 th iteration => 0.126699940186\n",
      "Loss for  3497 th iteration => 0.126654145323\n",
      "Loss for  3498 th iteration => 0.126615940387\n",
      "Loss for  3499 th iteration => 0.126566368636\n",
      "Loss for  3500 th iteration => 0.126528274624\n",
      "Loss for  3501 th iteration => 0.126487085383\n",
      "Loss for  3502 th iteration => 0.12644144482\n",
      "Loss for  3503 th iteration => 0.126398929166\n",
      "Loss for  3504 th iteration => 0.126358661775\n",
      "Loss for  3505 th iteration => 0.126314824544\n",
      "Loss for  3506 th iteration => 0.12627411039\n",
      "Loss for  3507 th iteration => 0.126239915349\n",
      "Loss for  3508 th iteration => 0.126195033649\n",
      "Loss for  3509 th iteration => 0.126154948653\n",
      "Loss for  3510 th iteration => 0.126105756161\n",
      "Loss for  3511 th iteration => 0.126065567278\n",
      "Loss for  3512 th iteration => 0.126027471073\n",
      "Loss for  3513 th iteration => 0.125982565804\n",
      "Loss for  3514 th iteration => 0.125945446747\n",
      "Loss for  3515 th iteration => 0.125903738615\n",
      "Loss for  3516 th iteration => 0.125861955674\n",
      "Loss for  3517 th iteration => 0.125814376136\n",
      "Loss for  3518 th iteration => 0.125773495678\n",
      "Loss for  3519 th iteration => 0.125730135149\n",
      "Loss for  3520 th iteration => 0.125691057103\n",
      "Loss for  3521 th iteration => 0.125654015385\n",
      "Loss for  3522 th iteration => 0.125612436771\n",
      "Loss for  3523 th iteration => 0.125570897906\n",
      "Loss for  3524 th iteration => 0.12552355846\n",
      "Loss for  3525 th iteration => 0.125482919435\n",
      "Loss for  3526 th iteration => 0.125439738048\n",
      "Loss for  3527 th iteration => 0.125400489757\n",
      "Loss for  3528 th iteration => 0.125363841893\n",
      "Loss for  3529 th iteration => 0.125322094236\n",
      "Loss for  3530 th iteration => 0.125281234523\n",
      "Loss for  3531 th iteration => 0.125233909585\n",
      "Loss for  3532 th iteration => 0.125193550121\n",
      "Loss for  3533 th iteration => 0.125150530287\n",
      "Loss for  3534 th iteration => 0.125110942289\n",
      "Loss for  3535 th iteration => 0.125074877579\n",
      "Loss for  3536 th iteration => 0.125032783037\n",
      "Loss for  3537 th iteration => 0.124992866981\n",
      "Loss for  3538 th iteration => 0.124945420351\n",
      "Loss for  3539 th iteration => 0.124905360509\n",
      "Loss for  3540 th iteration => 0.124862491369\n",
      "Loss for  3541 th iteration => 0.124822462539\n",
      "Loss for  3542 th iteration => 0.124787090395\n",
      "Loss for  3543 th iteration => 0.12474454673\n",
      "Loss for  3544 th iteration => 0.12470573163\n",
      "Loss for  3545 th iteration => 0.124658083142\n",
      "Loss for  3546 th iteration => 0.124618331678\n",
      "Loss for  3547 th iteration => 0.124575606548\n",
      "Loss for  3548 th iteration => 0.124535245426\n",
      "Loss for  3549 th iteration => 0.124495006778\n",
      "Loss for  3550 th iteration => 0.124453569929\n",
      "Loss for  3551 th iteration => 0.124412608186\n",
      "Loss for  3552 th iteration => 0.124371784167\n",
      "Loss for  3553 th iteration => 0.124332251616\n",
      "Loss for  3554 th iteration => 0.124289731439\n",
      "Loss for  3555 th iteration => 0.124249456107\n",
      "Loss for  3556 th iteration => 0.124209467737\n",
      "Loss for  3557 th iteration => 0.124167836558\n",
      "Loss for  3558 th iteration => 0.124127400738\n",
      "Loss for  3559 th iteration => 0.124086645636\n",
      "Loss for  3560 th iteration => 0.124048955081\n",
      "Loss for  3561 th iteration => 0.12401084301\n",
      "Loss for  3562 th iteration => 0.123964535259\n",
      "Loss for  3563 th iteration => 0.123925948122\n",
      "Loss for  3564 th iteration => 0.123882905833\n",
      "Loss for  3565 th iteration => 0.123843127807\n",
      "Loss for  3566 th iteration => 0.123805249934\n",
      "Loss for  3567 th iteration => 0.123766458718\n",
      "Loss for  3568 th iteration => 0.123721154986\n",
      "Loss for  3569 th iteration => 0.123681621606\n",
      "Loss for  3570 th iteration => 0.123642544496\n",
      "Loss for  3571 th iteration => 0.123599587237\n",
      "Loss for  3572 th iteration => 0.123562024259\n",
      "Loss for  3573 th iteration => 0.123526699566\n",
      "Loss for  3574 th iteration => 0.12347914351\n",
      "Loss for  3575 th iteration => 0.123439583548\n",
      "Loss for  3576 th iteration => 0.12340066205\n",
      "Loss for  3577 th iteration => 0.123364394493\n",
      "Loss for  3578 th iteration => 0.123324564351\n",
      "Loss for  3579 th iteration => 0.123278661051\n",
      "Loss for  3580 th iteration => 0.123239834631\n",
      "Loss for  3581 th iteration => 0.123197191648\n",
      "Loss for  3582 th iteration => 0.123159468391\n",
      "Loss for  3583 th iteration => 0.1231248344\n",
      "Loss for  3584 th iteration => 0.123077342086\n",
      "Loss for  3585 th iteration => 0.123038956163\n",
      "Loss for  3586 th iteration => 0.123004313654\n",
      "Loss for  3587 th iteration => 0.122962551714\n",
      "Loss for  3588 th iteration => 0.122924714307\n",
      "Loss for  3589 th iteration => 0.122877727109\n",
      "Loss for  3590 th iteration => 0.122840282501\n",
      "Loss for  3591 th iteration => 0.12280361524\n",
      "Loss for  3592 th iteration => 0.12275769729\n",
      "Loss for  3593 th iteration => 0.122719975046\n",
      "Loss for  3594 th iteration => 0.122677661409\n",
      "Loss for  3595 th iteration => 0.122638771117\n",
      "Loss for  3596 th iteration => 0.122606187234\n",
      "Loss for  3597 th iteration => 0.122562801171\n",
      "Loss for  3598 th iteration => 0.122519868422\n",
      "Loss for  3599 th iteration => 0.122485592778\n",
      "Loss for  3600 th iteration => 0.122440496229\n",
      "Loss for  3601 th iteration => 0.122407140366\n",
      "Loss for  3602 th iteration => 0.122359780991\n",
      "Loss for  3603 th iteration => 0.122322414917\n",
      "Loss for  3604 th iteration => 0.122287116808\n",
      "Loss for  3605 th iteration => 0.122240563624\n",
      "Loss for  3606 th iteration => 0.122204004527\n",
      "Loss for  3607 th iteration => 0.122161988747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  3608 th iteration => 0.122122151655\n",
      "Loss for  3609 th iteration => 0.122085131428\n",
      "Loss for  3610 th iteration => 0.122043651854\n",
      "Loss for  3611 th iteration => 0.122004449811\n",
      "Loss for  3612 th iteration => 0.121965332642\n",
      "Loss for  3613 th iteration => 0.121926310756\n",
      "Loss for  3614 th iteration => 0.121885940957\n",
      "Loss for  3615 th iteration => 0.121846062142\n",
      "Loss for  3616 th iteration => 0.121808969694\n",
      "Loss for  3617 th iteration => 0.121767698155\n",
      "Loss for  3618 th iteration => 0.121727640579\n",
      "Loss for  3619 th iteration => 0.121691876501\n",
      "Loss for  3620 th iteration => 0.121654171643\n",
      "Loss for  3621 th iteration => 0.121610754195\n",
      "Loss for  3622 th iteration => 0.121579054069\n",
      "Loss for  3623 th iteration => 0.121536512993\n",
      "Loss for  3624 th iteration => 0.121493196781\n",
      "Loss for  3625 th iteration => 0.121460353698\n",
      "Loss for  3626 th iteration => 0.121415131098\n",
      "Loss for  3627 th iteration => 0.121376016706\n",
      "Loss for  3628 th iteration => 0.121336852491\n",
      "Loss for  3629 th iteration => 0.121298337781\n",
      "Loss for  3630 th iteration => 0.121265653607\n",
      "Loss for  3631 th iteration => 0.121219222555\n",
      "Loss for  3632 th iteration => 0.121181741085\n",
      "Loss for  3633 th iteration => 0.121141780196\n",
      "Loss for  3634 th iteration => 0.12110184346\n",
      "Loss for  3635 th iteration => 0.121065179164\n",
      "Loss for  3636 th iteration => 0.121024790823\n",
      "Loss for  3637 th iteration => 0.120985129213\n",
      "Loss for  3638 th iteration => 0.120948283616\n",
      "Loss for  3639 th iteration => 0.120908048374\n",
      "Loss for  3640 th iteration => 0.120868611998\n",
      "Loss for  3641 th iteration => 0.120832239438\n",
      "Loss for  3642 th iteration => 0.120795689705\n",
      "Loss for  3643 th iteration => 0.120752906431\n",
      "Loss for  3644 th iteration => 0.120720576832\n",
      "Loss for  3645 th iteration => 0.120675661096\n",
      "Loss for  3646 th iteration => 0.120636633443\n",
      "Loss for  3647 th iteration => 0.12059855293\n",
      "Loss for  3648 th iteration => 0.120559518881\n",
      "Loss for  3649 th iteration => 0.120521089102\n",
      "Loss for  3650 th iteration => 0.12048220173\n",
      "Loss for  3651 th iteration => 0.120444381558\n",
      "Loss for  3652 th iteration => 0.120411985488\n",
      "Loss for  3653 th iteration => 0.120366320235\n",
      "Loss for  3654 th iteration => 0.120328782203\n",
      "Loss for  3655 th iteration => 0.120289547073\n",
      "Loss for  3656 th iteration => 0.120250293709\n",
      "Loss for  3657 th iteration => 0.120213981425\n",
      "Loss for  3658 th iteration => 0.12018016322\n",
      "Loss for  3659 th iteration => 0.120135044149\n",
      "Loss for  3660 th iteration => 0.12009871071\n",
      "Loss for  3661 th iteration => 0.120058503985\n",
      "Loss for  3662 th iteration => 0.120019611086\n",
      "Loss for  3663 th iteration => 0.119983871836\n",
      "Loss for  3664 th iteration => 0.11994345473\n",
      "Loss for  3665 th iteration => 0.119904470928\n",
      "Loss for  3666 th iteration => 0.119869303853\n",
      "Loss for  3667 th iteration => 0.11983363714\n",
      "Loss for  3668 th iteration => 0.119790196736\n",
      "Loss for  3669 th iteration => 0.119754311267\n",
      "Loss for  3670 th iteration => 0.119713696744\n",
      "Loss for  3671 th iteration => 0.11967566337\n",
      "Loss for  3672 th iteration => 0.11963972828\n",
      "Loss for  3673 th iteration => 0.119599223405\n",
      "Loss for  3674 th iteration => 0.119561353432\n",
      "Loss for  3675 th iteration => 0.119531437474\n",
      "Loss for  3676 th iteration => 0.11948558552\n",
      "Loss for  3677 th iteration => 0.119446895286\n",
      "Loss for  3678 th iteration => 0.119411154792\n",
      "Loss for  3679 th iteration => 0.119375957197\n",
      "Loss for  3680 th iteration => 0.119333455506\n",
      "Loss for  3681 th iteration => 0.119296661339\n",
      "Loss for  3682 th iteration => 0.11925739877\n",
      "Loss for  3683 th iteration => 0.119219659766\n",
      "Loss for  3684 th iteration => 0.119182902869\n",
      "Loss for  3685 th iteration => 0.119143715772\n",
      "Loss for  3686 th iteration => 0.119106047151\n",
      "Loss for  3687 th iteration => 0.119069345197\n",
      "Loss for  3688 th iteration => 0.119030227784\n",
      "Loss for  3689 th iteration => 0.118992617868\n",
      "Loss for  3690 th iteration => 0.118955985275\n",
      "Loss for  3691 th iteration => 0.11891693285\n",
      "Loss for  3692 th iteration => 0.118879372153\n",
      "Loss for  3693 th iteration => 0.118842820642\n",
      "Loss for  3694 th iteration => 0.118803829392\n",
      "Loss for  3695 th iteration => 0.118766310219\n",
      "Loss for  3696 th iteration => 0.11872984928\n",
      "Loss for  3697 th iteration => 0.118690916108\n",
      "Loss for  3698 th iteration => 0.118653432231\n",
      "Loss for  3699 th iteration => 0.118617069512\n",
      "Loss for  3700 th iteration => 0.118578191906\n",
      "Loss for  3701 th iteration => 0.118540738291\n",
      "Loss for  3702 th iteration => 0.118504479922\n",
      "Loss for  3703 th iteration => 0.118465655843\n",
      "Loss for  3704 th iteration => 0.118428228439\n",
      "Loss for  3705 th iteration => 0.118392079299\n",
      "Loss for  3706 th iteration => 0.118353307094\n",
      "Loss for  3707 th iteration => 0.118315902654\n",
      "Loss for  3708 th iteration => 0.11827986659\n",
      "Loss for  3709 th iteration => 0.118241144923\n",
      "Loss for  3710 th iteration => 0.118203760857\n",
      "Loss for  3711 th iteration => 0.118167840868\n",
      "Loss for  3712 th iteration => 0.118129168661\n",
      "Loss for  3713 th iteration => 0.118091802918\n",
      "Loss for  3714 th iteration => 0.118056001308\n",
      "Loss for  3715 th iteration => 0.118017377692\n",
      "Loss for  3716 th iteration => 0.117980028664\n",
      "Loss for  3717 th iteration => 0.117944347162\n",
      "Loss for  3718 th iteration => 0.117905771443\n",
      "Loss for  3719 th iteration => 0.117868437882\n",
      "Loss for  3720 th iteration => 0.117832877748\n",
      "Loss for  3721 th iteration => 0.117794349371\n",
      "Loss for  3722 th iteration => 0.117757030328\n",
      "Loss for  3723 th iteration => 0.117721592436\n",
      "Loss for  3724 th iteration => 0.117683110963\n",
      "Loss for  3725 th iteration => 0.117645992426\n",
      "Loss for  3726 th iteration => 0.117611861564\n",
      "Loss for  3727 th iteration => 0.117578228778\n",
      "Loss for  3728 th iteration => 0.117535873257\n",
      "Loss for  3729 th iteration => 0.117498732748\n",
      "Loss for  3730 th iteration => 0.117460900285\n",
      "Loss for  3731 th iteration => 0.117425419191\n",
      "Loss for  3732 th iteration => 0.117387587702\n",
      "Loss for  3733 th iteration => 0.117350233125\n",
      "Loss for  3734 th iteration => 0.117315114956\n",
      "Loss for  3735 th iteration => 0.117276655014\n",
      "Loss for  3736 th iteration => 0.117239809903\n",
      "Loss for  3737 th iteration => 0.117206236668\n",
      "Loss for  3738 th iteration => 0.117169991662\n",
      "Loss for  3739 th iteration => 0.117131695466\n",
      "Loss for  3740 th iteration => 0.117099810489\n",
      "Loss for  3741 th iteration => 0.117056143068\n",
      "Loss for  3742 th iteration => 0.117019382752\n",
      "Loss for  3743 th iteration => 0.11698404667\n",
      "Loss for  3744 th iteration => 0.116946135137\n",
      "Loss for  3745 th iteration => 0.116909312271\n",
      "Loss for  3746 th iteration => 0.116874200446\n",
      "Loss for  3747 th iteration => 0.116836305191\n",
      "Loss for  3748 th iteration => 0.116799460219\n",
      "Loss for  3749 th iteration => 0.116765922859\n",
      "Loss for  3750 th iteration => 0.116732680177\n",
      "Loss for  3751 th iteration => 0.116690921789\n",
      "Loss for  3752 th iteration => 0.116654091424\n",
      "Loss for  3753 th iteration => 0.11661688612\n",
      "Loss for  3754 th iteration => 0.116582019779\n",
      "Loss for  3755 th iteration => 0.116544156821\n",
      "Loss for  3756 th iteration => 0.11650768745\n",
      "Loss for  3757 th iteration => 0.116474203217\n",
      "Loss for  3758 th iteration => 0.116438730649\n",
      "Loss for  3759 th iteration => 0.11640015511\n",
      "Loss for  3760 th iteration => 0.116369452617\n",
      "Loss for  3761 th iteration => 0.116326048454\n",
      "Loss for  3762 th iteration => 0.116289562052\n",
      "Loss for  3763 th iteration => 0.116254711451\n",
      "Loss for  3764 th iteration => 0.116217234232\n",
      "Loss for  3765 th iteration => 0.116180663383\n",
      "Loss for  3766 th iteration => 0.116146066036\n",
      "Loss for  3767 th iteration => 0.116108595974\n",
      "Loss for  3768 th iteration => 0.116072533594\n",
      "Loss for  3769 th iteration => 0.116037467811\n",
      "Loss for  3770 th iteration => 0.116006668484\n",
      "Loss for  3771 th iteration => 0.115965230214\n",
      "Loss for  3772 th iteration => 0.115927724681\n",
      "Loss for  3773 th iteration => 0.115891682677\n",
      "Loss for  3774 th iteration => 0.115858080055\n",
      "Loss for  3775 th iteration => 0.115823429256\n",
      "Loss for  3776 th iteration => 0.115784419063\n",
      "Loss for  3777 th iteration => 0.115754901583\n",
      "Loss for  3778 th iteration => 0.11571168782\n",
      "Loss for  3779 th iteration => 0.115675435236\n",
      "Loss for  3780 th iteration => 0.115640992654\n",
      "Loss for  3781 th iteration => 0.115603872666\n",
      "Loss for  3782 th iteration => 0.115567846645\n",
      "Loss for  3783 th iteration => 0.11553321143\n",
      "Loss for  3784 th iteration => 0.115502519738\n",
      "Loss for  3785 th iteration => 0.115461456008\n",
      "Loss for  3786 th iteration => 0.11542443977\n",
      "Loss for  3787 th iteration => 0.115388560882\n",
      "Loss for  3788 th iteration => 0.115354902606\n",
      "Loss for  3789 th iteration => 0.11532087859\n",
      "Loss for  3790 th iteration => 0.115281579527\n",
      "Loss for  3791 th iteration => 0.115252926962\n",
      "Loss for  3792 th iteration => 0.115209984654\n",
      "Loss for  3793 th iteration => 0.115173941134\n",
      "Loss for  3794 th iteration => 0.115139797337\n",
      "Loss for  3795 th iteration => 0.11510298194\n",
      "Loss for  3796 th iteration => 0.115067337948\n",
      "Loss for  3797 th iteration => 0.115032355291\n",
      "Loss for  3798 th iteration => 0.114996170685\n",
      "Loss for  3799 th iteration => 0.114961385694\n",
      "Loss for  3800 th iteration => 0.114924987946\n",
      "Loss for  3801 th iteration => 0.114889467317\n",
      "Loss for  3802 th iteration => 0.114854968587\n",
      "Loss for  3803 th iteration => 0.114818430116\n",
      "Loss for  3804 th iteration => 0.11478271995\n",
      "Loss for  3805 th iteration => 0.114748625521\n",
      "Loss for  3806 th iteration => 0.114712054762\n",
      "Loss for  3807 th iteration => 0.114676362787\n",
      "Loss for  3808 th iteration => 0.114642165615\n",
      "Loss for  3809 th iteration => 0.114611996052\n",
      "Loss for  3810 th iteration => 0.114571579906\n",
      "Loss for  3811 th iteration => 0.114534912315\n",
      "Loss for  3812 th iteration => 0.114499765113\n",
      "Loss for  3813 th iteration => 0.114466103494\n",
      "Loss for  3814 th iteration => 0.11442889059\n",
      "Loss for  3815 th iteration => 0.114393472583\n",
      "Loss for  3816 th iteration => 0.114360386947\n",
      "Loss for  3817 th iteration => 0.11432313959\n",
      "Loss for  3818 th iteration => 0.114288276775\n",
      "Loss for  3819 th iteration => 0.114253992919\n",
      "Loss for  3820 th iteration => 0.114217578944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  3821 th iteration => 0.114183786851\n",
      "Loss for  3822 th iteration => 0.114147352959\n",
      "Loss for  3823 th iteration => 0.11411250435\n",
      "Loss for  3824 th iteration => 0.114078956953\n",
      "Loss for  3825 th iteration => 0.114042006693\n",
      "Loss for  3826 th iteration => 0.114006748627\n",
      "Loss for  3827 th iteration => 0.113973864289\n",
      "Loss for  3828 th iteration => 0.113936872568\n",
      "Loss for  3829 th iteration => 0.113902396355\n",
      "Loss for  3830 th iteration => 0.113867850295\n",
      "Loss for  3831 th iteration => 0.113831929806\n",
      "Loss for  3832 th iteration => 0.113798574696\n",
      "Loss for  3833 th iteration => 0.11376208704\n",
      "Loss for  3834 th iteration => 0.113727134371\n",
      "Loss for  3835 th iteration => 0.113694162431\n",
      "Loss for  3836 th iteration => 0.113657382144\n",
      "Loss for  3837 th iteration => 0.113622588136\n",
      "Loss for  3838 th iteration => 0.113589180956\n",
      "Loss for  3839 th iteration => 0.11355287151\n",
      "Loss for  3840 th iteration => 0.113519346117\n",
      "Loss for  3841 th iteration => 0.11348337244\n",
      "Loss for  3842 th iteration => 0.113448930902\n",
      "Loss for  3843 th iteration => 0.113415657043\n",
      "Loss for  3844 th iteration => 0.1133791169\n",
      "Loss for  3845 th iteration => 0.113344147663\n",
      "Loss for  3846 th iteration => 0.113311626539\n",
      "Loss for  3847 th iteration => 0.113275033783\n",
      "Loss for  3848 th iteration => 0.113241208019\n",
      "Loss for  3849 th iteration => 0.113206285704\n",
      "Loss for  3850 th iteration => 0.113171244929\n",
      "Loss for  3851 th iteration => 0.113138467804\n",
      "Loss for  3852 th iteration => 0.113106147777\n",
      "Loss for  3853 th iteration => 0.113067741812\n",
      "Loss for  3854 th iteration => 0.113034500774\n",
      "Loss for  3855 th iteration => 0.112998148157\n",
      "Loss for  3856 th iteration => 0.112964808396\n",
      "Loss for  3857 th iteration => 0.11293000957\n",
      "Loss for  3858 th iteration => 0.112895255243\n",
      "Loss for  3859 th iteration => 0.112861996122\n",
      "Loss for  3860 th iteration => 0.112825902761\n",
      "Loss for  3861 th iteration => 0.112791994008\n",
      "Loss for  3862 th iteration => 0.112758844699\n",
      "Loss for  3863 th iteration => 0.112722685258\n",
      "Loss for  3864 th iteration => 0.112689490226\n",
      "Loss for  3865 th iteration => 0.112654777778\n",
      "Loss for  3866 th iteration => 0.112620295905\n",
      "Loss for  3867 th iteration => 0.112587091154\n",
      "Loss for  3868 th iteration => 0.112551179141\n",
      "Loss for  3869 th iteration => 0.112517376914\n",
      "Loss for  3870 th iteration => 0.11248437499\n",
      "Loss for  3871 th iteration => 0.112448393436\n",
      "Loss for  3872 th iteration => 0.112415470989\n",
      "Loss for  3873 th iteration => 0.112380569386\n",
      "Loss for  3874 th iteration => 0.112346608817\n",
      "Loss for  3875 th iteration => 0.112313361052\n",
      "Loss for  3876 th iteration => 0.112277619427\n",
      "Loss for  3877 th iteration => 0.112243900008\n",
      "Loss for  3878 th iteration => 0.112211078672\n",
      "Loss for  3879 th iteration => 0.112175265598\n",
      "Loss for  3880 th iteration => 0.112142690444\n",
      "Loss for  3881 th iteration => 0.112107437729\n",
      "Loss for  3882 th iteration => 0.112074143887\n",
      "Loss for  3883 th iteration => 0.112040796172\n",
      "Loss for  3884 th iteration => 0.112005218089\n",
      "Loss for  3885 th iteration => 0.111971668031\n",
      "Loss for  3886 th iteration => 0.111938722443\n",
      "Loss for  3887 th iteration => 0.111903535634\n",
      "Loss for  3888 th iteration => 0.111871008097\n",
      "Loss for  3889 th iteration => 0.111835627682\n",
      "Loss for  3890 th iteration => 0.111802234956\n",
      "Loss for  3891 th iteration => 0.111769409639\n",
      "Loss for  3892 th iteration => 0.111733943735\n",
      "Loss for  3893 th iteration => 0.111701265912\n",
      "Loss for  3894 th iteration => 0.111666853699\n",
      "Loss for  3895 th iteration => 0.111633351021\n",
      "Loss for  3896 th iteration => 0.111600328452\n",
      "Loss for  3897 th iteration => 0.111565071094\n",
      "Loss for  3898 th iteration => 0.111531655612\n",
      "Loss for  3899 th iteration => 0.111499166079\n",
      "Loss for  3900 th iteration => 0.111463884943\n",
      "Loss for  3901 th iteration => 0.111431725926\n",
      "Loss for  3902 th iteration => 0.111396655984\n",
      "Loss for  3903 th iteration => 0.111363506121\n",
      "Loss for  3904 th iteration => 0.111330807695\n",
      "Loss for  3905 th iteration => 0.111295644128\n",
      "Loss for  3906 th iteration => 0.111263155144\n",
      "Loss for  3907 th iteration => 0.111228969853\n",
      "Loss for  3908 th iteration => 0.111195801676\n",
      "Loss for  3909 th iteration => 0.111162901296\n",
      "Loss for  3910 th iteration => 0.111127929489\n",
      "Loss for  3911 th iteration => 0.111094823933\n",
      "Loss for  3912 th iteration => 0.111062173015\n",
      "Loss for  3913 th iteration => 0.111027809848\n",
      "Loss for  3914 th iteration => 0.110995455781\n",
      "Loss for  3915 th iteration => 0.110960650947\n",
      "Loss for  3916 th iteration => 0.110927613843\n",
      "Loss for  3917 th iteration => 0.110895225269\n",
      "Loss for  3918 th iteration => 0.110860326239\n",
      "Loss for  3919 th iteration => 0.110828455359\n",
      "Loss for  3920 th iteration => 0.110793824925\n",
      "Loss for  3921 th iteration => 0.110761022319\n",
      "Loss for  3922 th iteration => 0.110728501162\n",
      "Loss for  3923 th iteration => 0.11069372776\n",
      "Loss for  3924 th iteration => 0.110661463438\n",
      "Loss for  3925 th iteration => 0.110627690919\n",
      "Loss for  3926 th iteration => 0.1105948558\n",
      "Loss for  3927 th iteration => 0.110562197802\n",
      "Loss for  3928 th iteration => 0.110527593701\n",
      "Loss for  3929 th iteration => 0.110494908262\n",
      "Loss for  3930 th iteration => 0.110462249977\n",
      "Loss for  3931 th iteration => 0.110428600653\n",
      "Loss for  3932 th iteration => 0.110396340257\n",
      "Loss for  3933 th iteration => 0.110361887024\n",
      "Loss for  3934 th iteration => 0.110329082594\n",
      "Loss for  3935 th iteration => 0.110297042484\n",
      "Loss for  3936 th iteration => 0.11026271965\n",
      "Loss for  3937 th iteration => 0.110230932302\n",
      "Loss for  3938 th iteration => 0.110196623085\n",
      "Loss for  3939 th iteration => 0.110163990947\n",
      "Loss for  3940 th iteration => 0.110131876185\n",
      "Loss for  3941 th iteration => 0.110097461943\n",
      "Loss for  3942 th iteration => 0.110065996408\n",
      "Loss for  3943 th iteration => 0.110031788147\n",
      "Loss for  3944 th iteration => 0.109999329232\n",
      "Loss for  3945 th iteration => 0.109967169874\n",
      "Loss for  3946 th iteration => 0.109932858903\n",
      "Loss for  3947 th iteration => 0.109901203275\n",
      "Loss for  3948 th iteration => 0.109867371133\n",
      "Loss for  3949 th iteration => 0.109835052467\n",
      "Loss for  3950 th iteration => 0.109802900176\n",
      "Loss for  3951 th iteration => 0.109768678989\n",
      "Loss for  3952 th iteration => 0.109736953033\n",
      "Loss for  3953 th iteration => 0.109703431122\n",
      "Loss for  3954 th iteration => 0.109671423531\n",
      "Loss for  3955 th iteration => 0.109639035088\n",
      "Loss for  3956 th iteration => 0.109604956894\n",
      "Loss for  3957 th iteration => 0.109573045992\n",
      "Loss for  3958 th iteration => 0.109540131431\n",
      "Loss for  3959 th iteration => 0.1095077699\n",
      "Loss for  3960 th iteration => 0.10947559997\n",
      "Loss for  3961 th iteration => 0.109441653556\n",
      "Loss for  3962 th iteration => 0.109409666629\n",
      "Loss for  3963 th iteration => 0.109377129968\n",
      "Loss for  3964 th iteration => 0.109344624382\n",
      "Loss for  3965 th iteration => 0.109312591612\n",
      "Loss for  3966 th iteration => 0.109278768764\n",
      "Loss for  3967 th iteration => 0.109246785334\n",
      "Loss for  3968 th iteration => 0.109214459107\n",
      "Loss for  3969 th iteration => 0.1091819626\n",
      "Loss for  3970 th iteration => 0.109150007853\n",
      "Loss for  3971 th iteration => 0.109116302495\n",
      "Loss for  3972 th iteration => 0.109084380453\n",
      "Loss for  3973 th iteration => 0.109052142639\n",
      "Loss for  3974 th iteration => 0.109019766634\n",
      "Loss for  3975 th iteration => 0.108987847034\n",
      "Loss for  3976 th iteration => 0.10895425462\n",
      "Loss for  3977 th iteration => 0.108922435963\n",
      "Loss for  3978 th iteration => 0.108890197846\n",
      "Loss for  3979 th iteration => 0.108858023149\n",
      "Loss for  3980 th iteration => 0.108826107713\n",
      "Loss for  3981 th iteration => 0.108792624801\n",
      "Loss for  3982 th iteration => 0.108760939855\n",
      "Loss for  3983 th iteration => 0.108728637125\n",
      "Loss for  3984 th iteration => 0.108696722079\n",
      "Loss for  3985 th iteration => 0.10866478853\n",
      "Loss for  3986 th iteration => 0.108631412478\n",
      "Loss for  3987 th iteration => 0.10859988301\n",
      "Loss for  3988 th iteration => 0.108567469233\n",
      "Loss for  3989 th iteration => 0.108535855711\n",
      "Loss for  3990 th iteration => 0.108503888144\n",
      "Loss for  3991 th iteration => 0.108470616893\n",
      "Loss for  3992 th iteration => 0.108439258391\n",
      "Loss for  3993 th iteration => 0.108406700231\n",
      "Loss for  3994 th iteration => 0.108375418036\n",
      "Loss for  3995 th iteration => 0.108343405207\n",
      "Loss for  3996 th iteration => 0.10831023712\n",
      "Loss for  3997 th iteration => 0.108279060473\n",
      "Loss for  3998 th iteration => 0.108246481997\n",
      "Loss for  3999 th iteration => 0.108214947773\n",
      "Loss for  4000 th iteration => 0.108183359725\n",
      "Loss for  4001 th iteration => 0.10815023338\n",
      "Loss for  4002 th iteration => 0.10811944399\n",
      "Loss for  4003 th iteration => 0.108086641238\n",
      "Loss for  4004 th iteration => 0.108055128004\n",
      "Loss for  4005 th iteration => 0.10802372958\n",
      "Loss for  4006 th iteration => 0.10799068705\n",
      "Loss for  4007 th iteration => 0.107960091177\n",
      "Loss for  4008 th iteration => 0.107927209232\n",
      "Loss for  4009 th iteration => 0.107895688371\n",
      "Loss for  4010 th iteration => 0.107864488861\n",
      "Loss for  4011 th iteration => 0.107831845251\n",
      "Loss for  4012 th iteration => 0.107801009287\n",
      "Loss for  4013 th iteration => 0.10776819966\n",
      "Loss for  4014 th iteration => 0.107736685503\n",
      "Loss for  4015 th iteration => 0.107705663105\n",
      "Loss for  4016 th iteration => 0.107673371406\n",
      "Loss for  4017 th iteration => 0.107642343307\n",
      "Loss for  4018 th iteration => 0.107609607955\n",
      "Loss for  4019 th iteration => 0.107578254424\n",
      "Loss for  4020 th iteration => 0.107546971381\n",
      "Loss for  4021 th iteration => 0.107515452632\n",
      "Loss for  4022 th iteration => 0.107484069559\n",
      "Loss for  4023 th iteration => 0.107451408858\n",
      "Loss for  4024 th iteration => 0.107420541822\n",
      "Loss for  4025 th iteration => 0.107388556127\n",
      "Loss for  4026 th iteration => 0.107357461587\n",
      "Loss for  4027 th iteration => 0.107326229371\n",
      "Loss for  4028 th iteration => 0.107293589806\n",
      "Loss for  4029 th iteration => 0.107263328725\n",
      "Loss for  4030 th iteration => 0.107230893788\n",
      "Loss for  4031 th iteration => 0.107199775543\n",
      "Loss for  4032 th iteration => 0.107168802208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  4033 th iteration => 0.107136491979\n",
      "Loss for  4034 th iteration => 0.107106062535\n",
      "Loss for  4035 th iteration => 0.107073641859\n",
      "Loss for  4036 th iteration => 0.107042484125\n",
      "Loss for  4037 th iteration => 0.107011760909\n",
      "Loss for  4038 th iteration => 0.106979944773\n",
      "Loss for  4039 th iteration => 0.106949173729\n",
      "Loss for  4040 th iteration => 0.106916813109\n",
      "Loss for  4041 th iteration => 0.106885967423\n",
      "Loss for  4042 th iteration => 0.106854664869\n",
      "Loss for  4043 th iteration => 0.1068239068\n",
      "Loss for  4044 th iteration => 0.106792674349\n",
      "Loss for  4045 th iteration => 0.10676037831\n",
      "Loss for  4046 th iteration => 0.106730123007\n",
      "Loss for  4047 th iteration => 0.10669824052\n",
      "Loss for  4048 th iteration => 0.10666742797\n",
      "Loss for  4049 th iteration => 0.106636604084\n",
      "Loss for  4050 th iteration => 0.106604562828\n",
      "Loss for  4051 th iteration => 0.106574426346\n",
      "Loss for  4052 th iteration => 0.106542324303\n",
      "Loss for  4053 th iteration => 0.106511438373\n",
      "Loss for  4054 th iteration => 0.106480919627\n",
      "Loss for  4055 th iteration => 0.106449478625\n",
      "Loss for  4056 th iteration => 0.10641888968\n",
      "Loss for  4057 th iteration => 0.106386837118\n",
      "Loss for  4058 th iteration => 0.106356344854\n",
      "Loss for  4059 th iteration => 0.10632509039\n",
      "Loss for  4060 th iteration => 0.106294547364\n",
      "Loss for  4061 th iteration => 0.106263756728\n",
      "Loss for  4062 th iteration => 0.106231716391\n",
      "Loss for  4063 th iteration => 0.106201974717\n",
      "Loss for  4064 th iteration => 0.106170092134\n",
      "Loss for  4065 th iteration => 0.106139440368\n",
      "Loss for  4066 th iteration => 0.106109012735\n",
      "Loss for  4067 th iteration => 0.106077687071\n",
      "Loss for  4068 th iteration => 0.106047374946\n",
      "Loss for  4069 th iteration => 0.106015532524\n",
      "Loss for  4070 th iteration => 0.105985165307\n",
      "Loss for  4071 th iteration => 0.10595419179\n",
      "Loss for  4072 th iteration => 0.105924103814\n",
      "Loss for  4073 th iteration => 0.105893165635\n",
      "Loss for  4074 th iteration => 0.105861372667\n",
      "Loss for  4075 th iteration => 0.105831759152\n",
      "Loss for  4076 th iteration => 0.1058001604\n",
      "Loss for  4077 th iteration => 0.105769744232\n",
      "Loss for  4078 th iteration => 0.105739383037\n",
      "Loss for  4079 th iteration => 0.105708157836\n",
      "Loss for  4080 th iteration => 0.105678134016\n",
      "Loss for  4081 th iteration => 0.105646516036\n",
      "Loss for  4082 th iteration => 0.105616229629\n",
      "Loss for  4083 th iteration => 0.105585621629\n",
      "Loss for  4084 th iteration => 0.105555594596\n",
      "Loss for  4085 th iteration => 0.1055248533\n",
      "Loss for  4086 th iteration => 0.105493276829\n",
      "Loss for  4087 th iteration => 0.105463835974\n",
      "Loss for  4088 th iteration => 0.105432442315\n",
      "Loss for  4089 th iteration => 0.105402204297\n",
      "Loss for  4090 th iteration => 0.105372000699\n",
      "Loss for  4091 th iteration => 0.105341055587\n",
      "Loss for  4092 th iteration => 0.105311131409\n",
      "Loss for  4093 th iteration => 0.10527972004\n",
      "Loss for  4094 th iteration => 0.105249701156\n",
      "Loss for  4095 th iteration => 0.105219108939\n",
      "Loss for  4096 th iteration => 0.105189116215\n",
      "Loss for  4097 th iteration => 0.105158790755\n",
      "Loss for  4098 th iteration => 0.105127541162\n",
      "Loss for  4099 th iteration => 0.105098151541\n",
      "Loss for  4100 th iteration => 0.105066879526\n",
      "Loss for  4101 th iteration => 0.105036737208\n",
      "Loss for  4102 th iteration => 0.105006843832\n",
      "Loss for  4103 th iteration => 0.104976439071\n",
      "Loss for  4104 th iteration => 0.104946343331\n",
      "Loss for  4105 th iteration => 0.104915097058\n",
      "Loss for  4106 th iteration => 0.104885648056\n",
      "Loss for  4107 th iteration => 0.104854855269\n",
      "Loss for  4108 th iteration => 0.104824966992\n",
      "Loss for  4109 th iteration => 0.104794934234\n",
      "Loss for  4110 th iteration => 0.104764187267\n",
      "Loss for  4111 th iteration => 0.104734663177\n",
      "Loss for  4112 th iteration => 0.104703559037\n",
      "Loss for  4113 th iteration => 0.104673774095\n",
      "Loss for  4114 th iteration => 0.104643534233\n",
      "Loss for  4115 th iteration => 0.104613842182\n",
      "Loss for  4116 th iteration => 0.104583764511\n",
      "Loss for  4117 th iteration => 0.104552865341\n",
      "Loss for  4118 th iteration => 0.104523715695\n",
      "Loss for  4119 th iteration => 0.1044927362\n",
      "Loss for  4120 th iteration => 0.104462873763\n",
      "Loss for  4121 th iteration => 0.104433263378\n",
      "Loss for  4122 th iteration => 0.104403276807\n",
      "Loss for  4123 th iteration => 0.104373350894\n",
      "Loss for  4124 th iteration => 0.104342390434\n",
      "Loss for  4125 th iteration => 0.104313403584\n",
      "Loss for  4126 th iteration => 0.104282729317\n",
      "Loss for  4127 th iteration => 0.104253094844\n",
      "Loss for  4128 th iteration => 0.104223383285\n",
      "Loss for  4129 th iteration => 0.104193128869\n",
      "Loss for  4130 th iteration => 0.104163694674\n",
      "Loss for  4131 th iteration => 0.104132868584\n",
      "Loss for  4132 th iteration => 0.104103621748\n",
      "Loss for  4133 th iteration => 0.104073420782\n",
      "Loss for  4134 th iteration => 0.104043964978\n",
      "Loss for  4135 th iteration => 0.104014233184\n",
      "Loss for  4136 th iteration => 0.103983873657\n",
      "Loss for  4137 th iteration => 0.10395476254\n",
      "Loss for  4138 th iteration => 0.103924055986\n",
      "Loss for  4139 th iteration => 0.103894718481\n",
      "Loss for  4140 th iteration => 0.103864819182\n",
      "Loss for  4141 th iteration => 0.103835509575\n",
      "Loss for  4142 th iteration => 0.103805811152\n",
      "Loss for  4143 th iteration => 0.103775453749\n",
      "Loss for  4144 th iteration => 0.103746554771\n",
      "Loss for  4145 th iteration => 0.103715958153\n",
      "Loss for  4146 th iteration => 0.103686641177\n",
      "Loss for  4147 th iteration => 0.103656938488\n",
      "Loss for  4148 th iteration => 0.103628036903\n",
      "Loss for  4149 th iteration => 0.103598107745\n",
      "Loss for  4150 th iteration => 0.103567745842\n",
      "Loss for  4151 th iteration => 0.103539065351\n",
      "Loss for  4152 th iteration => 0.10350862642\n",
      "Loss for  4153 th iteration => 0.103479259971\n",
      "Loss for  4154 th iteration => 0.103450066024\n",
      "Loss for  4155 th iteration => 0.103420748239\n",
      "Loss for  4156 th iteration => 0.10339115665\n",
      "Loss for  4157 th iteration => 0.103360724478\n",
      "Loss for  4158 th iteration => 0.103332405196\n",
      "Loss for  4159 th iteration => 0.103302053187\n",
      "Loss for  4160 th iteration => 0.103272866625\n",
      "Loss for  4161 th iteration => 0.103243644521\n",
      "Loss for  4162 th iteration => 0.103214200592\n",
      "Loss for  4163 th iteration => 0.103184949671\n",
      "Loss for  4164 th iteration => 0.103154636816\n",
      "Loss for  4165 th iteration => 0.103126204107\n",
      "Loss for  4166 th iteration => 0.103096172857\n",
      "Loss for  4167 th iteration => 0.103067132591\n",
      "Loss for  4168 th iteration => 0.103037937785\n",
      "Loss for  4169 th iteration => 0.10300847906\n",
      "Loss for  4170 th iteration => 0.102979453634\n",
      "Loss for  4171 th iteration => 0.102949250111\n",
      "Loss for  4172 th iteration => 0.102920819576\n",
      "Loss for  4173 th iteration => 0.102890991958\n",
      "Loss for  4174 th iteration => 0.102862075683\n",
      "Loss for  4175 th iteration => 0.102832944816\n",
      "Loss for  4176 th iteration => 0.102803544584\n",
      "Loss for  4177 th iteration => 0.102774668761\n",
      "Loss for  4178 th iteration => 0.102744568035\n",
      "Loss for  4179 th iteration => 0.102716215467\n",
      "Loss for  4180 th iteration => 0.102686514529\n",
      "Loss for  4181 th iteration => 0.102657707317\n",
      "Loss for  4182 th iteration => 0.102628664489\n",
      "Loss for  4183 th iteration => 0.102599370772\n",
      "Loss for  4184 th iteration => 0.102570594587\n",
      "Loss for  4185 th iteration => 0.102540592344\n",
      "Loss for  4186 th iteration => 0.102512367156\n",
      "Loss for  4187 th iteration => 0.102482742466\n",
      "Loss for  4188 th iteration => 0.102454034233\n",
      "Loss for  4189 th iteration => 0.102425095206\n",
      "Loss for  4190 th iteration => 0.102395939283\n",
      "Loss for  4191 th iteration => 0.102367229895\n",
      "Loss for  4192 th iteration => 0.102337323258\n",
      "Loss for  4193 th iteration => 0.102309257399\n",
      "Loss for  4194 th iteration => 0.102279676042\n",
      "Loss for  4195 th iteration => 0.102251059891\n",
      "Loss for  4196 th iteration => 0.102222234897\n",
      "Loss for  4197 th iteration => 0.10219323696\n",
      "Loss for  4198 th iteration => 0.102164572837\n",
      "Loss for  4199 th iteration => 0.102134759869\n",
      "Loss for  4200 th iteration => 0.102106873733\n",
      "Loss for  4201 th iteration => 0.102077314365\n",
      "Loss for  4202 th iteration => 0.102048785504\n",
      "Loss for  4203 th iteration => 0.102020081112\n",
      "Loss for  4204 th iteration => 0.101991254014\n",
      "Loss for  4205 th iteration => 0.101962621102\n",
      "Loss for  4206 th iteration => 0.101932900479\n",
      "Loss for  4207 th iteration => 0.101905206822\n",
      "Loss for  4208 th iteration => 0.10187565574\n",
      "Loss for  4209 th iteration => 0.101847210768\n",
      "Loss for  4210 th iteration => 0.101818631122\n",
      "Loss for  4211 th iteration => 0.101789982862\n",
      "Loss for  4212 th iteration => 0.101761372048\n",
      "Loss for  4213 th iteration => 0.101731742847\n",
      "Loss for  4214 th iteration => 0.101704249378\n",
      "Loss for  4215 th iteration => 0.101674697928\n",
      "Loss for  4216 th iteration => 0.10164633437\n",
      "Loss for  4217 th iteration => 0.101617882007\n",
      "Loss for  4218 th iteration => 0.101589417373\n",
      "Loss for  4219 th iteration => 0.101560822813\n",
      "Loss for  4220 th iteration => 0.101531361972\n",
      "Loss for  4221 th iteration => 0.101503831578\n",
      "Loss for  4222 th iteration => 0.101474416352\n",
      "Loss for  4223 th iteration => 0.101446101319\n",
      "Loss for  4224 th iteration => 0.101417801786\n",
      "Loss for  4225 th iteration => 0.101389645919\n",
      "Loss for  4226 th iteration => 0.101360940214\n",
      "Loss for  4227 th iteration => 0.10133178123\n",
      "Loss for  4228 th iteration => 0.101304146964\n",
      "Loss for  4229 th iteration => 0.10127484004\n",
      "Loss for  4230 th iteration => 0.101246765191\n",
      "Loss for  4231 th iteration => 0.101218243434\n",
      "Loss for  4232 th iteration => 0.101190253148\n",
      "Loss for  4233 th iteration => 0.101161726484\n",
      "Loss for  4234 th iteration => 0.101133074085\n",
      "Loss for  4235 th iteration => 0.101105128044\n",
      "Loss for  4236 th iteration => 0.101075884649\n",
      "Loss for  4237 th iteration => 0.101048351138\n",
      "Loss for  4238 th iteration => 0.101019486856\n",
      "Loss for  4239 th iteration => 0.10099151448\n",
      "Loss for  4240 th iteration => 0.100963223856\n",
      "Loss for  4241 th iteration => 0.100934970729\n",
      "Loss for  4242 th iteration => 0.100906819801\n",
      "Loss for  4243 th iteration => 0.100877648069\n",
      "Loss for  4244 th iteration => 0.100850543578\n",
      "Loss for  4245 th iteration => 0.100821447168\n",
      "Loss for  4246 th iteration => 0.100793511379\n",
      "Loss for  4247 th iteration => 0.100765421155\n",
      "Loss for  4248 th iteration => 0.100737495765\n",
      "Loss for  4249 th iteration => 0.100709211112\n",
      "Loss for  4250 th iteration => 0.100680331563\n",
      "Loss for  4251 th iteration => 0.100653065207\n",
      "Loss for  4252 th iteration => 0.100624087884\n",
      "Loss for  4253 th iteration => 0.100596320943\n",
      "Loss for  4254 th iteration => 0.100568131307\n",
      "Loss for  4255 th iteration => 0.100540470927\n",
      "Loss for  4256 th iteration => 0.100512231462\n",
      "Loss for  4257 th iteration => 0.10048396451\n",
      "Loss for  4258 th iteration => 0.100456277254\n",
      "Loss for  4259 th iteration => 0.100427354303\n",
      "Loss for  4260 th iteration => 0.100400237749\n",
      "Loss for  4261 th iteration => 0.100371594824\n",
      "Loss for  4262 th iteration => 0.100343930157\n",
      "Loss for  4263 th iteration => 0.100315961648\n",
      "Loss for  4264 th iteration => 0.100288162976\n",
      "Loss for  4265 th iteration => 0.100260198412\n",
      "Loss for  4266 th iteration => 0.100231481757\n",
      "Loss for  4267 th iteration => 0.100204496426\n",
      "Loss for  4268 th iteration => 0.100175752363\n",
      "Loss for  4269 th iteration => 0.100148155931\n",
      "Loss for  4270 th iteration => 0.100120234533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  4271 th iteration => 0.100092809155\n",
      "Loss for  4272 th iteration => 0.100064749461\n",
      "Loss for  4273 th iteration => 0.100036725269\n",
      "Loss for  4274 th iteration => 0.100009237352\n",
      "Loss for  4275 th iteration => 0.0999805408661\n",
      "Loss for  4276 th iteration => 0.0999536772754\n",
      "Loss for  4277 th iteration => 0.0999252192664\n",
      "Loss for  4278 th iteration => 0.0998977732414\n",
      "Loss for  4279 th iteration => 0.0998700109021\n",
      "Loss for  4280 th iteration => 0.0998425071535\n",
      "Loss for  4281 th iteration => 0.0998146875461\n",
      "Loss for  4282 th iteration => 0.0997862829451\n",
      "Loss for  4283 th iteration => 0.0997594239384\n",
      "Loss for  4284 th iteration => 0.0997308988608\n",
      "Loss for  4285 th iteration => 0.0997036278739\n",
      "Loss for  4286 th iteration => 0.0996758164623\n",
      "Loss for  4287 th iteration => 0.0996485941162\n",
      "Loss for  4288 th iteration => 0.0996207642788\n",
      "Loss for  4289 th iteration => 0.0995930791965\n",
      "Loss for  4290 th iteration => 0.0995656886783\n",
      "Loss for  4291 th iteration => 0.0995372083617\n",
      "Loss for  4292 th iteration => 0.0995106977805\n",
      "Loss for  4293 th iteration => 0.0994823206279\n",
      "Loss for  4294 th iteration => 0.0994550708419\n",
      "Loss for  4295 th iteration => 0.0994275472335\n",
      "Loss for  4296 th iteration => 0.0994003998484\n",
      "Loss for  4297 th iteration => 0.0993726583285\n",
      "Loss for  4298 th iteration => 0.0993446196824\n",
      "Loss for  4299 th iteration => 0.0993178281887\n",
      "Loss for  4300 th iteration => 0.0992895149204\n",
      "Loss for  4301 th iteration => 0.0992626259124\n",
      "Loss for  4302 th iteration => 0.0992348639011\n",
      "Loss for  4303 th iteration => 0.0992078302386\n",
      "Loss for  4304 th iteration => 0.0991802491719\n",
      "Loss for  4305 th iteration => 0.0991529362966\n",
      "Loss for  4306 th iteration => 0.0991256051044\n",
      "Loss for  4307 th iteration => 0.0990974447743\n",
      "Loss for  4308 th iteration => 0.0990710168791\n",
      "Loss for  4309 th iteration => 0.0990428531569\n",
      "Loss for  4310 th iteration => 0.0990158239999\n",
      "Loss for  4311 th iteration => 0.0989884388154\n",
      "Loss for  4312 th iteration => 0.0989615767874\n",
      "Loss for  4313 th iteration => 0.0989340132873\n",
      "Loss for  4314 th iteration => 0.0989066892227\n",
      "Loss for  4315 th iteration => 0.0988796109474\n",
      "Loss for  4316 th iteration => 0.0988514821594\n",
      "Loss for  4317 th iteration => 0.0988253342045\n",
      "Loss for  4318 th iteration => 0.098797261545\n",
      "Loss for  4319 th iteration => 0.0987703483904\n",
      "Loss for  4320 th iteration => 0.0987431297577\n",
      "Loss for  4321 th iteration => 0.0987164198682\n",
      "Loss for  4322 th iteration => 0.0986889110625\n",
      "Loss for  4323 th iteration => 0.0986613355266\n",
      "Loss for  4324 th iteration => 0.0986347489571\n",
      "Loss for  4325 th iteration => 0.0986067762273\n",
      "Loss for  4326 th iteration => 0.0985803568305\n",
      "Loss for  4327 th iteration => 0.0985527888497\n",
      "Loss for  4328 th iteration => 0.0985260689448\n",
      "Loss for  4329 th iteration => 0.0984988277094\n",
      "Loss for  4330 th iteration => 0.0984720176595\n",
      "Loss for  4331 th iteration => 0.0984448489386\n",
      "Loss for  4332 th iteration => 0.0984172091346\n",
      "Loss for  4333 th iteration => 0.0983909244114\n",
      "Loss for  4334 th iteration => 0.0983630936036\n",
      "Loss for  4335 th iteration => 0.0983365952455\n",
      "Loss for  4336 th iteration => 0.0983093394999\n",
      "Loss for  4337 th iteration => 0.0982827768829\n",
      "Loss for  4338 th iteration => 0.0982555737117\n",
      "Loss for  4339 th iteration => 0.0982287880539\n",
      "Loss for  4340 th iteration => 0.0982018323105\n",
      "Loss for  4341 th iteration => 0.0981742392756\n",
      "Loss for  4342 th iteration => 0.0981481436938\n",
      "Loss for  4343 th iteration => 0.0981204460752\n",
      "Loss for  4344 th iteration => 0.0980939832747\n",
      "Loss for  4345 th iteration => 0.0980669250498\n",
      "Loss for  4346 th iteration => 0.0980404980598\n",
      "Loss for  4347 th iteration => 0.0980133692927\n",
      "Loss for  4348 th iteration => 0.0979866814858\n",
      "Loss for  4349 th iteration => 0.0979598633869\n",
      "Loss for  4350 th iteration => 0.0979323824799\n",
      "Loss for  4351 th iteration => 0.0979064092925\n",
      "Loss for  4352 th iteration => 0.0978788392361\n",
      "Loss for  4353 th iteration => 0.0978524798072\n",
      "Loss for  4354 th iteration => 0.0978255507332\n",
      "Loss for  4355 th iteration => 0.0977992461622\n",
      "Loss for  4356 th iteration => 0.0977722135087\n",
      "Loss for  4357 th iteration => 0.0977456664295\n",
      "Loss for  4358 th iteration => 0.0977189414938\n",
      "Loss for  4359 th iteration => 0.097691610744\n",
      "Loss for  4360 th iteration => 0.0976657205917\n",
      "Loss for  4361 th iteration => 0.0976382742889\n",
      "Loss for  4362 th iteration => 0.0976120581068\n",
      "Loss for  4363 th iteration => 0.0975852174614\n",
      "Loss for  4364 th iteration => 0.0975590271192\n",
      "Loss for  4365 th iteration => 0.0975321035132\n",
      "Loss for  4366 th iteration => 0.097505721762\n",
      "Loss for  4367 th iteration => 0.097479063899\n",
      "Loss for  4368 th iteration => 0.0974519050077\n",
      "Loss for  4369 th iteration => 0.0974260748584\n",
      "Loss for  4370 th iteration => 0.0973987495797\n",
      "Loss for  4371 th iteration => 0.0973726998173\n",
      "Loss for  4372 th iteration => 0.097345923382\n",
      "Loss for  4373 th iteration => 0.0973198420666\n",
      "Loss for  4374 th iteration => 0.0972930351829\n",
      "Loss for  4375 th iteration => 0.097266832449\n",
      "Loss for  4376 th iteration => 0.0972402265337\n",
      "Loss for  4377 th iteration => 0.0972132514564\n",
      "Loss for  4378 th iteration => 0.0971874680128\n",
      "Loss for  4379 th iteration => 0.0971602616776\n",
      "Loss for  4380 th iteration => 0.0971343915246\n",
      "Loss for  4381 th iteration => 0.0971076649396\n",
      "Loss for  4382 th iteration => 0.0970816892332\n",
      "Loss for  4383 th iteration => 0.097055003594\n",
      "Loss for  4384 th iteration => 0.0970289870598\n",
      "Loss for  4385 th iteration => 0.0970024245042\n",
      "Loss for  4386 th iteration => 0.0969756393859\n",
      "Loss for  4387 th iteration => 0.0969498951536\n",
      "Loss for  4388 th iteration => 0.0969228060767\n",
      "Loss for  4389 th iteration => 0.0968971227548\n",
      "Loss for  4390 th iteration => 0.0968704375543\n",
      "Loss for  4391 th iteration => 0.0968445651071\n",
      "Loss for  4392 th iteration => 0.0968180033424\n",
      "Loss for  4393 th iteration => 0.0967921763156\n",
      "Loss for  4394 th iteration => 0.0967656524263\n",
      "Loss for  4395 th iteration => 0.0967390599535\n",
      "Loss for  4396 th iteration => 0.0967133508944\n",
      "Loss for  4397 th iteration => 0.0966863776358\n",
      "Loss for  4398 th iteration => 0.0966608847991\n",
      "Loss for  4399 th iteration => 0.0966342360432\n",
      "Loss for  4400 th iteration => 0.0966084651466\n",
      "Loss for  4401 th iteration => 0.0965820287483\n",
      "Loss for  4402 th iteration => 0.0965563922348\n",
      "Loss for  4403 th iteration => 0.0965299046349\n",
      "Loss for  4404 th iteration => 0.0965035054413\n",
      "Loss for  4405 th iteration => 0.0964778295723\n",
      "Loss for  4406 th iteration => 0.0964509708475\n",
      "Loss for  4407 th iteration => 0.0964256700176\n",
      "Loss for  4408 th iteration => 0.0963990548776\n",
      "Loss for  4409 th iteration => 0.0963733842092\n",
      "Loss for  4410 th iteration => 0.096347073982\n",
      "Loss for  4411 th iteration => 0.0963216276267\n",
      "Loss for  4412 th iteration => 0.0962951753132\n",
      "Loss for  4413 th iteration => 0.0962689688202\n",
      "Loss for  4414 th iteration => 0.0962433253752\n",
      "Loss for  4415 th iteration => 0.0962165800016\n",
      "Loss for  4416 th iteration => 0.0961914714264\n",
      "Loss for  4417 th iteration => 0.0961648883386\n",
      "Loss for  4418 th iteration => 0.0961393168102\n",
      "Loss for  4419 th iteration => 0.0961131331414\n",
      "Loss for  4420 th iteration => 0.0960878757907\n",
      "Loss for  4421 th iteration => 0.0960614585705\n",
      "Loss for  4422 th iteration => 0.0960354434899\n",
      "Loss for  4423 th iteration => 0.0960098324194\n",
      "Loss for  4424 th iteration => 0.0959831992842\n",
      "Loss for  4425 th iteration => 0.0959582824515\n",
      "Loss for  4426 th iteration => 0.0959317306105\n",
      "Loss for  4427 th iteration => 0.095906257277\n",
      "Loss for  4428 th iteration => 0.0958802002987\n",
      "Loss for  4429 th iteration => 0.0958551303364\n",
      "Loss for  4430 th iteration => 0.0958287484896\n",
      "Loss for  4431 th iteration => 0.0958029231242\n",
      "Loss for  4432 th iteration => 0.0957773447957\n",
      "Loss for  4433 th iteration => 0.0957508481351\n",
      "Loss for  4434 th iteration => 0.0957259854438\n",
      "Loss for  4435 th iteration => 0.0956995508101\n",
      "Loss for  4436 th iteration => 0.0956741463659\n",
      "Loss for  4437 th iteration => 0.0956484308111\n",
      "Loss for  4438 th iteration => 0.0956231977795\n",
      "Loss for  4439 th iteration => 0.0955969908107\n",
      "Loss for  4440 th iteration => 0.0955715527506\n",
      "Loss for  4441 th iteration => 0.0955458089829\n",
      "Loss for  4442 th iteration => 0.0955196809502\n",
      "Loss for  4443 th iteration => 0.0954946720465\n",
      "Loss for  4444 th iteration => 0.0954683527994\n",
      "Loss for  4445 th iteration => 0.0954432448411\n",
      "Loss for  4446 th iteration => 0.0954173659475\n",
      "Loss for  4447 th iteration => 0.0953921896559\n",
      "Loss for  4448 th iteration => 0.0953662225626\n",
      "Loss for  4449 th iteration => 0.0953412046952\n",
      "Loss for  4450 th iteration => 0.0953152610617\n",
      "Loss for  4451 th iteration => 0.0952895305425\n",
      "Loss for  4452 th iteration => 0.0952643452347\n",
      "Loss for  4453 th iteration => 0.0952381211967\n",
      "Loss for  4454 th iteration => 0.0952134191886\n",
      "Loss for  4455 th iteration => 0.0951873567902\n",
      "Loss for  4456 th iteration => 0.0951622382835\n",
      "Loss for  4457 th iteration => 0.0951364658242\n",
      "Loss for  4458 th iteration => 0.0951117717547\n",
      "Loss for  4459 th iteration => 0.0950857229586\n",
      "Loss for  4460 th iteration => 0.0950603034781\n",
      "Loss for  4461 th iteration => 0.0950350261602\n",
      "Loss for  4462 th iteration => 0.0950089617787\n",
      "Loss for  4463 th iteration => 0.0949843720692\n",
      "Loss for  4464 th iteration => 0.0949583314399\n",
      "Loss for  4465 th iteration => 0.0949332571015\n",
      "Loss for  4466 th iteration => 0.0949079516034\n",
      "Loss for  4467 th iteration => 0.0948830026325\n",
      "Loss for  4468 th iteration => 0.0948571309574\n",
      "Loss for  4469 th iteration => 0.0948321664157\n",
      "Loss for  4470 th iteration => 0.0948066524087\n",
      "Loss for  4471 th iteration => 0.0947810164662\n",
      "Loss for  4472 th iteration => 0.0947562173708\n",
      "Loss for  4473 th iteration => 0.0947302839055\n",
      "Loss for  4474 th iteration => 0.0947056121099\n",
      "Loss for  4475 th iteration => 0.0946799954063\n",
      "Loss for  4476 th iteration => 0.0946551295852\n",
      "Loss for  4477 th iteration => 0.0946295177874\n",
      "Loss for  4478 th iteration => 0.0946050099046\n",
      "Loss for  4479 th iteration => 0.0945792556906\n",
      "Loss for  4480 th iteration => 0.0945540492372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  4481 th iteration => 0.0945290380416\n",
      "Loss for  4482 th iteration => 0.0945032104834\n",
      "Loss for  4483 th iteration => 0.094478861835\n",
      "Loss for  4484 th iteration => 0.094453099278\n",
      "Loss for  4485 th iteration => 0.0944282542187\n",
      "Loss for  4486 th iteration => 0.0944031428528\n",
      "Loss for  4487 th iteration => 0.0943784686635\n",
      "Loss for  4488 th iteration => 0.0943528055512\n",
      "Loss for  4489 th iteration => 0.0943281221003\n",
      "Loss for  4490 th iteration => 0.0943028040395\n",
      "Loss for  4491 th iteration => 0.0942774653836\n",
      "Loss for  4492 th iteration => 0.0942528449852\n",
      "Loss for  4493 th iteration => 0.094227183142\n",
      "Loss for  4494 th iteration => 0.0942030315141\n",
      "Loss for  4495 th iteration => 0.0941809328397\n",
      "Loss for  4496 th iteration => 0.0941537045168\n",
      "Loss for  4497 th iteration => 0.0941277918049\n",
      "Loss for  4498 th iteration => 0.0941027076555\n",
      "Loss for  4499 th iteration => 0.0940777996271\n",
      "Loss for  4500 th iteration => 0.0940525729836\n",
      "Loss for  4501 th iteration => 0.0940280546988\n",
      "Loss for  4502 th iteration => 0.0940023990396\n",
      "Loss for  4503 th iteration => 0.0939782716374\n",
      "Loss for  4504 th iteration => 0.093956326442\n",
      "Loss for  4505 th iteration => 0.0939290342538\n",
      "Loss for  4506 th iteration => 0.0939034578918\n",
      "Loss for  4507 th iteration => 0.0938776463185\n",
      "Loss for  4508 th iteration => 0.0938539412246\n",
      "Loss for  4509 th iteration => 0.0938283504122\n",
      "Loss for  4510 th iteration => 0.0938041932128\n",
      "Loss for  4511 th iteration => 0.0937785430159\n",
      "Loss for  4512 th iteration => 0.0937547587327\n",
      "Loss for  4513 th iteration => 0.0937291622854\n",
      "Loss for  4514 th iteration => 0.093704572692\n",
      "Loss for  4515 th iteration => 0.093679814906\n",
      "Loss for  4516 th iteration => 0.093654508694\n",
      "Loss for  4517 th iteration => 0.0936304992992\n",
      "Loss for  4518 th iteration => 0.0936048736137\n",
      "Loss for  4519 th iteration => 0.0935810455976\n",
      "Loss for  4520 th iteration => 0.0935558863512\n",
      "Loss for  4521 th iteration => 0.0935317678792\n",
      "Loss for  4522 th iteration => 0.093506236985\n",
      "Loss for  4523 th iteration => 0.0934823591899\n",
      "Loss for  4524 th iteration => 0.0934571073181\n",
      "Loss for  4525 th iteration => 0.0934324609016\n",
      "Loss for  4526 th iteration => 0.0934080100844\n",
      "Loss for  4527 th iteration => 0.0933826799075\n",
      "Loss for  4528 th iteration => 0.0933589440115\n",
      "Loss for  4529 th iteration => 0.0933335144007\n",
      "Loss for  4530 th iteration => 0.0933097432668\n",
      "Loss for  4531 th iteration => 0.0932845779248\n",
      "Loss for  4532 th iteration => 0.0932607190878\n",
      "Loss for  4533 th iteration => 0.0932352953081\n",
      "Loss for  4534 th iteration => 0.0932116462807\n",
      "Loss for  4535 th iteration => 0.0931898915246\n",
      "Loss for  4536 th iteration => 0.0931631282\n",
      "Loss for  4537 th iteration => 0.0931377831115\n",
      "Loss for  4538 th iteration => 0.0931122230125\n",
      "Loss for  4539 th iteration => 0.0930889655799\n",
      "Loss for  4540 th iteration => 0.0930630698651\n",
      "Loss for  4541 th iteration => 0.093040068431\n",
      "Loss for  4542 th iteration => 0.0930146225264\n",
      "Loss for  4543 th iteration => 0.0929915248807\n",
      "Loss for  4544 th iteration => 0.092969380579\n",
      "Loss for  4545 th iteration => 0.0929428897168\n",
      "Loss for  4546 th iteration => 0.0929175350404\n",
      "Loss for  4547 th iteration => 0.0928924076485\n",
      "Loss for  4548 th iteration => 0.0928689337505\n",
      "Loss for  4549 th iteration => 0.0928431704272\n",
      "Loss for  4550 th iteration => 0.0928203627882\n",
      "Loss for  4551 th iteration => 0.0927940352774\n",
      "Loss for  4552 th iteration => 0.0927718218006\n",
      "Loss for  4553 th iteration => 0.0927459191195\n",
      "Loss for  4554 th iteration => 0.0927231616387\n",
      "Loss for  4555 th iteration => 0.0926978627874\n",
      "Loss for  4556 th iteration => 0.0926739287495\n",
      "Loss for  4557 th iteration => 0.0926494838372\n",
      "Loss for  4558 th iteration => 0.0926248182882\n",
      "Loss for  4559 th iteration => 0.0926011361492\n",
      "Loss for  4560 th iteration => 0.0925758198349\n",
      "Loss for  4561 th iteration => 0.0925528186939\n",
      "Loss for  4562 th iteration => 0.0925269244997\n",
      "Loss for  4563 th iteration => 0.0925045310172\n",
      "Loss for  4564 th iteration => 0.0924786376073\n",
      "Loss for  4565 th iteration => 0.0924567673588\n",
      "Loss for  4566 th iteration => 0.0924343357398\n",
      "Loss for  4567 th iteration => 0.0924083822774\n",
      "Loss for  4568 th iteration => 0.0923830365846\n",
      "Loss for  4569 th iteration => 0.0923584519433\n",
      "Loss for  4570 th iteration => 0.0923349377489\n",
      "Loss for  4571 th iteration => 0.0923096989868\n",
      "Loss for  4572 th iteration => 0.0922868689802\n",
      "Loss for  4573 th iteration => 0.0922610491808\n",
      "Loss for  4574 th iteration => 0.0922388297842\n",
      "Loss for  4575 th iteration => 0.092212802105\n",
      "Loss for  4576 th iteration => 0.0921905875945\n",
      "Loss for  4577 th iteration => 0.0921654491118\n",
      "Loss for  4578 th iteration => 0.0921423414542\n",
      "Loss for  4579 th iteration => 0.0921176888461\n",
      "Loss for  4580 th iteration => 0.0920936950986\n",
      "Loss for  4581 th iteration => 0.0920698427002\n",
      "Loss for  4582 th iteration => 0.0920451641587\n",
      "Loss for  4583 th iteration => 0.0920220267458\n",
      "Loss for  4584 th iteration => 0.0919967391719\n",
      "Loss for  4585 th iteration => 0.0919742402709\n",
      "Loss for  4586 th iteration => 0.0919484558747\n",
      "Loss for  4587 th iteration => 0.0919270313721\n",
      "Loss for  4588 th iteration => 0.091904490841\n",
      "Loss for  4589 th iteration => 0.091879270388\n",
      "Loss for  4590 th iteration => 0.0918539831956\n",
      "Loss for  4591 th iteration => 0.0918300584558\n",
      "Loss for  4592 th iteration => 0.0918063834521\n",
      "Loss for  4593 th iteration => 0.0917817625522\n",
      "Loss for  4594 th iteration => 0.0917588138717\n",
      "Loss for  4595 th iteration => 0.091733573597\n",
      "Loss for  4596 th iteration => 0.0917112736371\n",
      "Loss for  4597 th iteration => 0.0916857154756\n",
      "Loss for  4598 th iteration => 0.0916636975039\n",
      "Loss for  4599 th iteration => 0.0916416551799\n",
      "Loss for  4600 th iteration => 0.0916166134937\n",
      "Loss for  4601 th iteration => 0.0915911240334\n",
      "Loss for  4602 th iteration => 0.0915679226097\n",
      "Loss for  4603 th iteration => 0.0915442036369\n",
      "Loss for  4604 th iteration => 0.0915240839488\n",
      "Loss for  4605 th iteration => 0.091498217498\n",
      "Loss for  4606 th iteration => 0.0914724233919\n",
      "Loss for  4607 th iteration => 0.0914491570258\n",
      "Loss for  4608 th iteration => 0.0914245528465\n",
      "Loss for  4609 th iteration => 0.0914018967993\n",
      "Loss for  4610 th iteration => 0.0913772530597\n",
      "Loss for  4611 th iteration => 0.0913542649094\n",
      "Loss for  4612 th iteration => 0.0913300155793\n",
      "Loss for  4613 th iteration => 0.0913068122301\n",
      "Loss for  4614 th iteration => 0.0912827107614\n",
      "Loss for  4615 th iteration => 0.0912598539106\n",
      "Loss for  4616 th iteration => 0.0912355889341\n",
      "Loss for  4617 th iteration => 0.0912122807851\n",
      "Loss for  4618 th iteration => 0.0911885519869\n",
      "Loss for  4619 th iteration => 0.0911646091392\n",
      "Loss for  4620 th iteration => 0.0911415440366\n",
      "Loss for  4621 th iteration => 0.0911171011802\n",
      "Loss for  4622 th iteration => 0.0910950457572\n",
      "Loss for  4623 th iteration => 0.0910735224234\n",
      "Loss for  4624 th iteration => 0.0910479349038\n",
      "Loss for  4625 th iteration => 0.0910233704503\n",
      "Loss for  4626 th iteration => 0.0909997170233\n",
      "Loss for  4627 th iteration => 0.0909770704323\n",
      "Loss for  4628 th iteration => 0.0909574111372\n",
      "Loss for  4629 th iteration => 0.0909302890847\n",
      "Loss for  4630 th iteration => 0.0909068117409\n",
      "Loss for  4631 th iteration => 0.090886941704\n",
      "Loss for  4632 th iteration => 0.0908602800144\n",
      "Loss for  4633 th iteration => 0.0908358175226\n",
      "Loss for  4634 th iteration => 0.0908116914746\n",
      "Loss for  4635 th iteration => 0.0907891162338\n",
      "Loss for  4636 th iteration => 0.0907648550405\n",
      "Loss for  4637 th iteration => 0.0907423300289\n",
      "Loss for  4638 th iteration => 0.090721607462\n",
      "Loss for  4639 th iteration => 0.0906961249584\n",
      "Loss for  4640 th iteration => 0.090671755729\n",
      "Loss for  4641 th iteration => 0.0906482421077\n",
      "Loss for  4642 th iteration => 0.0906253637197\n",
      "Loss for  4643 th iteration => 0.0906062417261\n",
      "Loss for  4644 th iteration => 0.0905793554734\n",
      "Loss for  4645 th iteration => 0.0905558262068\n",
      "Loss for  4646 th iteration => 0.090536191649\n",
      "Loss for  4647 th iteration => 0.0905097057632\n",
      "Loss for  4648 th iteration => 0.0904853795318\n",
      "Loss for  4649 th iteration => 0.0904614769779\n",
      "Loss for  4650 th iteration => 0.090438991619\n",
      "Loss for  4651 th iteration => 0.0904149539517\n",
      "Loss for  4652 th iteration => 0.0903925903162\n",
      "Loss for  4653 th iteration => 0.0903720173416\n",
      "Loss for  4654 th iteration => 0.0903466260377\n",
      "Loss for  4655 th iteration => 0.0903224873142\n",
      "Loss for  4656 th iteration => 0.0902990324384\n",
      "Loss for  4657 th iteration => 0.0902766510215\n",
      "Loss for  4658 th iteration => 0.0902573264518\n",
      "Loss for  4659 th iteration => 0.090230485718\n",
      "Loss for  4660 th iteration => 0.0902077095121\n",
      "Loss for  4661 th iteration => 0.0901877695098\n",
      "Loss for  4662 th iteration => 0.0901614213574\n",
      "Loss for  4663 th iteration => 0.0901372889999\n",
      "Loss for  4664 th iteration => 0.09011378511\n",
      "Loss for  4665 th iteration => 0.0900912164991\n",
      "Loss for  4666 th iteration => 0.090067417092\n",
      "Loss for  4667 th iteration => 0.0900454970947\n",
      "Loss for  4668 th iteration => 0.0900247813423\n",
      "Loss for  4669 th iteration => 0.0899994778148\n",
      "Loss for  4670 th iteration => 0.089975581287\n",
      "Loss for  4671 th iteration => 0.0899522156349\n",
      "Loss for  4672 th iteration => 0.0899296023718\n",
      "Loss for  4673 th iteration => 0.0899063227546\n",
      "Loss for  4674 th iteration => 0.089883824741\n",
      "Loss for  4675 th iteration => 0.089864952186\n",
      "Loss for  4676 th iteration => 0.0898383755763\n",
      "Loss for  4677 th iteration => 0.0898156377217\n",
      "Loss for  4678 th iteration => 0.0897958693293\n",
      "Loss for  4679 th iteration => 0.0897697168046\n",
      "Loss for  4680 th iteration => 0.0897457405674\n",
      "Loss for  4681 th iteration => 0.0897224488517\n",
      "Loss for  4682 th iteration => 0.0897000168593\n",
      "Loss for  4683 th iteration => 0.0896764409632\n",
      "Loss for  4684 th iteration => 0.0896547873023\n",
      "Loss for  4685 th iteration => 0.0896341480227\n",
      "Loss for  4686 th iteration => 0.0896089741066\n",
      "Loss for  4687 th iteration => 0.089585308029\n",
      "Loss for  4688 th iteration => 0.0895621405473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  4689 th iteration => 0.0895396808491\n",
      "Loss for  4690 th iteration => 0.0895164918892\n",
      "Loss for  4691 th iteration => 0.0894946349319\n",
      "Loss for  4692 th iteration => 0.0894754456374\n",
      "Loss for  4693 th iteration => 0.0894489346489\n",
      "Loss for  4694 th iteration => 0.0894270737386\n",
      "Loss for  4695 th iteration => 0.0894069134671\n",
      "Loss for  4696 th iteration => 0.0893809172699\n",
      "Loss for  4697 th iteration => 0.0893571566851\n",
      "Loss for  4698 th iteration => 0.0893342609063\n",
      "Loss for  4699 th iteration => 0.0893117837426\n",
      "Loss for  4700 th iteration => 0.0892885016535\n",
      "Loss for  4701 th iteration => 0.0892664389202\n",
      "Loss for  4702 th iteration => 0.089243088976\n",
      "Loss for  4703 th iteration => 0.0892214432352\n",
      "Loss for  4704 th iteration => 0.0892011801901\n",
      "Loss for  4705 th iteration => 0.0891761556548\n",
      "Loss for  4706 th iteration => 0.0891527259729\n",
      "Loss for  4707 th iteration => 0.089129632245\n",
      "Loss for  4708 th iteration => 0.0891074840068\n",
      "Loss for  4709 th iteration => 0.0890844536161\n",
      "Loss for  4710 th iteration => 0.0890629470522\n",
      "Loss for  4711 th iteration => 0.0890437676945\n",
      "Loss for  4712 th iteration => 0.0890174908268\n",
      "Loss for  4713 th iteration => 0.0889945971365\n",
      "Loss for  4714 th iteration => 0.0889711655696\n",
      "Loss for  4715 th iteration => 0.0889503193613\n",
      "Loss for  4716 th iteration => 0.0889307761029\n",
      "Loss for  4717 th iteration => 0.0889050551132\n",
      "Loss for  4718 th iteration => 0.0888814282469\n",
      "Loss for  4719 th iteration => 0.0888585415834\n",
      "Loss for  4720 th iteration => 0.0888364717033\n",
      "Loss for  4721 th iteration => 0.088813350774\n",
      "Loss for  4722 th iteration => 0.0887924131193\n",
      "Loss for  4723 th iteration => 0.0887718063615\n",
      "Loss for  4724 th iteration => 0.0887469588257\n",
      "Loss for  4725 th iteration => 0.0887237615408\n",
      "Loss for  4726 th iteration => 0.0887010231384\n",
      "Loss for  4727 th iteration => 0.0886789077504\n",
      "Loss for  4728 th iteration => 0.0886559708262\n",
      "Loss for  4729 th iteration => 0.0886355427579\n",
      "Loss for  4730 th iteration => 0.0886156349942\n",
      "Loss for  4731 th iteration => 0.0885896834296\n",
      "Loss for  4732 th iteration => 0.0885668812081\n",
      "Loss for  4733 th iteration => 0.0885437850351\n",
      "Loss for  4734 th iteration => 0.0885222787203\n",
      "Loss for  4735 th iteration => 0.0884990533609\n",
      "Loss for  4736 th iteration => 0.0884786247508\n",
      "Loss for  4737 th iteration => 0.0884590643546\n",
      "Loss for  4738 th iteration => 0.0884335468485\n",
      "Loss for  4739 th iteration => 0.0884101478336\n",
      "Loss for  4740 th iteration => 0.0883874624889\n",
      "Loss for  4741 th iteration => 0.0883656089404\n",
      "Loss for  4742 th iteration => 0.0883427142493\n",
      "Loss for  4743 th iteration => 0.0883222490352\n",
      "Loss for  4744 th iteration => 0.0883015728475\n",
      "Loss for  4745 th iteration => 0.0882769280464\n",
      "Loss for  4746 th iteration => 0.0882539633234\n",
      "Loss for  4747 th iteration => 0.0882314663951\n",
      "Loss for  4748 th iteration => 0.0882095299525\n",
      "Loss for  4749 th iteration => 0.0881868065769\n",
      "Loss for  4750 th iteration => 0.0881651285799\n",
      "Loss for  4751 th iteration => 0.0881424422902\n",
      "Loss for  4752 th iteration => 0.0881222700896\n",
      "Loss for  4753 th iteration => 0.0881025315752\n",
      "Loss for  4754 th iteration => 0.0880768074897\n",
      "Loss for  4755 th iteration => 0.0880542298312\n",
      "Loss for  4756 th iteration => 0.0880313185482\n",
      "Loss for  4757 th iteration => 0.0880161598759\n",
      "Loss for  4758 th iteration => 0.0879937991175\n",
      "Loss for  4759 th iteration => 0.0879665106836\n",
      "Loss for  4760 th iteration => 0.0879428934145\n",
      "Loss for  4761 th iteration => 0.0879218132029\n",
      "Loss for  4762 th iteration => 0.0879033936894\n",
      "Loss for  4763 th iteration => 0.0878786803003\n",
      "Loss for  4764 th iteration => 0.0878549409223\n",
      "Loss for  4765 th iteration => 0.0878325574434\n",
      "Loss for  4766 th iteration => 0.0878108801147\n",
      "Loss for  4767 th iteration => 0.0877882596626\n",
      "Loss for  4768 th iteration => 0.0877668490932\n",
      "Loss for  4769 th iteration => 0.0877440709403\n",
      "Loss for  4770 th iteration => 0.087724651342\n",
      "Loss for  4771 th iteration => 0.0877034572873\n",
      "Loss for  4772 th iteration => 0.0876791899043\n",
      "Loss for  4773 th iteration => 0.0876563637242\n",
      "Loss for  4774 th iteration => 0.0876349753942\n",
      "Loss for  4775 th iteration => 0.0876158410125\n",
      "Loss for  4776 th iteration => 0.0875916937839\n",
      "Loss for  4777 th iteration => 0.0875709288048\n",
      "Loss for  4778 th iteration => 0.0875506973412\n",
      "Loss for  4779 th iteration => 0.0875253047477\n",
      "Loss for  4780 th iteration => 0.0875029021533\n",
      "Loss for  4781 th iteration => 0.0874810916619\n",
      "Loss for  4782 th iteration => 0.0874635251724\n",
      "Loss for  4783 th iteration => 0.0874380827442\n",
      "Loss for  4784 th iteration => 0.0874157341327\n",
      "Loss for  4785 th iteration => 0.0873925980275\n",
      "Loss for  4786 th iteration => 0.0873721000935\n",
      "Loss for  4787 th iteration => 0.0873488789779\n",
      "Loss for  4788 th iteration => 0.0873303913938\n",
      "Loss for  4789 th iteration => 0.0873098613916\n",
      "Loss for  4790 th iteration => 0.0872853152894\n",
      "Loss for  4791 th iteration => 0.0872619840168\n",
      "Loss for  4792 th iteration => 0.0872410155028\n",
      "Loss for  4793 th iteration => 0.0872230626795\n",
      "Loss for  4794 th iteration => 0.0871986142042\n",
      "Loss for  4795 th iteration => 0.0871752031169\n",
      "Loss for  4796 th iteration => 0.087152999201\n",
      "Loss for  4797 th iteration => 0.087131730837\n",
      "Loss for  4798 th iteration => 0.0871093487651\n",
      "Loss for  4799 th iteration => 0.0870882894282\n",
      "Loss for  4800 th iteration => 0.0870658343712\n",
      "Loss for  4801 th iteration => 0.0870468545537\n",
      "Loss for  4802 th iteration => 0.0870257971724\n",
      "Loss for  4803 th iteration => 0.0870018089595\n",
      "Loss for  4804 th iteration => 0.0869793170977\n",
      "Loss for  4805 th iteration => 0.0869585457311\n",
      "Loss for  4806 th iteration => 0.0869393655343\n",
      "Loss for  4807 th iteration => 0.0869154475949\n",
      "Loss for  4808 th iteration => 0.0868929272883\n",
      "Loss for  4809 th iteration => 0.0868707183326\n",
      "Loss for  4810 th iteration => 0.0868496772647\n",
      "Loss for  4811 th iteration => 0.0868275015991\n",
      "Loss for  4812 th iteration => 0.0868086640729\n",
      "Loss for  4813 th iteration => 0.0867886661504\n",
      "Loss for  4814 th iteration => 0.0867636547855\n",
      "Loss for  4815 th iteration => 0.0867415351857\n",
      "Loss for  4816 th iteration => 0.0867204771348\n",
      "Loss for  4817 th iteration => 0.0867027762899\n",
      "Loss for  4818 th iteration => 0.0866777572955\n",
      "Loss for  4819 th iteration => 0.086655663409\n",
      "Loss for  4820 th iteration => 0.086632985912\n",
      "Loss for  4821 th iteration => 0.0866126829456\n",
      "Loss for  4822 th iteration => 0.0865898615236\n",
      "Loss for  4823 th iteration => 0.0865724409802\n",
      "Loss for  4824 th iteration => 0.0865514654532\n",
      "Loss for  4825 th iteration => 0.0865272011425\n",
      "Loss for  4826 th iteration => 0.0865042924022\n",
      "Loss for  4827 th iteration => 0.0864845895593\n",
      "Loss for  4828 th iteration => 0.0864659866497\n",
      "Loss for  4829 th iteration => 0.0864418076684\n",
      "Loss for  4830 th iteration => 0.0864188377241\n",
      "Loss for  4831 th iteration => 0.0863971577416\n",
      "Loss for  4832 th iteration => 0.0863760322896\n",
      "Loss for  4833 th iteration => 0.0863541961737\n",
      "Loss for  4834 th iteration => 0.0863332577674\n",
      "Loss for  4835 th iteration => 0.0863112835638\n",
      "Loss for  4836 th iteration => 0.0862905147095\n",
      "Loss for  4837 th iteration => 0.0862684598099\n",
      "Loss for  4838 th iteration => 0.0862503624534\n",
      "Loss for  4839 th iteration => 0.0862290884549\n",
      "Loss for  4840 th iteration => 0.0862054245389\n",
      "Loss for  4841 th iteration => 0.086183341038\n",
      "Loss for  4842 th iteration => 0.0861635937854\n",
      "Loss for  4843 th iteration => 0.0861440510355\n",
      "Loss for  4844 th iteration => 0.0861204539193\n",
      "Loss for  4845 th iteration => 0.0860983488408\n",
      "Loss for  4846 th iteration => 0.0860770979708\n",
      "Loss for  4847 th iteration => 0.086059137298\n",
      "Loss for  4848 th iteration => 0.0860356069135\n",
      "Loss for  4849 th iteration => 0.0860134858568\n",
      "Loss for  4850 th iteration => 0.0859912541655\n",
      "Loss for  4851 th iteration => 0.0859710026044\n",
      "Loss for  4852 th iteration => 0.0859487024359\n",
      "Loss for  4853 th iteration => 0.0859285532308\n",
      "Loss for  4854 th iteration => 0.0859062452152\n",
      "Loss for  4855 th iteration => 0.0858894885883\n",
      "Loss for  4856 th iteration => 0.0858681512293\n",
      "Loss for  4857 th iteration => 0.0858441271993\n",
      "Loss for  4858 th iteration => 0.0858218523415\n",
      "Loss for  4859 th iteration => 0.0858031696328\n",
      "Loss for  4860 th iteration => 0.0857838307586\n",
      "Loss for  4861 th iteration => 0.0857598262819\n",
      "Loss for  4862 th iteration => 0.0857375611476\n",
      "Loss for  4863 th iteration => 0.085717156923\n",
      "Loss for  4864 th iteration => 0.0856996104925\n",
      "Loss for  4865 th iteration => 0.0856756531669\n",
      "Loss for  4866 th iteration => 0.0856533788694\n",
      "Loss for  4867 th iteration => 0.08563146963\n",
      "Loss for  4868 th iteration => 0.0856112512734\n",
      "Loss for  4869 th iteration => 0.0855892032469\n",
      "Loss for  4870 th iteration => 0.0855691537285\n",
      "Loss for  4871 th iteration => 0.0855469828437\n",
      "Loss for  4872 th iteration => 0.0855270868702\n",
      "Loss for  4873 th iteration => 0.0855048074809\n",
      "Loss for  4874 th iteration => 0.0854850512753\n",
      "Loss for  4875 th iteration => 0.0854627082443\n",
      "Loss for  4876 th iteration => 0.085447472332\n",
      "Loss for  4877 th iteration => 0.0854252355847\n",
      "Loss for  4878 th iteration => 0.0854014595871\n",
      "Loss for  4879 th iteration => 0.0853790764204\n",
      "Loss for  4880 th iteration => 0.0853619379004\n",
      "Loss for  4881 th iteration => 0.0853416838846\n",
      "Loss for  4882 th iteration => 0.0853179680606\n",
      "Loss for  4883 th iteration => 0.0852956682626\n",
      "Loss for  4884 th iteration => 0.0852808254087\n",
      "Loss for  4885 th iteration => 0.0852614364017\n",
      "Loss for  4886 th iteration => 0.085235024525\n",
      "Loss for  4887 th iteration => 0.0852151797759\n",
      "Loss for  4888 th iteration => 0.0851954692494\n",
      "Loss for  4889 th iteration => 0.0851748411092\n",
      "Loss for  4890 th iteration => 0.0851511354701\n",
      "Loss for  4891 th iteration => 0.0851288552494\n",
      "Loss for  4892 th iteration => 0.0851119881019\n",
      "Loss for  4893 th iteration => 0.0850947385181\n",
      "Loss for  4894 th iteration => 0.0850684099233\n",
      "Loss for  4895 th iteration => 0.0850486981579\n",
      "Loss for  4896 th iteration => 0.0850269700877\n",
      "Loss for  4897 th iteration => 0.0850084915169\n",
      "Loss for  4898 th iteration => 0.0849847715529\n",
      "Loss for  4899 th iteration => 0.0849627538916\n",
      "Loss for  4900 th iteration => 0.0849447937296\n",
      "Loss for  4901 th iteration => 0.0849235952266\n",
      "Loss for  4902 th iteration => 0.0848997464962\n",
      "Loss for  4903 th iteration => 0.0848824174445\n",
      "Loss for  4904 th iteration => 0.0848607111191\n",
      "Loss for  4905 th iteration => 0.0848377743569\n",
      "Loss for  4906 th iteration => 0.0848164552283\n",
      "Loss for  4907 th iteration => 0.0848008525939\n",
      "Loss for  4908 th iteration => 0.0847772953095\n",
      "Loss for  4909 th iteration => 0.0847551864043\n",
      "Loss for  4910 th iteration => 0.0847375059487\n",
      "Loss for  4911 th iteration => 0.0847163452779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  4912 th iteration => 0.0846926662924\n",
      "Loss for  4913 th iteration => 0.0846752961224\n",
      "Loss for  4914 th iteration => 0.0846537584081\n",
      "Loss for  4915 th iteration => 0.0846308307132\n",
      "Loss for  4916 th iteration => 0.0846098145\n",
      "Loss for  4917 th iteration => 0.0845940758256\n",
      "Loss for  4918 th iteration => 0.084570662692\n",
      "Loss for  4919 th iteration => 0.0845486041099\n",
      "Loss for  4920 th iteration => 0.0845311631588\n",
      "Loss for  4921 th iteration => 0.0845098771161\n",
      "Loss for  4922 th iteration => 0.0844866038905\n",
      "Loss for  4923 th iteration => 0.0844690488172\n",
      "Loss for  4924 th iteration => 0.0844477732287\n",
      "Loss for  4925 th iteration => 0.0844293197774\n",
      "Loss for  4926 th iteration => 0.0844060024129\n",
      "Loss for  4927 th iteration => 0.0843840016751\n",
      "Loss for  4928 th iteration => 0.0843668422156\n",
      "Loss for  4929 th iteration => 0.0843487425258\n",
      "Loss for  4930 th iteration => 0.0843245303424\n",
      "Loss for  4931 th iteration => 0.0843061785556\n",
      "Loss for  4932 th iteration => 0.0842828438001\n",
      "Loss for  4933 th iteration => 0.0842609680977\n",
      "Loss for  4934 th iteration => 0.0842439578687\n",
      "Loss for  4935 th iteration => 0.0842255898976\n",
      "Loss for  4936 th iteration => 0.084201628286\n",
      "Loss for  4937 th iteration => 0.0841833524029\n",
      "Loss for  4938 th iteration => 0.0841599979106\n",
      "Loss for  4939 th iteration => 0.0841382709783\n",
      "Loss for  4940 th iteration => 0.0841214238783\n",
      "Loss for  4941 th iteration => 0.0840992833161\n",
      "Loss for  4942 th iteration => 0.0840772687151\n",
      "Loss for  4943 th iteration => 0.0840593849409\n",
      "Loss for  4944 th iteration => 0.0840383584894\n",
      "Loss for  4945 th iteration => 0.0840199987815\n",
      "Loss for  4946 th iteration => 0.0839967252629\n",
      "Loss for  4947 th iteration => 0.0839751644101\n",
      "Loss for  4948 th iteration => 0.0839586256473\n",
      "Loss for  4949 th iteration => 0.0839394696654\n",
      "Loss for  4950 th iteration => 0.0839162250237\n",
      "Loss for  4951 th iteration => 0.083897914333\n",
      "Loss for  4952 th iteration => 0.0838746175223\n",
      "Loss for  4953 th iteration => 0.0838532334853\n",
      "Loss for  4954 th iteration => 0.0838368525642\n",
      "Loss for  4955 th iteration => 0.0838173466989\n",
      "Loss for  4956 th iteration => 0.0837944189871\n",
      "Loss for  4957 th iteration => 0.0837761458974\n",
      "Loss for  4958 th iteration => 0.0837528249052\n",
      "Loss for  4959 th iteration => 0.0837316284643\n",
      "Loss for  4960 th iteration => 0.0837154068399\n",
      "Loss for  4961 th iteration => 0.0836955346687\n",
      "Loss for  4962 th iteration => 0.0836729371964\n",
      "Loss for  4963 th iteration => 0.0836546933548\n",
      "Loss for  4964 th iteration => 0.0836313475997\n",
      "Loss for  4965 th iteration => 0.0836103467613\n",
      "Loss for  4966 th iteration => 0.0835942848232\n",
      "Loss for  4967 th iteration => 0.0835740351458\n",
      "Loss for  4968 th iteration => 0.0835517769962\n",
      "Loss for  4969 th iteration => 0.0835335563177\n",
      "Loss for  4970 th iteration => 0.0835101854472\n",
      "Loss for  4971 th iteration => 0.083489386152\n",
      "Loss for  4972 th iteration => 0.0834734835133\n",
      "Loss for  4973 th iteration => 0.0834528489938\n",
      "Loss for  4974 th iteration => 0.0834309361229\n",
      "Loss for  4975 th iteration => 0.0834127477321\n",
      "Loss for  4976 th iteration => 0.0833936211432\n",
      "Loss for  4977 th iteration => 0.0833688003484\n",
      "Loss for  4978 th iteration => 0.0833497403413\n",
      "Loss for  4979 th iteration => 0.0833355319503\n",
      "Loss for  4980 th iteration => 0.0833101844418\n",
      "Loss for  4981 th iteration => 0.0832882290665\n",
      "Loss for  4982 th iteration => 0.0832730879054\n",
      "Loss for  4983 th iteration => 0.0832516400792\n",
      "Loss for  4984 th iteration => 0.0832294087763\n",
      "Loss for  4985 th iteration => 0.083208557082\n",
      "Loss for  4986 th iteration => 0.0831923321729\n",
      "Loss for  4987 th iteration => 0.0831684728175\n",
      "Loss for  4988 th iteration => 0.0831485557438\n",
      "Loss for  4989 th iteration => 0.0831315822892\n",
      "Loss for  4990 th iteration => 0.0831087682683\n",
      "Loss for  4991 th iteration => 0.0830885415045\n",
      "Loss for  4992 th iteration => 0.083068111078\n",
      "Loss for  4993 th iteration => 0.0830518273893\n",
      "Loss for  4994 th iteration => 0.0830288601915\n",
      "Loss for  4995 th iteration => 0.0830087046771\n",
      "Loss for  4996 th iteration => 0.0829882517154\n",
      "Loss for  4997 th iteration => 0.0829687455852\n",
      "Loss for  4998 th iteration => 0.0829484590859\n",
      "Loss for  4999 th iteration => 0.0829321208217\n",
      "Loss for  5000 th iteration => 0.0829092252036\n",
      "Loss for  5001 th iteration => 0.0828891591837\n",
      "Loss for  5002 th iteration => 0.0828732434918\n",
      "Loss for  5003 th iteration => 0.0828503228054\n",
      "Loss for  5004 th iteration => 0.0828297009442\n",
      "Loss for  5005 th iteration => 0.0828124370901\n",
      "Loss for  5006 th iteration => 0.0827902218575\n",
      "Loss for  5007 th iteration => 0.0827696780013\n",
      "Loss for  5008 th iteration => 0.0827545292435\n",
      "Loss for  5009 th iteration => 0.0827306545972\n",
      "Loss for  5010 th iteration => 0.0827114220343\n",
      "Loss for  5011 th iteration => 0.0826973724122\n",
      "Loss for  5012 th iteration => 0.0826724082454\n",
      "Loss for  5013 th iteration => 0.0826510276142\n",
      "Loss for  5014 th iteration => 0.0826361141746\n",
      "Loss for  5015 th iteration => 0.0826157840538\n",
      "Loss for  5016 th iteration => 0.0825926781405\n",
      "Loss for  5017 th iteration => 0.0825725401804\n",
      "Loss for  5018 th iteration => 0.0825566121433\n",
      "Loss for  5019 th iteration => 0.0825326188147\n",
      "Loss for  5020 th iteration => 0.0825134997658\n",
      "Loss for  5021 th iteration => 0.0824971817933\n",
      "Loss for  5022 th iteration => 0.082473857778\n",
      "Loss for  5023 th iteration => 0.0824535150252\n",
      "Loss for  5024 th iteration => 0.0824347399812\n",
      "Loss for  5025 th iteration => 0.0824181433912\n",
      "Loss for  5026 th iteration => 0.0823951339078\n",
      "Loss for  5027 th iteration => 0.0823746248641\n",
      "Loss for  5028 th iteration => 0.0823556396428\n",
      "Loss for  5029 th iteration => 0.0823354522908\n",
      "Loss for  5030 th iteration => 0.0823195135776\n",
      "Loss for  5031 th iteration => 0.0822968394819\n",
      "Loss for  5032 th iteration => 0.0822770053337\n",
      "Loss for  5033 th iteration => 0.0822568847983\n",
      "Loss for  5034 th iteration => 0.0822376523464\n",
      "Loss for  5035 th iteration => 0.0822179284502\n",
      "Loss for  5036 th iteration => 0.0822016137124\n",
      "Loss for  5037 th iteration => 0.0821790056938\n",
      "Loss for  5038 th iteration => 0.0821596110303\n",
      "Loss for  5039 th iteration => 0.0821437817775\n",
      "Loss for  5040 th iteration => 0.0821207938336\n",
      "Loss for  5041 th iteration => 0.0821012247006\n",
      "Loss for  5042 th iteration => 0.0820837664375\n",
      "Loss for  5043 th iteration => 0.0820663451489\n",
      "Loss for  5044 th iteration => 0.0820421437048\n",
      "Loss for  5045 th iteration => 0.0820234211801\n",
      "Loss for  5046 th iteration => 0.0820065066506\n",
      "Loss for  5047 th iteration => 0.0819844070036\n",
      "Loss for  5048 th iteration => 0.0819646041706\n",
      "Loss for  5049 th iteration => 0.0819490393772\n",
      "Loss for  5050 th iteration => 0.0819293002898\n",
      "Loss for  5051 th iteration => 0.0819060914558\n",
      "Loss for  5052 th iteration => 0.0818860725653\n",
      "Loss for  5053 th iteration => 0.0818671618487\n",
      "Loss for  5054 th iteration => 0.0818511331527\n",
      "Loss for  5055 th iteration => 0.0818282838201\n",
      "Loss for  5056 th iteration => 0.0818080619043\n",
      "Loss for  5057 th iteration => 0.081789560992\n",
      "Loss for  5058 th iteration => 0.0817732848562\n",
      "Loss for  5059 th iteration => 0.0817539927387\n",
      "Loss for  5060 th iteration => 0.0817309424493\n",
      "Loss for  5061 th iteration => 0.0817119344773\n",
      "Loss for  5062 th iteration => 0.0816916291111\n",
      "Loss for  5063 th iteration => 0.0816763165683\n",
      "Loss for  5064 th iteration => 0.0816533127382\n",
      "Loss for  5065 th iteration => 0.081634855288\n",
      "Loss for  5066 th iteration => 0.0816185279605\n",
      "Loss for  5067 th iteration => 0.0815996295187\n",
      "Loss for  5068 th iteration => 0.0815761352238\n",
      "Loss for  5069 th iteration => 0.0815573083593\n",
      "Loss for  5070 th iteration => 0.0815416747924\n",
      "Loss for  5071 th iteration => 0.0815216098526\n",
      "Loss for  5072 th iteration => 0.0814991269544\n",
      "Loss for  5073 th iteration => 0.0814798938114\n",
      "Loss for  5074 th iteration => 0.0814651008899\n",
      "Loss for  5075 th iteration => 0.0814449879955\n",
      "Loss for  5076 th iteration => 0.0814226643605\n",
      "Loss for  5077 th iteration => 0.0814021231439\n",
      "Loss for  5078 th iteration => 0.0813843228823\n",
      "Loss for  5079 th iteration => 0.0813675493893\n",
      "Loss for  5080 th iteration => 0.0813456206599\n",
      "Loss for  5081 th iteration => 0.0813249743147\n",
      "Loss for  5082 th iteration => 0.0813069734599\n",
      "Loss for  5083 th iteration => 0.0812866599349\n",
      "Loss for  5084 th iteration => 0.0812683815997\n",
      "Loss for  5085 th iteration => 0.0812483675557\n",
      "Loss for  5086 th iteration => 0.0812298422152\n",
      "Loss for  5087 th iteration => 0.0812102005543\n",
      "Loss for  5088 th iteration => 0.0811946043541\n",
      "Loss for  5089 th iteration => 0.0811723454646\n",
      "Loss for  5090 th iteration => 0.0811536928452\n",
      "Loss for  5091 th iteration => 0.0811379122482\n",
      "Loss for  5092 th iteration => 0.0811185819219\n",
      "Loss for  5093 th iteration => 0.0810959710251\n",
      "Loss for  5094 th iteration => 0.0810769921965\n",
      "Loss for  5095 th iteration => 0.0810621941217\n",
      "Loss for  5096 th iteration => 0.081045743673\n",
      "Loss for  5097 th iteration => 0.0810204964691\n",
      "Loss for  5098 th iteration => 0.0809995913457\n",
      "Loss for  5099 th iteration => 0.0809826630296\n",
      "Loss for  5100 th iteration => 0.0809653190363\n",
      "Loss for  5101 th iteration => 0.0809440840053\n",
      "Loss for  5102 th iteration => 0.0809230962099\n",
      "Loss for  5103 th iteration => 0.080905755396\n",
      "Loss for  5104 th iteration => 0.0808851337579\n",
      "Loss for  5105 th iteration => 0.0808674845769\n",
      "Loss for  5106 th iteration => 0.0808471925985\n",
      "Loss for  5107 th iteration => 0.0808292682875\n",
      "Loss for  5108 th iteration => 0.0808092731349\n",
      "Loss for  5109 th iteration => 0.0807915190431\n",
      "Loss for  5110 th iteration => 0.0807757419457\n",
      "Loss for  5111 th iteration => 0.0807537752985\n",
      "Loss for  5112 th iteration => 0.0807343998037\n",
      "Loss for  5113 th iteration => 0.0807182029837\n",
      "Loss for  5114 th iteration => 0.0807009509542\n",
      "Loss for  5115 th iteration => 0.080681334343\n",
      "Loss for  5116 th iteration => 0.0806592395715\n",
      "Loss for  5117 th iteration => 0.0806402476504\n",
      "Loss for  5118 th iteration => 0.0806249629596\n",
      "Loss for  5119 th iteration => 0.0806054899286\n",
      "Loss for  5120 th iteration => 0.0805832038196\n",
      "Loss for  5121 th iteration => 0.080563368463\n",
      "Loss for  5122 th iteration => 0.080545954599\n",
      "Loss for  5123 th iteration => 0.0805294303857\n",
      "Loss for  5124 th iteration => 0.0805075561887\n",
      "Loss for  5125 th iteration => 0.0804879079973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  5126 th iteration => 0.0804728858708\n",
      "Loss for  5127 th iteration => 0.0804508206151\n",
      "Loss for  5128 th iteration => 0.0804319283208\n",
      "Loss for  5129 th iteration => 0.080412755878\n",
      "Loss for  5130 th iteration => 0.0803975750456\n",
      "Loss for  5131 th iteration => 0.0803797837981\n",
      "Loss for  5132 th iteration => 0.0803569699295\n",
      "Loss for  5133 th iteration => 0.0803383679104\n",
      "Loss for  5134 th iteration => 0.0803220953942\n",
      "Loss for  5135 th iteration => 0.0803048536295\n",
      "Loss for  5136 th iteration => 0.080285423027\n",
      "Loss for  5137 th iteration => 0.080263515346\n",
      "Loss for  5138 th iteration => 0.0802434951979\n",
      "Loss for  5139 th iteration => 0.080226204289\n",
      "Loss for  5140 th iteration => 0.0802097911244\n",
      "Loss for  5141 th iteration => 0.0801883400988\n",
      "Loss for  5142 th iteration => 0.0801682674684\n",
      "Loss for  5143 th iteration => 0.0801506365354\n",
      "Loss for  5144 th iteration => 0.0801309469224\n",
      "Loss for  5145 th iteration => 0.0801129921638\n",
      "Loss for  5146 th iteration => 0.0800938844904\n",
      "Loss for  5147 th iteration => 0.080078719617\n",
      "Loss for  5148 th iteration => 0.0800612855645\n",
      "Loss for  5149 th iteration => 0.0800387350536\n",
      "Loss for  5150 th iteration => 0.0800201178511\n",
      "Loss for  5151 th iteration => 0.0800038334051\n",
      "Loss for  5152 th iteration => 0.0799868414572\n",
      "Loss for  5153 th iteration => 0.0799676285494\n",
      "Loss for  5154 th iteration => 0.0799457304463\n",
      "Loss for  5155 th iteration => 0.0799259546018\n",
      "Loss for  5156 th iteration => 0.0799087007358\n",
      "Loss for  5157 th iteration => 0.0798924865367\n",
      "Loss for  5158 th iteration => 0.0798710631363\n",
      "Loss for  5159 th iteration => 0.0798512414465\n",
      "Loss for  5160 th iteration => 0.0798336164768\n",
      "Loss for  5161 th iteration => 0.0798141907322\n",
      "Loss for  5162 th iteration => 0.0797962301189\n",
      "Loss for  5163 th iteration => 0.0797775844355\n",
      "Loss for  5164 th iteration => 0.0797623281593\n",
      "Loss for  5165 th iteration => 0.0797449316513\n",
      "Loss for  5166 th iteration => 0.0797224532481\n",
      "Loss for  5167 th iteration => 0.0797033863922\n",
      "Loss for  5168 th iteration => 0.0796853597029\n",
      "Loss for  5169 th iteration => 0.0796708288285\n",
      "Loss for  5170 th iteration => 0.0796520879482\n",
      "Loss for  5171 th iteration => 0.0796299064649\n",
      "Loss for  5172 th iteration => 0.0796107188953\n",
      "Loss for  5173 th iteration => 0.0795933418983\n",
      "Loss for  5174 th iteration => 0.0795774892355\n",
      "Loss for  5175 th iteration => 0.0795557780106\n",
      "Loss for  5176 th iteration => 0.0795370965932\n",
      "Loss for  5177 th iteration => 0.0795218445456\n",
      "Loss for  5178 th iteration => 0.079500269473\n",
      "Loss for  5179 th iteration => 0.0794817125933\n",
      "Loss for  5180 th iteration => 0.0794680167118\n",
      "Loss for  5181 th iteration => 0.0794489122744\n",
      "Loss for  5182 th iteration => 0.0794265247533\n",
      "Loss for  5183 th iteration => 0.0794083663503\n",
      "Loss for  5184 th iteration => 0.0793939820054\n",
      "Loss for  5185 th iteration => 0.0793755528395\n",
      "Loss for  5186 th iteration => 0.0793535563323\n",
      "Loss for  5187 th iteration => 0.0793343874374\n",
      "Loss for  5188 th iteration => 0.0793169928024\n",
      "Loss for  5189 th iteration => 0.0793013605652\n",
      "Loss for  5190 th iteration => 0.0792798556701\n",
      "Loss for  5191 th iteration => 0.0792610663915\n",
      "Loss for  5192 th iteration => 0.0792461337329\n",
      "Loss for  5193 th iteration => 0.0792245943415\n",
      "Loss for  5194 th iteration => 0.0792062999888\n",
      "Loss for  5195 th iteration => 0.0791924937218\n",
      "Loss for  5196 th iteration => 0.0791735301375\n",
      "Loss for  5197 th iteration => 0.0791512841532\n",
      "Loss for  5198 th iteration => 0.0791334538141\n",
      "Loss for  5199 th iteration => 0.0791189928553\n",
      "Loss for  5200 th iteration => 0.0791005919976\n",
      "Loss for  5201 th iteration => 0.0790788463227\n",
      "Loss for  5202 th iteration => 0.0790596582512\n",
      "Loss for  5203 th iteration => 0.0790426676066\n",
      "Loss for  5204 th iteration => 0.0790268351285\n",
      "Loss for  5205 th iteration => 0.0790055877846\n",
      "Loss for  5206 th iteration => 0.0789869442183\n",
      "Loss for  5207 th iteration => 0.0789720760716\n",
      "Loss for  5208 th iteration => 0.0789505734182\n",
      "Loss for  5209 th iteration => 0.078932707197\n",
      "Loss for  5210 th iteration => 0.0789188383706\n",
      "Loss for  5211 th iteration => 0.0788996834275\n",
      "Loss for  5212 th iteration => 0.0788777079241\n",
      "Loss for  5213 th iteration => 0.078860424411\n",
      "Loss for  5214 th iteration => 0.0788457827099\n",
      "Loss for  5215 th iteration => 0.078827273298\n",
      "Loss for  5216 th iteration => 0.0788058062266\n",
      "Loss for  5217 th iteration => 0.0787865838726\n",
      "Loss for  5218 th iteration => 0.0787701960521\n",
      "Loss for  5219 th iteration => 0.0787539641733\n",
      "Loss for  5220 th iteration => 0.0787329950564\n",
      "Loss for  5221 th iteration => 0.0787146227117\n",
      "Loss for  5222 th iteration => 0.0786996920935\n",
      "Loss for  5223 th iteration => 0.0786782282609\n",
      "Loss for  5224 th iteration => 0.078660866044\n",
      "Loss for  5225 th iteration => 0.0786469621418\n",
      "Loss for  5226 th iteration => 0.0786274558983\n",
      "Loss for  5227 th iteration => 0.0786059551078\n",
      "Loss for  5228 th iteration => 0.0785876721103\n",
      "Loss for  5229 th iteration => 0.0785708366211\n",
      "Loss for  5230 th iteration => 0.0785551856507\n",
      "Loss for  5231 th iteration => 0.0785346226947\n",
      "Loss for  5232 th iteration => 0.0785148960581\n",
      "Loss for  5233 th iteration => 0.078498235308\n",
      "Loss for  5234 th iteration => 0.0784790587151\n",
      "Loss for  5235 th iteration => 0.0784654024801\n",
      "Loss for  5236 th iteration => 0.078448211643\n",
      "Loss for  5237 th iteration => 0.078426521459\n",
      "Loss for  5238 th iteration => 0.0784082902328\n",
      "Loss for  5239 th iteration => 0.0783934054975\n",
      "Loss for  5240 th iteration => 0.0783766324301\n",
      "Loss for  5241 th iteration => 0.0783576840468\n",
      "Loss for  5242 th iteration => 0.0783367874366\n",
      "Loss for  5243 th iteration => 0.0783173474534\n",
      "Loss for  5244 th iteration => 0.0783018335806\n",
      "Loss for  5245 th iteration => 0.0782850685241\n",
      "Loss for  5246 th iteration => 0.0782646984424\n",
      "Loss for  5247 th iteration => 0.0782455735612\n",
      "Loss for  5248 th iteration => 0.078228619231\n",
      "Loss for  5249 th iteration => 0.0782147416122\n",
      "Loss for  5250 th iteration => 0.0781967211362\n",
      "Loss for  5251 th iteration => 0.0781747024536\n",
      "Loss for  5252 th iteration => 0.0781574125985\n",
      "Loss for  5253 th iteration => 0.0781432459281\n",
      "Loss for  5254 th iteration => 0.0781250790994\n",
      "Loss for  5255 th iteration => 0.0781039281902\n",
      "Loss for  5256 th iteration => 0.078084939181\n",
      "Loss for  5257 th iteration => 0.0780688723985\n",
      "Loss for  5258 th iteration => 0.0780528311016\n",
      "Loss for  5259 th iteration => 0.0780322152328\n",
      "Loss for  5260 th iteration => 0.0780141954795\n",
      "Loss for  5261 th iteration => 0.077999444081\n",
      "Loss for  5262 th iteration => 0.0779782561845\n",
      "Loss for  5263 th iteration => 0.0779615245501\n",
      "Loss for  5264 th iteration => 0.0779466447921\n",
      "Loss for  5265 th iteration => 0.0779253154599\n",
      "Loss for  5266 th iteration => 0.0779071738559\n",
      "Loss for  5267 th iteration => 0.077893304098\n",
      "Loss for  5268 th iteration => 0.0778724534165\n",
      "Loss for  5269 th iteration => 0.0778535423644\n",
      "Loss for  5270 th iteration => 0.0778404545922\n",
      "Loss for  5271 th iteration => 0.0778225102928\n",
      "Loss for  5272 th iteration => 0.0778017200442\n",
      "Loss for  5273 th iteration => 0.0777871681393\n",
      "Loss for  5274 th iteration => 0.0777696905462\n",
      "Loss for  5275 th iteration => 0.0777473235295\n",
      "Loss for  5276 th iteration => 0.0777313343856\n",
      "Loss for  5277 th iteration => 0.0777158813087\n",
      "Loss for  5278 th iteration => 0.0776945827491\n",
      "Loss for  5279 th iteration => 0.0776772635213\n",
      "Loss for  5280 th iteration => 0.0776627440933\n",
      "Loss for  5281 th iteration => 0.0776421776621\n",
      "Loss for  5282 th iteration => 0.0776234150916\n",
      "Loss for  5283 th iteration => 0.0776096954764\n",
      "Loss for  5284 th iteration => 0.0775886277461\n",
      "Loss for  5285 th iteration => 0.0775716933441\n",
      "Loss for  5286 th iteration => 0.0775577709431\n",
      "Loss for  5287 th iteration => 0.0775392835056\n",
      "Loss for  5288 th iteration => 0.0775179722635\n",
      "Loss for  5289 th iteration => 0.0775042345313\n",
      "Loss for  5290 th iteration => 0.0774840196282\n",
      "Loss for  5291 th iteration => 0.0774645874652\n",
      "Loss for  5292 th iteration => 0.0774489770479\n",
      "Loss for  5293 th iteration => 0.0774329300369\n",
      "Loss for  5294 th iteration => 0.0774134672494\n",
      "Loss for  5295 th iteration => 0.0773986947563\n",
      "Loss for  5296 th iteration => 0.077382087142\n",
      "Loss for  5297 th iteration => 0.0773597000542\n",
      "Loss for  5298 th iteration => 0.0773435577063\n",
      "Loss for  5299 th iteration => 0.077328541762\n",
      "Loss for  5300 th iteration => 0.077307284394\n",
      "Loss for  5301 th iteration => 0.0772901858059\n",
      "Loss for  5302 th iteration => 0.0772757224838\n",
      "Loss for  5303 th iteration => 0.0772554474364\n",
      "Loss for  5304 th iteration => 0.0772366101658\n",
      "Loss for  5305 th iteration => 0.0772232540428\n",
      "Loss for  5306 th iteration => 0.0772023432762\n",
      "Loss for  5307 th iteration => 0.0771857028188\n",
      "Loss for  5308 th iteration => 0.0771716449475\n",
      "Loss for  5309 th iteration => 0.0771533566678\n",
      "Loss for  5310 th iteration => 0.077132568018\n",
      "Loss for  5311 th iteration => 0.0771185345913\n",
      "Loss for  5312 th iteration => 0.0770986516894\n",
      "Loss for  5313 th iteration => 0.0770792080652\n",
      "Loss for  5314 th iteration => 0.0770642616694\n",
      "Loss for  5315 th iteration => 0.0770478303611\n",
      "Loss for  5316 th iteration => 0.0770289811502\n",
      "Loss for  5317 th iteration => 0.0770141209429\n",
      "Loss for  5318 th iteration => 0.0769974182952\n",
      "Loss for  5319 th iteration => 0.0769753879621\n",
      "Loss for  5320 th iteration => 0.076959745245\n",
      "Loss for  5321 th iteration => 0.0769445092945\n",
      "Loss for  5322 th iteration => 0.0769232624521\n",
      "Loss for  5323 th iteration => 0.0769070257618\n",
      "Loss for  5324 th iteration => 0.076891980384\n",
      "Loss for  5325 th iteration => 0.0768720728288\n",
      "Loss for  5326 th iteration => 0.0768535584336\n",
      "Loss for  5327 th iteration => 0.0768402135373\n",
      "Loss for  5328 th iteration => 0.076823795069\n",
      "Loss for  5329 th iteration => 0.0768029540607\n",
      "Loss for  5330 th iteration => 0.076785730974\n",
      "Loss for  5331 th iteration => 0.0767710678192\n",
      "Loss for  5332 th iteration => 0.0767547276941\n",
      "Loss for  5333 th iteration => 0.0767365854199\n",
      "Loss for  5334 th iteration => 0.0767163654459\n",
      "Loss for  5335 th iteration => 0.0766976759737\n",
      "Loss for  5336 th iteration => 0.0766813749961\n",
      "Loss for  5337 th iteration => 0.0766635196002\n",
      "Loss for  5338 th iteration => 0.0766496141462\n",
      "Loss for  5339 th iteration => 0.0766295238645\n",
      "Loss for  5340 th iteration => 0.0766126804221\n",
      "Loss for  5341 th iteration => 0.0765997619204\n",
      "Loss for  5342 th iteration => 0.0765846008611\n",
      "Loss for  5343 th iteration => 0.0765614963696\n",
      "Loss for  5344 th iteration => 0.0765423755512\n",
      "Loss for  5345 th iteration => 0.0765280522791\n",
      "Loss for  5346 th iteration => 0.0765113746798\n",
      "Loss for  5347 th iteration => 0.0764921529483\n",
      "Loss for  5348 th iteration => 0.0764735998447\n",
      "Loss for  5349 th iteration => 0.0764613043182\n",
      "Loss for  5350 th iteration => 0.0764442917445\n",
      "Loss for  5351 th iteration => 0.076423092849\n",
      "Loss for  5352 th iteration => 0.0764070719503\n",
      "Loss for  5353 th iteration => 0.076394949229\n",
      "Loss for  5354 th iteration => 0.0763726669467\n",
      "Loss for  5355 th iteration => 0.0763531368829\n",
      "Loss for  5356 th iteration => 0.0763379331645\n",
      "Loss for  5357 th iteration => 0.0763190111604\n",
      "Loss for  5358 th iteration => 0.0763040903708\n",
      "Loss for  5359 th iteration => 0.0762891512615\n",
      "Loss for  5360 th iteration => 0.0762724426847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  5361 th iteration => 0.0762522964957\n",
      "Loss for  5362 th iteration => 0.0762378707209\n",
      "Loss for  5363 th iteration => 0.076219805349\n",
      "Loss for  5364 th iteration => 0.0762017544184\n",
      "Loss for  5365 th iteration => 0.0761859553495\n",
      "Loss for  5366 th iteration => 0.0761682888508\n",
      "Loss for  5367 th iteration => 0.0761511944578\n",
      "Loss for  5368 th iteration => 0.0761350725121\n",
      "Loss for  5369 th iteration => 0.0761161441386\n",
      "Loss for  5370 th iteration => 0.0760976447663\n",
      "Loss for  5371 th iteration => 0.0760840063502\n",
      "Loss for  5372 th iteration => 0.0760652188256\n",
      "Loss for  5373 th iteration => 0.0760497746018\n",
      "Loss for  5374 th iteration => 0.0760314823906\n",
      "Loss for  5375 th iteration => 0.0760157829857\n",
      "Loss for  5376 th iteration => 0.075998278749\n",
      "Loss for  5377 th iteration => 0.0759780991343\n",
      "Loss for  5378 th iteration => 0.0759635059438\n",
      "Loss for  5379 th iteration => 0.0759487221415\n",
      "Loss for  5380 th iteration => 0.0759313154237\n",
      "Loss for  5381 th iteration => 0.0759124694338\n",
      "Loss for  5382 th iteration => 0.0758972081601\n",
      "Loss for  5383 th iteration => 0.0758783882588\n",
      "Loss for  5384 th iteration => 0.075859167931\n",
      "Loss for  5385 th iteration => 0.0758443596757\n",
      "Loss for  5386 th iteration => 0.0758296656669\n",
      "Loss for  5387 th iteration => 0.075810740137\n",
      "Loss for  5388 th iteration => 0.0757927068668\n",
      "Loss for  5389 th iteration => 0.0757837635205\n",
      "Loss for  5390 th iteration => 0.0757642079146\n",
      "Loss for  5391 th iteration => 0.075741466109\n",
      "Loss for  5392 th iteration => 0.0757269105992\n",
      "Loss for  5393 th iteration => 0.0757110241293\n",
      "Loss for  5394 th iteration => 0.0756919148773\n",
      "Loss for  5395 th iteration => 0.0756737514349\n",
      "Loss for  5396 th iteration => 0.0756582966469\n",
      "Loss for  5397 th iteration => 0.0756450647389\n",
      "Loss for  5398 th iteration => 0.0756274615808\n",
      "Loss for  5399 th iteration => 0.0756076758923\n",
      "Loss for  5400 th iteration => 0.0755937159732\n",
      "Loss for  5401 th iteration => 0.0755744351372\n",
      "Loss for  5402 th iteration => 0.0755557178029\n",
      "Loss for  5403 th iteration => 0.0755403233248\n",
      "Loss for  5404 th iteration => 0.0755223784715\n",
      "Loss for  5405 th iteration => 0.0755102230325\n",
      "Loss for  5406 th iteration => 0.0754937856808\n",
      "Loss for  5407 th iteration => 0.0754728308819\n",
      "Loss for  5408 th iteration => 0.0754578981482\n",
      "Loss for  5409 th iteration => 0.075445278766\n",
      "Loss for  5410 th iteration => 0.0754234178439\n",
      "Loss for  5411 th iteration => 0.0754046129462\n",
      "Loss for  5412 th iteration => 0.0753896010609\n",
      "Loss for  5413 th iteration => 0.0753754596211\n",
      "Loss for  5414 th iteration => 0.0753566050506\n",
      "Loss for  5415 th iteration => 0.0753391753694\n",
      "Loss for  5416 th iteration => 0.0753297483226\n",
      "Loss for  5417 th iteration => 0.0753105202457\n",
      "Loss for  5418 th iteration => 0.0752880561357\n",
      "Loss for  5419 th iteration => 0.0752739286808\n",
      "Loss for  5420 th iteration => 0.0752579347667\n",
      "Loss for  5421 th iteration => 0.0752388729949\n",
      "Loss for  5422 th iteration => 0.075221416163\n",
      "Loss for  5423 th iteration => 0.0752087859513\n",
      "Loss for  5424 th iteration => 0.0751928874639\n",
      "Loss for  5425 th iteration => 0.0751718939978\n",
      "Loss for  5426 th iteration => 0.0751575054124\n",
      "Loss for  5427 th iteration => 0.0751445270501\n",
      "Loss for  5428 th iteration => 0.075122779489\n",
      "Loss for  5429 th iteration => 0.0751044791583\n",
      "Loss for  5430 th iteration => 0.0750890855012\n",
      "Loss for  5431 th iteration => 0.0750755493201\n",
      "Loss for  5432 th iteration => 0.0750564052531\n",
      "Loss for  5433 th iteration => 0.07503980851\n",
      "Loss for  5434 th iteration => 0.075029708105\n",
      "Loss for  5435 th iteration => 0.0750106271923\n",
      "Loss for  5436 th iteration => 0.0749886272586\n",
      "Loss for  5437 th iteration => 0.0749727918314\n",
      "Loss for  5438 th iteration => 0.0749555993284\n",
      "Loss for  5439 th iteration => 0.0749391625709\n",
      "Loss for  5440 th iteration => 0.07492292474\n",
      "Loss for  5441 th iteration => 0.0749091989976\n",
      "Loss for  5442 th iteration => 0.0748951509707\n",
      "Loss for  5443 th iteration => 0.0748768717633\n",
      "Loss for  5444 th iteration => 0.074857237577\n",
      "Loss for  5445 th iteration => 0.074839660286\n",
      "Loss for  5446 th iteration => 0.0748236568215\n",
      "Loss for  5447 th iteration => 0.0748067506541\n",
      "Loss for  5448 th iteration => 0.0747901664953\n",
      "Loss for  5449 th iteration => 0.0747749041953\n",
      "Loss for  5450 th iteration => 0.0747647796538\n",
      "Loss for  5451 th iteration => 0.0747464534221\n",
      "Loss for  5452 th iteration => 0.0747242740113\n",
      "Loss for  5453 th iteration => 0.0747101448523\n",
      "Loss for  5454 th iteration => 0.0746945449132\n",
      "Loss for  5455 th iteration => 0.0746753145823\n",
      "Loss for  5456 th iteration => 0.0746588326965\n",
      "Loss for  5457 th iteration => 0.0746455462057\n",
      "Loss for  5458 th iteration => 0.07463031876\n",
      "Loss for  5459 th iteration => 0.0746135853865\n",
      "Loss for  5460 th iteration => 0.07459386697\n",
      "Loss for  5461 th iteration => 0.0745761579389\n",
      "Loss for  5462 th iteration => 0.0745618785723\n",
      "Loss for  5463 th iteration => 0.0745466744223\n",
      "Loss for  5464 th iteration => 0.0745308763073\n",
      "Loss for  5465 th iteration => 0.0745154525462\n",
      "Loss for  5466 th iteration => 0.0744953447566\n",
      "Loss for  5467 th iteration => 0.0744795351178\n",
      "Loss for  5468 th iteration => 0.0744696008483\n",
      "Loss for  5469 th iteration => 0.0744501344309\n",
      "Loss for  5470 th iteration => 0.0744288349749\n",
      "Loss for  5471 th iteration => 0.0744130027128\n",
      "Loss for  5472 th iteration => 0.0743964460019\n",
      "Loss for  5473 th iteration => 0.0743828979142\n",
      "Loss for  5474 th iteration => 0.0743641830048\n",
      "Loss for  5475 th iteration => 0.0743481241472\n",
      "Loss for  5476 th iteration => 0.0743359264184\n",
      "Loss for  5477 th iteration => 0.0743184466052\n",
      "Loss for  5478 th iteration => 0.0742989538202\n",
      "Loss for  5479 th iteration => 0.0742818822662\n",
      "Loss for  5480 th iteration => 0.074265820873\n",
      "Loss for  5481 th iteration => 0.0742501531196\n",
      "Loss for  5482 th iteration => 0.0742365295646\n",
      "Loss for  5483 th iteration => 0.0742216686659\n",
      "Loss for  5484 th iteration => 0.0742047946867\n",
      "Loss for  5485 th iteration => 0.0741851721207\n",
      "Loss for  5486 th iteration => 0.0741678737513\n",
      "Loss for  5487 th iteration => 0.0741520998739\n",
      "Loss for  5488 th iteration => 0.0741360342301\n",
      "Loss for  5489 th iteration => 0.0741225801714\n",
      "Loss for  5490 th iteration => 0.0741077949132\n",
      "Loss for  5491 th iteration => 0.0740913504466\n",
      "Loss for  5492 th iteration => 0.074071642245\n",
      "Loss for  5493 th iteration => 0.0740543270596\n",
      "Loss for  5494 th iteration => 0.0740401645795\n",
      "Loss for  5495 th iteration => 0.0740254186967\n",
      "Loss for  5496 th iteration => 0.0740094626902\n",
      "Loss for  5497 th iteration => 0.0739942841435\n",
      "Loss for  5498 th iteration => 0.0739744175087\n",
      "Loss for  5499 th iteration => 0.073957574996\n",
      "Loss for  5500 th iteration => 0.0739468040451\n",
      "Loss for  5501 th iteration => 0.0739293636736\n",
      "Loss for  5502 th iteration => 0.0739093213859\n",
      "Loss for  5503 th iteration => 0.0738958780591\n",
      "Loss for  5504 th iteration => 0.073877380361\n",
      "Loss for  5505 th iteration => 0.0738605647555\n",
      "Loss for  5506 th iteration => 0.0738500370414\n",
      "Loss for  5507 th iteration => 0.0738353910872\n",
      "Loss for  5508 th iteration => 0.0738130744225\n",
      "Loss for  5509 th iteration => 0.0737958996692\n",
      "Loss for  5510 th iteration => 0.0737802673824\n",
      "Loss for  5511 th iteration => 0.0737642253017\n",
      "Loss for  5512 th iteration => 0.0737508367145\n",
      "Loss for  5513 th iteration => 0.0737363639391\n",
      "Loss for  5514 th iteration => 0.0737160861631\n",
      "Loss for  5515 th iteration => 0.0737009301173\n",
      "Loss for  5516 th iteration => 0.073688721497\n",
      "Loss for  5517 th iteration => 0.0736711813497\n",
      "Loss for  5518 th iteration => 0.0736520789344\n",
      "Loss for  5519 th iteration => 0.0736380452486\n",
      "Loss for  5520 th iteration => 0.0736202110051\n",
      "Loss for  5521 th iteration => 0.0736029832414\n",
      "Loss for  5522 th iteration => 0.073592731313\n",
      "Loss for  5523 th iteration => 0.0735753448667\n",
      "Loss for  5524 th iteration => 0.0735555103458\n",
      "Loss for  5525 th iteration => 0.0735397580867\n",
      "Loss for  5526 th iteration => 0.0735260101098\n",
      "Loss for  5527 th iteration => 0.0735081502573\n",
      "Loss for  5528 th iteration => 0.0734908920998\n",
      "Loss for  5529 th iteration => 0.0734806393058\n",
      "Loss for  5530 th iteration => 0.0734634806629\n",
      "Loss for  5531 th iteration => 0.0734436749865\n",
      "Loss for  5532 th iteration => 0.0734278104326\n",
      "Loss for  5533 th iteration => 0.0734142524422\n",
      "Loss for  5534 th iteration => 0.0733963677137\n",
      "Loss for  5535 th iteration => 0.0733792338343\n",
      "Loss for  5536 th iteration => 0.0733701325139\n",
      "Loss for  5537 th iteration => 0.0733544981302\n",
      "Loss for  5538 th iteration => 0.0733323934789\n",
      "Loss for  5539 th iteration => 0.0733158176227\n",
      "Loss for  5540 th iteration => 0.0732999599359\n",
      "Loss for  5541 th iteration => 0.0732845209597\n",
      "Loss for  5542 th iteration => 0.0732708745693\n",
      "Loss for  5543 th iteration => 0.0732578409044\n",
      "Loss for  5544 th iteration => 0.073240475801\n",
      "Loss for  5545 th iteration => 0.0732211215329\n",
      "Loss for  5546 th iteration => 0.0732045414973\n",
      "Loss for  5547 th iteration => 0.0731887687021\n",
      "Loss for  5548 th iteration => 0.0731733777275\n",
      "Loss for  5549 th iteration => 0.0731597876858\n",
      "Loss for  5550 th iteration => 0.0731467477656\n",
      "Loss for  5551 th iteration => 0.0731294850029\n",
      "Loss for  5552 th iteration => 0.0731101372624\n",
      "Loss for  5553 th iteration => 0.0730935778921\n",
      "Loss for  5554 th iteration => 0.0730778673222\n",
      "Loss for  5555 th iteration => 0.0730625719801\n",
      "Loss for  5556 th iteration => 0.0730490243271\n",
      "Loss for  5557 th iteration => 0.0730359733002\n",
      "Loss for  5558 th iteration => 0.0730187766879\n",
      "Loss for  5559 th iteration => 0.0729994446615\n",
      "Loss for  5560 th iteration => 0.0729829239251\n",
      "Loss for  5561 th iteration => 0.0729672591306\n",
      "Loss for  5562 th iteration => 0.0729520938697\n",
      "Loss for  5563 th iteration => 0.0729385786394\n",
      "Loss for  5564 th iteration => 0.0729255129566\n",
      "Loss for  5565 th iteration => 0.072908355927\n",
      "Loss for  5566 th iteration => 0.072889046386\n",
      "Loss for  5567 th iteration => 0.0728725772166\n",
      "Loss for  5568 th iteration => 0.0728569462931\n",
      "Loss for  5569 th iteration => 0.0728419359095\n",
      "Loss for  5570 th iteration => 0.0728284460516\n",
      "Loss for  5571 th iteration => 0.0728153631174\n",
      "Loss for  5572 th iteration => 0.0727982261499\n",
      "Loss for  5573 th iteration => 0.0727789440919\n",
      "Loss for  5574 th iteration => 0.0727625357367\n",
      "Loss for  5575 th iteration => 0.072746930103\n",
      "Loss for  5576 th iteration => 0.0727320923283\n",
      "Loss for  5577 th iteration => 0.0727186229194\n",
      "Loss for  5578 th iteration => 0.0727055208381\n",
      "Loss for  5579 th iteration => 0.0726883895732\n",
      "Loss for  5580 th iteration => 0.0726691386934\n",
      "Loss for  5581 th iteration => 0.0726528298353\n",
      "Loss for  5582 th iteration => 0.0726402624498\n",
      "Loss for  5583 th iteration => 0.0726226455693\n",
      "Loss for  5584 th iteration => 0.0726102950496\n",
      "Loss for  5585 th iteration => 0.0725972578354\n",
      "Loss for  5586 th iteration => 0.0725775800618\n",
      "Loss for  5587 th iteration => 0.0725613359758\n",
      "Loss for  5588 th iteration => 0.0725466375616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  5589 th iteration => 0.0725283004109\n",
      "Loss for  5590 th iteration => 0.0725124854734\n",
      "Loss for  5591 th iteration => 0.0725007000386\n",
      "Loss for  5592 th iteration => 0.0724862088773\n",
      "Loss for  5593 th iteration => 0.0724695777468\n",
      "Loss for  5594 th iteration => 0.0724509036112\n",
      "Loss for  5595 th iteration => 0.0724340517408\n",
      "Loss for  5596 th iteration => 0.0724192738953\n",
      "Loss for  5597 th iteration => 0.0724077103859\n",
      "Loss for  5598 th iteration => 0.0723919301574\n",
      "Loss for  5599 th iteration => 0.0723736645637\n",
      "Loss for  5600 th iteration => 0.0723618865927\n",
      "Loss for  5601 th iteration => 0.0723455126959\n",
      "Loss for  5602 th iteration => 0.0723256367475\n",
      "Loss for  5603 th iteration => 0.0723100512606\n",
      "Loss for  5604 th iteration => 0.0722955938069\n",
      "Loss for  5605 th iteration => 0.0722859998712\n",
      "Loss for  5606 th iteration => 0.0722694448623\n",
      "Loss for  5607 th iteration => 0.0722483950867\n",
      "Loss for  5608 th iteration => 0.0722345889628\n",
      "Loss for  5609 th iteration => 0.0722205255909\n",
      "Loss for  5610 th iteration => 0.0722047406926\n",
      "Loss for  5611 th iteration => 0.0721868577124\n",
      "Loss for  5612 th iteration => 0.072171545539\n",
      "Loss for  5613 th iteration => 0.0721614558233\n",
      "Loss for  5614 th iteration => 0.0721462861248\n",
      "Loss for  5615 th iteration => 0.0721251788836\n",
      "Loss for  5616 th iteration => 0.0721087294279\n",
      "Loss for  5617 th iteration => 0.0720938263375\n",
      "Loss for  5618 th iteration => 0.0720826528022\n",
      "Loss for  5619 th iteration => 0.0720664565836\n",
      "Loss for  5620 th iteration => 0.0720488554041\n",
      "Loss for  5621 th iteration => 0.0720371134306\n",
      "Loss for  5622 th iteration => 0.0720203905369\n",
      "Loss for  5623 th iteration => 0.0720012044836\n",
      "Loss for  5624 th iteration => 0.0719854443349\n",
      "Loss for  5625 th iteration => 0.0719717012749\n",
      "Loss for  5626 th iteration => 0.0719616917826\n",
      "Loss for  5627 th iteration => 0.0719449057245\n",
      "Loss for  5628 th iteration => 0.0719242932331\n",
      "Loss for  5629 th iteration => 0.0719089389152\n",
      "Loss for  5630 th iteration => 0.071893995127\n",
      "Loss for  5631 th iteration => 0.0718806066002\n",
      "Loss for  5632 th iteration => 0.0718642352951\n",
      "Loss for  5633 th iteration => 0.0718517161374\n",
      "Loss for  5634 th iteration => 0.0718368880193\n",
      "Loss for  5635 th iteration => 0.0718169144472\n",
      "Loss for  5636 th iteration => 0.0718013318054\n",
      "Loss for  5637 th iteration => 0.0717870177587\n",
      "Loss for  5638 th iteration => 0.0717732603503\n",
      "Loss for  5639 th iteration => 0.0717613090517\n",
      "Loss for  5640 th iteration => 0.0717443157673\n",
      "Loss for  5641 th iteration => 0.0717252805574\n",
      "Loss for  5642 th iteration => 0.071709827873\n",
      "Loss for  5643 th iteration => 0.0716970639571\n",
      "Loss for  5644 th iteration => 0.0716804149901\n",
      "Loss for  5645 th iteration => 0.0716676545429\n",
      "Loss for  5646 th iteration => 0.0716552651175\n",
      "Loss for  5647 th iteration => 0.0716356200988\n",
      "Loss for  5648 th iteration => 0.0716207772167\n",
      "Loss for  5649 th iteration => 0.0716056331611\n",
      "Loss for  5650 th iteration => 0.0715875947613\n",
      "Loss for  5651 th iteration => 0.0715730308376\n",
      "Loss for  5652 th iteration => 0.0715607512524\n",
      "Loss for  5653 th iteration => 0.0715473128371\n",
      "Loss for  5654 th iteration => 0.0715302916786\n",
      "Loss for  5655 th iteration => 0.0715119678577\n",
      "Loss for  5656 th iteration => 0.0714962296187\n",
      "Loss for  5657 th iteration => 0.0714842887005\n",
      "Loss for  5658 th iteration => 0.0714709810567\n",
      "Loss for  5659 th iteration => 0.071454953973\n",
      "Loss for  5660 th iteration => 0.0714364040152\n",
      "Loss for  5661 th iteration => 0.0714202459474\n",
      "Loss for  5662 th iteration => 0.0714053553\n",
      "Loss for  5663 th iteration => 0.0713905000953\n",
      "Loss for  5664 th iteration => 0.07137836651\n",
      "Loss for  5665 th iteration => 0.0713649701222\n",
      "Loss for  5666 th iteration => 0.0713484291863\n",
      "Loss for  5667 th iteration => 0.0713300371581\n",
      "Loss for  5668 th iteration => 0.0713142133828\n",
      "Loss for  5669 th iteration => 0.0712993099538\n",
      "Loss for  5670 th iteration => 0.0712890429683\n",
      "Loss for  5671 th iteration => 0.0712764044242\n",
      "Loss for  5672 th iteration => 0.0712548451949\n",
      "Loss for  5673 th iteration => 0.0712385660421\n",
      "Loss for  5674 th iteration => 0.0712239141761\n",
      "Loss for  5675 th iteration => 0.071208880238\n",
      "Loss for  5676 th iteration => 0.071196950029\n",
      "Loss for  5677 th iteration => 0.0711835403857\n",
      "Loss for  5678 th iteration => 0.0711672779245\n",
      "Loss for  5679 th iteration => 0.0711489137636\n",
      "Loss for  5680 th iteration => 0.0711330342358\n",
      "Loss for  5681 th iteration => 0.0711182820029\n",
      "Loss for  5682 th iteration => 0.0711079144188\n",
      "Loss for  5683 th iteration => 0.0710956056447\n",
      "Loss for  5684 th iteration => 0.0710740857893\n",
      "Loss for  5685 th iteration => 0.071060268734\n",
      "Loss for  5686 th iteration => 0.0710480766739\n",
      "Loss for  5687 th iteration => 0.0710313748298\n",
      "Loss for  5688 th iteration => 0.0710158175222\n",
      "Loss for  5689 th iteration => 0.0710024364106\n",
      "Loss for  5690 th iteration => 0.0709836609453\n",
      "Loss for  5691 th iteration => 0.0709691990866\n",
      "Loss for  5692 th iteration => 0.0709564521524\n",
      "Loss for  5693 th iteration => 0.0709381261609\n",
      "Loss for  5694 th iteration => 0.0709240273091\n",
      "Loss for  5695 th iteration => 0.0709111730842\n",
      "Loss for  5696 th iteration => 0.0708976249881\n",
      "Loss for  5697 th iteration => 0.0708821921718\n",
      "Loss for  5698 th iteration => 0.0708636557699\n",
      "Loss for  5699 th iteration => 0.070848671282\n",
      "Loss for  5700 th iteration => 0.070836080177\n",
      "Loss for  5701 th iteration => 0.0708227606183\n",
      "Loss for  5702 th iteration => 0.0708040965596\n",
      "Loss for  5703 th iteration => 0.0707896493546\n",
      "Loss for  5704 th iteration => 0.070777006775\n",
      "Loss for  5705 th iteration => 0.0707587126655\n",
      "Loss for  5706 th iteration => 0.0707447710885\n",
      "Loss for  5707 th iteration => 0.0707319422252\n",
      "Loss for  5708 th iteration => 0.0707184715825\n",
      "Loss for  5709 th iteration => 0.0707030655215\n",
      "Loss for  5710 th iteration => 0.0706845717712\n",
      "Loss for  5711 th iteration => 0.0706697880253\n",
      "Loss for  5712 th iteration => 0.0706572106916\n",
      "Loss for  5713 th iteration => 0.0706438663621\n",
      "Loss for  5714 th iteration => 0.070625305158\n",
      "Loss for  5715 th iteration => 0.0706110152814\n",
      "Loss for  5716 th iteration => 0.0705984084118\n",
      "Loss for  5717 th iteration => 0.0705831047917\n",
      "Loss for  5718 th iteration => 0.0705658647762\n",
      "Loss for  5719 th iteration => 0.0705510571541\n",
      "Loss for  5720 th iteration => 0.0705401827537\n",
      "Loss for  5721 th iteration => 0.070524622057\n",
      "Loss for  5722 th iteration => 0.0705062796785\n",
      "Loss for  5723 th iteration => 0.0704917625437\n",
      "Loss for  5724 th iteration => 0.0704792981836\n",
      "Loss for  5725 th iteration => 0.0704658869643\n",
      "Loss for  5726 th iteration => 0.0704510529313\n",
      "Loss for  5727 th iteration => 0.0704325551567\n",
      "Loss for  5728 th iteration => 0.0704173880944\n",
      "Loss for  5729 th iteration => 0.070405223791\n",
      "Loss for  5730 th iteration => 0.0703879829968\n",
      "Loss for  5731 th iteration => 0.0703738592383\n",
      "Loss for  5732 th iteration => 0.0703626614796\n",
      "Loss for  5733 th iteration => 0.0703469702353\n",
      "Loss for  5734 th iteration => 0.0703287644913\n",
      "Loss for  5735 th iteration => 0.070314663351\n",
      "Loss for  5736 th iteration => 0.0703022371143\n",
      "Loss for  5737 th iteration => 0.070288701244\n",
      "Loss for  5738 th iteration => 0.0702737507524\n",
      "Loss for  5739 th iteration => 0.0702553852195\n",
      "Loss for  5740 th iteration => 0.0702406165465\n",
      "Loss for  5741 th iteration => 0.0702283898935\n",
      "Loss for  5742 th iteration => 0.070215004552\n",
      "Loss for  5743 th iteration => 0.0701967471942\n",
      "Loss for  5744 th iteration => 0.070182438084\n",
      "Loss for  5745 th iteration => 0.0701724483379\n",
      "Loss for  5746 th iteration => 0.0701580611632\n",
      "Loss for  5747 th iteration => 0.0701381463874\n",
      "Loss for  5748 th iteration => 0.0701223935334\n",
      "Loss for  5749 th iteration => 0.0701128738854\n",
      "Loss for  5750 th iteration => 0.070097463454\n",
      "Loss for  5751 th iteration => 0.0700787722669\n",
      "Loss for  5752 th iteration => 0.0700652397496\n",
      "Loss for  5753 th iteration => 0.0700516943141\n",
      "Loss for  5754 th iteration => 0.0700353875257\n",
      "Loss for  5755 th iteration => 0.0700245821846\n",
      "Loss for  5756 th iteration => 0.0700098347272\n",
      "Loss for  5757 th iteration => 0.0699917089512\n",
      "Loss for  5758 th iteration => 0.069978791552\n",
      "Loss for  5759 th iteration => 0.0699621714353\n",
      "Loss for  5760 th iteration => 0.069950960144\n",
      "Loss for  5761 th iteration => 0.0699373924563\n",
      "Loss for  5762 th iteration => 0.0699184698903\n",
      "Loss for  5763 th iteration => 0.069907957399\n",
      "Loss for  5764 th iteration => 0.0698922223164\n",
      "Loss for  5765 th iteration => 0.0698743291558\n",
      "Loss for  5766 th iteration => 0.0698634032018\n",
      "Loss for  5767 th iteration => 0.0698494050696\n",
      "Loss for  5768 th iteration => 0.0698337424443\n",
      "Loss for  5769 th iteration => 0.0698166567361\n",
      "Loss for  5770 th iteration => 0.0698012267344\n",
      "Loss for  5771 th iteration => 0.0697907141942\n",
      "Loss for  5772 th iteration => 0.0697730136686\n",
      "Loss for  5773 th iteration => 0.0697585257233\n",
      "Loss for  5774 th iteration => 0.0697474895273\n",
      "Loss for  5775 th iteration => 0.069731378745\n",
      "Loss for  5776 th iteration => 0.0697143194449\n",
      "Loss for  5777 th iteration => 0.0697006595532\n",
      "Loss for  5778 th iteration => 0.0696891809331\n",
      "Loss for  5779 th iteration => 0.0696749226891\n",
      "Loss for  5780 th iteration => 0.0696591471205\n",
      "Loss for  5781 th iteration => 0.0696420645373\n",
      "Loss for  5782 th iteration => 0.0696277807967\n",
      "Loss for  5783 th iteration => 0.0696165764162\n",
      "Loss for  5784 th iteration => 0.0696024917204\n",
      "Loss for  5785 th iteration => 0.0695870013367\n",
      "Loss for  5786 th iteration => 0.0695698939154\n",
      "Loss for  5787 th iteration => 0.0695551488539\n",
      "Loss for  5788 th iteration => 0.0695441392546\n",
      "Loss for  5789 th iteration => 0.0695302329716\n",
      "Loss for  5790 th iteration => 0.0695119403736\n",
      "Loss for  5791 th iteration => 0.0694981055944\n",
      "Loss for  5792 th iteration => 0.0694853837815\n",
      "Loss for  5793 th iteration => 0.069472140184\n",
      "Loss for  5794 th iteration => 0.0694545662437\n",
      "Loss for  5795 th iteration => 0.069440152602\n",
      "Loss for  5796 th iteration => 0.0694291939565\n",
      "Loss for  5797 th iteration => 0.0694133840109\n",
      "Loss for  5798 th iteration => 0.0693963195521\n",
      "Loss for  5799 th iteration => 0.0693829806919\n",
      "Loss for  5800 th iteration => 0.069371405278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  5801 th iteration => 0.0693572311426\n",
      "Loss for  5802 th iteration => 0.0693417188664\n",
      "Loss for  5803 th iteration => 0.069324632818\n",
      "Loss for  5804 th iteration => 0.0693107743714\n",
      "Loss for  5805 th iteration => 0.0692994258016\n",
      "Loss for  5806 th iteration => 0.0692854021523\n",
      "Loss for  5807 th iteration => 0.0692701438759\n",
      "Loss for  5808 th iteration => 0.0692530350684\n",
      "Loss for  5809 th iteration => 0.0692387983036\n",
      "Loss for  5810 th iteration => 0.0692276077586\n",
      "Loss for  5811 th iteration => 0.0692137063327\n",
      "Loss for  5812 th iteration => 0.0691986663347\n",
      "Loss for  5813 th iteration => 0.0691815341276\n",
      "Loss for  5814 th iteration => 0.0691670918088\n",
      "Loss for  5815 th iteration => 0.0691575997394\n",
      "Loss for  5816 th iteration => 0.0691406859841\n",
      "Loss for  5817 th iteration => 0.069125176664\n",
      "Loss for  5818 th iteration => 0.0691125969825\n",
      "Loss for  5819 th iteration => 0.0690984618279\n",
      "Loss for  5820 th iteration => 0.0690850921998\n",
      "Loss for  5821 th iteration => 0.0690671748045\n",
      "Loss for  5822 th iteration => 0.0690535339041\n",
      "Loss for  5823 th iteration => 0.0690412238042\n",
      "Loss for  5824 th iteration => 0.0690270295473\n",
      "Loss for  5825 th iteration => 0.0690139588738\n",
      "Loss for  5826 th iteration => 0.0689960505194\n",
      "Loss for  5827 th iteration => 0.0689821039641\n",
      "Loss for  5828 th iteration => 0.0689701358909\n",
      "Loss for  5829 th iteration => 0.0689529406054\n",
      "Loss for  5830 th iteration => 0.0689391244643\n",
      "Loss for  5831 th iteration => 0.0689279293496\n",
      "Loss for  5832 th iteration => 0.0689143059468\n",
      "Loss for  5833 th iteration => 0.0688993411961\n",
      "Loss for  5834 th iteration => 0.0688820532349\n",
      "Loss for  5835 th iteration => 0.0688678865071\n",
      "Loss for  5836 th iteration => 0.0688567938331\n",
      "Loss for  5837 th iteration => 0.068843218158\n",
      "Loss for  5838 th iteration => 0.0688286210659\n",
      "Loss for  5839 th iteration => 0.0688112629097\n",
      "Loss for  5840 th iteration => 0.0687968404875\n",
      "Loss for  5841 th iteration => 0.0687858231351\n",
      "Loss for  5842 th iteration => 0.0687722853622\n",
      "Loss for  5843 th iteration => 0.0687579855929\n",
      "Loss for  5844 th iteration => 0.0687405756105\n",
      "Loss for  5845 th iteration => 0.0687259745424\n",
      "Loss for  5846 th iteration => 0.0687150106827\n",
      "Loss for  5847 th iteration => 0.0687015028223\n",
      "Loss for  5848 th iteration => 0.0686874431456\n",
      "Loss for  5849 th iteration => 0.0686699963361\n",
      "Loss for  5850 th iteration => 0.068655279122\n",
      "Loss for  5851 th iteration => 0.0686443512132\n",
      "Loss for  5852 th iteration => 0.0686309150836\n",
      "Loss for  5853 th iteration => 0.0686132541237\n",
      "Loss for  5854 th iteration => 0.068600104321\n",
      "Loss for  5855 th iteration => 0.0685879065047\n",
      "Loss for  5856 th iteration => 0.0685737963438\n",
      "Loss for  5857 th iteration => 0.0685606067274\n",
      "Loss for  5858 th iteration => 0.0685429507434\n",
      "Loss for  5859 th iteration => 0.0685295788476\n",
      "Loss for  5860 th iteration => 0.0685175174968\n",
      "Loss for  5861 th iteration => 0.0685033958882\n",
      "Loss for  5862 th iteration => 0.0684904017846\n",
      "Loss for  5863 th iteration => 0.0684727461462\n",
      "Loss for  5864 th iteration => 0.0684592372965\n",
      "Loss for  5865 th iteration => 0.0684472583279\n",
      "Loss for  5866 th iteration => 0.06843314504\n",
      "Loss for  5867 th iteration => 0.068420305014\n",
      "Loss for  5868 th iteration => 0.0684026461823\n",
      "Loss for  5869 th iteration => 0.0683890691029\n",
      "Loss for  5870 th iteration => 0.0683771291527\n",
      "Loss for  5871 th iteration => 0.0683630399082\n",
      "Loss for  5872 th iteration => 0.0683503202973\n",
      "Loss for  5873 th iteration => 0.0683326556089\n",
      "Loss for  5874 th iteration => 0.0683190656847\n",
      "Loss for  5875 th iteration => 0.0683071693434\n",
      "Loss for  5876 th iteration => 0.068290190587\n",
      "Loss for  5877 th iteration => 0.068276897694\n",
      "Loss for  5878 th iteration => 0.0682657457228\n",
      "Loss for  5879 th iteration => 0.0682525410511\n",
      "Loss for  5880 th iteration => 0.0682375270125\n",
      "Loss for  5881 th iteration => 0.0682204908008\n",
      "Loss for  5882 th iteration => 0.0682069899174\n",
      "Loss for  5883 th iteration => 0.0681959037303\n",
      "Loss for  5884 th iteration => 0.0681827260043\n",
      "Loss for  5885 th iteration => 0.0681679727443\n",
      "Loss for  5886 th iteration => 0.0681508949546\n",
      "Loss for  5887 th iteration => 0.0681372527352\n",
      "Loss for  5888 th iteration => 0.0681262134536\n",
      "Loss for  5889 th iteration => 0.0681130558548\n",
      "Loss for  5890 th iteration => 0.0680985137936\n",
      "Loss for  5891 th iteration => 0.0680814073975\n",
      "Loss for  5892 th iteration => 0.0680676779523\n",
      "Loss for  5893 th iteration => 0.0680566703877\n",
      "Loss for  5894 th iteration => 0.0680435273707\n",
      "Loss for  5895 th iteration => 0.0680291561135\n",
      "Loss for  5896 th iteration => 0.0680120317049\n",
      "Loss for  5897 th iteration => 0.0679982589196\n",
      "Loss for  5898 th iteration => 0.0679872708773\n",
      "Loss for  5899 th iteration => 0.0679741379289\n",
      "Loss for  5900 th iteration => 0.0679599045582\n",
      "Loss for  5901 th iteration => 0.067942770788\n",
      "Loss for  5902 th iteration => 0.0679289902197\n",
      "Loss for  5903 th iteration => 0.0679180119341\n",
      "Loss for  5904 th iteration => 0.0679048853779\n",
      "Loss for  5905 th iteration => 0.0678907630624\n",
      "Loss for  5906 th iteration => 0.0678736269949\n",
      "Loss for  5907 th iteration => 0.0678598674205\n",
      "Loss for  5908 th iteration => 0.0678488910965\n",
      "Loss for  5909 th iteration => 0.0678357679354\n",
      "Loss for  5910 th iteration => 0.067821734796\n",
      "Loss for  5911 th iteration => 0.0678046022013\n",
      "Loss for  5912 th iteration => 0.0677908868794\n",
      "Loss for  5913 th iteration => 0.0677799063205\n",
      "Loss for  5914 th iteration => 0.0677667841094\n",
      "Loss for  5915 th iteration => 0.0677528222957\n",
      "Loss for  5916 th iteration => 0.0677356978883\n",
      "Loss for  5917 th iteration => 0.0677220455887\n",
      "Loss for  5918 th iteration => 0.0677110558947\n",
      "Loss for  5919 th iteration => 0.067697932637\n",
      "Loss for  5920 th iteration => 0.0676840275764\n",
      "Loss for  5921 th iteration => 0.0676669152104\n",
      "Loss for  5922 th iteration => 0.067653341052\n",
      "Loss for  5923 th iteration => 0.0676423383738\n",
      "Loss for  5924 th iteration => 0.0676292124377\n",
      "Loss for  5925 th iteration => 0.0676153522241\n",
      "Loss for  5926 th iteration => 0.0675982550521\n",
      "Loss for  5927 th iteration => 0.0675847711852\n",
      "Loss for  5928 th iteration => 0.0675737525258\n",
      "Loss for  5929 th iteration => 0.0675606225767\n",
      "Loss for  5930 th iteration => 0.067546797473\n",
      "Loss for  5931 th iteration => 0.0675297180761\n",
      "Loss for  5932 th iteration => 0.0675163342373\n",
      "Loss for  5933 th iteration => 0.0675052972901\n",
      "Loss for  5934 th iteration => 0.0674921622354\n",
      "Loss for  5935 th iteration => 0.0674783642694\n",
      "Loss for  5936 th iteration => 0.0674613047628\n",
      "Loss for  5937 th iteration => 0.0674480287272\n",
      "Loss for  5938 th iteration => 0.0674369717445\n",
      "Loss for  5939 th iteration => 0.0674238306891\n",
      "Loss for  5940 th iteration => 0.0674100533244\n",
      "Loss for  5941 th iteration => 0.0673930154434\n",
      "Loss for  5942 th iteration => 0.0673798533918\n",
      "Loss for  5943 th iteration => 0.0673687750786\n",
      "Loss for  5944 th iteration => 0.0673556272884\n",
      "Loss for  5945 th iteration => 0.0673418651572\n",
      "Loss for  5946 th iteration => 0.0673248503275\n",
      "Loss for  5947 th iteration => 0.0673118071453\n",
      "Loss for  5948 th iteration => 0.0673007065722\n",
      "Loss for  5949 th iteration => 0.0672875514451\n",
      "Loss for  5950 th iteration => 0.0672738001304\n",
      "Loss for  5951 th iteration => 0.0672568095251\n",
      "Loss for  5952 th iteration => 0.0672438890455\n",
      "Loss for  5953 th iteration => 0.0672327655788\n",
      "Loss for  5954 th iteration => 0.0672196026203\n",
      "Loss for  5955 th iteration => 0.067205919854\n",
      "Loss for  5956 th iteration => 0.0671917935005\n",
      "Loss for  5957 th iteration => 0.0671753712455\n",
      "Loss for  5958 th iteration => 0.0671627396352\n",
      "Loss for  5959 th iteration => 0.0671517990441\n",
      "Loss for  5960 th iteration => 0.0671380934092\n",
      "Loss for  5961 th iteration => 0.0671239855928\n",
      "Loss for  5962 th iteration => 0.0671075444293\n",
      "Loss for  5963 th iteration => 0.0670952215448\n",
      "Loss for  5964 th iteration => 0.0670841184552\n",
      "Loss for  5965 th iteration => 0.067070393841\n",
      "Loss for  5966 th iteration => 0.0670563091144\n",
      "Loss for  5967 th iteration => 0.0670398502839\n",
      "Loss for  5968 th iteration => 0.0670278149006\n",
      "Loss for  5969 th iteration => 0.0670165610863\n",
      "Loss for  5970 th iteration => 0.0670028205776\n",
      "Loss for  5971 th iteration => 0.0669889010229\n",
      "Loss for  5972 th iteration => 0.0669761958068\n",
      "Loss for  5973 th iteration => 0.0669595322054\n",
      "Loss for  5974 th iteration => 0.0669461249963\n",
      "Loss for  5975 th iteration => 0.0669353543581\n",
      "Loss for  5976 th iteration => 0.066921593512\n",
      "Loss for  5977 th iteration => 0.0669087219906\n",
      "Loss for  5978 th iteration => 0.0668920246428\n",
      "Loss for  5979 th iteration => 0.0668790903458\n",
      "Loss for  5980 th iteration => 0.0668680173209\n",
      "Loss for  5981 th iteration => 0.066854398915\n",
      "Loss for  5982 th iteration => 0.0668413886756\n",
      "Loss for  5983 th iteration => 0.0668246616117\n",
      "Loss for  5984 th iteration => 0.0668121500532\n",
      "Loss for  5985 th iteration => 0.0668008083443\n",
      "Loss for  5986 th iteration => 0.0667873189001\n",
      "Loss for  5987 th iteration => 0.0667742731805\n",
      "Loss for  5988 th iteration => 0.0667610791089\n",
      "Loss for  5989 th iteration => 0.0667443506389\n",
      "Loss for  5990 th iteration => 0.0667309186219\n",
      "Loss for  5991 th iteration => 0.0667203967792\n",
      "Loss for  5992 th iteration => 0.0667073189707\n",
      "Loss for  5993 th iteration => 0.0666939434331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  5994 th iteration => 0.0666772871958\n",
      "Loss for  5995 th iteration => 0.0666641939803\n",
      "Loss for  5996 th iteration => 0.0666535858128\n",
      "Loss for  5997 th iteration => 0.0666404801251\n",
      "Loss for  5998 th iteration => 0.0666269553258\n",
      "Loss for  5999 th iteration => 0.0666103628456\n",
      "Loss for  6000 th iteration => 0.0665975711326\n",
      "Loss for  6001 th iteration => 0.0665868878447\n",
      "Loss for  6002 th iteration => 0.0665737577058\n",
      "Loss for  6003 th iteration => 0.0665602097839\n",
      "Loss for  6004 th iteration => 0.0665464440496\n",
      "Loss for  6005 th iteration => 0.0665301744782\n",
      "Loss for  6006 th iteration => 0.0665182319115\n",
      "Loss for  6007 th iteration => 0.0665071718197\n",
      "Loss for  6008 th iteration => 0.0664935651908\n",
      "Loss for  6009 th iteration => 0.0664799224917\n",
      "Loss for  6010 th iteration => 0.0664673622381\n",
      "Loss for  6011 th iteration => 0.0664508521506\n",
      "Loss for  6012 th iteration => 0.0664378505205\n",
      "Loss for  6013 th iteration => 0.066427028672\n",
      "Loss for  6014 th iteration => 0.0664135838823\n",
      "Loss for  6015 th iteration => 0.0664007650202\n",
      "Loss for  6016 th iteration => 0.0663842151741\n",
      "Loss for  6017 th iteration => 0.0663718374963\n",
      "Loss for  6018 th iteration => 0.0663606216177\n",
      "Loss for  6019 th iteration => 0.0663473494433\n",
      "Loss for  6020 th iteration => 0.0663344027251\n",
      "Loss for  6021 th iteration => 0.0663213483212\n",
      "Loss for  6022 th iteration => 0.0663048392917\n",
      "Loss for  6023 th iteration => 0.0662916541022\n",
      "Loss for  6024 th iteration => 0.0662812636547\n",
      "Loss for  6025 th iteration => 0.0662682736552\n",
      "Loss for  6026 th iteration => 0.0662549603311\n",
      "Loss for  6027 th iteration => 0.0662385431914\n",
      "Loss for  6028 th iteration => 0.0662257818004\n",
      "Loss for  6029 th iteration => 0.0662152819743\n",
      "Loss for  6030 th iteration => 0.0662022547675\n",
      "Loss for  6031 th iteration => 0.0661888042692\n",
      "Loss for  6032 th iteration => 0.0661752463899\n",
      "Loss for  6033 th iteration => 0.0661590970591\n",
      "Loss for  6034 th iteration => 0.0661473962942\n",
      "Loss for  6035 th iteration => 0.0661363669029\n",
      "Loss for  6036 th iteration => 0.0661228334819\n",
      "Loss for  6037 th iteration => 0.0661094567276\n",
      "Loss for  6038 th iteration => 0.0660969008519\n",
      "Loss for  6039 th iteration => 0.0660804971879\n",
      "Loss for  6040 th iteration => 0.0660679266117\n",
      "Loss for  6041 th iteration => 0.0660569721169\n",
      "Loss for  6042 th iteration => 0.0660438129537\n",
      "Loss for  6043 th iteration => 0.0660309478167\n",
      "Loss for  6044 th iteration => 0.0660145017704\n",
      "Loss for  6045 th iteration => 0.0660026371628\n",
      "Loss for  6046 th iteration => 0.0659912401142\n",
      "Loss for  6047 th iteration => 0.065978268326\n",
      "Loss for  6048 th iteration => 0.0659653946711\n",
      "Loss for  6049 th iteration => 0.0659522523986\n",
      "Loss for  6050 th iteration => 0.0659359900205\n",
      "Loss for  6051 th iteration => 0.065923176931\n",
      "Loss for  6052 th iteration => 0.0659128687645\n",
      "Loss for  6053 th iteration => 0.0658999466418\n",
      "Loss for  6054 th iteration => 0.0658865222429\n",
      "Loss for  6055 th iteration => 0.0658731943978\n",
      "Loss for  6056 th iteration => 0.0658571496184\n",
      "Loss for  6057 th iteration => 0.0658455773302\n",
      "Loss for  6058 th iteration => 0.065834624858\n",
      "Loss for  6059 th iteration => 0.065821096978\n",
      "Loss for  6060 th iteration => 0.0658079705387\n",
      "Loss for  6061 th iteration => 0.0657954504057\n",
      "Loss for  6062 th iteration => 0.065779105442\n",
      "Loss for  6063 th iteration => 0.0657668886453\n",
      "Loss for  6064 th iteration => 0.0657557825017\n",
      "Loss for  6065 th iteration => 0.0657428911132\n",
      "Loss for  6066 th iteration => 0.0657300627422\n",
      "Loss for  6067 th iteration => 0.0657172102257\n",
      "Loss for  6068 th iteration => 0.0657010421022\n",
      "Loss for  6069 th iteration => 0.0656880660608\n",
      "Loss for  6070 th iteration => 0.0656779478149\n",
      "Loss for  6071 th iteration => 0.0656650607314\n",
      "Loss for  6072 th iteration => 0.0656518369146\n",
      "Loss for  6073 th iteration => 0.0656357887566\n",
      "Loss for  6074 th iteration => 0.0656233617543\n",
      "Loss for  6075 th iteration => 0.0656130991641\n",
      "Loss for  6076 th iteration => 0.0656001620704\n",
      "Loss for  6077 th iteration => 0.0655868105937\n",
      "Loss for  6078 th iteration => 0.0655736846234\n",
      "Loss for  6079 th iteration => 0.0655613622647\n",
      "Loss for  6080 th iteration => 0.0655451283751\n",
      "Loss for  6081 th iteration => 0.0655328619421\n",
      "Loss for  6082 th iteration => 0.0655219140636\n",
      "Loss for  6083 th iteration => 0.0655090515995\n",
      "Loss for  6084 th iteration => 0.0654963144313\n",
      "Loss for  6085 th iteration => 0.0654800309452\n",
      "Loss for  6086 th iteration => 0.0654686447807\n",
      "Loss for  6087 th iteration => 0.0654571482615\n",
      "Loss for  6088 th iteration => 0.0654445088352\n",
      "Loss for  6089 th iteration => 0.0654317930893\n",
      "Loss for  6090 th iteration => 0.0654186557785\n",
      "Loss for  6091 th iteration => 0.0654027009885\n",
      "Loss for  6092 th iteration => 0.0653903181618\n",
      "Loss for  6093 th iteration => 0.0653801039944\n",
      "Loss for  6094 th iteration => 0.0653673288722\n",
      "Loss for  6095 th iteration => 0.0653539767006\n",
      "Loss for  6096 th iteration => 0.0653409614262\n",
      "Loss for  6097 th iteration => 0.0653287647164\n",
      "Loss for  6098 th iteration => 0.0653125701496\n",
      "Loss for  6099 th iteration => 0.0653005305315\n",
      "Loss for  6100 th iteration => 0.0652894981672\n",
      "Loss for  6101 th iteration => 0.0652767630474\n",
      "Loss for  6102 th iteration => 0.0652641713521\n",
      "Loss for  6103 th iteration => 0.0652514223864\n",
      "Loss for  6104 th iteration => 0.0652354579336\n",
      "Loss for  6105 th iteration => 0.0652227950581\n",
      "Loss for  6106 th iteration => 0.0652126931234\n",
      "Loss for  6107 th iteration => 0.0652000292381\n",
      "Loss for  6108 th iteration => 0.0651868307561\n",
      "Loss for  6109 th iteration => 0.0651710070444\n",
      "Loss for  6110 th iteration => 0.065158979175\n",
      "Loss for  6111 th iteration => 0.0651487114277\n",
      "Loss for  6112 th iteration => 0.0651359859121\n",
      "Loss for  6113 th iteration => 0.0651227034956\n",
      "Loss for  6114 th iteration => 0.0651098263029\n",
      "Loss for  6115 th iteration => 0.0650975264715\n",
      "Loss for  6116 th iteration => 0.0650814199537\n",
      "Loss for  6117 th iteration => 0.0650698211686\n",
      "Loss for  6118 th iteration => 0.0650586439685\n",
      "Loss for  6119 th iteration => 0.0650460462933\n",
      "Loss for  6120 th iteration => 0.0650335014836\n",
      "Loss for  6121 th iteration => 0.0650206639968\n",
      "Loss for  6122 th iteration => 0.0650048358057\n",
      "Loss for  6123 th iteration => 0.0649925584262\n",
      "Loss for  6124 th iteration => 0.0649823946818\n",
      "Loss for  6125 th iteration => 0.0649697761941\n",
      "Loss for  6126 th iteration => 0.0649566134347\n",
      "Loss for  6127 th iteration => 0.0649437138243\n",
      "Loss for  6128 th iteration => 0.0649316776197\n",
      "Loss for  6129 th iteration => 0.0649156359726\n",
      "Loss for  6130 th iteration => 0.0649038467037\n",
      "Loss for  6131 th iteration => 0.0648928439826\n",
      "Loss for  6132 th iteration => 0.0648802539946\n",
      "Loss for  6133 th iteration => 0.0648677990857\n",
      "Loss for  6134 th iteration => 0.0648551292077\n",
      "Loss for  6135 th iteration => 0.0648393478784\n",
      "Loss for  6136 th iteration => 0.0648270352605\n",
      "Loss for  6137 th iteration => 0.0648169178136\n",
      "Loss for  6138 th iteration => 0.0648043818399\n",
      "Loss for  6139 th iteration => 0.0647912849095\n",
      "Loss for  6140 th iteration => 0.0647784308305\n",
      "Loss for  6141 th iteration => 0.0647665146142\n",
      "Loss for  6142 th iteration => 0.064750525562\n",
      "Loss for  6143 th iteration => 0.0647387869988\n",
      "Loss for  6144 th iteration => 0.0647278080314\n",
      "Loss for  6145 th iteration => 0.0647152796829\n",
      "Loss for  6146 th iteration => 0.0647028969054\n",
      "Loss for  6147 th iteration => 0.0646902801739\n",
      "Loss for  6148 th iteration => 0.0646745753624\n",
      "Loss for  6149 th iteration => 0.0646623554431\n",
      "Loss for  6150 th iteration => 0.0646522497039\n",
      "Loss for  6151 th iteration => 0.0646397812478\n",
      "Loss for  6152 th iteration => 0.064626719609\n",
      "Loss for  6153 th iteration => 0.0646139500452\n",
      "Loss for  6154 th iteration => 0.0646020678794\n",
      "Loss for  6155 th iteration => 0.0645861248832\n",
      "Loss for  6156 th iteration => 0.064574578473\n",
      "Loss for  6157 th iteration => 0.0645635361683\n",
      "Loss for  6158 th iteration => 0.06455110011\n",
      "Loss for  6159 th iteration => 0.064538778481\n",
      "Loss for  6160 th iteration => 0.0645261475518\n",
      "Loss for  6161 th iteration => 0.0645105359435\n",
      "Loss for  6162 th iteration => 0.0644984849736\n",
      "Loss for  6163 th iteration => 0.0644883701529\n",
      "Loss for  6164 th iteration => 0.0644759598886\n",
      "Loss for  6165 th iteration => 0.0644629168178\n",
      "Loss for  6166 th iteration => 0.0644502534844\n",
      "Loss for  6167 th iteration => 0.0644383539089\n",
      "Loss for  6168 th iteration => 0.0644224537923\n",
      "Loss for  6169 th iteration => 0.064411181436\n",
      "Loss for  6170 th iteration => 0.0644000265888\n",
      "Loss for  6171 th iteration => 0.0643876995966\n",
      "Loss for  6172 th iteration => 0.0643754321845\n",
      "Loss for  6173 th iteration => 0.0643627478145\n",
      "Loss for  6174 th iteration => 0.0643472383658\n",
      "Loss for  6175 th iteration => 0.0643354016696\n",
      "Loss for  6176 th iteration => 0.0643252652364\n",
      "Loss for  6177 th iteration => 0.0643129072273\n",
      "Loss for  6178 th iteration => 0.0642998741712\n",
      "Loss for  6179 th iteration => 0.0642873285206\n",
      "Loss for  6180 th iteration => 0.064275380676\n",
      "Loss for  6181 th iteration => 0.0642595222263\n",
      "Loss for  6182 th iteration => 0.0642485703124\n",
      "Loss for  6183 th iteration => 0.0642372762435\n",
      "Loss for  6184 th iteration => 0.0642250668585\n",
      "Loss for  6185 th iteration => 0.0642128491585\n",
      "Loss for  6186 th iteration => 0.0642000888504\n",
      "Loss for  6187 th iteration => 0.0641847138437\n",
      "Loss for  6188 th iteration => 0.0641718182611\n",
      "Loss for  6189 th iteration => 0.0641613170833\n",
      "Loss for  6190 th iteration => 0.0641502992173\n",
      "Loss for  6191 th iteration => 0.064138269629\n",
      "Loss for  6192 th iteration => 0.0641249392547\n",
      "Loss for  6193 th iteration => 0.0641128784817\n",
      "Loss for  6194 th iteration => 0.0640975611329\n",
      "Loss for  6195 th iteration => 0.0640875843686\n",
      "Loss for  6196 th iteration => 0.064072621132\n",
      "Loss for  6197 th iteration => 0.0640609886076\n",
      "Loss for  6198 th iteration => 0.0640515420345\n",
      "Loss for  6199 th iteration => 0.0640384779379\n",
      "Loss for  6200 th iteration => 0.0640250841294\n",
      "Loss for  6201 th iteration => 0.0640104275962\n",
      "Loss for  6202 th iteration => 0.0640012024387\n",
      "Loss for  6203 th iteration => 0.0639858446867\n",
      "Loss for  6204 th iteration => 0.0639739111864\n",
      "Loss for  6205 th iteration => 0.0639641673296\n",
      "Loss for  6206 th iteration => 0.0639518334636\n",
      "Loss for  6207 th iteration => 0.0639389236802\n",
      "Loss for  6208 th iteration => 0.0639266435047\n",
      "Loss for  6209 th iteration => 0.0639146359956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  6210 th iteration => 0.0638989214386\n",
      "Loss for  6211 th iteration => 0.0638881358605\n",
      "Loss for  6212 th iteration => 0.063876959654\n",
      "Loss for  6213 th iteration => 0.0638650420153\n",
      "Loss for  6214 th iteration => 0.0638528329177\n",
      "Loss for  6215 th iteration => 0.0638400602059\n",
      "Loss for  6216 th iteration => 0.0638248912448\n",
      "Loss for  6217 th iteration => 0.0638170409317\n",
      "Loss for  6218 th iteration => 0.0638022171862\n",
      "Loss for  6219 th iteration => 0.0637890689114\n",
      "Loss for  6220 th iteration => 0.0637781801592\n",
      "Loss for  6221 th iteration => 0.0637662836859\n",
      "Loss for  6222 th iteration => 0.0637541089516\n",
      "Loss for  6223 th iteration => 0.0637416334018\n",
      "Loss for  6224 th iteration => 0.0637264609268\n",
      "Loss for  6225 th iteration => 0.0637146296281\n",
      "Loss for  6226 th iteration => 0.0637049921172\n",
      "Loss for  6227 th iteration => 0.0636927126302\n",
      "Loss for  6228 th iteration => 0.063679782024\n",
      "Loss for  6229 th iteration => 0.0636677572122\n",
      "Loss for  6230 th iteration => 0.0636557375988\n",
      "Loss for  6231 th iteration => 0.063640060271\n",
      "Loss for  6232 th iteration => 0.0636296777959\n",
      "Loss for  6233 th iteration => 0.0636182765426\n",
      "Loss for  6234 th iteration => 0.0636066216758\n",
      "Loss for  6235 th iteration => 0.0635944596184\n",
      "Loss for  6236 th iteration => 0.0635816196815\n",
      "Loss for  6237 th iteration => 0.0635668717171\n",
      "Loss for  6238 th iteration => 0.0635575776068\n",
      "Loss for  6239 th iteration => 0.0635422966704\n",
      "Loss for  6240 th iteration => 0.0635310628361\n",
      "Loss for  6241 th iteration => 0.0635209792248\n",
      "Loss for  6242 th iteration => 0.063508303218\n",
      "Loss for  6243 th iteration => 0.0634961628173\n",
      "Loss for  6244 th iteration => 0.0634844040982\n",
      "Loss for  6245 th iteration => 0.0634714274447\n",
      "Loss for  6246 th iteration => 0.0634562677293\n",
      "Loss for  6247 th iteration => 0.0634443492206\n",
      "Loss for  6248 th iteration => 0.0634331469865\n",
      "Loss for  6249 th iteration => 0.0634228264965\n",
      "Loss for  6250 th iteration => 0.0634114682994\n",
      "Loss for  6251 th iteration => 0.0633981188059\n",
      "Loss for  6252 th iteration => 0.0633863582589\n",
      "Loss for  6253 th iteration => 0.0633745791753\n",
      "Loss for  6254 th iteration => 0.0633622173158\n",
      "Loss for  6255 th iteration => 0.0633469576849\n",
      "Loss for  6256 th iteration => 0.0633355743281\n",
      "Loss for  6257 th iteration => 0.0633253486098\n",
      "Loss for  6258 th iteration => 0.0633139887125\n",
      "Loss for  6259 th iteration => 0.0633008533491\n",
      "Loss for  6260 th iteration => 0.0632857094069\n",
      "Loss for  6261 th iteration => 0.0632767809211\n",
      "Loss for  6262 th iteration => 0.0632651657058\n",
      "Loss for  6263 th iteration => 0.0632495488748\n",
      "Loss for  6264 th iteration => 0.063239085358\n",
      "Loss for  6265 th iteration => 0.0632278515609\n",
      "Loss for  6266 th iteration => 0.0632163686106\n",
      "Loss for  6267 th iteration => 0.0632045174619\n",
      "Loss for  6268 th iteration => 0.0631916980549\n",
      "Loss for  6269 th iteration => 0.0631769758815\n",
      "Loss for  6270 th iteration => 0.0631691873241\n",
      "Loss for  6271 th iteration => 0.0631545119956\n",
      "Loss for  6272 th iteration => 0.0631420698489\n",
      "Loss for  6273 th iteration => 0.0631308963797\n",
      "Loss for  6274 th iteration => 0.0631195103293\n",
      "Loss for  6275 th iteration => 0.0631076654959\n",
      "Loss for  6276 th iteration => 0.0630949673648\n",
      "Loss for  6277 th iteration => 0.0630802317383\n",
      "Loss for  6278 th iteration => 0.0630723651642\n",
      "Loss for  6279 th iteration => 0.063057919746\n",
      "Loss for  6280 th iteration => 0.0630453946692\n",
      "Loss for  6281 th iteration => 0.0630342089037\n",
      "Loss for  6282 th iteration => 0.0630229438098\n",
      "Loss for  6283 th iteration => 0.0630110984625\n",
      "Loss for  6284 th iteration => 0.0629984655841\n",
      "Loss for  6285 th iteration => 0.0629837777088\n",
      "Loss for  6286 th iteration => 0.062975849014\n",
      "Loss for  6287 th iteration => 0.0629615312437\n",
      "Loss for  6288 th iteration => 0.0629490398882\n",
      "Loss for  6289 th iteration => 0.0629377900013\n",
      "Loss for  6290 th iteration => 0.062926662358\n",
      "Loss for  6291 th iteration => 0.0629148116993\n",
      "Loss for  6292 th iteration => 0.0629022035006\n",
      "Loss for  6293 th iteration => 0.0628876094594\n",
      "Loss for  6294 th iteration => 0.0628761570982\n",
      "Loss for  6295 th iteration => 0.0628651229754\n",
      "Loss for  6296 th iteration => 0.0628529210945\n",
      "Loss for  6297 th iteration => 0.06284132519\n",
      "Loss for  6298 th iteration => 0.0628315073479\n",
      "Loss for  6299 th iteration => 0.0628183968884\n",
      "Loss for  6300 th iteration => 0.0628059407844\n",
      "Loss for  6301 th iteration => 0.0627925502286\n",
      "Loss for  6302 th iteration => 0.062783286737\n",
      "Loss for  6303 th iteration => 0.0627692616373\n",
      "Loss for  6304 th iteration => 0.0627571161427\n",
      "Loss for  6305 th iteration => 0.0627454578572\n",
      "Loss for  6306 th iteration => 0.0627357487411\n",
      "Loss for  6307 th iteration => 0.0627226612072\n",
      "Loss for  6308 th iteration => 0.0627102169573\n",
      "Loss for  6309 th iteration => 0.0626969020088\n",
      "Loss for  6310 th iteration => 0.062687625719\n",
      "Loss for  6311 th iteration => 0.0626736235482\n",
      "Loss for  6312 th iteration => 0.0626616115592\n",
      "Loss for  6313 th iteration => 0.062649857438\n",
      "Loss for  6314 th iteration => 0.0626402674943\n",
      "Loss for  6315 th iteration => 0.062627199255\n",
      "Loss for  6316 th iteration => 0.0626147402622\n",
      "Loss for  6317 th iteration => 0.0626015301817\n",
      "Loss for  6318 th iteration => 0.0625922493601\n",
      "Loss for  6319 th iteration => 0.0625782200749\n",
      "Loss for  6320 th iteration => 0.0625663971769\n",
      "Loss for  6321 th iteration => 0.0625545239074\n",
      "Loss for  6322 th iteration => 0.0625450599168\n",
      "Loss for  6323 th iteration => 0.0625320083176\n",
      "Loss for  6324 th iteration => 0.0625195155469\n",
      "Loss for  6325 th iteration => 0.0625064312139\n",
      "Loss for  6326 th iteration => 0.0624971523318\n",
      "Loss for  6327 th iteration => 0.0624830592944\n",
      "Loss for  6328 th iteration => 0.0624714652992\n",
      "Loss for  6329 th iteration => 0.0624594569734\n",
      "Loss for  6330 th iteration => 0.0624501230501\n",
      "Loss for  6331 th iteration => 0.062437086149\n",
      "Loss for  6332 th iteration => 0.0624245460973\n",
      "Loss for  6333 th iteration => 0.0624116022582\n",
      "Loss for  6334 th iteration => 0.0624023304547\n",
      "Loss for  6335 th iteration => 0.0623881468608\n",
      "Loss for  6336 th iteration => 0.0623768100296\n",
      "Loss for  6337 th iteration => 0.0623647198217\n",
      "Loss for  6338 th iteration => 0.0623536365632\n",
      "Loss for  6339 th iteration => 0.0623405111985\n",
      "Loss for  6340 th iteration => 0.0623294446136\n",
      "Loss for  6341 th iteration => 0.0623179481348\n",
      "Loss for  6342 th iteration => 0.0623073792217\n",
      "Loss for  6343 th iteration => 0.0622932141798\n",
      "Loss for  6344 th iteration => 0.0622818920481\n",
      "Loss for  6345 th iteration => 0.0622714547647\n",
      "Loss for  6346 th iteration => 0.0622587040778\n",
      "Loss for  6347 th iteration => 0.0622470975817\n",
      "Loss for  6348 th iteration => 0.0622383234713\n",
      "Loss for  6349 th iteration => 0.0622217345307\n",
      "Loss for  6350 th iteration => 0.062210703336\n",
      "Loss for  6351 th iteration => 0.0622000062487\n",
      "Loss for  6352 th iteration => 0.062187501885\n",
      "Loss for  6353 th iteration => 0.0621779909463\n",
      "Loss for  6354 th iteration => 0.0621639386653\n",
      "Loss for  6355 th iteration => 0.0621516063823\n",
      "Loss for  6356 th iteration => 0.0621422259642\n",
      "Loss for  6357 th iteration => 0.0621295583513\n",
      "Loss for  6358 th iteration => 0.0621177460964\n",
      "Loss for  6359 th iteration => 0.0621054846407\n",
      "Loss for  6360 th iteration => 0.0620932864397\n",
      "Loss for  6361 th iteration => 0.0620847023897\n",
      "Loss for  6362 th iteration => 0.0620728985637\n",
      "Loss for  6363 th iteration => 0.0620578594886\n",
      "Loss for  6364 th iteration => 0.0620484763884\n",
      "Loss for  6365 th iteration => 0.062035392554\n",
      "Loss for  6366 th iteration => 0.0620242396441\n",
      "Loss for  6367 th iteration => 0.0620115312846\n",
      "Loss for  6368 th iteration => 0.0619997150128\n",
      "Loss for  6369 th iteration => 0.0619902539998\n",
      "Loss for  6370 th iteration => 0.0619772324058\n",
      "Loss for  6371 th iteration => 0.0619640921412\n",
      "Loss for  6372 th iteration => 0.0619556151443\n",
      "Loss for  6373 th iteration => 0.0619414299829\n",
      "Loss for  6374 th iteration => 0.0619286692006\n",
      "Loss for  6375 th iteration => 0.0619165550489\n",
      "Loss for  6376 th iteration => 0.0619083493274\n",
      "Loss for  6377 th iteration => 0.0618976313955\n",
      "Loss for  6378 th iteration => 0.0618834020589\n",
      "Loss for  6379 th iteration => 0.0618704545474\n",
      "Loss for  6380 th iteration => 0.0618631923983\n",
      "Loss for  6381 th iteration => 0.0618509514375\n",
      "Loss for  6382 th iteration => 0.0618353147761\n",
      "Loss for  6383 th iteration => 0.0618237520581\n",
      "Loss for  6384 th iteration => 0.0618168311396\n",
      "Loss for  6385 th iteration => 0.0618009874563\n",
      "Loss for  6386 th iteration => 0.0617913229656\n",
      "Loss for  6387 th iteration => 0.0617769425402\n",
      "Loss for  6388 th iteration => 0.0617667576915\n",
      "Loss for  6389 th iteration => 0.0617554938337\n",
      "Loss for  6390 th iteration => 0.061742990643\n",
      "Loss for  6391 th iteration => 0.0617348107472\n",
      "Loss for  6392 th iteration => 0.061723109048\n",
      "Loss for  6393 th iteration => 0.0617079528758\n",
      "Loss for  6394 th iteration => 0.0616992424304\n",
      "Loss for  6395 th iteration => 0.0616859158373\n",
      "Loss for  6396 th iteration => 0.0616743516145\n",
      "Loss for  6397 th iteration => 0.0616625658727\n",
      "Loss for  6398 th iteration => 0.0616530490589\n",
      "Loss for  6399 th iteration => 0.0616411632118\n",
      "Loss for  6400 th iteration => 0.061628346964\n",
      "Loss for  6401 th iteration => 0.06161544466\n",
      "Loss for  6402 th iteration => 0.0616068050683\n",
      "Loss for  6403 th iteration => 0.0615928791204\n",
      "Loss for  6404 th iteration => 0.0615803164518\n",
      "Loss for  6405 th iteration => 0.0615681902018\n",
      "Loss for  6406 th iteration => 0.0615587164837\n",
      "Loss for  6407 th iteration => 0.0615475354106\n",
      "Loss for  6408 th iteration => 0.0615350049272\n",
      "Loss for  6409 th iteration => 0.0615237017391\n",
      "Loss for  6410 th iteration => 0.0615150681458\n",
      "Loss for  6411 th iteration => 0.0615029276114\n",
      "Loss for  6412 th iteration => 0.061488886304\n",
      "Loss for  6413 th iteration => 0.0614793594829\n",
      "Loss for  6414 th iteration => 0.0614694177975\n",
      "Loss for  6415 th iteration => 0.0614541367028\n",
      "Loss for  6416 th iteration => 0.0614442665688\n",
      "Loss for  6417 th iteration => 0.0614323834767\n",
      "Loss for  6418 th iteration => 0.0614205923157\n",
      "Loss for  6419 th iteration => 0.0614085543367\n",
      "Loss for  6420 th iteration => 0.0613970508138\n",
      "Loss for  6421 th iteration => 0.0613880227013\n",
      "Loss for  6422 th iteration => 0.0613767034127\n",
      "Loss for  6423 th iteration => 0.0613623823051\n",
      "Loss for  6424 th iteration => 0.0613525876351\n",
      "Loss for  6425 th iteration => 0.0613398474365\n",
      "Loss for  6426 th iteration => 0.0613292205153\n",
      "Loss for  6427 th iteration => 0.061319980979\n",
      "Loss for  6428 th iteration => 0.0613037867671\n",
      "Loss for  6429 th iteration => 0.0612938962085\n",
      "Loss for  6430 th iteration => 0.0612826128787\n",
      "Loss for  6431 th iteration => 0.0612706096632\n",
      "Loss for  6432 th iteration => 0.0612620665627\n",
      "Loss for  6433 th iteration => 0.0612507321246\n",
      "Loss for  6434 th iteration => 0.0612360844179\n",
      "Loss for  6435 th iteration => 0.0612270512978\n",
      "Loss for  6436 th iteration => 0.0612139775169\n",
      "Loss for  6437 th iteration => 0.0612029304742\n",
      "Loss for  6438 th iteration => 0.0611908023001\n",
      "Loss for  6439 th iteration => 0.06118198284\n",
      "Loss for  6440 th iteration => 0.0611699998249\n",
      "Loss for  6441 th iteration => 0.0611571137281\n",
      "Loss for  6442 th iteration => 0.0611449067881\n",
      "Loss for  6443 th iteration => 0.0611360283829\n",
      "Loss for  6444 th iteration => 0.0611254426253\n",
      "Loss for  6445 th iteration => 0.0611105258319\n",
      "Loss for  6446 th iteration => 0.0611014219069\n",
      "Loss for  6447 th iteration => 0.0610887607637\n",
      "Loss for  6448 th iteration => 0.0610774166352\n",
      "Loss for  6449 th iteration => 0.0610654897889\n",
      "Loss for  6450 th iteration => 0.0610541332742\n",
      "Loss for  6451 th iteration => 0.0610443457111\n",
      "Loss for  6452 th iteration => 0.0610320711665\n",
      "Loss for  6453 th iteration => 0.0610194669785\n",
      "Loss for  6454 th iteration => 0.0610113326799\n",
      "Loss for  6455 th iteration => 0.0609970265299\n",
      "Loss for  6456 th iteration => 0.0609848601725\n",
      "Loss for  6457 th iteration => 0.0609737026881\n",
      "Loss for  6458 th iteration => 0.060963332\n",
      "Loss for  6459 th iteration => 0.0609527582188\n",
      "Loss for  6460 th iteration => 0.0609440509541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  6461 th iteration => 0.0609302514847\n",
      "Loss for  6462 th iteration => 0.0609181934598\n",
      "Loss for  6463 th iteration => 0.0609067540677\n",
      "Loss for  6464 th iteration => 0.0608952829218\n",
      "Loss for  6465 th iteration => 0.0608864609401\n",
      "Loss for  6466 th iteration => 0.0608752679399\n",
      "Loss for  6467 th iteration => 0.0608611592186\n",
      "Loss for  6468 th iteration => 0.0608518060307\n",
      "Loss for  6469 th iteration => 0.0608388726256\n",
      "Loss for  6470 th iteration => 0.0608283610564\n",
      "Loss for  6471 th iteration => 0.0608194525388\n",
      "Loss for  6472 th iteration => 0.0608058923116\n",
      "Loss for  6473 th iteration => 0.0607939778642\n",
      "Loss for  6474 th iteration => 0.0607824239478\n",
      "Loss for  6475 th iteration => 0.0607711176972\n",
      "Loss for  6476 th iteration => 0.0607621207764\n",
      "Loss for  6477 th iteration => 0.0607510777745\n",
      "Loss for  6478 th iteration => 0.0607371272071\n",
      "Loss for  6479 th iteration => 0.060727633709\n",
      "Loss for  6480 th iteration => 0.0607148346826\n",
      "Loss for  6481 th iteration => 0.0607028277527\n",
      "Loss for  6482 th iteration => 0.0606935183586\n",
      "Loss for  6483 th iteration => 0.0606791832636\n",
      "Loss for  6484 th iteration => 0.0606705974248\n",
      "Loss for  6485 th iteration => 0.060658438272\n",
      "Loss for  6486 th iteration => 0.0606493465915\n",
      "Loss for  6487 th iteration => 0.0606384320036\n",
      "Loss for  6488 th iteration => 0.0606252632341\n",
      "Loss for  6489 th iteration => 0.0606128740589\n",
      "Loss for  6490 th iteration => 0.0606049838428\n",
      "Loss for  6491 th iteration => 0.0605941232245\n",
      "Loss for  6492 th iteration => 0.060579072021\n",
      "Loss for  6493 th iteration => 0.0605675891376\n",
      "Loss for  6494 th iteration => 0.0605576948222\n",
      "Loss for  6495 th iteration => 0.0605473539921\n",
      "Loss for  6496 th iteration => 0.0605382704168\n",
      "Loss for  6497 th iteration => 0.060522400863\n",
      "Loss for  6498 th iteration => 0.0605128313589\n",
      "Loss for  6499 th iteration => 0.0605018008056\n",
      "Loss for  6500 th iteration => 0.0604929156182\n",
      "Loss for  6501 th iteration => 0.060480852893\n",
      "Loss for  6502 th iteration => 0.0604686920022\n",
      "Loss for  6503 th iteration => 0.0604566122997\n",
      "Loss for  6504 th iteration => 0.060447879713\n",
      "Loss for  6505 th iteration => 0.0604343669213\n",
      "Loss for  6506 th iteration => 0.0604226111709\n",
      "Loss for  6507 th iteration => 0.0604140506571\n",
      "Loss for  6508 th iteration => 0.0604014666109\n",
      "Loss for  6509 th iteration => 0.0603909420701\n",
      "Loss for  6510 th iteration => 0.0603786162406\n",
      "Loss for  6511 th iteration => 0.0603698965258\n",
      "Loss for  6512 th iteration => 0.0603589396936\n",
      "Loss for  6513 th iteration => 0.0603456755283\n",
      "Loss for  6514 th iteration => 0.0603337328836\n",
      "Loss for  6515 th iteration => 0.0603258452371\n",
      "Loss for  6516 th iteration => 0.0603147746959\n",
      "Loss for  6517 th iteration => 0.0603002228072\n",
      "Loss for  6518 th iteration => 0.0602916629624\n",
      "Loss for  6519 th iteration => 0.0602789491296\n",
      "Loss for  6520 th iteration => 0.0602667609529\n",
      "Loss for  6521 th iteration => 0.0602548892764\n",
      "Loss for  6522 th iteration => 0.0602491902767\n",
      "Loss for  6523 th iteration => 0.0602338667213\n",
      "Loss for  6524 th iteration => 0.0602209987283\n",
      "Loss for  6525 th iteration => 0.0602103534635\n",
      "Loss for  6526 th iteration => 0.0602026806832\n",
      "Loss for  6527 th iteration => 0.0601950258408\n",
      "Loss for  6528 th iteration => 0.0601778417644\n",
      "Loss for  6529 th iteration => 0.060168351944\n",
      "Loss for  6530 th iteration => 0.0601588827025\n",
      "Loss for  6531 th iteration => 0.060146224054\n",
      "Loss for  6532 th iteration => 0.0601360450483\n",
      "Loss for  6533 th iteration => 0.0601265281472\n",
      "Loss for  6534 th iteration => 0.0601129063363\n",
      "Loss for  6535 th iteration => 0.060102308481\n",
      "Loss for  6536 th iteration => 0.06008825232\n",
      "Loss for  6537 th iteration => 0.0600805973793\n",
      "Loss for  6538 th iteration => 0.060071339895\n",
      "Loss for  6539 th iteration => 0.0600556628357\n",
      "Loss for  6540 th iteration => 0.0600464526287\n",
      "Loss for  6541 th iteration => 0.0600353814494\n",
      "Loss for  6542 th iteration => 0.0600269169167\n",
      "Loss for  6543 th iteration => 0.0600149039048\n",
      "Loss for  6544 th iteration => 0.0600026649677\n",
      "Loss for  6545 th iteration => 0.0599911284898\n",
      "Loss for  6546 th iteration => 0.0599822686603\n",
      "Loss for  6547 th iteration => 0.0599688701149\n",
      "Loss for  6548 th iteration => 0.0599576206974\n",
      "Loss for  6549 th iteration => 0.0599491137985\n",
      "Loss for  6550 th iteration => 0.0599396629982\n",
      "Loss for  6551 th iteration => 0.0599245534062\n",
      "Loss for  6552 th iteration => 0.0599124694986\n",
      "Loss for  6553 th iteration => 0.0599037706166\n",
      "Loss for  6554 th iteration => 0.0598931844604\n",
      "Loss for  6555 th iteration => 0.0598843915209\n",
      "Loss for  6556 th iteration => 0.0598689166777\n",
      "Loss for  6557 th iteration => 0.0598606702854\n",
      "Loss for  6558 th iteration => 0.0598501340266\n",
      "Loss for  6559 th iteration => 0.0598378384896\n",
      "Loss for  6560 th iteration => 0.0598285559608\n",
      "Loss for  6561 th iteration => 0.0598180803014\n",
      "Loss for  6562 th iteration => 0.0598048419399\n",
      "Loss for  6563 th iteration => 0.0597949722018\n",
      "Loss for  6564 th iteration => 0.0597826607177\n",
      "Loss for  6565 th iteration => 0.0597715369658\n",
      "Loss for  6566 th iteration => 0.0597620739349\n",
      "Loss for  6567 th iteration => 0.0597536537976\n",
      "Loss for  6568 th iteration => 0.0597386405595\n",
      "Loss for  6569 th iteration => 0.0597290796849\n",
      "Loss for  6570 th iteration => 0.0597155226412\n",
      "Loss for  6571 th iteration => 0.0597081431775\n",
      "Loss for  6572 th iteration => 0.0596968271298\n",
      "Loss for  6573 th iteration => 0.0596844778566\n",
      "Loss for  6574 th iteration => 0.0596761485594\n",
      "Loss for  6575 th iteration => 0.0596649241055\n",
      "Loss for  6576 th iteration => 0.0596516431753\n",
      "Loss for  6577 th iteration => 0.0596426976211\n",
      "Loss for  6578 th iteration => 0.0596329677156\n",
      "Loss for  6579 th iteration => 0.0596188588927\n",
      "Loss for  6580 th iteration => 0.0596093691165\n",
      "Loss for  6581 th iteration => 0.0596009659934\n",
      "Loss for  6582 th iteration => 0.0595861225044\n",
      "Loss for  6583 th iteration => 0.059576658403\n",
      "Loss for  6584 th iteration => 0.0595629847591\n",
      "Loss for  6585 th iteration => 0.0595558713683\n",
      "Loss for  6586 th iteration => 0.0595443696273\n",
      "Loss for  6587 th iteration => 0.0595322190718\n",
      "Loss for  6588 th iteration => 0.0595239009947\n",
      "Loss for  6589 th iteration => 0.0595126012946\n",
      "Loss for  6590 th iteration => 0.059499543165\n",
      "Loss for  6591 th iteration => 0.0594906238055\n",
      "Loss for  6592 th iteration => 0.0594807814195\n",
      "Loss for  6593 th iteration => 0.0594669168347\n",
      "Loss for  6594 th iteration => 0.0594574664191\n",
      "Loss for  6595 th iteration => 0.0594489178561\n",
      "Loss for  6596 th iteration => 0.0594343376726\n",
      "Loss for  6597 th iteration => 0.0594249487028\n",
      "Loss for  6598 th iteration => 0.0594111441528\n",
      "Loss for  6599 th iteration => 0.0594031395893\n",
      "Loss for  6600 th iteration => 0.059391149194\n",
      "Loss for  6601 th iteration => 0.0593789127477\n",
      "Loss for  6602 th iteration => 0.0593713589238\n",
      "Loss for  6603 th iteration => 0.0593603885868\n",
      "Loss for  6604 th iteration => 0.059348679443\n",
      "Loss for  6605 th iteration => 0.0593394961626\n",
      "Loss for  6606 th iteration => 0.0593287672223\n",
      "Loss for  6607 th iteration => 0.059316192757\n",
      "Loss for  6608 th iteration => 0.0593064483951\n",
      "Loss for  6609 th iteration => 0.059297097915\n",
      "Loss for  6610 th iteration => 0.0592837542779\n",
      "Loss for  6611 th iteration => 0.0592738728472\n",
      "Loss for  6612 th iteration => 0.0592620899874\n",
      "Loss for  6613 th iteration => 0.0592525782606\n",
      "Loss for  6614 th iteration => 0.0592432856059\n",
      "Loss for  6615 th iteration => 0.059227944531\n",
      "Loss for  6616 th iteration => 0.0592193666819\n",
      "Loss for  6617 th iteration => 0.0592082396002\n",
      "Loss for  6618 th iteration => 0.0591983984226\n",
      "Loss for  6619 th iteration => 0.0591868857319\n",
      "Loss for  6620 th iteration => 0.0591793107748\n",
      "Loss for  6621 th iteration => 0.0591641078441\n",
      "Loss for  6622 th iteration => 0.0591550701836\n",
      "Loss for  6623 th iteration => 0.0591456698917\n",
      "Loss for  6624 th iteration => 0.059133957612\n",
      "Loss for  6625 th iteration => 0.059123829173\n",
      "Loss for  6626 th iteration => 0.0591111354817\n",
      "Loss for  6627 th iteration => 0.0591013458656\n",
      "Loss for  6628 th iteration => 0.0590914423005\n",
      "Loss for  6629 th iteration => 0.0590824829052\n",
      "Loss for  6630 th iteration => 0.0590691343641\n",
      "Loss for  6631 th iteration => 0.05905912359\n",
      "Loss for  6632 th iteration => 0.0590476527712\n",
      "Loss for  6633 th iteration => 0.0590381843705\n",
      "Loss for  6634 th iteration => 0.0590288514193\n",
      "Loss for  6635 th iteration => 0.0590135939543\n",
      "Loss for  6636 th iteration => 0.0590052051154\n",
      "Loss for  6637 th iteration => 0.0589939791357\n",
      "Loss for  6638 th iteration => 0.0589817346006\n",
      "Loss for  6639 th iteration => 0.0589740736939\n",
      "Loss for  6640 th iteration => 0.0589669478393\n",
      "Loss for  6641 th iteration => 0.0589502502069\n",
      "Loss for  6642 th iteration => 0.0589412414737\n",
      "Loss for  6643 th iteration => 0.058931920641\n",
      "Loss for  6644 th iteration => 0.0589203266696\n",
      "Loss for  6645 th iteration => 0.0589101838099\n",
      "Loss for  6646 th iteration => 0.0588976078188\n",
      "Loss for  6647 th iteration => 0.058887938657\n",
      "Loss for  6648 th iteration => 0.0588781101989\n",
      "Loss for  6649 th iteration => 0.0588690974412\n",
      "Loss for  6650 th iteration => 0.0588559481059\n",
      "Loss for  6651 th iteration => 0.0588459326068\n",
      "Loss for  6652 th iteration => 0.0588344923406\n",
      "Loss for  6653 th iteration => 0.0588236484109\n",
      "Loss for  6654 th iteration => 0.0588141326559\n",
      "Loss for  6655 th iteration => 0.0588035608416\n",
      "Loss for  6656 th iteration => 0.0587928728951\n",
      "Loss for  6657 th iteration => 0.0587837015164\n",
      "Loss for  6658 th iteration => 0.0587690292202\n",
      "Loss for  6659 th iteration => 0.0587619766506\n",
      "Loss for  6660 th iteration => 0.0587508156035\n",
      "Loss for  6661 th iteration => 0.0587391874335\n",
      "Loss for  6662 th iteration => 0.0587308227642\n",
      "Loss for  6663 th iteration => 0.0587197128783\n",
      "Loss for  6664 th iteration => 0.0587073380399\n",
      "Loss for  6665 th iteration => 0.0586984726481\n",
      "Loss for  6666 th iteration => 0.0586885708438\n",
      "Loss for  6667 th iteration => 0.0586755333332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  6668 th iteration => 0.0586662256172\n",
      "Loss for  6669 th iteration => 0.0586573961947\n",
      "Loss for  6670 th iteration => 0.0586437714966\n",
      "Loss for  6671 th iteration => 0.0586342738456\n",
      "Loss for  6672 th iteration => 0.0586237100838\n",
      "Loss for  6673 th iteration => 0.0586117717564\n",
      "Loss for  6674 th iteration => 0.0586028146884\n",
      "Loss for  6675 th iteration => 0.0585915681486\n",
      "Loss for  6676 th iteration => 0.0585816732039\n",
      "Loss for  6677 th iteration => 0.0585698263078\n",
      "Loss for  6678 th iteration => 0.0585576796216\n",
      "Loss for  6679 th iteration => 0.0585506781437\n",
      "Loss for  6680 th iteration => 0.0585398313874\n",
      "Loss for  6681 th iteration => 0.0585264101154\n",
      "Loss for  6682 th iteration => 0.0585184823333\n",
      "Loss for  6683 th iteration => 0.0585118196693\n",
      "Loss for  6684 th iteration => 0.0584952935259\n",
      "Loss for  6685 th iteration => 0.0584860894424\n",
      "Loss for  6686 th iteration => 0.0584772944363\n",
      "Loss for  6687 th iteration => 0.058464000242\n",
      "Loss for  6688 th iteration => 0.0584541719764\n",
      "Loss for  6689 th iteration => 0.0584494263512\n",
      "Loss for  6690 th iteration => 0.0584328665271\n",
      "Loss for  6691 th iteration => 0.0584221401973\n",
      "Loss for  6692 th iteration => 0.058415246155\n",
      "Loss for  6693 th iteration => 0.0584015766496\n",
      "Loss for  6694 th iteration => 0.0583904854747\n",
      "Loss for  6695 th iteration => 0.0583811767596\n",
      "Loss for  6696 th iteration => 0.0583717338764\n",
      "Loss for  6697 th iteration => 0.0583607312306\n",
      "Loss for  6698 th iteration => 0.0583493224332\n",
      "Loss for  6699 th iteration => 0.0583399227489\n",
      "Loss for  6700 th iteration => 0.0583294181725\n",
      "Loss for  6701 th iteration => 0.0583188549408\n",
      "Loss for  6702 th iteration => 0.0583082245824\n",
      "Loss for  6703 th iteration => 0.058298222308\n",
      "Loss for  6704 th iteration => 0.0582870655106\n",
      "Loss for  6705 th iteration => 0.058278173333\n",
      "Loss for  6706 th iteration => 0.0582686137638\n",
      "Loss for  6707 th iteration => 0.05825369037\n",
      "Loss for  6708 th iteration => 0.0582473724871\n",
      "Loss for  6709 th iteration => 0.0582392262768\n",
      "Loss for  6710 th iteration => 0.0582228574837\n",
      "Loss for  6711 th iteration => 0.058215331178\n",
      "Loss for  6712 th iteration => 0.0582049897639\n",
      "Loss for  6713 th iteration => 0.0581918452468\n",
      "Loss for  6714 th iteration => 0.0581835515416\n",
      "Loss for  6715 th iteration => 0.0581773971115\n",
      "Loss for  6716 th iteration => 0.0581609923042\n",
      "Loss for  6717 th iteration => 0.0581516362074\n",
      "Loss for  6718 th iteration => 0.0581430547649\n",
      "Loss for  6719 th iteration => 0.0581320088236\n",
      "Loss for  6720 th iteration => 0.058121297125\n",
      "Loss for  6721 th iteration => 0.0581123941496\n",
      "Loss for  6722 th iteration => 0.0581007845923\n",
      "Loss for  6723 th iteration => 0.0580899178063\n",
      "Loss for  6724 th iteration => 0.058078653781\n",
      "Loss for  6725 th iteration => 0.0580692548194\n",
      "Loss for  6726 th iteration => 0.0580589366692\n",
      "Loss for  6727 th iteration => 0.0580483736915\n",
      "Loss for  6728 th iteration => 0.0580378361716\n",
      "Loss for  6729 th iteration => 0.058027978183\n",
      "Loss for  6730 th iteration => 0.058016816948\n",
      "Loss for  6731 th iteration => 0.0580063853575\n",
      "Loss for  6732 th iteration => 0.0579970927843\n",
      "Loss for  6733 th iteration => 0.0579865913339\n",
      "Loss for  6734 th iteration => 0.0579750445886\n",
      "Loss for  6735 th iteration => 0.0579661927524\n",
      "Loss for  6736 th iteration => 0.0579551107761\n",
      "Loss for  6737 th iteration => 0.0579455917304\n",
      "Loss for  6738 th iteration => 0.0579336387476\n",
      "Loss for  6739 th iteration => 0.0579218116134\n",
      "Loss for  6740 th iteration => 0.0579156891664\n",
      "Loss for  6741 th iteration => 0.0579074913826\n",
      "Loss for  6742 th iteration => 0.0578912598508\n",
      "Loss for  6743 th iteration => 0.0578839614688\n",
      "Loss for  6744 th iteration => 0.0578735516854\n",
      "Loss for  6745 th iteration => 0.0578605344252\n",
      "Loss for  6746 th iteration => 0.0578527158367\n",
      "Loss for  6747 th iteration => 0.0578462310943\n",
      "Loss for  6748 th iteration => 0.0578299719134\n",
      "Loss for  6749 th iteration => 0.0578211948973\n",
      "Loss for  6750 th iteration => 0.0578156761857\n",
      "Loss for  6751 th iteration => 0.0577993994685\n",
      "Loss for  6752 th iteration => 0.0577898336449\n",
      "Loss for  6753 th iteration => 0.0577816372029\n",
      "Loss for  6754 th iteration => 0.0577706018732\n",
      "Loss for  6755 th iteration => 0.0577600143054\n",
      "Loss for  6756 th iteration => 0.0577512669553\n",
      "Loss for  6757 th iteration => 0.0577397487654\n",
      "Loss for  6758 th iteration => 0.0577289819074\n",
      "Loss for  6759 th iteration => 0.0577178501008\n",
      "Loss for  6760 th iteration => 0.0577085908745\n",
      "Loss for  6761 th iteration => 0.0576984270932\n",
      "Loss for  6762 th iteration => 0.0576878610508\n",
      "Loss for  6763 th iteration => 0.0576775417845\n",
      "Loss for  6764 th iteration => 0.0576677907476\n",
      "Loss for  6765 th iteration => 0.0576566760595\n",
      "Loss for  6766 th iteration => 0.0576464595924\n",
      "Loss for  6767 th iteration => 0.0576373493856\n",
      "Loss for  6768 th iteration => 0.0576292435923\n",
      "Loss for  6769 th iteration => 0.0576157585022\n",
      "Loss for  6770 th iteration => 0.0576065484876\n",
      "Loss for  6771 th iteration => 0.0575963069216\n",
      "Loss for  6772 th iteration => 0.0575862339507\n",
      "Loss for  6773 th iteration => 0.0575771626777\n",
      "Loss for  6774 th iteration => 0.0575628715864\n",
      "Loss for  6775 th iteration => 0.0575567553352\n",
      "Loss for  6776 th iteration => 0.0575454431248\n",
      "Loss for  6777 th iteration => 0.05753245308\n",
      "Loss for  6778 th iteration => 0.057525786219\n",
      "Loss for  6779 th iteration => 0.0575182964217\n",
      "Loss for  6780 th iteration => 0.0575022012456\n",
      "Loss for  6781 th iteration => 0.0574946153185\n",
      "Loss for  6782 th iteration => 0.0574880522192\n",
      "Loss for  6783 th iteration => 0.0574719406657\n",
      "Loss for  6784 th iteration => 0.0574636570652\n",
      "Loss for  6785 th iteration => 0.0574543953737\n",
      "Loss for  6786 th iteration => 0.0574415197762\n",
      "Loss for  6787 th iteration => 0.0574329610744\n",
      "Loss for  6788 th iteration => 0.0574273908559\n",
      "Loss for  6789 th iteration => 0.057411273433\n",
      "Loss for  6790 th iteration => 0.0574020591677\n",
      "Loss for  6791 th iteration => 0.0573971534332\n",
      "Loss for  6792 th iteration => 0.0573810276032\n",
      "Loss for  6793 th iteration => 0.0573712489624\n",
      "Loss for  6794 th iteration => 0.0573669170034\n",
      "Loss for  6795 th iteration => 0.0573507855402\n",
      "Loss for  6796 th iteration => 0.0573406094333\n",
      "Loss for  6797 th iteration => 0.0573332336194\n",
      "Loss for  6798 th iteration => 0.0573223672715\n",
      "Loss for  6799 th iteration => 0.0573114180178\n",
      "Loss for  6800 th iteration => 0.0573009376713\n",
      "Loss for  6801 th iteration => 0.0572917001513\n",
      "Loss for  6802 th iteration => 0.0572812251265\n",
      "Loss for  6803 th iteration => 0.0572701324958\n",
      "Loss for  6804 th iteration => 0.0572609997798\n",
      "Loss for  6805 th iteration => 0.0572511116023\n",
      "Loss for  6806 th iteration => 0.0572405474393\n",
      "Loss for  6807 th iteration => 0.0572304055318\n",
      "Loss for  6808 th iteration => 0.0572209981449\n",
      "Loss for  6809 th iteration => 0.0572127180506\n",
      "Loss for  6810 th iteration => 0.0572001177523\n",
      "Loss for  6811 th iteration => 0.0571906294715\n",
      "Loss for  6812 th iteration => 0.0571796833775\n",
      "Loss for  6813 th iteration => 0.057169523938\n",
      "Loss for  6814 th iteration => 0.0571606319508\n",
      "Loss for  6815 th iteration => 0.0571501737007\n",
      "Loss for  6816 th iteration => 0.0571390351408\n",
      "Loss for  6817 th iteration => 0.0571306685884\n",
      "Loss for  6818 th iteration => 0.0571224626218\n",
      "Loss for  6819 th iteration => 0.0571088520953\n",
      "Loss for  6820 th iteration => 0.0571003500605\n",
      "Loss for  6821 th iteration => 0.0570894190698\n",
      "Loss for  6822 th iteration => 0.0570783846552\n",
      "Loss for  6823 th iteration => 0.0570677417339\n",
      "Loss for  6824 th iteration => 0.0570621381423\n",
      "Loss for  6825 th iteration => 0.0570489484713\n",
      "Loss for  6826 th iteration => 0.0570399354532\n",
      "Loss for  6827 th iteration => 0.0570297386984\n",
      "Loss for  6828 th iteration => 0.057018584397\n",
      "Loss for  6829 th iteration => 0.0570100573472\n",
      "Loss for  6830 th iteration => 0.0570022213654\n",
      "Loss for  6831 th iteration => 0.0569885242165\n",
      "Loss for  6832 th iteration => 0.0569799283685\n",
      "Loss for  6833 th iteration => 0.0569691891194\n",
      "Loss for  6834 th iteration => 0.0569600354695\n",
      "Loss for  6835 th iteration => 0.0569484420099\n",
      "Loss for  6836 th iteration => 0.0569392258929\n",
      "Loss for  6837 th iteration => 0.0569303295191\n",
      "Loss for  6838 th iteration => 0.056921336128\n",
      "Loss for  6839 th iteration => 0.0569070013443\n",
      "Loss for  6840 th iteration => 0.0569010434009\n",
      "Loss for  6841 th iteration => 0.0568931142618\n",
      "Loss for  6842 th iteration => 0.0568772863247\n",
      "Loss for  6843 th iteration => 0.0568706453834\n",
      "Loss for  6844 th iteration => 0.0568634077473\n",
      "Loss for  6845 th iteration => 0.0568475704668\n",
      "Loss for  6846 th iteration => 0.0568404055563\n",
      "Loss for  6847 th iteration => 0.0568304941933\n",
      "Loss for  6848 th iteration => 0.0568176993493\n",
      "Loss for  6849 th iteration => 0.056810477809\n",
      "Loss for  6850 th iteration => 0.0568038423985\n",
      "Loss for  6851 th iteration => 0.0567880108128\n",
      "Loss for  6852 th iteration => 0.0567802949677\n",
      "Loss for  6853 th iteration => 0.0567741633439\n",
      "Loss for  6854 th iteration => 0.0567583281114\n",
      "Loss for  6855 th iteration => 0.0567501896337\n",
      "Loss for  6856 th iteration => 0.0567444905168\n",
      "Loss for  6857 th iteration => 0.0567286538106\n",
      "Loss for  6858 th iteration => 0.0567201561848\n",
      "Loss for  6859 th iteration => 0.0567148264448\n",
      "Loss for  6860 th iteration => 0.0566989901929\n",
      "Loss for  6861 th iteration => 0.0566902124016\n",
      "Loss for  6862 th iteration => 0.0566819809815\n",
      "Loss for  6863 th iteration => 0.0566691822612\n",
      "Loss for  6864 th iteration => 0.0566606393353\n",
      "Loss for  6865 th iteration => 0.0566553795849\n",
      "Loss for  6866 th iteration => 0.0566395652994\n",
      "Loss for  6867 th iteration => 0.0566307536529\n",
      "Loss for  6868 th iteration => 0.0566257733459\n",
      "Loss for  6869 th iteration => 0.0566099622499\n",
      "Loss for  6870 th iteration => 0.0566009277003\n",
      "Loss for  6871 th iteration => 0.0565961812842\n",
      "Loss for  6872 th iteration => 0.0565803747711\n",
      "Loss for  6873 th iteration => 0.0565711578856\n",
      "Loss for  6874 th iteration => 0.0565666050294\n",
      "Loss for  6875 th iteration => 0.0565508043331\n",
      "Loss for  6876 th iteration => 0.0565414410259\n",
      "Loss for  6877 th iteration => 0.0565370460254\n",
      "Loss for  6878 th iteration => 0.0565212522377\n",
      "Loss for  6879 th iteration => 0.0565118074576\n",
      "Loss for  6880 th iteration => 0.0565049330541\n",
      "Loss for  6881 th iteration => 0.0564915715905\n",
      "Loss for  6882 th iteration => 0.0564824854288\n",
      "Loss for  6883 th iteration => 0.056477839861\n",
      "Loss for  6884 th iteration => 0.0564620777842\n",
      "Loss for  6885 th iteration => 0.0564528983416\n",
      "Loss for  6886 th iteration => 0.0564453036602\n",
      "Loss for  6887 th iteration => 0.0564324449472\n",
      "Loss for  6888 th iteration => 0.056423717294\n",
      "Loss for  6889 th iteration => 0.0564166934285\n",
      "Loss for  6890 th iteration => 0.0564028931208\n",
      "Loss for  6891 th iteration => 0.0563943678501\n",
      "Loss for  6892 th iteration => 0.0563892020857\n",
      "Loss for  6893 th iteration => 0.0563734920649\n",
      "Loss for  6894 th iteration => 0.0563648281007\n",
      "Loss for  6895 th iteration => 0.0563598124159\n",
      "Loss for  6896 th iteration => 0.0563441094716\n",
      "Loss for  6897 th iteration => 0.0563353375051\n",
      "Loss for  6898 th iteration => 0.0563304413882\n",
      "Loss for  6899 th iteration => 0.0563147464508\n",
      "Loss for  6900 th iteration => 0.0563058936527\n",
      "Loss for  6901 th iteration => 0.0563010900915\n",
      "Loss for  6902 th iteration => 0.0562854039838\n",
      "Loss for  6903 th iteration => 0.0562764944048\n",
      "Loss for  6904 th iteration => 0.0562717594883\n",
      "Loss for  6905 th iteration => 0.0562560829375\n",
      "Loss for  6906 th iteration => 0.0562471662412\n",
      "Loss for  6907 th iteration => 0.0562400574476\n",
      "Loss for  6908 th iteration => 0.0562266339277\n",
      "Loss for  6909 th iteration => 0.0562181546832\n",
      "Loss for  6910 th iteration => 0.056213016738\n",
      "Loss for  6911 th iteration => 0.0561973763556\n",
      "Loss for  6912 th iteration => 0.0561888410297\n",
      "Loss for  6913 th iteration => 0.0561837709859\n",
      "Loss for  6914 th iteration => 0.0561681401453\n",
      "Loss for  6915 th iteration => 0.0561595920519\n",
      "Loss for  6916 th iteration => 0.0561517334531\n",
      "Loss for  6917 th iteration => 0.0561387649073\n",
      "Loss for  6918 th iteration => 0.0561307394061\n",
      "Loss for  6919 th iteration => 0.0561233250131\n",
      "Loss for  6920 th iteration => 0.0561094737848\n",
      "Loss for  6921 th iteration => 0.0561017290541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  6922 th iteration => 0.0560959101559\n",
      "Loss for  6923 th iteration => 0.0560803379903\n",
      "Loss for  6924 th iteration => 0.0560725102462\n",
      "Loss for  6925 th iteration => 0.056066785943\n",
      "Loss for  6926 th iteration => 0.056051222445\n",
      "Loss for  6927 th iteration => 0.0560433357076\n",
      "Loss for  6928 th iteration => 0.0560376821186\n",
      "Loss for  6929 th iteration => 0.0560221280125\n",
      "Loss for  6930 th iteration => 0.0560142035486\n",
      "Loss for  6931 th iteration => 0.0560085995293\n",
      "Loss for  6932 th iteration => 0.0559930554552\n",
      "Loss for  6933 th iteration => 0.0559851120911\n",
      "Loss for  6934 th iteration => 0.0559795389224\n",
      "Loss for  6935 th iteration => 0.0559640054455\n",
      "Loss for  6936 th iteration => 0.055956059844\n",
      "Loss for  6937 th iteration => 0.0559505009569\n",
      "Loss for  6938 th iteration => 0.0559349785762\n",
      "Loss for  6939 th iteration => 0.0559270454823\n",
      "Loss for  6940 th iteration => 0.0559214862133\n",
      "Loss for  6941 th iteration => 0.0559059753687\n",
      "Loss for  6942 th iteration => 0.0558980681628\n",
      "Loss for  6943 th iteration => 0.0558904323241\n",
      "Loss for  6944 th iteration => 0.0558768423109\n",
      "Loss for  6945 th iteration => 0.0558694632071\n",
      "Loss for  6946 th iteration => 0.0558633776994\n",
      "Loss for  6947 th iteration => 0.0558479064612\n",
      "Loss for  6948 th iteration => 0.0558405178216\n",
      "Loss for  6949 th iteration => 0.055834453782\n",
      "Loss for  6950 th iteration => 0.0558189933668\n",
      "Loss for  6951 th iteration => 0.0558116106345\n",
      "Loss for  6952 th iteration => 0.0558055527058\n",
      "Loss for  6953 th iteration => 0.0557901035743\n",
      "Loss for  6954 th iteration => 0.0557827404165\n",
      "Loss for  6955 th iteration => 0.0557766750061\n",
      "Loss for  6956 th iteration => 0.0557612375647\n",
      "Loss for  6957 th iteration => 0.0557539060736\n",
      "Loss for  6958 th iteration => 0.0557478211538\n",
      "Loss for  6959 th iteration => 0.0557323957607\n",
      "Loss for  6960 th iteration => 0.0557251221513\n",
      "Loss for  6961 th iteration => 0.05571665455\n",
      "Loss for  6962 th iteration => 0.0557034118256\n",
      "Loss for  6963 th iteration => 0.0556967341672\n",
      "Loss for  6964 th iteration => 0.0556885215355\n",
      "Loss for  6965 th iteration => 0.0556745609262\n",
      "Loss for  6966 th iteration => 0.0556676621194\n",
      "Loss for  6967 th iteration => 0.0556598223876\n",
      "Loss for  6968 th iteration => 0.0556458809797\n",
      "Loss for  6969 th iteration => 0.0556385911178\n",
      "Loss for  6970 th iteration => 0.055631157696\n",
      "Loss for  6971 th iteration => 0.0556172346817\n",
      "Loss for  6972 th iteration => 0.0556095364498\n",
      "Loss for  6973 th iteration => 0.0556027918022\n",
      "Loss for  6974 th iteration => 0.0555903257002\n",
      "Loss for  6975 th iteration => 0.0555809013859\n",
      "Loss for  6976 th iteration => 0.0555740331135\n",
      "Loss for  6977 th iteration => 0.0555616829728\n",
      "Loss for  6978 th iteration => 0.0555523015539\n",
      "Loss for  6979 th iteration => 0.0555453016715\n",
      "Loss for  6980 th iteration => 0.0555330612209\n",
      "Loss for  6981 th iteration => 0.0555237359849\n",
      "Loss for  6982 th iteration => 0.0555165975862\n",
      "Loss for  6983 th iteration => 0.0555044610786\n",
      "Loss for  6984 th iteration => 0.0554952038092\n",
      "Loss for  6985 th iteration => 0.0554879362762\n",
      "Loss for  6986 th iteration => 0.0554740725792\n",
      "Loss for  6987 th iteration => 0.0554663227516\n",
      "Loss for  6988 th iteration => 0.0554582951296\n",
      "Loss for  6989 th iteration => 0.055447200549\n",
      "Loss for  6990 th iteration => 0.055437407928\n",
      "Loss for  6991 th iteration => 0.0554297485052\n",
      "Loss for  6992 th iteration => 0.0554187427842\n",
      "Loss for  6993 th iteration => 0.0554085798229\n",
      "Loss for  6994 th iteration => 0.0554012200586\n",
      "Loss for  6995 th iteration => 0.0553903217268\n",
      "Loss for  6996 th iteration => 0.0553798398883\n",
      "Loss for  6997 th iteration => 0.0553712756446\n",
      "Loss for  6998 th iteration => 0.0553636168179\n",
      "Loss for  6999 th iteration => 0.0553520085826\n",
      "Loss for  7000 th iteration => 0.0553442790407\n",
      "Loss for  7001 th iteration => 0.0553351861744\n",
      "Loss for  7002 th iteration => 0.0553235860417\n",
      "Loss for  7003 th iteration => 0.0553158156454\n",
      "Loss for  7004 th iteration => 0.0553067763597\n",
      "Loss for  7005 th iteration => 0.0552951912846\n",
      "Loss for  7006 th iteration => 0.055287385051\n",
      "Loss for  7007 th iteration => 0.0552783879854\n",
      "Loss for  7008 th iteration => 0.0552668242037\n",
      "Loss for  7009 th iteration => 0.0552589866415\n",
      "Loss for  7010 th iteration => 0.0552500385\n",
      "Loss for  7011 th iteration => 0.0552378491941\n",
      "Loss for  7012 th iteration => 0.0552293379219\n",
      "Loss for  7013 th iteration => 0.0552217006145\n",
      "Loss for  7014 th iteration => 0.0552091904694\n",
      "Loss for  7015 th iteration => 0.0552010563242\n",
      "Loss for  7016 th iteration => 0.0551934054366\n",
      "Loss for  7017 th iteration => 0.0551805928942\n",
      "Loss for  7018 th iteration => 0.0551732124591\n",
      "Loss for  7019 th iteration => 0.0551674155056\n",
      "Loss for  7020 th iteration => 0.0551522161866\n",
      "Loss for  7021 th iteration => 0.0551451467335\n",
      "Loss for  7022 th iteration => 0.0551390525534\n",
      "Loss for  7023 th iteration => 0.0551238735449\n",
      "Loss for  7024 th iteration => 0.0551170938595\n",
      "Loss for  7025 th iteration => 0.0551107234986\n",
      "Loss for  7026 th iteration => 0.0550955641338\n",
      "Loss for  7027 th iteration => 0.0550890553668\n",
      "Loss for  7028 th iteration => 0.0550824275239\n",
      "Loss for  7029 th iteration => 0.0550672872096\n",
      "Loss for  7030 th iteration => 0.0550610326091\n",
      "Loss for  7031 th iteration => 0.0550541639013\n",
      "Loss for  7032 th iteration => 0.055039095276\n",
      "Loss for  7033 th iteration => 0.0550324874709\n",
      "Loss for  7034 th iteration => 0.0550243425985\n",
      "Loss for  7035 th iteration => 0.0550109830929\n",
      "Loss for  7036 th iteration => 0.0550040483658\n",
      "Loss for  7037 th iteration => 0.0549962466063\n",
      "Loss for  7038 th iteration => 0.0549829094916\n",
      "Loss for  7039 th iteration => 0.0549756151372\n",
      "Loss for  7040 th iteration => 0.0549685080739\n",
      "Loss for  7041 th iteration => 0.0549568662672\n",
      "Loss for  7042 th iteration => 0.0549476498406\n",
      "Loss for  7043 th iteration => 0.0549403603409\n",
      "Loss for  7044 th iteration => 0.0549287638115\n",
      "Loss for  7045 th iteration => 0.054919710074\n",
      "Loss for  7046 th iteration => 0.0549122405114\n",
      "Loss for  7047 th iteration => 0.0549006879652\n",
      "Loss for  7048 th iteration => 0.0548917958049\n",
      "Loss for  7049 th iteration => 0.0548841485329\n",
      "Loss for  7050 th iteration => 0.0548727257624\n",
      "Loss for  7051 th iteration => 0.0548628206771\n",
      "Loss for  7052 th iteration => 0.054853680511\n",
      "Loss for  7053 th iteration => 0.0548447818595\n",
      "Loss for  7054 th iteration => 0.054834277164\n",
      "Loss for  7055 th iteration => 0.0548257847435\n",
      "Loss for  7056 th iteration => 0.0548193255516\n",
      "Loss for  7057 th iteration => 0.0548060059335\n",
      "Loss for  7058 th iteration => 0.0547982558098\n",
      "Loss for  7059 th iteration => 0.0547929791539\n",
      "Loss for  7060 th iteration => 0.054777914519\n",
      "Loss for  7061 th iteration => 0.0547706842052\n",
      "Loss for  7062 th iteration => 0.0547649028352\n",
      "Loss for  7063 th iteration => 0.0547498647225\n",
      "Loss for  7064 th iteration => 0.054743109397\n",
      "Loss for  7065 th iteration => 0.054736867778\n",
      "Loss for  7066 th iteration => 0.0547218547887\n",
      "Loss for  7067 th iteration => 0.0547155346476\n",
      "Loss for  7068 th iteration => 0.0547088722689\n",
      "Loss for  7069 th iteration => 0.0546938948887\n",
      "Loss for  7070 th iteration => 0.054687335425\n",
      "Loss for  7071 th iteration => 0.0546793091173\n",
      "Loss for  7072 th iteration => 0.0546660899407\n",
      "Loss for  7073 th iteration => 0.0546587636369\n",
      "Loss for  7074 th iteration => 0.0546503209776\n",
      "Loss for  7075 th iteration => 0.0546383676684\n",
      "Loss for  7076 th iteration => 0.0546304182637\n",
      "Loss for  7077 th iteration => 0.0546231458027\n",
      "Loss for  7078 th iteration => 0.0546126310357\n",
      "Loss for  7079 th iteration => 0.0546024424516\n",
      "Loss for  7080 th iteration => 0.0545958688193\n",
      "Loss for  7081 th iteration => 0.0545865412119\n",
      "Loss for  7082 th iteration => 0.054574752725\n",
      "Loss for  7083 th iteration => 0.0545682078569\n",
      "Loss for  7084 th iteration => 0.0545587351126\n",
      "Loss for  7085 th iteration => 0.0545470885061\n",
      "Loss for  7086 th iteration => 0.054540568341\n",
      "Loss for  7087 th iteration => 0.0545309603769\n",
      "Loss for  7088 th iteration => 0.0545194498096\n",
      "Loss for  7089 th iteration => 0.0545129507212\n",
      "Loss for  7090 th iteration => 0.0545032164523\n",
      "Loss for  7091 th iteration => 0.054491836646\n",
      "Loss for  7092 th iteration => 0.054485355398\n",
      "Loss for  7093 th iteration => 0.054475566735\n",
      "Loss for  7094 th iteration => 0.0544636065389\n",
      "Loss for  7095 th iteration => 0.0544565813398\n",
      "Loss for  7096 th iteration => 0.0544479266293\n",
      "Loss for  7097 th iteration => 0.0544356201969\n",
      "Loss for  7098 th iteration => 0.0544291195684\n",
      "Loss for  7099 th iteration => 0.0544203318538\n",
      "Loss for  7100 th iteration => 0.0544077005449\n",
      "Loss for  7101 th iteration => 0.0544021417931\n",
      "Loss for  7102 th iteration => 0.0543948511351\n",
      "Loss for  7103 th iteration => 0.0543800228377\n",
      "Loss for  7104 th iteration => 0.0543740492627\n",
      "Loss for  7105 th iteration => 0.0543655909448\n",
      "Loss for  7106 th iteration => 0.0543525260529\n",
      "Loss for  7107 th iteration => 0.054345570932\n",
      "Loss for  7108 th iteration => 0.0543371151968\n",
      "Loss for  7109 th iteration => 0.054327322632\n",
      "Loss for  7110 th iteration => 0.0543178665592\n",
      "Loss for  7111 th iteration => 0.0543099097124\n",
      "Loss for  7112 th iteration => 0.0543016426313\n",
      "Loss for  7113 th iteration => 0.0542904441951\n",
      "Loss for  7114 th iteration => 0.0542825778253\n",
      "Loss for  7115 th iteration => 0.0542740475659\n",
      "Loss for  7116 th iteration => 0.054263046568\n",
      "Loss for  7117 th iteration => 0.0542552614883\n",
      "Loss for  7118 th iteration => 0.0542465469271\n",
      "Loss for  7119 th iteration => 0.0542346547716\n",
      "Loss for  7120 th iteration => 0.0542267694948\n",
      "Loss for  7121 th iteration => 0.0542190934784\n",
      "Loss for  7122 th iteration => 0.0542068359559\n",
      "Loss for  7123 th iteration => 0.0541998819019\n",
      "Loss for  7124 th iteration => 0.0541940580461\n",
      "Loss for  7125 th iteration => 0.0541792510556\n",
      "Loss for  7126 th iteration => 0.0541729866338\n",
      "Loss for  7127 th iteration => 0.0541664892283\n",
      "Loss for  7128 th iteration => 0.0541517136041\n",
      "Loss for  7129 th iteration => 0.0541460749262\n",
      "Loss for  7130 th iteration => 0.0541389673377\n",
      "Loss for  7131 th iteration => 0.0541242210931\n",
      "Loss for  7132 th iteration => 0.0541191513984\n",
      "Loss for  7133 th iteration => 0.0541114899304\n",
      "Loss for  7134 th iteration => 0.0540969332446\n",
      "Loss for  7135 th iteration => 0.0540903772765\n",
      "Loss for  7136 th iteration => 0.0540814685546\n",
      "Loss for  7137 th iteration => 0.054069734729\n",
      "Loss for  7138 th iteration => 0.0540625806979\n",
      "Loss for  7139 th iteration => 0.0540548474016\n",
      "Loss for  7140 th iteration => 0.0540464465556\n",
      "Loss for  7141 th iteration => 0.0540354020816\n",
      "Loss for  7142 th iteration => 0.0540277775633\n",
      "Loss for  7143 th iteration => 0.0540190642036\n",
      "Loss for  7144 th iteration => 0.0540082476572\n",
      "Loss for  7145 th iteration => 0.0540007205089\n",
      "Loss for  7146 th iteration => 0.0539918205392\n",
      "Loss for  7147 th iteration => 0.053979977918\n",
      "Loss for  7148 th iteration => 0.0539725368946\n",
      "Loss for  7149 th iteration => 0.0539645952025\n",
      "Loss for  7150 th iteration => 0.0539523806921\n",
      "Loss for  7151 th iteration => 0.0539460118133\n",
      "Loss for  7152 th iteration => 0.0539397096389\n",
      "Loss for  7153 th iteration => 0.0539250190582\n",
      "Loss for  7154 th iteration => 0.0539193831657\n",
      "Loss for  7155 th iteration => 0.0539123642911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  7156 th iteration => 0.0538977062366\n",
      "Loss for  7157 th iteration => 0.0538927346237\n",
      "Loss for  7158 th iteration => 0.0538850671907\n",
      "Loss for  7159 th iteration => 0.0538705919188\n",
      "Loss for  7160 th iteration => 0.0538641092245\n",
      "Loss for  7161 th iteration => 0.0538552470897\n",
      "Loss for  7162 th iteration => 0.053843599877\n",
      "Loss for  7163 th iteration => 0.0538365314247\n",
      "Loss for  7164 th iteration => 0.0538290023276\n",
      "Loss for  7165 th iteration => 0.053820463153\n",
      "Loss for  7166 th iteration => 0.0538095675693\n",
      "Loss for  7167 th iteration => 0.0538021621236\n",
      "Loss for  7168 th iteration => 0.053793269955\n",
      "Loss for  7169 th iteration => 0.0537826274543\n",
      "Loss for  7170 th iteration => 0.0537753324366\n",
      "Loss for  7171 th iteration => 0.0537662330912\n",
      "Loss for  7172 th iteration => 0.0537544565274\n",
      "Loss for  7173 th iteration => 0.0537473980283\n",
      "Loss for  7174 th iteration => 0.0537392097658\n",
      "Loss for  7175 th iteration => 0.0537270564258\n",
      "Loss for  7176 th iteration => 0.0537212189335\n",
      "Loss for  7177 th iteration => 0.0537144798185\n",
      "Loss for  7178 th iteration => 0.0536998927555\n",
      "Loss for  7179 th iteration => 0.0536948234518\n",
      "Loss for  7180 th iteration => 0.0536873325919\n",
      "Loss for  7181 th iteration => 0.0536728276226\n",
      "Loss for  7182 th iteration => 0.0536664415144\n",
      "Loss for  7183 th iteration => 0.0536575871689\n",
      "Loss for  7184 th iteration => 0.053646014855\n",
      "Loss for  7185 th iteration => 0.0536390545789\n",
      "Loss for  7186 th iteration => 0.0536316667463\n",
      "Loss for  7187 th iteration => 0.0536230996768\n",
      "Loss for  7188 th iteration => 0.0536122779942\n",
      "Loss for  7189 th iteration => 0.0536050304953\n",
      "Loss for  7190 th iteration => 0.0535960666424\n",
      "Loss for  7191 th iteration => 0.0535855247433\n",
      "Loss for  7192 th iteration => 0.0535784024061\n",
      "Loss for  7193 th iteration => 0.0535691570048\n",
      "Loss for  7194 th iteration => 0.0535574848436\n",
      "Loss for  7195 th iteration => 0.0535506405666\n",
      "Loss for  7196 th iteration => 0.0535423077095\n",
      "Loss for  7197 th iteration => 0.0535302530692\n",
      "Loss for  7198 th iteration => 0.0535247732125\n",
      "Loss for  7199 th iteration => 0.0535177579744\n",
      "Loss for  7200 th iteration => 0.0535032582143\n",
      "Loss for  7201 th iteration => 0.0534985874343\n",
      "Loss for  7202 th iteration => 0.0534907797864\n",
      "Loss for  7203 th iteration => 0.053476390159\n",
      "Loss for  7204 th iteration => 0.0534701398052\n",
      "Loss for  7205 th iteration => 0.0534612934342\n",
      "Loss for  7206 th iteration => 0.0534517184844\n",
      "Loss for  7207 th iteration => 0.0534432063046\n",
      "Loss for  7208 th iteration => 0.0534353510047\n",
      "Loss for  7209 th iteration => 0.0534270982846\n",
      "Loss for  7210 th iteration => 0.0534166117347\n",
      "Loss for  7211 th iteration => 0.0534089234055\n",
      "Loss for  7212 th iteration => 0.0534002416428\n",
      "Loss for  7213 th iteration => 0.0533886725122\n",
      "Loss for  7214 th iteration => 0.0533813507343\n",
      "Loss for  7215 th iteration => 0.0533762202299\n",
      "Loss for  7216 th iteration => 0.0533617197592\n",
      "Loss for  7217 th iteration => 0.0533554676072\n",
      "Loss for  7218 th iteration => 0.0533492862807\n",
      "Loss for  7219 th iteration => 0.0533348304915\n",
      "Loss for  7220 th iteration => 0.0533281927193\n",
      "Loss for  7221 th iteration => 0.0533209966079\n",
      "Loss for  7222 th iteration => 0.0533106618195\n",
      "Loss for  7223 th iteration => 0.0533020175514\n",
      "Loss for  7224 th iteration => 0.0532944094576\n",
      "Loss for  7225 th iteration => 0.0532838524595\n",
      "Loss for  7226 th iteration => 0.05327583919\n",
      "Loss for  7227 th iteration => 0.0532678489097\n",
      "Loss for  7228 th iteration => 0.0532572142778\n",
      "Loss for  7229 th iteration => 0.0532485154423\n",
      "Loss for  7230 th iteration => 0.0532404182464\n",
      "Loss for  7231 th iteration => 0.0532325012374\n",
      "Loss for  7232 th iteration => 0.0532221036968\n",
      "Loss for  7233 th iteration => 0.0532142038272\n",
      "Loss for  7234 th iteration => 0.0532058962148\n",
      "Loss for  7235 th iteration => 0.0531942779334\n",
      "Loss for  7236 th iteration => 0.0531869367381\n",
      "Loss for  7237 th iteration => 0.0531819028089\n",
      "Loss for  7238 th iteration => 0.0531674762487\n",
      "Loss for  7239 th iteration => 0.0531612880399\n",
      "Loss for  7240 th iteration => 0.053155120558\n",
      "Loss for  7241 th iteration => 0.0531407389329\n",
      "Loss for  7242 th iteration => 0.05313558861\n",
      "Loss for  7243 th iteration => 0.0531284016422\n",
      "Loss for  7244 th iteration => 0.0531142327887\n",
      "Loss for  7245 th iteration => 0.0531083377407\n",
      "Loss for  7246 th iteration => 0.0531005164462\n",
      "Loss for  7247 th iteration => 0.0530901679012\n",
      "Loss for  7248 th iteration => 0.053081360029\n",
      "Loss for  7249 th iteration => 0.0530731849807\n",
      "Loss for  7250 th iteration => 0.0530637097826\n",
      "Loss for  7251 th iteration => 0.0530548575887\n",
      "Loss for  7252 th iteration => 0.0530474861617\n",
      "Loss for  7253 th iteration => 0.0530389216121\n",
      "Loss for  7254 th iteration => 0.0530274078287\n",
      "Loss for  7255 th iteration => 0.0530203318031\n",
      "Loss for  7256 th iteration => 0.0530125135772\n",
      "Loss for  7257 th iteration => 0.0530005805579\n",
      "Loss for  7258 th iteration => 0.0529950651153\n",
      "Loss for  7259 th iteration => 0.0529882962588\n",
      "Loss for  7260 th iteration => 0.0529739995028\n",
      "Loss for  7261 th iteration => 0.0529695144207\n",
      "Loss for  7262 th iteration => 0.0529617334435\n",
      "Loss for  7263 th iteration => 0.0529476451033\n",
      "Loss for  7264 th iteration => 0.0529416387472\n",
      "Loss for  7265 th iteration => 0.0529329116858\n",
      "Loss for  7266 th iteration => 0.0529232880645\n",
      "Loss for  7267 th iteration => 0.052915202725\n",
      "Loss for  7268 th iteration => 0.0529074275763\n",
      "Loss for  7269 th iteration => 0.0528991284197\n",
      "Loss for  7270 th iteration => 0.0528876658596\n",
      "Loss for  7271 th iteration => 0.0528805116397\n",
      "Loss for  7272 th iteration => 0.0528754137384\n",
      "Loss for  7273 th iteration => 0.0528611114787\n",
      "Loss for  7274 th iteration => 0.0528552110204\n",
      "Loss for  7275 th iteration => 0.0528488795318\n",
      "Loss for  7276 th iteration => 0.0528346250729\n",
      "Loss for  7277 th iteration => 0.0528298523527\n",
      "Loss for  7278 th iteration => 0.0528224121443\n",
      "Loss for  7279 th iteration => 0.0528083865796\n",
      "Loss for  7280 th iteration => 0.0528019557943\n",
      "Loss for  7281 th iteration => 0.0527938743232\n",
      "Loss for  7282 th iteration => 0.052784585174\n",
      "Loss for  7283 th iteration => 0.0527757175374\n",
      "Loss for  7284 th iteration => 0.0527684835887\n",
      "Loss for  7285 th iteration => 0.052760010159\n",
      "Loss for  7286 th iteration => 0.0527497497623\n",
      "Loss for  7287 th iteration => 0.0527427340135\n",
      "Loss for  7288 th iteration => 0.0527338060697\n",
      "Loss for  7289 th iteration => 0.0527223506884\n",
      "Loss for  7290 th iteration => 0.0527159898381\n",
      "Loss for  7291 th iteration => 0.0527101700305\n",
      "Loss for  7292 th iteration => 0.0526959552204\n",
      "Loss for  7293 th iteration => 0.0526908271534\n",
      "Loss for  7294 th iteration => 0.0526837944506\n",
      "Loss for  7295 th iteration => 0.0526697898166\n",
      "Loss for  7296 th iteration => 0.0526630470213\n",
      "Loss for  7297 th iteration => 0.0526554014098\n",
      "Loss for  7298 th iteration => 0.0526480657123\n",
      "Loss for  7299 th iteration => 0.0526371877238\n",
      "Loss for  7300 th iteration => 0.0526298195306\n",
      "Loss for  7301 th iteration => 0.0526217575304\n",
      "Loss for  7302 th iteration => 0.0526113499316\n",
      "Loss for  7303 th iteration => 0.0526042295305\n",
      "Loss for  7304 th iteration => 0.0525956032786\n",
      "Loss for  7305 th iteration => 0.0525842221615\n",
      "Loss for  7306 th iteration => 0.0525775597258\n",
      "Loss for  7307 th iteration => 0.0525695774235\n",
      "Loss for  7308 th iteration => 0.0525577606709\n",
      "Loss for  7309 th iteration => 0.0525528760674\n",
      "Loss for  7310 th iteration => 0.0525456563094\n",
      "Loss for  7311 th iteration => 0.0525316163237\n",
      "Loss for  7312 th iteration => 0.0525266610953\n",
      "Loss for  7313 th iteration => 0.0525184524579\n",
      "Loss for  7314 th iteration => 0.0525075945104\n",
      "Loss for  7315 th iteration => 0.0524999542413\n",
      "Loss for  7316 th iteration => 0.0524914471895\n",
      "Loss for  7317 th iteration => 0.0524840381625\n",
      "Loss for  7318 th iteration => 0.0524726960906\n",
      "Loss for  7319 th iteration => 0.0524651465667\n",
      "Loss for  7320 th iteration => 0.0524606043262\n",
      "Loss for  7321 th iteration => 0.052446449912\n",
      "Loss for  7322 th iteration => 0.0524403599455\n",
      "Loss for  7323 th iteration => 0.0524343800252\n",
      "Loss for  7324 th iteration => 0.0524202794424\n",
      "Loss for  7325 th iteration => 0.0524155006681\n",
      "Loss for  7326 th iteration => 0.0524082300115\n",
      "Loss for  7327 th iteration => 0.0523942640232\n",
      "Loss for  7328 th iteration => 0.0523883896218\n",
      "Loss for  7329 th iteration => 0.0523798155865\n",
      "Loss for  7330 th iteration => 0.0523704026991\n",
      "Loss for  7331 th iteration => 0.0523624717373\n",
      "Loss for  7332 th iteration => 0.0523549387018\n",
      "Loss for  7333 th iteration => 0.0523466601431\n",
      "Loss for  7334 th iteration => 0.052335357209\n",
      "Loss for  7335 th iteration => 0.0523287351901\n",
      "Loss for  7336 th iteration => 0.0523233267547\n",
      "Loss for  7337 th iteration => 0.0523092531852\n",
      "Loss for  7338 th iteration => 0.0523040420395\n",
      "Loss for  7339 th iteration => 0.0522972440637\n",
      "Loss for  7340 th iteration => 0.0522833225575\n",
      "Loss for  7341 th iteration => 0.0522777300639\n",
      "Loss for  7342 th iteration => 0.0522701597858\n",
      "Loss for  7343 th iteration => 0.052259912707\n",
      "Loss for  7344 th iteration => 0.0522514950983\n",
      "Loss for  7345 th iteration => 0.0522437080712\n",
      "Loss for  7346 th iteration => 0.052235984877\n",
      "Loss for  7347 th iteration => 0.0522260109223\n",
      "Loss for  7348 th iteration => 0.0522185136954\n",
      "Loss for  7349 th iteration => 0.0522102293032\n",
      "Loss for  7350 th iteration => 0.0521988816361\n",
      "Loss for  7351 th iteration => 0.0521925936812\n",
      "Loss for  7352 th iteration => 0.0521869072774\n",
      "Loss for  7353 th iteration => 0.0521728959464\n",
      "Loss for  7354 th iteration => 0.0521680355744\n",
      "Loss for  7355 th iteration => 0.0521609430035\n",
      "Loss for  7356 th iteration => 0.0521471396575\n",
      "Loss for  7357 th iteration => 0.0521417943039\n",
      "Loss for  7358 th iteration => 0.0521340583924\n",
      "Loss for  7359 th iteration => 0.0521238530141\n",
      "Loss for  7360 th iteration => 0.0521155759317\n",
      "Loss for  7361 th iteration => 0.052107854256\n",
      "Loss for  7362 th iteration => 0.0521000250353\n",
      "Loss for  7363 th iteration => 0.0520888196319\n",
      "Loss for  7364 th iteration => 0.0520818274172\n",
      "Loss for  7365 th iteration => 0.0520768826187\n",
      "Loss for  7366 th iteration => 0.05206289234\n",
      "Loss for  7367 th iteration => 0.0520574462059\n",
      "Loss for  7368 th iteration => 0.0520509778312\n",
      "Loss for  7369 th iteration => 0.0520371010236\n",
      "Loss for  7370 th iteration => 0.0520315885418\n",
      "Loss for  7371 th iteration => 0.0520239571594\n",
      "Loss for  7372 th iteration => 0.0520139365859\n",
      "Loss for  7373 th iteration => 0.0520055941326\n",
      "Loss for  7374 th iteration => 0.0519977698829\n",
      "Loss for  7375 th iteration => 0.0519901759618\n",
      "Loss for  7376 th iteration => 0.0519790421231\n",
      "Loss for  7377 th iteration => 0.0519717809297\n",
      "Loss for  7378 th iteration => 0.0519671457244\n",
      "Loss for  7379 th iteration => 0.0519531894507\n",
      "Loss for  7380 th iteration => 0.0519475449963\n",
      "Loss for  7381 th iteration => 0.0519413161959\n",
      "Loss for  7382 th iteration => 0.0519274346517\n",
      "Loss for  7383 th iteration => 0.0519219240487\n",
      "Loss for  7384 th iteration => 0.0519142915435\n",
      "Loss for  7385 th iteration => 0.0519043848651\n",
      "Loss for  7386 th iteration => 0.0518960553723\n",
      "Loss for  7387 th iteration => 0.0518882053873\n",
      "Loss for  7388 th iteration => 0.0518807081703\n",
      "Loss for  7389 th iteration => 0.0518696018789\n",
      "Loss for  7390 th iteration => 0.0518623446533\n",
      "Loss for  7391 th iteration => 0.0518577481088\n",
      "Loss for  7392 th iteration => 0.051843833898\n",
      "Loss for  7393 th iteration => 0.0518382339728\n",
      "Loss for  7394 th iteration => 0.0518320036182\n",
      "Loss for  7395 th iteration => 0.0518181603221\n",
      "Loss for  7396 th iteration => 0.0518127569863\n",
      "Loss for  7397 th iteration => 0.0518050581077\n",
      "Loss for  7398 th iteration => 0.0517952199951\n",
      "Loss for  7399 th iteration => 0.0517869526137\n",
      "Loss for  7400 th iteration => 0.051779125342\n",
      "Loss for  7401 th iteration => 0.0517716374364\n",
      "Loss for  7402 th iteration => 0.0517605314594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  7403 th iteration => 0.0517534489223\n",
      "Loss for  7404 th iteration => 0.0517487215097\n",
      "Loss for  7405 th iteration => 0.0517348543235\n",
      "Loss for  7406 th iteration => 0.0517294507329\n",
      "Loss for  7407 th iteration => 0.0517230680222\n",
      "Loss for  7408 th iteration => 0.0517092897363\n",
      "Loss for  7409 th iteration => 0.0517040590011\n",
      "Loss for  7410 th iteration => 0.0516962538999\n",
      "Loss for  7411 th iteration => 0.0516864551494\n",
      "Loss for  7412 th iteration => 0.0516782806473\n",
      "Loss for  7413 th iteration => 0.0516705066433\n",
      "Loss for  7414 th iteration => 0.0516629729089\n",
      "Loss for  7415 th iteration => 0.0516518505673\n",
      "Loss for  7416 th iteration => 0.0516450490819\n",
      "Loss for  7417 th iteration => 0.0516400851003\n",
      "Loss for  7418 th iteration => 0.0516262679468\n",
      "Loss for  7419 th iteration => 0.0516211551784\n",
      "Loss for  7420 th iteration => 0.0516145261671\n",
      "Loss for  7421 th iteration => 0.0516008665107\n",
      "Loss for  7422 th iteration => 0.0515948947506\n",
      "Loss for  7423 th iteration => 0.0515869282963\n",
      "Loss for  7424 th iteration => 0.0515799275105\n",
      "Loss for  7425 th iteration => 0.0515700121825\n",
      "Loss for  7426 th iteration => 0.0515624181482\n",
      "Loss for  7427 th iteration => 0.0515546745807\n",
      "Loss for  7428 th iteration => 0.0515435118094\n",
      "Loss for  7429 th iteration => 0.0515372278125\n",
      "Loss for  7430 th iteration => 0.0515317925854\n",
      "Loss for  7431 th iteration => 0.0515180323801\n",
      "Loss for  7432 th iteration => 0.0515134209942\n",
      "Loss for  7433 th iteration => 0.0515063366198\n",
      "Loss for  7434 th iteration => 0.0514928825555\n",
      "Loss for  7435 th iteration => 0.0514869518241\n",
      "Loss for  7436 th iteration => 0.0514791581154\n",
      "Loss for  7437 th iteration => 0.0514718745501\n",
      "Loss for  7438 th iteration => 0.0514621668984\n",
      "Loss for  7439 th iteration => 0.0514547420112\n",
      "Loss for  7440 th iteration => 0.0514468024956\n",
      "Loss for  7441 th iteration => 0.0514356027521\n",
      "Loss for  7442 th iteration => 0.0514298123569\n",
      "Loss for  7443 th iteration => 0.0514239293378\n",
      "Loss for  7444 th iteration => 0.0514102251371\n",
      "Loss for  7445 th iteration => 0.0514060935745\n",
      "Loss for  7446 th iteration => 0.0513985749877\n",
      "Loss for  7447 th iteration => 0.0513853176434\n",
      "Loss for  7448 th iteration => 0.0513794304519\n",
      "Loss for  7449 th iteration => 0.051371800557\n",
      "Loss for  7450 th iteration => 0.0513643268053\n",
      "Loss for  7451 th iteration => 0.0513533369436\n",
      "Loss for  7452 th iteration => 0.0513466154638\n",
      "Loss for  7453 th iteration => 0.0513416875898\n",
      "Loss for  7454 th iteration => 0.0513279871012\n",
      "Loss for  7455 th iteration => 0.0513230542023\n",
      "Loss for  7456 th iteration => 0.0513163622962\n",
      "Loss for  7457 th iteration => 0.0513028853786\n",
      "Loss for  7458 th iteration => 0.0512969566035\n",
      "Loss for  7459 th iteration => 0.0512892289466\n",
      "Loss for  7460 th iteration => 0.0512820850931\n",
      "Loss for  7461 th iteration => 0.0512723445034\n",
      "Loss for  7462 th iteration => 0.0512650068356\n",
      "Loss for  7463 th iteration => 0.0512571125964\n",
      "Loss for  7464 th iteration => 0.0512460127977\n",
      "Loss for  7465 th iteration => 0.05124023229\n",
      "Loss for  7466 th iteration => 0.0512344126734\n",
      "Loss for  7467 th iteration => 0.0512207813114\n",
      "Loss for  7468 th iteration => 0.0512167267534\n",
      "Loss for  7469 th iteration => 0.0512092050304\n",
      "Loss for  7470 th iteration => 0.0511960416865\n",
      "Loss for  7471 th iteration => 0.0511901863363\n",
      "Loss for  7472 th iteration => 0.0511827064761\n",
      "Loss for  7473 th iteration => 0.0511751612004\n",
      "Loss for  7474 th iteration => 0.0511642328061\n",
      "Loss for  7475 th iteration => 0.0511577551778\n",
      "Loss for  7476 th iteration => 0.0511526583612\n",
      "Loss for  7477 th iteration => 0.0511390375137\n",
      "Loss for  7478 th iteration => 0.051134389678\n",
      "Loss for  7479 th iteration => 0.0511274879441\n",
      "Loss for  7480 th iteration => 0.0511141652943\n",
      "Loss for  7481 th iteration => 0.0511082852449\n",
      "Loss for  7482 th iteration => 0.051100749748\n",
      "Loss for  7483 th iteration => 0.0510934219653\n",
      "Loss for  7484 th iteration => 0.0510838435219\n",
      "Loss for  7485 th iteration => 0.0510767048858\n",
      "Loss for  7486 th iteration => 0.0510686446073\n",
      "Loss for  7487 th iteration => 0.0510575743766\n",
      "Loss for  7488 th iteration => 0.0510522300302\n",
      "Loss for  7489 th iteration => 0.0510460504253\n",
      "Loss for  7490 th iteration => 0.0510325306781\n",
      "Loss for  7491 th iteration => 0.051026638777\n",
      "Loss for  7492 th iteration => 0.0510189983746\n",
      "Loss for  7493 th iteration => 0.0510120379518\n",
      "Loss for  7494 th iteration => 0.0510022712535\n",
      "Loss for  7495 th iteration => 0.0509950497288\n",
      "Loss for  7496 th iteration => 0.0509872047516\n",
      "Loss for  7497 th iteration => 0.0509762509339\n",
      "Loss for  7498 th iteration => 0.0509705105955\n",
      "Loss for  7499 th iteration => 0.0509647549332\n",
      "Loss for  7500 th iteration => 0.0509512279461\n",
      "Loss for  7501 th iteration => 0.0509473044851\n",
      "Loss for  7502 th iteration => 0.0509397565827\n",
      "Loss for  7503 th iteration => 0.0509267097336\n",
      "Loss for  7504 th iteration => 0.0509209316302\n",
      "Loss for  7505 th iteration => 0.0509136537406\n",
      "Loss for  7506 th iteration => 0.0509059984943\n",
      "Loss for  7507 th iteration => 0.0508951635959\n",
      "Loss for  7508 th iteration => 0.0508890451498\n",
      "Loss for  7509 th iteration => 0.0508836955115\n",
      "Loss for  7510 th iteration => 0.0508701882772\n",
      "Loss for  7511 th iteration => 0.0508659553724\n",
      "Loss for  7512 th iteration => 0.0508587455099\n",
      "Loss for  7513 th iteration => 0.0508456217201\n",
      "Loss for  7514 th iteration => 0.050839841698\n",
      "Loss for  7515 th iteration => 0.0508325658804\n",
      "Loss for  7516 th iteration => 0.0508249949015\n",
      "Loss for  7517 th iteration => 0.0508142446\n",
      "Loss for  7518 th iteration => 0.0508079580455\n",
      "Loss for  7519 th iteration => 0.0508028060815\n",
      "Loss for  7520 th iteration => 0.050789324794\n",
      "Loss for  7521 th iteration => 0.0507849696463\n",
      "Loss for  7522 th iteration => 0.0507779120383\n",
      "Loss for  7523 th iteration => 0.050764766881\n",
      "Loss for  7524 th iteration => 0.0507589980615\n",
      "Loss for  7525 th iteration => 0.0507517645039\n",
      "Loss for  7526 th iteration => 0.0507442258269\n",
      "Loss for  7527 th iteration => 0.0507348704307\n",
      "Loss for  7528 th iteration => 0.050728051445\n",
      "Loss for  7529 th iteration => 0.0507197478629\n",
      "Loss for  7530 th iteration => 0.0507087829877\n",
      "Loss for  7531 th iteration => 0.0507040697484\n",
      "Loss for  7532 th iteration => 0.0506973980949\n",
      "Loss for  7533 th iteration => 0.0506841532466\n",
      "Loss for  7534 th iteration => 0.0506784020463\n",
      "Loss for  7535 th iteration => 0.0506711292721\n",
      "Loss for  7536 th iteration => 0.0506637797117\n",
      "Loss for  7537 th iteration => 0.0506543468322\n",
      "Loss for  7538 th iteration => 0.0506475026742\n",
      "Loss for  7539 th iteration => 0.050639299739\n",
      "Loss for  7540 th iteration => 0.0506284110676\n",
      "Loss for  7541 th iteration => 0.0506235388221\n",
      "Loss for  7542 th iteration => 0.0506170555744\n",
      "Loss for  7543 th iteration => 0.0506037704958\n",
      "Loss for  7544 th iteration => 0.0505980499086\n",
      "Loss for  7545 th iteration => 0.0505907819611\n",
      "Loss for  7546 th iteration => 0.0505835139327\n",
      "Loss for  7547 th iteration => 0.0505740666164\n",
      "Loss for  7548 th iteration => 0.050567236657\n",
      "Loss for  7549 th iteration => 0.0505590634392\n",
      "Loss for  7550 th iteration => 0.0505482280433\n",
      "Loss for  7551 th iteration => 0.0505433375303\n",
      "Loss for  7552 th iteration => 0.0505369030161\n",
      "Loss for  7553 th iteration => 0.0505236194522\n",
      "Loss for  7554 th iteration => 0.0505179392544\n",
      "Loss for  7555 th iteration => 0.0505107065316\n",
      "Loss for  7556 th iteration => 0.0505034443408\n",
      "Loss for  7557 th iteration => 0.0504940274565\n",
      "Loss for  7558 th iteration => 0.0504872387922\n",
      "Loss for  7559 th iteration => 0.0504790456178\n",
      "Loss for  7560 th iteration => 0.0504682471663\n",
      "Loss for  7561 th iteration => 0.0504634377511\n",
      "Loss for  7562 th iteration => 0.0504569533289\n",
      "Loss for  7563 th iteration => 0.0504437004656\n",
      "Loss for  7564 th iteration => 0.0504380681261\n",
      "Loss for  7565 th iteration => 0.0504308913744\n",
      "Loss for  7566 th iteration => 0.0504235819269\n",
      "Loss for  7567 th iteration => 0.0504142274358\n",
      "Loss for  7568 th iteration => 0.0504074984834\n",
      "Loss for  7569 th iteration => 0.0503992507316\n",
      "Loss for  7570 th iteration => 0.0503884775751\n",
      "Loss for  7571 th iteration => 0.0503838192878\n",
      "Loss for  7572 th iteration => 0.0503772154045\n",
      "Loss for  7573 th iteration => 0.0503640135278\n",
      "Loss for  7574 th iteration => 0.0503584348573\n",
      "Loss for  7575 th iteration => 0.0503513280033\n",
      "Loss for  7576 th iteration => 0.0503439342168\n",
      "Loss for  7577 th iteration => 0.0503346649137\n",
      "Loss for  7578 th iteration => 0.0503280079591\n",
      "Loss for  7579 th iteration => 0.0503196816649\n",
      "Loss for  7580 th iteration => 0.0503089254783\n",
      "Loss for  7581 th iteration => 0.0503044675495\n",
      "Loss for  7582 th iteration => 0.0502976952745\n",
      "Loss for  7583 th iteration => 0.0502845583663\n",
      "Loss for  7584 th iteration => 0.0502790379796\n",
      "Loss for  7585 th iteration => 0.0502720101379\n",
      "Loss for  7586 th iteration => 0.0502645062715\n",
      "Loss for  7587 th iteration => 0.0502553384393\n",
      "Loss for  7588 th iteration => 0.0502487614441\n",
      "Loss for  7589 th iteration => 0.050240340182\n",
      "Loss for  7590 th iteration => 0.050229595002\n",
      "Loss for  7591 th iteration => 0.0502253719128\n",
      "Loss for  7592 th iteration => 0.0502183969387\n",
      "Loss for  7593 th iteration => 0.0502053345169\n",
      "Loss for  7594 th iteration => 0.0501998761623\n",
      "Loss for  7595 th iteration => 0.0501929330571\n",
      "Loss for  7596 th iteration => 0.050185301402\n",
      "Loss for  7597 th iteration => 0.0501762466934\n",
      "Loss for  7598 th iteration => 0.0501697545749\n",
      "Loss for  7599 th iteration => 0.0501612272506\n",
      "Loss for  7600 th iteration => 0.0501504887935\n",
      "Loss for  7601 th iteration => 0.0501465245642\n",
      "Loss for  7602 th iteration => 0.0501393229545\n",
      "Loss for  7603 th iteration => 0.0501263413783\n",
      "Loss for  7604 th iteration => 0.0501209481712\n",
      "Loss for  7605 th iteration => 0.0501140931442\n",
      "Loss for  7606 th iteration => 0.0501063216773\n",
      "Loss for  7607 th iteration => 0.0500973884503\n",
      "Loss for  7608 th iteration => 0.0500909839879\n",
      "Loss for  7609 th iteration => 0.0500823432738\n",
      "Loss for  7610 th iteration => 0.0500716084517\n",
      "Loss for  7611 th iteration => 0.0500679196819\n",
      "Loss for  7612 th iteration => 0.0500604748571\n",
      "Loss for  7613 th iteration => 0.0500475782511\n",
      "Loss for  7614 th iteration => 0.0500422528411\n",
      "Loss for  7615 th iteration => 0.0500354875651\n",
      "Loss for  7616 th iteration => 0.0500275682863\n",
      "Loss for  7617 th iteration => 0.0500187625514\n",
      "Loss for  7618 th iteration => 0.0500124470287\n",
      "Loss for  7619 th iteration => 0.0500036882551\n",
      "Loss for  7620 th iteration => 0.0499929548325\n",
      "Loss for  7621 th iteration => 0.0499895528572\n",
      "Loss for  7622 th iteration => 0.0499818534573\n",
      "Loss for  7623 th iteration => 0.0499690443677\n",
      "Loss for  7624 th iteration => 0.0499637890571\n",
      "Loss for  7625 th iteration => 0.0499571140415\n",
      "Loss for  7626 th iteration => 0.0499490437533\n",
      "Loss for  7627 th iteration => 0.0499386077757\n",
      "Loss for  7628 th iteration => 0.0499337690438\n",
      "Loss for  7629 th iteration => 0.0499275146663\n",
      "Loss for  7630 th iteration => 0.0499144104558\n",
      "Loss for  7631 th iteration => 0.0499116413337\n",
      "Loss for  7632 th iteration => 0.0499033441483\n",
      "Loss for  7633 th iteration => 0.049890733369\n",
      "Loss for  7634 th iteration => 0.0498855488093\n",
      "Loss for  7635 th iteration => 0.0498790702672\n",
      "Loss for  7636 th iteration => 0.0498707193477\n",
      "Loss for  7637 th iteration => 0.0498602456699\n",
      "Loss for  7638 th iteration => 0.049855975093\n",
      "Loss for  7639 th iteration => 0.0498491870609\n",
      "Loss for  7640 th iteration => 0.0498361294521\n",
      "Loss for  7641 th iteration => 0.0498338937484\n",
      "Loss for  7642 th iteration => 0.049825097221\n",
      "Loss for  7643 th iteration => 0.0498126538711\n",
      "Loss for  7644 th iteration => 0.0498075354384\n",
      "Loss for  7645 th iteration => 0.0498012289954\n",
      "Loss for  7646 th iteration => 0.0497926381485\n",
      "Loss for  7647 th iteration => 0.0497821391207\n",
      "Loss for  7648 th iteration => 0.0497783580794\n",
      "Loss for  7649 th iteration => 0.0497711141876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  7650 th iteration => 0.0497581818609\n",
      "Loss for  7651 th iteration => 0.0497530047782\n",
      "Loss for  7652 th iteration => 0.0497464110348\n",
      "Loss for  7653 th iteration => 0.0497385044325\n",
      "Loss for  7654 th iteration => 0.0497297698389\n",
      "Loss for  7655 th iteration => 0.0497236468751\n",
      "Loss for  7656 th iteration => 0.0497147639922\n",
      "Loss for  7657 th iteration => 0.0497042081345\n",
      "Loss for  7658 th iteration => 0.0497010648578\n",
      "Loss for  7659 th iteration => 0.0496932180436\n",
      "Loss for  7660 th iteration => 0.0496804765361\n",
      "Loss for  7661 th iteration => 0.0496753962759\n",
      "Loss for  7662 th iteration => 0.049668965177\n",
      "Loss for  7663 th iteration => 0.0496607183622\n",
      "Loss for  7664 th iteration => 0.0496522291973\n",
      "Loss for  7665 th iteration => 0.0496462596623\n",
      "Loss for  7666 th iteration => 0.0496371343952\n",
      "Loss for  7667 th iteration => 0.0496265364496\n",
      "Loss for  7668 th iteration => 0.0496239360767\n",
      "Loss for  7669 th iteration => 0.0496155802373\n",
      "Loss for  7670 th iteration => 0.0496029998573\n",
      "Loss for  7671 th iteration => 0.0495980126615\n",
      "Loss for  7672 th iteration => 0.0495917200558\n",
      "Loss for  7673 th iteration => 0.0495832114067\n",
      "Loss for  7674 th iteration => 0.0495728853578\n",
      "Loss for  7675 th iteration => 0.0495689965163\n",
      "Loss for  7676 th iteration => 0.0495619382825\n",
      "Loss for  7677 th iteration => 0.0495490011886\n",
      "Loss for  7678 th iteration => 0.0495471986884\n",
      "Loss for  7679 th iteration => 0.0495380809102\n",
      "Loss for  7680 th iteration => 0.0495257456285\n",
      "Loss for  7681 th iteration => 0.0495208444876\n",
      "Loss for  7682 th iteration => 0.0495147787345\n",
      "Loss for  7683 th iteration => 0.0495059278422\n",
      "Loss for  7684 th iteration => 0.0494955417276\n",
      "Loss for  7685 th iteration => 0.04949234983\n",
      "Loss for  7686 th iteration => 0.0494846297281\n",
      "Loss for  7687 th iteration => 0.0494718509164\n",
      "Loss for  7688 th iteration => 0.0494669039632\n",
      "Loss for  7689 th iteration => 0.0494605812695\n",
      "Loss for  7690 th iteration => 0.0494523442216\n",
      "Loss for  7691 th iteration => 0.0494439197326\n",
      "Loss for  7692 th iteration => 0.0494380733053\n",
      "Loss for  7693 th iteration => 0.0494288594216\n",
      "Loss for  7694 th iteration => 0.0494183909078\n",
      "Loss for  7695 th iteration => 0.0494159849584\n",
      "Loss for  7696 th iteration => 0.0494075146156\n",
      "Loss for  7697 th iteration => 0.049394968699\n",
      "Loss for  7698 th iteration => 0.0493901328453\n",
      "Loss for  7699 th iteration => 0.0493839992729\n",
      "Loss for  7700 th iteration => 0.0493753448708\n",
      "Loss for  7701 th iteration => 0.0493672153546\n",
      "Loss for  7702 th iteration => 0.0493615454233\n",
      "Loss for  7703 th iteration => 0.0493520404324\n",
      "Loss for  7704 th iteration => 0.0493415112809\n",
      "Loss for  7705 th iteration => 0.0493397527094\n",
      "Loss for  7706 th iteration => 0.0493306693377\n",
      "Loss for  7707 th iteration => 0.0493183133747\n",
      "Loss for  7708 th iteration => 0.049313582292\n",
      "Loss for  7709 th iteration => 0.049307603628\n",
      "Loss for  7710 th iteration => 0.0492986844066\n",
      "Loss for  7711 th iteration => 0.0492884153731\n",
      "Loss for  7712 th iteration => 0.0492854338475\n",
      "Loss for  7713 th iteration => 0.0492775828522\n",
      "Loss for  7714 th iteration => 0.0492648327043\n",
      "Loss for  7715 th iteration => 0.0492600676248\n",
      "Loss for  7716 th iteration => 0.0492538670409\n",
      "Loss for  7717 th iteration => 0.049245521321\n",
      "Loss for  7718 th iteration => 0.0492372637661\n",
      "Loss for  7719 th iteration => 0.0492315484892\n",
      "Loss for  7720 th iteration => 0.049222184743\n",
      "Loss for  7721 th iteration => 0.0492118057085\n",
      "Loss for  7722 th iteration => 0.0492097752659\n",
      "Loss for  7723 th iteration => 0.0492010100518\n",
      "Loss for  7724 th iteration => 0.0491885412234\n",
      "Loss for  7725 th iteration => 0.0491838999688\n",
      "Loss for  7726 th iteration => 0.0491779222021\n",
      "Loss for  7727 th iteration => 0.0491690711042\n",
      "Loss for  7728 th iteration => 0.0491611620619\n",
      "Loss for  7729 th iteration => 0.049155653169\n",
      "Loss for  7730 th iteration => 0.0491459412802\n",
      "Loss for  7731 th iteration => 0.0491354816864\n",
      "Loss for  7732 th iteration => 0.049134215762\n",
      "Loss for  7733 th iteration => 0.049124721092\n",
      "Loss for  7734 th iteration => 0.0491124760429\n",
      "Loss for  7735 th iteration => 0.0491079495682\n",
      "Loss for  7736 th iteration => 0.0491021492166\n",
      "Loss for  7737 th iteration => 0.049092989368\n",
      "Loss for  7738 th iteration => 0.0490827779568\n",
      "Loss for  7739 th iteration => 0.0490803582836\n",
      "Loss for  7740 th iteration => 0.0490720271654\n",
      "Loss for  7741 th iteration => 0.0490594078804\n",
      "Loss for  7742 th iteration => 0.0490548535721\n",
      "Loss for  7743 th iteration => 0.0490488439541\n",
      "Loss for  7744 th iteration => 0.0490402037206\n",
      "Loss for  7745 th iteration => 0.0490322278893\n",
      "Loss for  7746 th iteration => 0.0490267044404\n",
      "Loss for  7747 th iteration => 0.0490170715369\n",
      "Loss for  7748 th iteration => 0.0490067391269\n",
      "Loss for  7749 th iteration => 0.0490053319093\n",
      "Loss for  7750 th iteration => 0.0489960254252\n",
      "Loss for  7751 th iteration => 0.0489837043954\n",
      "Loss for  7752 th iteration => 0.048979280658\n",
      "Loss for  7753 th iteration => 0.0489735035138\n",
      "Loss for  7754 th iteration => 0.0489643548062\n",
      "Loss for  7755 th iteration => 0.048954248311\n",
      "Loss for  7756 th iteration => 0.048951822611\n",
      "Loss for  7757 th iteration => 0.0489435453949\n",
      "Loss for  7758 th iteration => 0.0489308942629\n",
      "Loss for  7759 th iteration => 0.0489282014517\n",
      "Loss for  7760 th iteration => 0.0489203837178\n",
      "Loss for  7761 th iteration => 0.0489102098695\n",
      "Loss for  7762 th iteration => 0.0489039456482\n",
      "Loss for  7763 th iteration => 0.0488984072271\n",
      "Loss for  7764 th iteration => 0.048888816831\n",
      "Loss for  7765 th iteration => 0.0488785832284\n",
      "Loss for  7766 th iteration => 0.048877157252\n",
      "Loss for  7767 th iteration => 0.0488679171357\n",
      "Loss for  7768 th iteration => 0.0488555481819\n",
      "Loss for  7769 th iteration => 0.0488512606\n",
      "Loss for  7770 th iteration => 0.048845498971\n",
      "Loss for  7771 th iteration => 0.048836349536\n",
      "Loss for  7772 th iteration => 0.0488288107689\n",
      "Loss for  7773 th iteration => 0.0488235265927\n",
      "Loss for  7774 th iteration => 0.0488134980608\n",
      "Loss for  7775 th iteration => 0.0488031527608\n",
      "Loss for  7776 th iteration => 0.0488026750668\n",
      "Loss for  7777 th iteration => 0.0487925228479\n",
      "Loss for  7778 th iteration => 0.0487805173536\n",
      "Loss for  7779 th iteration => 0.0487788100856\n",
      "Loss for  7780 th iteration => 0.0487698499059\n",
      "Loss for  7781 th iteration => 0.0487598989574\n",
      "Loss for  7782 th iteration => 0.0487535634864\n",
      "Loss for  7783 th iteration => 0.048748037067\n",
      "Loss for  7784 th iteration => 0.0487397121773\n",
      "Loss for  7785 th iteration => 0.0487304362522\n",
      "Loss for  7786 th iteration => 0.048724140991\n",
      "Loss for  7787 th iteration => 0.0487177653305\n",
      "Loss for  7788 th iteration => 0.0487091165163\n",
      "Loss for  7789 th iteration => 0.0486990717878\n",
      "Loss for  7790 th iteration => 0.0486966143575\n",
      "Loss for  7791 th iteration => 0.0486884633915\n",
      "Loss for  7792 th iteration => 0.04867592322\n",
      "Loss for  7793 th iteration => 0.0486736496456\n",
      "Loss for  7794 th iteration => 0.0486655504912\n",
      "Loss for  7795 th iteration => 0.0486547925227\n",
      "Loss for  7796 th iteration => 0.0486495401169\n",
      "Loss for  7797 th iteration => 0.0486434885056\n",
      "Loss for  7798 th iteration => 0.0486343079719\n",
      "Loss for  7799 th iteration => 0.0486267926107\n",
      "Loss for  7800 th iteration => 0.0486218574147\n",
      "Loss for  7801 th iteration => 0.0486119840263\n",
      "Loss for  7802 th iteration => 0.0486038839046\n",
      "Loss for  7803 th iteration => 0.0485998351423\n",
      "Loss for  7804 th iteration => 0.0485897254113\n",
      "Loss for  7805 th iteration => 0.0485819301232\n",
      "Loss for  7806 th iteration => 0.0485769614539\n",
      "Loss for  7807 th iteration => 0.048567496138\n",
      "Loss for  7808 th iteration => 0.048560367197\n",
      "Loss for  7809 th iteration => 0.0485537421853\n",
      "Loss for  7810 th iteration => 0.0485452873468\n",
      "Loss for  7811 th iteration => 0.0485387561886\n",
      "Loss for  7812 th iteration => 0.0485306311795\n",
      "Loss for  7813 th iteration => 0.0485230980752\n",
      "Loss for  7814 th iteration => 0.0485171028739\n",
      "Loss for  7815 th iteration => 0.0485079491037\n",
      "Loss for  7816 th iteration => 0.0484977060071\n",
      "Loss for  7817 th iteration => 0.0484966035907\n",
      "Loss for  7818 th iteration => 0.0484871839676\n",
      "Loss for  7819 th iteration => 0.0484749773938\n",
      "Loss for  7820 th iteration => 0.048473207056\n",
      "Loss for  7821 th iteration => 0.0484643858669\n",
      "Loss for  7822 th iteration => 0.0484551079381\n",
      "Loss for  7823 th iteration => 0.048448093567\n",
      "Loss for  7824 th iteration => 0.0484431564174\n",
      "Loss for  7825 th iteration => 0.0484349854245\n",
      "Loss for  7826 th iteration => 0.0484250941068\n",
      "Loss for  7827 th iteration => 0.0484218710945\n",
      "Loss for  7828 th iteration => 0.048412612109\n",
      "Loss for  7829 th iteration => 0.0484026820055\n",
      "Loss for  7830 th iteration => 0.0483972051587\n",
      "Loss for  7831 th iteration => 0.0483911776547\n",
      "Loss for  7832 th iteration => 0.0483828252986\n",
      "Loss for  7833 th iteration => 0.0483741950246\n",
      "Loss for  7834 th iteration => 0.0483698169502\n",
      "Loss for  7835 th iteration => 0.04836065126\n",
      "Loss for  7836 th iteration => 0.048351581767\n",
      "Loss for  7837 th iteration => 0.0483432919864\n",
      "Loss for  7838 th iteration => 0.0483400342646\n",
      "Loss for  7839 th iteration => 0.048332807809\n",
      "Loss for  7840 th iteration => 0.0483203533093\n",
      "Loss for  7841 th iteration => 0.048319492678\n",
      "Loss for  7842 th iteration => 0.048309898887\n",
      "Loss for  7843 th iteration => 0.0482977412199\n",
      "Loss for  7844 th iteration => 0.0482965674002\n",
      "Loss for  7845 th iteration => 0.0482874700329\n",
      "Loss for  7846 th iteration => 0.0482777624621\n",
      "Loss for  7847 th iteration => 0.0482717010571\n",
      "Loss for  7848 th iteration => 0.0482662172181\n",
      "Loss for  7849 th iteration => 0.0482577236267\n",
      "Loss for  7850 th iteration => 0.0482489314609\n",
      "Loss for  7851 th iteration => 0.0482427252503\n",
      "Loss for  7852 th iteration => 0.0482362878013\n",
      "Loss for  7853 th iteration => 0.0482281520055\n",
      "Loss for  7854 th iteration => 0.0482178854935\n",
      "Loss for  7855 th iteration => 0.0482159510608\n",
      "Loss for  7856 th iteration => 0.0482074593546\n",
      "Loss for  7857 th iteration => 0.0481951660695\n",
      "Loss for  7858 th iteration => 0.0481933655586\n",
      "Loss for  7859 th iteration => 0.0481850594828\n",
      "Loss for  7860 th iteration => 0.0481745098489\n",
      "Loss for  7861 th iteration => 0.0481667608909\n",
      "Loss for  7862 th iteration => 0.0481644192949\n",
      "Loss for  7863 th iteration => 0.0481563499281\n",
      "Loss for  7864 th iteration => 0.0481439885746\n",
      "Loss for  7865 th iteration => 0.0481440001518\n",
      "Loss for  7866 th iteration => 0.0481336067609\n",
      "Loss for  7867 th iteration => 0.0481219416652\n",
      "Loss for  7868 th iteration => 0.0481177275092\n",
      "Loss for  7869 th iteration => 0.0481121367545\n",
      "Loss for  7870 th iteration => 0.0481044065797\n",
      "Loss for  7871 th iteration => 0.0480949367414\n",
      "Loss for  7872 th iteration => 0.0480912022159\n",
      "Loss for  7873 th iteration => 0.048082297947\n",
      "Loss for  7874 th iteration => 0.0480723560404\n",
      "Loss for  7875 th iteration => 0.0480675170449\n",
      "Loss for  7876 th iteration => 0.048060929546\n",
      "Loss for  7877 th iteration => 0.0480526476652\n",
      "Loss for  7878 th iteration => 0.0480448763514\n",
      "Loss for  7879 th iteration => 0.0480398404168\n",
      "Loss for  7880 th iteration => 0.0480308174168\n",
      "Loss for  7881 th iteration => 0.0480222517729\n",
      "Loss for  7882 th iteration => 0.0480159918844\n",
      "Loss for  7883 th iteration => 0.0480097890888\n",
      "Loss for  7884 th iteration => 0.0480022670605\n",
      "Loss for  7885 th iteration => 0.0479934894495\n",
      "Loss for  7886 th iteration => 0.0479890034377\n",
      "Loss for  7887 th iteration => 0.0479802294374\n",
      "Loss for  7888 th iteration => 0.0479710026885\n",
      "Loss for  7889 th iteration => 0.047967847657\n",
      "Loss for  7890 th iteration => 0.0479582453377\n",
      "Loss for  7891 th iteration => 0.0479493805631\n",
      "Loss for  7892 th iteration => 0.047943246109\n",
      "Loss for  7893 th iteration => 0.0479372466786\n",
      "Loss for  7894 th iteration => 0.0479298774604\n",
      "Loss for  7895 th iteration => 0.047920597337\n",
      "Loss for  7896 th iteration => 0.0479162943244\n",
      "Loss for  7897 th iteration => 0.0479080834214\n",
      "Loss for  7898 th iteration => 0.0478984121865\n",
      "Loss for  7899 th iteration => 0.0478928866808\n",
      "Loss for  7900 th iteration => 0.0478869762442\n",
      "Loss for  7901 th iteration => 0.0478784310767\n",
      "Loss for  7902 th iteration => 0.0478702523314\n",
      "Loss for  7903 th iteration => 0.0478660146871\n",
      "Loss for  7904 th iteration => 0.0478567502764\n",
      "Loss for  7905 th iteration => 0.0478483669921\n",
      "Loss for  7906 th iteration => 0.0478422168665\n",
      "Loss for  7907 th iteration => 0.047835908675\n",
      "Loss for  7908 th iteration => 0.0478283228418\n",
      "Loss for  7909 th iteration => 0.047819611995\n",
      "Loss for  7910 th iteration => 0.0478150712837\n",
      "Loss for  7911 th iteration => 0.0478066141686\n",
      "Loss for  7912 th iteration => 0.0477973977151\n",
      "Loss for  7913 th iteration => 0.0477937877875\n",
      "Loss for  7914 th iteration => 0.0477849657888\n",
      "Loss for  7915 th iteration => 0.0477762602697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  7916 th iteration => 0.0477715341105\n",
      "Loss for  7917 th iteration => 0.0477633391825\n",
      "Loss for  7918 th iteration => 0.0477554897473\n",
      "Loss for  7919 th iteration => 0.0477490686094\n",
      "Loss for  7920 th iteration => 0.0477390495838\n",
      "Loss for  7921 th iteration => 0.0477355718694\n",
      "Loss for  7922 th iteration => 0.0477287846236\n",
      "Loss for  7923 th iteration => 0.0477165495481\n",
      "Loss for  7924 th iteration => 0.0477156677316\n",
      "Loss for  7925 th iteration => 0.0477063164741\n",
      "Loss for  7926 th iteration => 0.0476945551591\n",
      "Loss for  7927 th iteration => 0.0476932062326\n",
      "Loss for  7928 th iteration => 0.0476843767816\n",
      "Loss for  7929 th iteration => 0.0476749788623\n",
      "Loss for  7930 th iteration => 0.0476714329218\n",
      "Loss for  7931 th iteration => 0.04766261854\n",
      "Loss for  7932 th iteration => 0.0476545345718\n",
      "Loss for  7933 th iteration => 0.0476490959555\n",
      "Loss for  7934 th iteration => 0.0476405758776\n",
      "Loss for  7935 th iteration => 0.0476341726943\n",
      "Loss for  7936 th iteration => 0.0476273179127\n",
      "Loss for  7937 th iteration => 0.047618099964\n",
      "Loss for  7938 th iteration => 0.0476137263726\n",
      "Loss for  7939 th iteration => 0.0476055901381\n",
      "Loss for  7940 th iteration => 0.0475959366608\n",
      "Loss for  7941 th iteration => 0.0475904198649\n",
      "Loss for  7942 th iteration => 0.047584760914\n",
      "Loss for  7943 th iteration => 0.0475759796279\n",
      "Loss for  7944 th iteration => 0.0475661032169\n",
      "Loss for  7945 th iteration => 0.0475649635533\n",
      "Loss for  7946 th iteration => 0.0475559226616\n",
      "Loss for  7947 th iteration => 0.0475441582793\n",
      "Loss for  7948 th iteration => 0.0475428431997\n",
      "Loss for  7949 th iteration => 0.0475338873456\n",
      "Loss for  7950 th iteration => 0.0475247869972\n",
      "Loss for  7951 th iteration => 0.0475184848338\n",
      "Loss for  7952 th iteration => 0.0475133926279\n",
      "Loss for  7953 th iteration => 0.0475054694336\n",
      "Loss for  7954 th iteration => 0.0474962651716\n",
      "Loss for  7955 th iteration => 0.0474907232835\n",
      "Loss for  7956 th iteration => 0.0474845822862\n",
      "Loss for  7957 th iteration => 0.0474762152871\n",
      "Loss for  7958 th iteration => 0.0474688655754\n",
      "Loss for  7959 th iteration => 0.0474640254036\n",
      "Loss for  7960 th iteration => 0.0474548744287\n",
      "Loss for  7961 th iteration => 0.0474468216815\n",
      "Loss for  7962 th iteration => 0.0474406581029\n",
      "Loss for  7963 th iteration => 0.0474343689799\n",
      "Loss for  7964 th iteration => 0.0474274304673\n",
      "Loss for  7965 th iteration => 0.0474183306757\n",
      "Loss for  7966 th iteration => 0.0474141070338\n",
      "Loss for  7967 th iteration => 0.0474058673438\n",
      "Loss for  7968 th iteration => 0.0473964246418\n",
      "Loss for  7969 th iteration => 0.0473933428123\n",
      "Loss for  7970 th iteration => 0.047384357346\n",
      "Loss for  7971 th iteration => 0.0473754540261\n",
      "Loss for  7972 th iteration => 0.0473717993153\n",
      "Loss for  7973 th iteration => 0.0473628454832\n",
      "Loss for  7974 th iteration => 0.0473552877388\n",
      "Loss for  7975 th iteration => 0.0473495793268\n",
      "Loss for  7976 th iteration => 0.047339233968\n",
      "Loss for  7977 th iteration => 0.04733579893\n",
      "Loss for  7978 th iteration => 0.0473291145432\n",
      "Loss for  7979 th iteration => 0.0473170295347\n",
      "Loss for  7980 th iteration => 0.0473162984031\n",
      "Loss for  7981 th iteration => 0.0473069431711\n",
      "Loss for  7982 th iteration => 0.0472955879374\n",
      "Loss for  7983 th iteration => 0.0472959250585\n",
      "Loss for  7984 th iteration => 0.0472846468767\n",
      "Loss for  7985 th iteration => 0.0472756271768\n",
      "Loss for  7986 th iteration => 0.0472717584415\n",
      "Loss for  7987 th iteration => 0.0472635618777\n",
      "Loss for  7988 th iteration => 0.0472571675837\n",
      "Loss for  7989 th iteration => 0.047249581361\n",
      "Loss for  7990 th iteration => 0.0472424587512\n",
      "Loss for  7991 th iteration => 0.0472366997705\n",
      "Loss for  7992 th iteration => 0.0472280982538\n",
      "Loss for  7993 th iteration => 0.0472206441429\n",
      "Loss for  7994 th iteration => 0.0472163497456\n",
      "Loss for  7995 th iteration => 0.0472069674542\n",
      "Loss for  7996 th iteration => 0.0471991698147\n",
      "Loss for  7997 th iteration => 0.0471952231417\n",
      "Loss for  7998 th iteration => 0.0471859046399\n",
      "Loss for  7999 th iteration => 0.0471788978462\n",
      "Loss for  8000 th iteration => 0.0471729988359\n",
      "Loss for  8001 th iteration => 0.047164863907\n",
      "Loss for  8002 th iteration => 0.047158567665\n",
      "Loss for  8003 th iteration => 0.0471509317082\n",
      "Loss for  8004 th iteration => 0.0471413574388\n",
      "Loss for  8005 th iteration => 0.0471388157563\n",
      "Loss for  8006 th iteration => 0.0471313178882\n",
      "Loss for  8007 th iteration => 0.0471193321778\n",
      "Loss for  8008 th iteration => 0.0471194577305\n",
      "Loss for  8009 th iteration => 0.0471093251365\n",
      "Loss for  8010 th iteration => 0.0470983769476\n",
      "Loss for  8011 th iteration => 0.0470964742565\n",
      "Loss for  8012 th iteration => 0.0470880230319\n",
      "Loss for  8013 th iteration => 0.0470803199061\n",
      "Loss for  8014 th iteration => 0.0470743552615\n",
      "Loss for  8015 th iteration => 0.047067059489\n",
      "Loss for  8016 th iteration => 0.0470600623784\n",
      "Loss for  8017 th iteration => 0.0470528161156\n",
      "Loss for  8018 th iteration => 0.0470455088946\n",
      "Loss for  8019 th iteration => 0.0470399159964\n",
      "Loss for  8020 th iteration => 0.047031824003\n",
      "Loss for  8021 th iteration => 0.0470235903301\n",
      "Loss for  8022 th iteration => 0.0470156767651\n",
      "Loss for  8023 th iteration => 0.0470117237191\n",
      "Loss for  8024 th iteration => 0.0470056690996\n",
      "Loss for  8025 th iteration => 0.0469936890942\n",
      "Loss for  8026 th iteration => 0.0469925816719\n",
      "Loss for  8027 th iteration => 0.0469837161746\n",
      "Loss for  8028 th iteration => 0.0469722036338\n",
      "Loss for  8029 th iteration => 0.0469686178109\n",
      "Loss for  8030 th iteration => 0.0469630758367\n",
      "Loss for  8031 th iteration => 0.0469544274945\n",
      "Loss for  8032 th iteration => 0.0469472501258\n",
      "Loss for  8033 th iteration => 0.0469429776859\n",
      "Loss for  8034 th iteration => 0.0469335259135\n",
      "Loss for  8035 th iteration => 0.046926010546\n",
      "Loss for  8036 th iteration => 0.0469220602304\n",
      "Loss for  8037 th iteration => 0.0469126913839\n",
      "Loss for  8038 th iteration => 0.0469059834883\n",
      "Loss for  8039 th iteration => 0.0469000354313\n",
      "Loss for  8040 th iteration => 0.046891879233\n",
      "Loss for  8041 th iteration => 0.0468858967977\n",
      "Loss for  8042 th iteration => 0.0468781375727\n",
      "Loss for  8043 th iteration => 0.0468710877769\n",
      "Loss for  8044 th iteration => 0.0468657567607\n",
      "Loss for  8045 th iteration => 0.0468569515337\n",
      "Loss for  8046 th iteration => 0.0468495718442\n",
      "Loss for  8047 th iteration => 0.0468457294209\n",
      "Loss for  8048 th iteration => 0.0468361322611\n",
      "Loss for  8049 th iteration => 0.0468288232615\n",
      "Loss for  8050 th iteration => 0.046824486947\n",
      "Loss for  8051 th iteration => 0.0468153796134\n",
      "Loss for  8052 th iteration => 0.046808867677\n",
      "Loss for  8053 th iteration => 0.0468025559819\n",
      "Loss for  8054 th iteration => 0.0467946490916\n",
      "Loss for  8055 th iteration => 0.0467888532423\n",
      "Loss for  8056 th iteration => 0.0467808816981\n",
      "Loss for  8057 th iteration => 0.0467713347148\n",
      "Loss for  8058 th iteration => 0.0467695644579\n",
      "Loss for  8059 th iteration => 0.0467614355652\n",
      "Loss for  8060 th iteration => 0.0467498863731\n",
      "Loss for  8061 th iteration => 0.0467461839098\n",
      "Loss for  8062 th iteration => 0.0467406636264\n",
      "Loss for  8063 th iteration => 0.0467322216843\n",
      "Loss for  8064 th iteration => 0.0467225899294\n",
      "Loss for  8065 th iteration => 0.0467217067471\n",
      "Loss for  8066 th iteration => 0.0467127166236\n",
      "Loss for  8067 th iteration => 0.0467013959475\n",
      "Loss for  8068 th iteration => 0.0466978108814\n",
      "Loss for  8069 th iteration => 0.0466925214538\n",
      "Loss for  8070 th iteration => 0.0466837823547\n",
      "Loss for  8071 th iteration => 0.046676596759\n",
      "Loss for  8072 th iteration => 0.0466726558539\n",
      "Loss for  8073 th iteration => 0.0466631065906\n",
      "Loss for  8074 th iteration => 0.0466558861514\n",
      "Loss for  8075 th iteration => 0.0466516535437\n",
      "Loss for  8076 th iteration => 0.0466424969028\n",
      "Loss for  8077 th iteration => 0.0466360874837\n",
      "Loss for  8078 th iteration => 0.0466298442948\n",
      "Loss for  8079 th iteration => 0.0466219096092\n",
      "Loss for  8080 th iteration => 0.0466162288798\n",
      "Loss for  8081 th iteration => 0.0466082207145\n",
      "Loss for  8082 th iteration => 0.0465987991902\n",
      "Loss for  8083 th iteration => 0.0465970522177\n",
      "Loss for  8084 th iteration => 0.0465889621569\n",
      "Loss for  8085 th iteration => 0.0465774313894\n",
      "Loss for  8086 th iteration => 0.0465737820156\n",
      "Loss for  8087 th iteration => 0.0465684082063\n",
      "Loss for  8088 th iteration => 0.0465598716917\n",
      "Loss for  8089 th iteration => 0.046550345309\n",
      "Loss for  8090 th iteration => 0.0465496046083\n",
      "Loss for  8091 th iteration => 0.0465405351282\n",
      "Loss for  8092 th iteration => 0.0465293636483\n",
      "Loss for  8093 th iteration => 0.0465301014036\n",
      "Loss for  8094 th iteration => 0.0465188081216\n",
      "Loss for  8095 th iteration => 0.0465104272021\n",
      "Loss for  8096 th iteration => 0.0465089337775\n",
      "Loss for  8097 th iteration => 0.0464970088866\n",
      "Loss for  8098 th iteration => 0.046492055279\n",
      "Loss for  8099 th iteration => 0.0464871771176\n",
      "Loss for  8100 th iteration => 0.0464753605326\n",
      "Loss for  8101 th iteration => 0.0464734896176\n",
      "Loss for  8102 th iteration => 0.0464655664669\n",
      "Loss for  8103 th iteration => 0.0464540725017\n",
      "Loss for  8104 th iteration => 0.0464503290034\n",
      "Loss for  8105 th iteration => 0.0464451419658\n",
      "Loss for  8106 th iteration => 0.0464365334044\n",
      "Loss for  8107 th iteration => 0.0464271032293\n",
      "Loss for  8108 th iteration => 0.046426354194\n",
      "Loss for  8109 th iteration => 0.0464173370543\n",
      "Loss for  8110 th iteration => 0.0464063090354\n",
      "Loss for  8111 th iteration => 0.0464068748652\n",
      "Loss for  8112 th iteration => 0.0463957011632\n",
      "Loss for  8113 th iteration => 0.0463874162552\n",
      "Loss for  8114 th iteration => 0.0463858715129\n",
      "Loss for  8115 th iteration => 0.0463739951694\n",
      "Loss for  8116 th iteration => 0.0463691582142\n",
      "Loss for  8117 th iteration => 0.0463642083504\n",
      "Loss for  8118 th iteration => 0.0463524407788\n",
      "Loss for  8119 th iteration => 0.0463507055126\n",
      "Loss for  8120 th iteration => 0.0463426918752\n",
      "Loss for  8121 th iteration => 0.0463313096162\n",
      "Loss for  8122 th iteration => 0.0463274953765\n",
      "Loss for  8123 th iteration => 0.0463225453167\n",
      "Loss for  8124 th iteration => 0.0463137666357\n",
      "Loss for  8125 th iteration => 0.0463043978224\n",
      "Loss for  8126 th iteration => 0.0463038460055\n",
      "Loss for  8127 th iteration => 0.0462946773035\n",
      "Loss for  8128 th iteration => 0.0462839132354\n",
      "Loss for  8129 th iteration => 0.0462842631883\n",
      "Loss for  8130 th iteration => 0.0462731390024\n",
      "Loss for  8131 th iteration => 0.0462651257875\n",
      "Loss for  8132 th iteration => 0.0462633556001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  8133 th iteration => 0.0462515327126\n",
      "Loss for  8134 th iteration => 0.046246969727\n",
      "Loss for  8135 th iteration => 0.0462417921314\n",
      "Loss for  8136 th iteration => 0.0462300778956\n",
      "Loss for  8137 th iteration => 0.0462286191727\n",
      "Loss for  8138 th iteration => 0.0462203752173\n",
      "Loss for  8139 th iteration => 0.0462091809058\n",
      "Loss for  8140 th iteration => 0.0462071825413\n",
      "Loss for  8141 th iteration => 0.0461998699973\n",
      "Loss for  8142 th iteration => 0.0461907867468\n",
      "Loss for  8143 th iteration => 0.0461867325721\n",
      "Loss for  8144 th iteration => 0.0461795462055\n",
      "Loss for  8145 th iteration => 0.0461714462687\n",
      "Loss for  8146 th iteration => 0.0461661033149\n",
      "Loss for  8147 th iteration => 0.0461583414091\n",
      "Loss for  8148 th iteration => 0.0461521944625\n",
      "Loss for  8149 th iteration => 0.0461457525882\n",
      "Loss for  8150 th iteration => 0.0461370212488\n",
      "Loss for  8151 th iteration => 0.0461309410717\n",
      "Loss for  8152 th iteration => 0.0461259482557\n",
      "Loss for  8153 th iteration => 0.0461174984395\n",
      "Loss for  8154 th iteration => 0.0461107812801\n",
      "Loss for  8155 th iteration => 0.0461064564191\n",
      "Loss for  8156 th iteration => 0.0460970944394\n",
      "Loss for  8157 th iteration => 0.0460901675045\n",
      "Loss for  8158 th iteration => 0.0460840582011\n",
      "Loss for  8159 th iteration => 0.0460779818495\n",
      "Loss for  8160 th iteration => 0.0460716855896\n",
      "Loss for  8161 th iteration => 0.0460627293064\n",
      "Loss for  8162 th iteration => 0.0460589528859\n",
      "Loss for  8163 th iteration => 0.0460512177463\n",
      "Loss for  8164 th iteration => 0.0460424060331\n",
      "Loss for  8165 th iteration => 0.0460388484271\n",
      "Loss for  8166 th iteration => 0.0460308034509\n",
      "Loss for  8167 th iteration => 0.046022609369\n",
      "Loss for  8168 th iteration => 0.0460183589437\n",
      "Loss for  8169 th iteration => 0.0460103937237\n",
      "Loss for  8170 th iteration => 0.0460036287057\n",
      "Loss for  8171 th iteration => 0.0459977610253\n",
      "Loss for  8172 th iteration => 0.04598923866\n",
      "Loss for  8173 th iteration => 0.0459847084547\n",
      "Loss for  8174 th iteration => 0.0459773384838\n",
      "Loss for  8175 th iteration => 0.045968610732\n",
      "Loss for  8176 th iteration => 0.0459650210818\n",
      "Loss for  8177 th iteration => 0.0459569732057\n",
      "Loss for  8178 th iteration => 0.0459486843471\n",
      "Loss for  8179 th iteration => 0.0459423115484\n",
      "Loss for  8180 th iteration => 0.0459375716774\n",
      "Loss for  8181 th iteration => 0.0459306554933\n",
      "Loss for  8182 th iteration => 0.0459214915393\n",
      "Loss for  8183 th iteration => 0.0459179720813\n",
      "Loss for  8184 th iteration => 0.045910548646\n",
      "Loss for  8185 th iteration => 0.0459019149721\n",
      "Loss for  8186 th iteration => 0.0458973668175\n",
      "Loss for  8187 th iteration => 0.0458878640733\n",
      "Loss for  8188 th iteration => 0.0458838136701\n",
      "Loss for  8189 th iteration => 0.0458782572418\n",
      "Loss for  8190 th iteration => 0.0458666919138\n",
      "Loss for  8191 th iteration => 0.0458657840292\n",
      "Loss for  8192 th iteration => 0.0458571233753\n",
      "Loss for  8193 th iteration => 0.0458466369541\n",
      "Loss for  8194 th iteration => 0.0458446721097\n",
      "Loss for  8195 th iteration => 0.0458366177091\n",
      "Loss for  8196 th iteration => 0.0458285205694\n",
      "Loss for  8197 th iteration => 0.0458243844598\n",
      "Loss for  8198 th iteration => 0.0458163603434\n",
      "Loss for  8199 th iteration => 0.0458097186969\n",
      "Loss for  8200 th iteration => 0.0458038313309\n",
      "Loss for  8201 th iteration => 0.0457954494482\n",
      "Loss for  8202 th iteration => 0.0457909739436\n",
      "Loss for  8203 th iteration => 0.0457835623682\n",
      "Loss for  8204 th iteration => 0.0457749902731\n",
      "Loss for  8205 th iteration => 0.0457714316447\n",
      "Loss for  8206 th iteration => 0.0457633509222\n",
      "Loss for  8207 th iteration => 0.0457552516589\n",
      "Loss for  8208 th iteration => 0.045748835024\n",
      "Loss for  8209 th iteration => 0.0457441667807\n",
      "Loss for  8210 th iteration => 0.0457373345758\n",
      "Loss for  8211 th iteration => 0.0457281403423\n",
      "Loss for  8212 th iteration => 0.0457247538481\n",
      "Loss for  8213 th iteration => 0.0457173838883\n",
      "Loss for  8214 th iteration => 0.0457088215259\n",
      "Loss for  8215 th iteration => 0.0457042568568\n",
      "Loss for  8216 th iteration => 0.0456947629672\n",
      "Loss for  8217 th iteration => 0.0456909915614\n",
      "Loss for  8218 th iteration => 0.0456852269453\n",
      "Loss for  8219 th iteration => 0.0456737954015\n",
      "Loss for  8220 th iteration => 0.0456707357575\n",
      "Loss for  8221 th iteration => 0.0456648017505\n",
      "Loss for  8222 th iteration => 0.0456551503158\n",
      "Loss for  8223 th iteration => 0.045649731776\n",
      "Loss for  8224 th iteration => 0.0456455432135\n",
      "Loss for  8225 th iteration => 0.0456363246626\n",
      "Loss for  8226 th iteration => 0.0456298594803\n",
      "Loss for  8227 th iteration => 0.0456259977862\n",
      "Loss for  8228 th iteration => 0.0456165519422\n",
      "Loss for  8229 th iteration => 0.0456106158141\n",
      "Loss for  8230 th iteration => 0.0456052765766\n",
      "Loss for  8231 th iteration => 0.0455968026048\n",
      "Loss for  8232 th iteration => 0.045591689208\n",
      "Loss for  8233 th iteration => 0.0455842913445\n",
      "Loss for  8234 th iteration => 0.0455770757744\n",
      "Loss for  8235 th iteration => 0.0455727016679\n",
      "Loss for  8236 th iteration => 0.0455637345976\n",
      "Loss for  8237 th iteration => 0.0455547111572\n",
      "Loss for  8238 th iteration => 0.0455544939991\n",
      "Loss for  8239 th iteration => 0.0455448779287\n",
      "Loss for  8240 th iteration => 0.0455354997339\n",
      "Loss for  8241 th iteration => 0.0455327228204\n",
      "Loss for  8242 th iteration => 0.0455247322922\n",
      "Loss for  8243 th iteration => 0.0455187460295\n",
      "Loss for  8244 th iteration => 0.0455117902261\n",
      "Loss for  8245 th iteration => 0.0455050585656\n",
      "Loss for  8246 th iteration => 0.0454998263307\n",
      "Loss for  8247 th iteration => 0.0454917369054\n",
      "Loss for  8248 th iteration => 0.0454846344366\n",
      "Loss for  8249 th iteration => 0.0454784276965\n",
      "Loss for  8250 th iteration => 0.0454729442389\n",
      "Loss for  8251 th iteration => 0.0454660882731\n",
      "Loss for  8252 th iteration => 0.0454562720238\n",
      "Loss for  8253 th iteration => 0.0454552967871\n",
      "Loss for  8254 th iteration => 0.0454468475421\n",
      "Loss for  8255 th iteration => 0.0454364669372\n",
      "Loss for  8256 th iteration => 0.0454346591521\n",
      "Loss for  8257 th iteration => 0.0454265696656\n",
      "Loss for  8258 th iteration => 0.0454188566077\n",
      "Loss for  8259 th iteration => 0.0454145753512\n",
      "Loss for  8260 th iteration => 0.0454066387197\n",
      "Loss for  8261 th iteration => 0.0454004096106\n",
      "Loss for  8262 th iteration => 0.0453943254499\n",
      "Loss for  8263 th iteration => 0.0453860588921\n",
      "Loss for  8264 th iteration => 0.0453820146157\n",
      "Loss for  8265 th iteration => 0.045374384788\n",
      "Loss for  8266 th iteration => 0.0453660887968\n",
      "Loss for  8267 th iteration => 0.0453626455515\n",
      "Loss for  8268 th iteration => 0.0453545006208\n",
      "Loss for  8269 th iteration => 0.0453468468722\n",
      "Loss for  8270 th iteration => 0.0453426879793\n",
      "Loss for  8271 th iteration => 0.0453346240196\n",
      "Loss for  8272 th iteration => 0.0453284706992\n",
      "Loss for  8273 th iteration => 0.0453223494845\n",
      "Loss for  8274 th iteration => 0.045314231198\n",
      "Loss for  8275 th iteration => 0.045310144897\n",
      "Loss for  8276 th iteration => 0.045302463839\n",
      "Loss for  8277 th iteration => 0.0452943030165\n",
      "Loss for  8278 th iteration => 0.0452908476979\n",
      "Loss for  8279 th iteration => 0.0452826351596\n",
      "Loss for  8280 th iteration => 0.0452751046405\n",
      "Loss for  8281 th iteration => 0.0452709603562\n",
      "Loss for  8282 th iteration => 0.0452628144266\n",
      "Loss for  8283 th iteration => 0.0452567939815\n",
      "Loss for  8284 th iteration => 0.0452505780827\n",
      "Loss for  8285 th iteration => 0.0452425703078\n",
      "Loss for  8286 th iteration => 0.0452385324861\n",
      "Loss for  8287 th iteration => 0.0452307487905\n",
      "Loss for  8288 th iteration => 0.0452227281046\n",
      "Loss for  8289 th iteration => 0.0452192634855\n",
      "Loss for  8290 th iteration => 0.0452109765153\n",
      "Loss for  8291 th iteration => 0.045203612435\n",
      "Loss for  8292 th iteration => 0.0451994068479\n",
      "Loss for  8293 th iteration => 0.04519121267\n",
      "Loss for  8294 th iteration => 0.0451853634418\n",
      "Loss for  8295 th iteration => 0.0451790140822\n",
      "Loss for  8296 th iteration => 0.045171088503\n",
      "Loss for  8297 th iteration => 0.0451671627073\n",
      "Loss for  8298 th iteration => 0.0451592420259\n",
      "Loss for  8299 th iteration => 0.0451513631383\n",
      "Loss for  8300 th iteration => 0.0451478912124\n",
      "Loss for  8301 th iteration => 0.0451395267394\n",
      "Loss for  8302 th iteration => 0.0451323575419\n",
      "Loss for  8303 th iteration => 0.0451280375467\n",
      "Loss for  8304 th iteration => 0.0451198204345\n",
      "Loss for  8305 th iteration => 0.0451141674095\n",
      "Loss for  8306 th iteration => 0.0451076592424\n",
      "Loss for  8307 th iteration => 0.0450998354302\n",
      "Loss for  8308 th iteration => 0.0450937416812\n",
      "Loss for  8309 th iteration => 0.0450885743172\n",
      "Loss for  8310 th iteration => 0.045081380597\n",
      "Loss for  8311 th iteration => 0.0450744184645\n",
      "Loss for  8312 th iteration => 0.0450700216179\n",
      "Loss for  8313 th iteration => 0.0450613496713\n",
      "Loss for  8314 th iteration => 0.045055173051\n",
      "Loss for  8315 th iteration => 0.0450507841978\n",
      "Loss for  8316 th iteration => 0.0450420371185\n",
      "Loss for  8317 th iteration => 0.0450360213339\n",
      "Loss for  8318 th iteration => 0.045030880543\n",
      "Loss for  8319 th iteration => 0.0450227475285\n",
      "Loss for  8320 th iteration => 0.0450175879845\n",
      "Loss for  8321 th iteration => 0.0450103136585\n",
      "Loss for  8322 th iteration => 0.0450034811554\n",
      "Loss for  8323 th iteration => 0.0449990917902\n",
      "Loss for  8324 th iteration => 0.0449904518438\n",
      "Loss for  8325 th iteration => 0.0449842463855\n",
      "Loss for  8326 th iteration => 0.0449798468246\n",
      "Loss for  8327 th iteration => 0.0449711957961\n",
      "Loss for  8328 th iteration => 0.0449651953842\n",
      "Loss for  8329 th iteration => 0.0449599567787\n",
      "Loss for  8330 th iteration => 0.0449519629069\n",
      "Loss for  8331 th iteration => 0.0449468176096\n",
      "Loss for  8332 th iteration => 0.0449394538034\n",
      "Loss for  8333 th iteration => 0.0449305771231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  8334 th iteration => 0.0449267534209\n",
      "Loss for  8335 th iteration => 0.0449217427505\n",
      "Loss for  8336 th iteration => 0.0449122924529\n",
      "Loss for  8337 th iteration => 0.0449084819646\n",
      "Loss for  8338 th iteration => 0.0449024054955\n",
      "Loss for  8339 th iteration => 0.0448927541694\n",
      "Loss for  8340 th iteration => 0.0448897021266\n",
      "Loss for  8341 th iteration => 0.0448825304683\n",
      "Loss for  8342 th iteration => 0.044874682693\n",
      "Loss for  8343 th iteration => 0.0448703553989\n",
      "Loss for  8344 th iteration => 0.0448620117355\n",
      "Loss for  8345 th iteration => 0.0448565304146\n",
      "Loss for  8346 th iteration => 0.0448510453487\n",
      "Loss for  8347 th iteration => 0.0448416381775\n",
      "Loss for  8348 th iteration => 0.0448361188691\n",
      "Loss for  8349 th iteration => 0.0448322165307\n",
      "Loss for  8350 th iteration => 0.0448241017333\n",
      "Loss for  8351 th iteration => 0.0448166437283\n",
      "Loss for  8352 th iteration => 0.0448136075713\n",
      "Loss for  8353 th iteration => 0.0448046409255\n",
      "Loss for  8354 th iteration => 0.0447980124037\n",
      "Loss for  8355 th iteration => 0.0447939045908\n",
      "Loss for  8356 th iteration => 0.0447851907608\n",
      "Loss for  8357 th iteration => 0.0447801314006\n",
      "Loss for  8358 th iteration => 0.0447735251329\n",
      "Loss for  8359 th iteration => 0.0447640901484\n",
      "Loss for  8360 th iteration => 0.0447626559298\n",
      "Loss for  8361 th iteration => 0.0447549026424\n",
      "Loss for  8362 th iteration => 0.0447446022871\n",
      "Loss for  8363 th iteration => 0.044744669706\n",
      "Loss for  8364 th iteration => 0.0447345710553\n",
      "Loss for  8365 th iteration => 0.0447267401256\n",
      "Loss for  8366 th iteration => 0.0447253267493\n",
      "Loss for  8367 th iteration => 0.0447140752471\n",
      "Loss for  8368 th iteration => 0.0447100312189\n",
      "Loss for  8369 th iteration => 0.0447048783763\n",
      "Loss for  8370 th iteration => 0.0446938443922\n",
      "Loss for  8371 th iteration => 0.0446907648887\n",
      "Loss for  8372 th iteration => 0.0446852052543\n",
      "Loss for  8373 th iteration => 0.0446757758468\n",
      "Loss for  8374 th iteration => 0.0446685831578\n",
      "Loss for  8375 th iteration => 0.0446672272483\n",
      "Loss for  8376 th iteration => 0.0446594295873\n",
      "Loss for  8377 th iteration => 0.0446491961068\n",
      "Loss for  8378 th iteration => 0.044649296236\n",
      "Loss for  8379 th iteration => 0.0446391695061\n",
      "Loss for  8380 th iteration => 0.0446314804951\n",
      "Loss for  8381 th iteration => 0.0446299596575\n",
      "Loss for  8382 th iteration => 0.0446187474199\n",
      "Loss for  8383 th iteration => 0.0446148506023\n",
      "Loss for  8384 th iteration => 0.0446095850638\n",
      "Loss for  8385 th iteration => 0.0445985854693\n",
      "Loss for  8386 th iteration => 0.0445956200628\n",
      "Loss for  8387 th iteration => 0.0445900383895\n",
      "Loss for  8388 th iteration => 0.0445805844583\n",
      "Loss for  8389 th iteration => 0.0445734218991\n",
      "Loss for  8390 th iteration => 0.0445722426759\n",
      "Loss for  8391 th iteration => 0.0445643030818\n",
      "Loss for  8392 th iteration => 0.0445541708373\n",
      "Loss for  8393 th iteration => 0.0445543292087\n",
      "Loss for  8394 th iteration => 0.0445441175314\n",
      "Loss for  8395 th iteration => 0.0445366547045\n",
      "Loss for  8396 th iteration => 0.0445349428905\n",
      "Loss for  8397 th iteration => 0.0445237721978\n",
      "Loss for  8398 th iteration => 0.0445200982254\n",
      "Loss for  8399 th iteration => 0.0445146449718\n",
      "Loss for  8400 th iteration => 0.044503696559\n",
      "Loss for  8401 th iteration => 0.0445031293071\n",
      "Loss for  8402 th iteration => 0.0444944091634\n",
      "Loss for  8403 th iteration => 0.0444850147543\n",
      "Loss for  8404 th iteration => 0.0444808210329\n",
      "Loss for  8405 th iteration => 0.0444757996603\n",
      "Loss for  8406 th iteration => 0.0444696497874\n",
      "Loss for  8407 th iteration => 0.0444607280444\n",
      "Loss for  8408 th iteration => 0.0444554580413\n",
      "Loss for  8409 th iteration => 0.0444514144676\n",
      "Loss for  8410 th iteration => 0.0444436363991\n",
      "Loss for  8411 th iteration => 0.0444357922693\n",
      "Loss for  8412 th iteration => 0.0444334162962\n",
      "Loss for  8413 th iteration => 0.0444244712809\n",
      "Loss for  8414 th iteration => 0.0444178226272\n",
      "Loss for  8415 th iteration => 0.0444136536167\n",
      "Loss for  8416 th iteration => 0.0444053182096\n",
      "Loss for  8417 th iteration => 0.0444002572061\n",
      "Loss for  8418 th iteration => 0.0443935997809\n",
      "Loss for  8419 th iteration => 0.0443842576558\n",
      "Loss for  8420 th iteration => 0.0443833739564\n",
      "Loss for  8421 th iteration => 0.0443752071691\n",
      "Loss for  8422 th iteration => 0.0443649429213\n",
      "Loss for  8423 th iteration => 0.0443616561965\n",
      "Loss for  8424 th iteration => 0.0443565840607\n",
      "Loss for  8425 th iteration => 0.044349846871\n",
      "Loss for  8426 th iteration => 0.044341611303\n",
      "Loss for  8427 th iteration => 0.0443387906681\n",
      "Loss for  8428 th iteration => 0.0443309714733\n",
      "Loss for  8429 th iteration => 0.0443234746867\n",
      "Loss for  8430 th iteration => 0.0443190407959\n",
      "Loss for  8431 th iteration => 0.0443121442585\n",
      "Loss for  8432 th iteration => 0.0443057263441\n",
      "Loss for  8433 th iteration => 0.0442997552789\n",
      "Loss for  8434 th iteration => 0.0442924745101\n",
      "Loss for  8435 th iteration => 0.044288044162\n",
      "Loss for  8436 th iteration => 0.0442809052802\n",
      "Loss for  8437 th iteration => 0.0442728174608\n",
      "Loss for  8438 th iteration => 0.0442698642891\n",
      "Loss for  8439 th iteration => 0.0442621048426\n",
      "Loss for  8440 th iteration => 0.0442551687725\n",
      "Loss for  8441 th iteration => 0.0442498752988\n",
      "Loss for  8442 th iteration => 0.0442409427772\n",
      "Loss for  8443 th iteration => 0.0442383896847\n",
      "Loss for  8444 th iteration => 0.0442319245962\n",
      "Loss for  8445 th iteration => 0.0442210477841\n",
      "Loss for  8446 th iteration => 0.0442195290167\n",
      "Loss for  8447 th iteration => 0.0442126288517\n",
      "Loss for  8448 th iteration => 0.044204316932\n",
      "Loss for  8449 th iteration => 0.044200611767\n",
      "Loss for  8450 th iteration => 0.0441938870328\n",
      "Loss for  8451 th iteration => 0.0441866868426\n",
      "Loss for  8452 th iteration => 0.0441815640162\n",
      "Loss for  8453 th iteration => 0.0441741304452\n",
      "Loss for  8454 th iteration => 0.0441691203149\n",
      "Loss for  8455 th iteration => 0.0441628000684\n",
      "Loss for  8456 th iteration => 0.0441542328126\n",
      "Loss for  8457 th iteration => 0.0441514775433\n",
      "Loss for  8458 th iteration => 0.0441440706177\n",
      "Loss for  8459 th iteration => 0.0441362917251\n",
      "Loss for  8460 th iteration => 0.0441318392894\n",
      "Loss for  8461 th iteration => 0.0441230025239\n",
      "Loss for  8462 th iteration => 0.0441196480602\n",
      "Loss for  8463 th iteration => 0.0441140164447\n",
      "Loss for  8464 th iteration => 0.0441031387133\n",
      "Loss for  8465 th iteration => 0.0441011314776\n",
      "Loss for  8466 th iteration => 0.0440944182873\n",
      "Loss for  8467 th iteration => 0.0440860329379\n",
      "Loss for  8468 th iteration => 0.0440830260539\n",
      "Loss for  8469 th iteration => 0.0440757630232\n",
      "Loss for  8470 th iteration => 0.0440685308339\n",
      "Loss for  8471 th iteration => 0.0440635118357\n",
      "Loss for  8472 th iteration => 0.0440566014622\n",
      "Loss for  8473 th iteration => 0.0440510886713\n",
      "Loss for  8474 th iteration => 0.0440448352902\n",
      "Loss for  8475 th iteration => 0.04403675146\n",
      "Loss for  8476 th iteration => 0.0440335678719\n",
      "Loss for  8477 th iteration => 0.0440261944398\n",
      "Loss for  8478 th iteration => 0.0440184848277\n",
      "Loss for  8479 th iteration => 0.0440144182154\n",
      "Loss for  8480 th iteration => 0.0440076017357\n",
      "Loss for  8481 th iteration => 0.0440010014024\n",
      "Loss for  8482 th iteration => 0.0439953812814\n",
      "Loss for  8483 th iteration => 0.0439881263627\n",
      "Loss for  8484 th iteration => 0.0439835800081\n",
      "Loss for  8485 th iteration => 0.0439767668047\n",
      "Loss for  8486 th iteration => 0.0439686160095\n",
      "Loss for  8487 th iteration => 0.0439639052514\n",
      "Loss for  8488 th iteration => 0.0439589828975\n",
      "Loss for  8489 th iteration => 0.0439516985627\n",
      "Loss for  8490 th iteration => 0.0439442634879\n",
      "Loss for  8491 th iteration => 0.0439370923798\n",
      "Loss for  8492 th iteration => 0.0439349069348\n",
      "Loss for  8493 th iteration => 0.0439281820861\n",
      "Loss for  8494 th iteration => 0.0439174524331\n",
      "Loss for  8495 th iteration => 0.0439168103556\n",
      "Loss for  8496 th iteration => 0.0439086933025\n",
      "Loss for  8497 th iteration => 0.0439013922797\n",
      "Loss for  8498 th iteration => 0.0438977507633\n",
      "Loss for  8499 th iteration => 0.0438899304213\n",
      "Loss for  8500 th iteration => 0.0438842723312\n",
      "Loss for  8501 th iteration => 0.04387840102\n",
      "Loss for  8502 th iteration => 0.0438706970923\n",
      "Loss for  8503 th iteration => 0.0438671837026\n",
      "Loss for  8504 th iteration => 0.0438596374251\n",
      "Loss for  8505 th iteration => 0.043851748038\n",
      "Loss for  8506 th iteration => 0.0438468353921\n",
      "Loss for  8507 th iteration => 0.0438418512623\n",
      "Loss for  8508 th iteration => 0.04383536976\n",
      "Loss for  8509 th iteration => 0.0438271766709\n",
      "Loss for  8510 th iteration => 0.0438220135282\n",
      "Loss for  8511 th iteration => 0.0438177405412\n",
      "Loss for  8512 th iteration => 0.0438105156029\n",
      "Loss for  8513 th iteration => 0.0438025448294\n",
      "Loss for  8514 th iteration => 0.0438006411798\n",
      "Loss for  8515 th iteration => 0.0437918307542\n",
      "Loss for  8516 th iteration => 0.0437851091765\n",
      "Loss for  8517 th iteration => 0.043781254969\n",
      "Loss for  8518 th iteration => 0.043773159273\n",
      "Loss for  8519 th iteration => 0.043768085642\n",
      "Loss for  8520 th iteration => 0.0437616905739\n",
      "Loss for  8521 th iteration => 0.043754314099\n",
      "Loss for  8522 th iteration => 0.043751092115\n",
      "Loss for  8523 th iteration => 0.0437430189733\n",
      "Loss for  8524 th iteration => 0.0437356431268\n",
      "Loss for  8525 th iteration => 0.043733006747\n",
      "Loss for  8526 th iteration => 0.0437243630818\n",
      "Loss for  8527 th iteration => 0.0437187145888\n",
      "Loss for  8528 th iteration => 0.0437132968098\n",
      "Loss for  8529 th iteration => 0.0437057695182\n",
      "Loss for  8530 th iteration => 0.0437016808072\n",
      "Loss for  8531 th iteration => 0.0436943312599\n",
      "Loss for  8532 th iteration => 0.0436865237485\n",
      "Loss for  8533 th iteration => 0.043684681511\n",
      "Loss for  8534 th iteration => 0.0436757349437\n",
      "Loss for  8535 th iteration => 0.0436692384117\n",
      "Loss for  8536 th iteration => 0.0436653236146\n",
      "Loss for  8537 th iteration => 0.0436571524646\n",
      "Loss for  8538 th iteration => 0.043652311817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  8539 th iteration => 0.0436457921485\n",
      "Loss for  8540 th iteration => 0.0436366914482\n",
      "Loss for  8541 th iteration => 0.0436360890629\n",
      "Loss for  8542 th iteration => 0.0436278999056\n",
      "Loss for  8543 th iteration => 0.0436182494213\n",
      "Loss for  8544 th iteration => 0.0436171470079\n",
      "Loss for  8545 th iteration => 0.0436089683698\n",
      "Loss for  8546 th iteration => 0.0436029331123\n",
      "Loss for  8547 th iteration => 0.0435975833412\n",
      "Loss for  8548 th iteration => 0.043590299829\n",
      "Loss for  8549 th iteration => 0.0435861166246\n",
      "Loss for  8550 th iteration => 0.0435790200579\n",
      "Loss for  8551 th iteration => 0.0435709080067\n",
      "Loss for  8552 th iteration => 0.0435644041462\n",
      "Loss for  8553 th iteration => 0.0435622960594\n",
      "Loss for  8554 th iteration => 0.0435556216269\n",
      "Loss for  8555 th iteration => 0.0435450323601\n",
      "Loss for  8556 th iteration => 0.043541822708\n",
      "Loss for  8557 th iteration => 0.0435373137469\n",
      "Loss for  8558 th iteration => 0.0435304685332\n",
      "Loss for  8559 th iteration => 0.0435228641697\n",
      "Loss for  8560 th iteration => 0.0435204778416\n",
      "Loss for  8561 th iteration => 0.0435119868894\n",
      "Loss for  8562 th iteration => 0.0435051762952\n",
      "Loss for  8563 th iteration => 0.0435017726849\n",
      "Loss for  8564 th iteration => 0.0434935204424\n",
      "Loss for  8565 th iteration => 0.0434884056417\n",
      "Loss for  8566 th iteration => 0.0434822668319\n",
      "Loss for  8567 th iteration => 0.0434751150336\n",
      "Loss for  8568 th iteration => 0.0434715318241\n",
      "Loss for  8569 th iteration => 0.0434637909199\n",
      "Loss for  8570 th iteration => 0.0434563709291\n",
      "Loss for  8571 th iteration => 0.0434516234526\n",
      "Loss for  8572 th iteration => 0.0434463152986\n",
      "Loss for  8573 th iteration => 0.0434403135332\n",
      "Loss for  8574 th iteration => 0.0434322525222\n",
      "Loss for  8575 th iteration => 0.0434291342643\n",
      "Loss for  8576 th iteration => 0.0434221686114\n",
      "Loss for  8577 th iteration => 0.0434146898041\n",
      "Loss for  8578 th iteration => 0.0434102804081\n",
      "Loss for  8579 th iteration => 0.0434019065763\n",
      "Loss for  8580 th iteration => 0.0433982949677\n",
      "Loss for  8581 th iteration => 0.0433931630781\n",
      "Loss for  8582 th iteration => 0.0433825189354\n",
      "Loss for  8583 th iteration => 0.0433825419534\n",
      "Loss for  8584 th iteration => 0.0433738201576\n",
      "Loss for  8585 th iteration => 0.0433648995143\n",
      "Loss for  8586 th iteration => 0.0433650270956\n",
      "Loss for  8587 th iteration => 0.0433542408455\n",
      "Loss for  8588 th iteration => 0.0433494821073\n",
      "Loss for  8589 th iteration => 0.0433455010063\n",
      "Loss for  8590 th iteration => 0.0433348413655\n",
      "Loss for  8591 th iteration => 0.043333838922\n",
      "Loss for  8592 th iteration => 0.0433261481354\n",
      "Loss for  8593 th iteration => 0.0433164757385\n",
      "Loss for  8594 th iteration => 0.0433170646898\n",
      "Loss for  8595 th iteration => 0.0433067057733\n",
      "Loss for  8596 th iteration => 0.043300722744\n",
      "Loss for  8597 th iteration => 0.0432979684649\n",
      "Loss for  8598 th iteration => 0.0432872909791\n",
      "Loss for  8599 th iteration => 0.0432851934117\n",
      "Loss for  8600 th iteration => 0.0432786023292\n",
      "Loss for  8601 th iteration => 0.0432682968862\n",
      "Loss for  8602 th iteration => 0.0432669374711\n",
      "Loss for  8603 th iteration => 0.0432601128918\n",
      "Loss for  8604 th iteration => 0.0432526977788\n",
      "Loss for  8605 th iteration => 0.0432483114915\n",
      "Loss for  8606 th iteration => 0.0432419384374\n",
      "Loss for  8607 th iteration => 0.0432359550452\n",
      "Loss for  8608 th iteration => 0.0432302575888\n",
      "Loss for  8609 th iteration => 0.0432226302475\n",
      "Loss for  8610 th iteration => 0.043219129779\n",
      "Loss for  8611 th iteration => 0.0432122419057\n",
      "Loss for  8612 th iteration => 0.0432046434855\n",
      "Loss for  8613 th iteration => 0.043200944179\n",
      "Loss for  8614 th iteration => 0.0431942741122\n",
      "Loss for  8615 th iteration => 0.0431878390193\n",
      "Loss for  8616 th iteration => 0.0431824964388\n",
      "Loss for  8617 th iteration => 0.0431754069194\n",
      "Loss for  8618 th iteration => 0.0431710873543\n",
      "Loss for  8619 th iteration => 0.0431645094799\n",
      "Loss for  8620 th iteration => 0.0431568833103\n",
      "Loss for  8621 th iteration => 0.0431534971491\n",
      "Loss for  8622 th iteration => 0.0431465820629\n",
      "Loss for  8623 th iteration => 0.0431398308618\n",
      "Loss for  8624 th iteration => 0.0431348373565\n",
      "Loss for  8625 th iteration => 0.0431282751364\n",
      "Loss for  8626 th iteration => 0.0431231516613\n",
      "Loss for  8627 th iteration => 0.0431168788429\n",
      "Loss for  8628 th iteration => 0.0431093293227\n",
      "Loss for  8629 th iteration => 0.0431060359687\n",
      "Loss for  8630 th iteration => 0.0430989828916\n",
      "Loss for  8631 th iteration => 0.0430920187676\n",
      "Loss for  8632 th iteration => 0.0430874156769\n",
      "Loss for  8633 th iteration => 0.0430811101413\n",
      "Loss for  8634 th iteration => 0.0430752750276\n",
      "Loss for  8635 th iteration => 0.0430693881364\n",
      "Loss for  8636 th iteration => 0.0430620416695\n",
      "Loss for  8637 th iteration => 0.0430585852119\n",
      "Loss for  8638 th iteration => 0.0430514958886\n",
      "Loss for  8639 th iteration => 0.0430442152361\n",
      "Loss for  8640 th iteration => 0.0430404794386\n",
      "Loss for  8641 th iteration => 0.0430336515544\n",
      "Loss for  8642 th iteration => 0.0430275428745\n",
      "Loss for  8643 th iteration => 0.0430219616244\n",
      "Loss for  8644 th iteration => 0.0430150875803\n",
      "Loss for  8645 th iteration => 0.0430109215153\n",
      "Loss for  8646 th iteration => 0.0430040986612\n",
      "Loss for  8647 th iteration => 0.0429967793196\n",
      "Loss for  8648 th iteration => 0.0429933582089\n",
      "Loss for  8649 th iteration => 0.0429862948036\n",
      "Loss for  8650 th iteration => 0.0429799067613\n",
      "Loss for  8651 th iteration => 0.0429746367065\n",
      "Loss for  8652 th iteration => 0.0429682358487\n",
      "Loss for  8653 th iteration => 0.042963353649\n",
      "Loss for  8654 th iteration => 0.0429568026011\n",
      "Loss for  8655 th iteration => 0.0429495553134\n",
      "Loss for  8656 th iteration => 0.0429462162272\n",
      "Loss for  8657 th iteration => 0.0429390303009\n",
      "Loss for  8658 th iteration => 0.0429324559294\n",
      "Loss for  8659 th iteration => 0.042927626338\n",
      "Loss for  8660 th iteration => 0.0429212819152\n",
      "Loss for  8661 th iteration => 0.0429158361886\n",
      "Loss for  8662 th iteration => 0.0429096455935\n",
      "Loss for  8663 th iteration => 0.0429024691951\n",
      "Loss for  8664 th iteration => 0.0428968181018\n",
      "Loss for  8665 th iteration => 0.0428928003236\n",
      "Loss for  8666 th iteration => 0.0428857875148\n",
      "Loss for  8667 th iteration => 0.0428786793568\n",
      "Loss for  8668 th iteration => 0.042876285734\n",
      "Loss for  8669 th iteration => 0.0428677947479\n",
      "Loss for  8670 th iteration => 0.0428617502421\n",
      "Loss for  8671 th iteration => 0.042857885687\n",
      "Loss for  8672 th iteration => 0.0428498218937\n",
      "Loss for  8673 th iteration => 0.0428454916917\n",
      "Loss for  8674 th iteration => 0.0428388969646\n",
      "Loss for  8675 th iteration => 0.04283007876\n",
      "Loss for  8676 th iteration => 0.0428298823949\n",
      "Loss for  8677 th iteration => 0.0428215640624\n",
      "Loss for  8678 th iteration => 0.042812744904\n",
      "Loss for  8679 th iteration => 0.0428129569871\n",
      "Loss for  8680 th iteration => 0.0428023735073\n",
      "Loss for  8681 th iteration => 0.0427978044008\n",
      "Loss for  8682 th iteration => 0.0427938210706\n",
      "Loss for  8683 th iteration => 0.0427833660611\n",
      "Loss for  8684 th iteration => 0.0427826354904\n",
      "Loss for  8685 th iteration => 0.0427748614242\n",
      "Loss for  8686 th iteration => 0.0427656400231\n",
      "Loss for  8687 th iteration => 0.0427660899817\n",
      "Loss for  8688 th iteration => 0.0427558205558\n",
      "Loss for  8689 th iteration => 0.0427504600974\n",
      "Loss for  8690 th iteration => 0.0427472748547\n",
      "Loss for  8691 th iteration => 0.0427368140686\n",
      "Loss for  8692 th iteration => 0.0427353775099\n",
      "Loss for  8693 th iteration => 0.0427283175408\n",
      "Loss for  8694 th iteration => 0.0427187817717\n",
      "Loss for  8695 th iteration => 0.0427171151434\n",
      "Loss for  8696 th iteration => 0.042710157645\n",
      "Loss for  8697 th iteration => 0.0427038284838\n",
      "Loss for  8698 th iteration => 0.0426986636868\n",
      "Loss for  8699 th iteration => 0.0426902499782\n",
      "Loss for  8700 th iteration => 0.042688395042\n",
      "Loss for  8701 th iteration => 0.0426817646118\n",
      "Loss for  8702 th iteration => 0.0426720658477\n",
      "Loss for  8703 th iteration => 0.042672467713\n",
      "Loss for  8704 th iteration => 0.0426627512892\n",
      "Loss for  8705 th iteration => 0.0426564800758\n",
      "Loss for  8706 th iteration => 0.0426542264093\n",
      "Loss for  8707 th iteration => 0.0426437733757\n",
      "Loss for  8708 th iteration => 0.0426415302453\n",
      "Loss for  8709 th iteration => 0.0426352993668\n",
      "Loss for  8710 th iteration => 0.0426254130131\n",
      "Loss for  8711 th iteration => 0.0426258567177\n",
      "Loss for  8712 th iteration => 0.0426163054286\n",
      "Loss for  8713 th iteration => 0.0426097300685\n",
      "Loss for  8714 th iteration => 0.0426077925487\n",
      "Loss for  8715 th iteration => 0.0425973475549\n",
      "Loss for  8716 th iteration => 0.0425948378524\n",
      "Loss for  8717 th iteration => 0.0425888861641\n",
      "Loss for  8718 th iteration => 0.0425788960745\n",
      "Loss for  8719 th iteration => 0.0425793311501\n",
      "Loss for  8720 th iteration => 0.042569915835\n",
      "Loss for  8721 th iteration => 0.0425631412516\n",
      "Loss for  8722 th iteration => 0.0425614161053\n",
      "Loss for  8723 th iteration => 0.0425509821247\n",
      "Loss for  8724 th iteration => 0.0425483004638\n",
      "Loss for  8725 th iteration => 0.0425425343335\n",
      "Loss for  8726 th iteration => 0.0425325055043\n",
      "Loss for  8727 th iteration => 0.0425328931598\n",
      "Loss for  8728 th iteration => 0.0425235908902\n",
      "Loss for  8729 th iteration => 0.0425167044314\n",
      "Loss for  8730 th iteration => 0.0425133148277\n",
      "Loss for  8731 th iteration => 0.0425055628968\n",
      "Loss for  8732 th iteration => 0.0425021194397\n",
      "Loss for  8733 th iteration => 0.0424947255412\n",
      "Loss for  8734 th iteration => 0.0424877423761\n",
      "Loss for  8735 th iteration => 0.0424836342612\n",
      "Loss for  8736 th iteration => 0.0424780099247\n",
      "Loss for  8737 th iteration => 0.0424722310387\n",
      "Loss for  8738 th iteration => 0.0424652351522\n",
      "Loss for  8739 th iteration => 0.0424614636554\n",
      "Loss for  8740 th iteration => 0.0424548186492\n",
      "Loss for  8741 th iteration => 0.0424479358405\n",
      "Loss for  8742 th iteration => 0.0424438010516\n",
      "Loss for  8743 th iteration => 0.042437437413\n",
      "Loss for  8744 th iteration => 0.0424315985276\n",
      "Loss for  8745 th iteration => 0.0424260626231\n",
      "Loss for  8746 th iteration => 0.0424191334685\n",
      "Loss for  8747 th iteration => 0.0424154440394\n",
      "Loss for  8748 th iteration => 0.0424086547088\n",
      "Loss for  8749 th iteration => 0.042401835135\n",
      "Loss for  8750 th iteration => 0.0423977791843\n",
      "Loss for  8751 th iteration => 0.0423913042121\n",
      "Loss for  8752 th iteration => 0.042385298332\n",
      "Loss for  8753 th iteration => 0.0423799581675\n",
      "Loss for  8754 th iteration => 0.0423733709424\n",
      "Loss for  8755 th iteration => 0.0423692007904\n",
      "Loss for  8756 th iteration => 0.0423625799807\n",
      "Loss for  8757 th iteration => 0.042355831696\n",
      "Loss for  8758 th iteration => 0.0423518331827\n",
      "Loss for  8759 th iteration => 0.0423452610421\n",
      "Loss for  8760 th iteration => 0.0423391543887\n",
      "Loss for  8761 th iteration => 0.042333942621\n",
      "Loss for  8762 th iteration => 0.0423276294732\n",
      "Loss for  8763 th iteration => 0.0423231085533\n",
      "Loss for  8764 th iteration => 0.0423165953051\n",
      "Loss for  8765 th iteration => 0.0423099240908\n",
      "Loss for  8766 th iteration => 0.0423059660052\n",
      "Loss for  8767 th iteration => 0.0422993086021\n",
      "Loss for  8768 th iteration => 0.0422931533869\n",
      "Loss for  8769 th iteration => 0.0422880249239\n",
      "Loss for  8770 th iteration => 0.0422797614757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  8771 th iteration => 0.0422780326753\n",
      "Loss for  8772 th iteration => 0.042271414276\n",
      "Loss for  8773 th iteration => 0.0422622143903\n",
      "Loss for  8774 th iteration => 0.0422621052097\n",
      "Loss for  8775 th iteration => 0.0422526960008\n",
      "Loss for  8776 th iteration => 0.0422468097464\n",
      "Loss for  8777 th iteration => 0.0422443115906\n",
      "Loss for  8778 th iteration => 0.0422340171555\n",
      "Loss for  8779 th iteration => 0.0422321897622\n",
      "Loss for  8780 th iteration => 0.042225684066\n",
      "Loss for  8781 th iteration => 0.0422164920471\n",
      "Loss for  8782 th iteration => 0.0422163122206\n",
      "Loss for  8783 th iteration => 0.0422069946865\n",
      "Loss for  8784 th iteration => 0.0422010533317\n",
      "Loss for  8785 th iteration => 0.0421986247505\n",
      "Loss for  8786 th iteration => 0.0421883451454\n",
      "Loss for  8787 th iteration => 0.0421864754635\n",
      "Loss for  8788 th iteration => 0.0421800267385\n",
      "Loss for  8789 th iteration => 0.042170880881\n",
      "Loss for  8790 th iteration => 0.0421706080641\n",
      "Loss for  8791 th iteration => 0.0421613681677\n",
      "Loss for  8792 th iteration => 0.0421554202737\n",
      "Loss for  8793 th iteration => 0.0421530132232\n",
      "Loss for  8794 th iteration => 0.0421427497972\n",
      "Loss for  8795 th iteration => 0.0421408816256\n",
      "Loss for  8796 th iteration => 0.0421344465108\n",
      "Loss for  8797 th iteration => 0.042125376381\n",
      "Loss for  8798 th iteration => 0.0421249936174\n",
      "Loss for  8799 th iteration => 0.0421158202181\n",
      "Loss for  8800 th iteration => 0.0421099035423\n",
      "Loss for  8801 th iteration => 0.0421074806591\n",
      "Loss for  8802 th iteration => 0.0420972344513\n",
      "Loss for  8803 th iteration => 0.0420954018398\n",
      "Loss for  8804 th iteration => 0.0420889466172\n",
      "Loss for  8805 th iteration => 0.0420799749793\n",
      "Loss for  8806 th iteration => 0.0420794695072\n",
      "Loss for  8807 th iteration => 0.0420703537254\n",
      "Loss for  8808 th iteration => 0.0420644976024\n",
      "Loss for  8809 th iteration => 0.0420620298493\n",
      "Loss for  8810 th iteration => 0.0420518016568\n",
      "Loss for  8811 th iteration => 0.0420500310546\n",
      "Loss for  8812 th iteration => 0.0420435295242\n",
      "Loss for  8813 th iteration => 0.042034673841\n",
      "Loss for  8814 th iteration => 0.0420340361632\n",
      "Loss for  8815 th iteration => 0.0420249708849\n",
      "Loss for  8816 th iteration => 0.0420192066429\n",
      "Loss for  8817 th iteration => 0.0420149057636\n",
      "Loss for  8818 th iteration => 0.042007289971\n",
      "Loss for  8819 th iteration => 0.0420049971355\n",
      "Loss for  8820 th iteration => 0.041996696026\n",
      "Loss for  8821 th iteration => 0.0419908576214\n",
      "Loss for  8822 th iteration => 0.0419881936746\n",
      "Loss for  8823 th iteration => 0.0419793465405\n",
      "Loss for  8824 th iteration => 0.0419753571862\n",
      "Loss for  8825 th iteration => 0.0419696983639\n",
      "Loss for  8826 th iteration => 0.0419620643586\n",
      "Loss for  8827 th iteration => 0.0419597471752\n",
      "Loss for  8828 th iteration => 0.0419514925161\n",
      "Loss for  8829 th iteration => 0.041945642124\n",
      "Loss for  8830 th iteration => 0.0419430285752\n",
      "Loss for  8831 th iteration => 0.0419341753983\n",
      "Loss for  8832 th iteration => 0.0419301794152\n",
      "Loss for  8833 th iteration => 0.0419245624412\n",
      "Loss for  8834 th iteration => 0.0419169258062\n",
      "Loss for  8835 th iteration => 0.0419146069371\n",
      "Loss for  8836 th iteration => 0.0419063759231\n",
      "Loss for  8837 th iteration => 0.0419005350449\n",
      "Loss for  8838 th iteration => 0.0418979371889\n",
      "Loss for  8839 th iteration => 0.0418890917256\n",
      "Loss for  8840 th iteration => 0.0418851085301\n",
      "Loss for  8841 th iteration => 0.0418795018877\n",
      "Loss for  8842 th iteration => 0.0418718751886\n",
      "Loss for  8843 th iteration => 0.0418695721612\n",
      "Loss for  8844 th iteration => 0.0418613471388\n",
      "Loss for  8845 th iteration => 0.0418555323787\n",
      "Loss for  8846 th iteration => 0.0418529229198\n",
      "Loss for  8847 th iteration => 0.041844096279\n",
      "Loss for  8848 th iteration => 0.0418401408268\n",
      "Loss for  8849 th iteration => 0.0418345197467\n",
      "Loss for  8850 th iteration => 0.0418269131452\n",
      "Loss for  8851 th iteration => 0.0418246394218\n",
      "Loss for  8852 th iteration => 0.0418164068152\n",
      "Loss for  8853 th iteration => 0.0418106309014\n",
      "Loss for  8854 th iteration => 0.041807988422\n",
      "Loss for  8855 th iteration => 0.0417991896027\n",
      "Loss for  8856 th iteration => 0.0417952733201\n",
      "Loss for  8857 th iteration => 0.0417896183847\n",
      "Loss for  8858 th iteration => 0.0417820401266\n",
      "Loss for  8859 th iteration => 0.0417798059543\n",
      "Loss for  8860 th iteration => 0.041771555413\n",
      "Loss for  8861 th iteration => 0.0417658280083\n",
      "Loss for  8862 th iteration => 0.0417631357523\n",
      "Loss for  8863 th iteration => 0.0417543720713\n",
      "Loss for  8864 th iteration => 0.0417505035943\n",
      "Loss for  8865 th iteration => 0.0417447996301\n",
      "Loss for  8866 th iteration => 0.0417372564327\n",
      "Loss for  8867 th iteration => 0.0417350695176\n",
      "Loss for  8868 th iteration => 0.0417267932399\n",
      "Loss for  8869 th iteration => 0.0417211215848\n",
      "Loss for  8870 th iteration => 0.0417183664925\n",
      "Loss for  8871 th iteration => 0.0417096439241\n",
      "Loss for  8872 th iteration => 0.0417058296846\n",
      "Loss for  8873 th iteration => 0.0417000648829\n",
      "Loss for  8874 th iteration => 0.0416925622438\n",
      "Loss for  8875 th iteration => 0.0416904282853\n",
      "Loss for  8876 th iteration => 0.0416821204819\n",
      "Loss for  8877 th iteration => 0.041676509905\n",
      "Loss for  8878 th iteration => 0.0416736818468\n",
      "Loss for  8879 th iteration => 0.0416650052929\n",
      "Loss for  8880 th iteration => 0.0416612499839\n",
      "Loss for  8881 th iteration => 0.041655415203\n",
      "Loss for  8882 th iteration => 0.0416479576443\n",
      "Loss for  8883 th iteration => 0.0416458807601\n",
      "Loss for  8884 th iteration => 0.0416375372286\n",
      "Loss for  8885 th iteration => 0.0416319915513\n",
      "Loss for  8886 th iteration => 0.0416290827192\n",
      "Loss for  8887 th iteration => 0.041620456224\n",
      "Loss for  8888 th iteration => 0.0416167631685\n",
      "Loss for  8889 th iteration => 0.0416108513798\n",
      "Loss for  8890 th iteration => 0.0416034426433\n",
      "Loss for  8891 th iteration => 0.0416014257054\n",
      "Loss for  8892 th iteration => 0.0415930434927\n",
      "Loss for  8893 th iteration => 0.0415875653503\n",
      "Loss for  8894 th iteration => 0.0415845697756\n",
      "Loss for  8895 th iteration => 0.041575996696\n",
      "Loss for  8896 th iteration => 0.0415723681398\n",
      "Loss for  8897 th iteration => 0.041566373988\n",
      "Loss for  8898 th iteration => 0.0415590171899\n",
      "Loss for  8899 th iteration => 0.0415570620915\n",
      "Loss for  8900 th iteration => 0.0415486392264\n",
      "Loss for  8901 th iteration => 0.0415432303225\n",
      "Loss for  8902 th iteration => 0.0415401434925\n",
      "Loss for  8903 th iteration => 0.041531626634\n",
      "Loss for  8904 th iteration => 0.0415280639776\n",
      "Loss for  8905 th iteration => 0.0415219834319\n",
      "Loss for  8906 th iteration => 0.0415146811856\n",
      "Loss for  8907 th iteration => 0.0415127890531\n",
      "Loss for  8908 th iteration => 0.041504324334\n",
      "Loss for  8909 th iteration => 0.0414989856424\n",
      "Loss for  8910 th iteration => 0.0414958041964\n",
      "Loss for  8911 th iteration => 0.041487345921\n",
      "Loss for  8912 th iteration => 0.0414838499034\n",
      "Loss for  8913 th iteration => 0.0414776799806\n",
      "Loss for  8914 th iteration => 0.0414704344951\n",
      "Loss for  8915 th iteration => 0.0414686058553\n",
      "Loss for  8916 th iteration => 0.041460098682\n",
      "Loss for  8917 th iteration => 0.0414548306068\n",
      "Loss for  8918 th iteration => 0.0414515520945\n",
      "Loss for  8919 th iteration => 0.0414431544065\n",
      "Loss for  8920 th iteration => 0.0414397252514\n",
      "Loss for  8921 th iteration => 0.041433463796\n",
      "Loss for  8922 th iteration => 0.0414262769532\n",
      "Loss for  8923 th iteration => 0.0414245118667\n",
      "Loss for  8924 th iteration => 0.041415962107\n",
      "Loss for  8925 th iteration => 0.0414107646094\n",
      "Loss for  8926 th iteration => 0.0414073872998\n",
      "Loss for  8927 th iteration => 0.041399051914\n",
      "Loss for  8928 th iteration => 0.0413956894448\n",
      "Loss for  8929 th iteration => 0.0413893349548\n",
      "Loss for  8930 th iteration => 0.041382208372\n",
      "Loss for  8931 th iteration => 0.0413805065383\n",
      "Loss for  8932 th iteration => 0.0413719144226\n",
      "Loss for  8933 th iteration => 0.0413667871212\n",
      "Loss for  8934 th iteration => 0.0413633098501\n",
      "Loss for  8935 th iteration => 0.0413550382465\n",
      "Loss for  8936 th iteration => 0.0413517419785\n",
      "Loss for  8937 th iteration => 0.0413452934664\n",
      "Loss for  8938 th iteration => 0.0413382337336\n",
      "Loss for  8939 th iteration => 0.0413344020658\n",
      "Loss for  8940 th iteration => 0.0413287591245\n",
      "Loss for  8941 th iteration => 0.0413236040689\n",
      "Loss for  8942 th iteration => 0.0413172321849\n",
      "Loss for  8943 th iteration => 0.0413131431636\n",
      "Loss for  8944 th iteration => 0.041307040219\n",
      "Loss for  8945 th iteration => 0.0413007842628\n",
      "Loss for  8946 th iteration => 0.0412963191178\n",
      "Loss for  8947 th iteration => 0.0412905101021\n",
      "Loss for  8948 th iteration => 0.0412849849988\n",
      "Loss for  8949 th iteration => 0.041279733582\n",
      "Loss for  8950 th iteration => 0.0412734059266\n",
      "Loss for  8951 th iteration => 0.0412692840252\n",
      "Loss for  8952 th iteration => 0.0412632034786\n",
      "Loss for  8953 th iteration => 0.0412569909556\n",
      "Loss for  8954 th iteration => 0.0412524948027\n",
      "Loss for  8955 th iteration => 0.0412467069144\n",
      "Loss for  8956 th iteration => 0.0412412920204\n",
      "Loss for  8957 th iteration => 0.0412359518869\n",
      "Loss for  8958 th iteration => 0.0412296665747\n",
      "Loss for  8959 th iteration => 0.0412255142239\n",
      "Loss for  8960 th iteration => 0.0412194553202\n",
      "Loss for  8961 th iteration => 0.0412132844085\n",
      "Loss for  8962 th iteration => 0.0412087595183\n",
      "Loss for  8963 th iteration => 0.0412029921182\n",
      "Loss for  8964 th iteration => 0.0411976810673\n",
      "Loss for  8965 th iteration => 0.0411922585836\n",
      "Loss for  8966 th iteration => 0.0411860139068\n",
      "Loss for  8967 th iteration => 0.0411818331689\n",
      "Loss for  8968 th iteration => 0.0411757953657\n",
      "Loss for  8969 th iteration => 0.0411696644107\n",
      "Loss for  8970 th iteration => 0.041165112715\n",
      "Loss for  8971 th iteration => 0.0411593653535\n",
      "Loss for  8972 th iteration => 0.0411541528026\n",
      "Loss for  8973 th iteration => 0.0411486532996\n",
      "Loss for  8974 th iteration => 0.0411424476982\n",
      "Loss for  8975 th iteration => 0.0411382403369\n",
      "Loss for  8976 th iteration => 0.0411322232604\n",
      "Loss for  8977 th iteration => 0.0411261307468\n",
      "Loss for  8978 th iteration => 0.0411215539026\n",
      "Loss for  8979 th iteration => 0.0411158262805\n",
      "Loss for  8980 th iteration => 0.041110707716\n",
      "Loss for  8981 th iteration => 0.0411051356849\n",
      "Loss for  8982 th iteration => 0.0410989677227\n",
      "Loss for  8983 th iteration => 0.0410947352589\n",
      "Loss for  8984 th iteration => 0.0410887386686\n",
      "Loss for  8985 th iteration => 0.0410826831981\n",
      "Loss for  8986 th iteration => 0.0410780826384\n",
      "Loss for  8987 th iteration => 0.0410723745754\n",
      "Loss for  8988 th iteration => 0.0410673461574\n",
      "Loss for  8989 th iteration => 0.0410617054078\n",
      "Loss for  8990 th iteration => 0.0410555737527\n",
      "Loss for  8991 th iteration => 0.0410513175093\n",
      "Loss for  8992 th iteration => 0.0410453412698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  8993 th iteration => 0.0410393215428\n",
      "Loss for  8994 th iteration => 0.0410347024594\n",
      "Loss for  8995 th iteration => 0.0410271583971\n",
      "Loss for  8996 th iteration => 0.0410246143087\n",
      "Loss for  8997 th iteration => 0.0410192131954\n",
      "Loss for  8998 th iteration => 0.0410104271495\n",
      "Loss for  8999 th iteration => 0.041009778901\n",
      "Loss for  9000 th iteration => 0.0410014600391\n",
      "Loss for  9001 th iteration => 0.0409953639026\n",
      "Loss for  9002 th iteration => 0.0409934822155\n",
      "Loss for  9003 th iteration => 0.0409836287258\n",
      "Loss for  9004 th iteration => 0.0409818004524\n",
      "Loss for  9005 th iteration => 0.0409757056799\n",
      "Loss for  9006 th iteration => 0.0409672264378\n",
      "Loss for  9007 th iteration => 0.0409646604082\n",
      "Loss for  9008 th iteration => 0.0409587958726\n",
      "Loss for  9009 th iteration => 0.040953055488\n",
      "Loss for  9010 th iteration => 0.0409481953851\n",
      "Loss for  9011 th iteration => 0.0409421569228\n",
      "Loss for  9012 th iteration => 0.0409360327111\n",
      "Loss for  9013 th iteration => 0.0409324491665\n",
      "Loss for  9014 th iteration => 0.0409267239829\n",
      "Loss for  9015 th iteration => 0.0409195144357\n",
      "Loss for  9016 th iteration => 0.0409176501852\n",
      "Loss for  9017 th iteration => 0.040909441637\n",
      "Loss for  9018 th iteration => 0.0409042692142\n",
      "Loss for  9019 th iteration => 0.0409011389277\n",
      "Loss for  9020 th iteration => 0.040892894397\n",
      "Loss for  9021 th iteration => 0.0408895940842\n",
      "Loss for  9022 th iteration => 0.0408834267928\n",
      "Loss for  9023 th iteration => 0.0408764518295\n",
      "Loss for  9024 th iteration => 0.0408747588685\n",
      "Loss for  9025 th iteration => 0.0408663444477\n",
      "Loss for  9026 th iteration => 0.0408615381471\n",
      "Loss for  9027 th iteration => 0.0408578019094\n",
      "Loss for  9028 th iteration => 0.0408498398907\n",
      "Loss for  9029 th iteration => 0.0408468715133\n",
      "Loss for  9030 th iteration => 0.0408401506591\n",
      "Loss for  9031 th iteration => 0.0408337492501\n",
      "Loss for  9032 th iteration => 0.0408317369555\n",
      "Loss for  9033 th iteration => 0.0408233487007\n",
      "Loss for  9034 th iteration => 0.0408188407021\n",
      "Loss for  9035 th iteration => 0.0408146041303\n",
      "Loss for  9036 th iteration => 0.040806884674\n",
      "Loss for  9037 th iteration => 0.0408041865857\n",
      "Loss for  9038 th iteration => 0.040797008382\n",
      "Loss for  9039 th iteration => 0.0407910960566\n",
      "Loss for  9040 th iteration => 0.0407888042105\n",
      "Loss for  9041 th iteration => 0.0407804507277\n",
      "Loss for  9042 th iteration => 0.0407761869345\n",
      "Loss for  9043 th iteration => 0.0407715343325\n",
      "Loss for  9044 th iteration => 0.0407640255171\n",
      "Loss for  9045 th iteration => 0.0407615485395\n",
      "Loss for  9046 th iteration => 0.0407540280974\n",
      "Loss for  9047 th iteration => 0.0407469039017\n",
      "Loss for  9048 th iteration => 0.040744653792\n",
      "Loss for  9049 th iteration => 0.0407385744384\n",
      "Loss for  9050 th iteration => 0.0407329246512\n",
      "Loss for  9051 th iteration => 0.0407282514851\n",
      "Loss for  9052 th iteration => 0.040722466124\n",
      "Loss for  9053 th iteration => 0.0407178433415\n",
      "Loss for  9054 th iteration => 0.0407119760977\n",
      "Loss for  9055 th iteration => 0.0407062511797\n",
      "Loss for  9056 th iteration => 0.0407019021415\n",
      "Loss for  9057 th iteration => 0.0406958679266\n",
      "Loss for  9058 th iteration => 0.0406902683838\n",
      "Loss for  9059 th iteration => 0.0406838545493\n",
      "Loss for  9060 th iteration => 0.0406805407708\n",
      "Loss for  9061 th iteration => 0.0406757164623\n",
      "Loss for  9062 th iteration => 0.0406679584461\n",
      "Loss for  9063 th iteration => 0.0406650738145\n",
      "Loss for  9064 th iteration => 0.0406585143705\n",
      "Loss for  9065 th iteration => 0.040653123313\n",
      "Loss for  9066 th iteration => 0.0406490506189\n",
      "Loss for  9067 th iteration => 0.040641196754\n",
      "Loss for  9068 th iteration => 0.0406379575485\n",
      "Loss for  9069 th iteration => 0.0406330869969\n",
      "Loss for  9070 th iteration => 0.0406256812147\n",
      "Loss for  9071 th iteration => 0.0406224626974\n",
      "Loss for  9072 th iteration => 0.0406156832331\n",
      "Loss for  9073 th iteration => 0.040610860935\n",
      "Loss for  9074 th iteration => 0.0406064753042\n",
      "Loss for  9075 th iteration => 0.0405987171492\n",
      "Loss for  9076 th iteration => 0.040595544705\n",
      "Loss for  9077 th iteration => 0.0405903865608\n",
      "Loss for  9078 th iteration => 0.0405835422044\n",
      "Loss for  9079 th iteration => 0.0405799303182\n",
      "Loss for  9080 th iteration => 0.0405728676676\n",
      "Loss for  9081 th iteration => 0.0405687327694\n",
      "Loss for  9082 th iteration => 0.0405639795411\n",
      "Loss for  9083 th iteration => 0.040556319863\n",
      "Loss for  9084 th iteration => 0.0405533450404\n",
      "Loss for  9085 th iteration => 0.040547655694\n",
      "Loss for  9086 th iteration => 0.0405414394428\n",
      "Loss for  9087 th iteration => 0.0405374863764\n",
      "Loss for  9088 th iteration => 0.0405301977084\n",
      "Loss for  9089 th iteration => 0.0405266442021\n",
      "Loss for  9090 th iteration => 0.0405215711769\n",
      "Loss for  9091 th iteration => 0.0405141732547\n",
      "Loss for  9092 th iteration => 0.0405110132258\n",
      "Loss for  9093 th iteration => 0.0405049795354\n",
      "Loss for  9094 th iteration => 0.0404994644035\n",
      "Loss for  9095 th iteration => 0.040495101314\n",
      "Loss for  9096 th iteration => 0.0404877025913\n",
      "Loss for  9097 th iteration => 0.0404844656624\n",
      "Loss for  9098 th iteration => 0.0404792495951\n",
      "Loss for  9099 th iteration => 0.0404722534803\n",
      "Loss for  9100 th iteration => 0.0404687086098\n",
      "Loss for  9101 th iteration => 0.0404623642145\n",
      "Loss for  9102 th iteration => 0.0404575546227\n",
      "Loss for  9103 th iteration => 0.0404528334061\n",
      "Loss for  9104 th iteration => 0.0404455155514\n",
      "Loss for  9105 th iteration => 0.0404422075781\n",
      "Loss for  9106 th iteration => 0.040437015896\n",
      "Loss for  9107 th iteration => 0.0404303679992\n",
      "Loss for  9108 th iteration => 0.0404264926086\n",
      "Loss for  9109 th iteration => 0.0404198959438\n",
      "Loss for  9110 th iteration => 0.0404156825343\n",
      "Loss for  9111 th iteration => 0.0404106529972\n",
      "Loss for  9112 th iteration => 0.0404034135568\n",
      "Loss for  9113 th iteration => 0.0404000749146\n",
      "Loss for  9114 th iteration => 0.040392862321\n",
      "Loss for  9115 th iteration => 0.04038920546\n",
      "Loss for  9116 th iteration => 0.0403851087772\n",
      "Loss for  9117 th iteration => 0.0403755390302\n",
      "Loss for  9118 th iteration => 0.0403760009429\n",
      "Loss for  9119 th iteration => 0.0403678376312\n",
      "Loss for  9120 th iteration => 0.0403609454662\n",
      "Loss for  9121 th iteration => 0.0403600538202\n",
      "Loss for  9122 th iteration => 0.0403504024217\n",
      "Loss for  9123 th iteration => 0.0403479371436\n",
      "Loss for  9124 th iteration => 0.0403426762071\n",
      "Loss for  9125 th iteration => 0.0403336490406\n",
      "Loss for  9126 th iteration => 0.0403341461413\n",
      "Loss for  9127 th iteration => 0.0403252713324\n",
      "Loss for  9128 th iteration => 0.0403199736933\n",
      "Loss for  9129 th iteration => 0.0403175201901\n",
      "Loss for  9130 th iteration => 0.0403079285605\n",
      "Loss for  9131 th iteration => 0.0403069126572\n",
      "Loss for  9132 th iteration => 0.0403002324124\n",
      "Loss for  9133 th iteration => 0.0402922763601\n",
      "Loss for  9134 th iteration => 0.0402920678948\n",
      "Loss for  9135 th iteration => 0.0402829079621\n",
      "Loss for  9136 th iteration => 0.0402788938585\n",
      "Loss for  9137 th iteration => 0.0402751854562\n",
      "Loss for  9138 th iteration => 0.0402656446058\n",
      "Loss for  9139 th iteration => 0.0402657967735\n",
      "Loss for  9140 th iteration => 0.0402579750625\n",
      "Loss for  9141 th iteration => 0.0402509153243\n",
      "Loss for  9142 th iteration => 0.0402487313956\n",
      "Loss for  9143 th iteration => 0.0402411058198\n",
      "Loss for  9144 th iteration => 0.0402382660866\n",
      "Loss for  9145 th iteration => 0.0402315044999\n",
      "Loss for  9146 th iteration => 0.0402253750503\n",
      "Loss for  9147 th iteration => 0.0402219360683\n",
      "Loss for  9148 th iteration => 0.0402159119706\n",
      "Loss for  9149 th iteration => 0.0402117302663\n",
      "Loss for  9150 th iteration => 0.0402046967497\n",
      "Loss for  9151 th iteration => 0.040201360558\n",
      "Loss for  9152 th iteration => 0.0401960525455\n",
      "Loss for  9153 th iteration => 0.0401890970432\n",
      "Loss for  9154 th iteration => 0.0401856622484\n",
      "Loss for  9155 th iteration => 0.0401798072173\n",
      "Loss for  9156 th iteration => 0.0401746267136\n",
      "Loss for  9157 th iteration => 0.0401699879454\n",
      "Loss for  9158 th iteration => 0.0401630105092\n",
      "Loss for  9159 th iteration => 0.040159615971\n",
      "Loss for  9160 th iteration => 0.0401543727248\n",
      "Loss for  9161 th iteration => 0.0401478581528\n",
      "Loss for  9162 th iteration => 0.0401439980229\n",
      "Loss for  9163 th iteration => 0.0401377565127\n",
      "Loss for  9164 th iteration => 0.0401333935486\n",
      "Loss for  9165 th iteration => 0.0401283607981\n",
      "Loss for  9166 th iteration => 0.0401214497474\n",
      "Loss for  9167 th iteration => 0.0401179752746\n",
      "Loss for  9168 th iteration => 0.0401127797438\n",
      "Loss for  9169 th iteration => 0.0401066422558\n",
      "Loss for  9170 th iteration => 0.0401024215077\n",
      "Loss for  9171 th iteration => 0.0400958651899\n",
      "Loss for  9172 th iteration => 0.0400921874511\n",
      "Loss for  9173 th iteration => 0.040086819992\n",
      "Loss for  9174 th iteration => 0.0400800201035\n",
      "Loss for  9175 th iteration => 0.0400750314112\n",
      "Loss for  9176 th iteration => 0.0400715771032\n",
      "Loss for  9177 th iteration => 0.0400659528692\n",
      "Loss for  9178 th iteration => 0.0400594908994\n",
      "Loss for  9179 th iteration => 0.0400558002949\n",
      "Loss for  9180 th iteration => 0.0400501375694\n",
      "Loss for  9181 th iteration => 0.0400451264447\n",
      "Loss for  9182 th iteration => 0.0400401500865\n",
      "Loss for  9183 th iteration => 0.0400336600462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  9184 th iteration => 0.0400300719188\n",
      "Loss for  9185 th iteration => 0.0400245235282\n",
      "Loss for  9186 th iteration => 0.0400184804353\n",
      "Loss for  9187 th iteration => 0.0400143913691\n",
      "Loss for  9188 th iteration => 0.0400082728662\n",
      "Loss for  9189 th iteration => 0.0400042198648\n",
      "Loss for  9190 th iteration => 0.0399987434521\n",
      "Loss for  9191 th iteration => 0.0399923785972\n",
      "Loss for  9192 th iteration => 0.0399886649914\n",
      "Loss for  9193 th iteration => 0.0399831542434\n",
      "Loss for  9194 th iteration => 0.0399775812935\n",
      "Loss for  9195 th iteration => 0.0399730385648\n",
      "Loss for  9196 th iteration => 0.0399666289408\n",
      "Loss for  9197 th iteration => 0.0399631735503\n",
      "Loss for  9198 th iteration => 0.0399574505677\n",
      "Loss for  9199 th iteration => 0.0399511641642\n",
      "Loss for  9200 th iteration => 0.0399473312876\n",
      "Loss for  9201 th iteration => 0.0399400957087\n",
      "Loss for  9202 th iteration => 0.0399373331045\n",
      "Loss for  9203 th iteration => 0.0399324969399\n",
      "Loss for  9204 th iteration => 0.0399236854458\n",
      "Loss for  9205 th iteration => 0.0399237910375\n",
      "Loss for  9206 th iteration => 0.0399154762881\n",
      "Loss for  9207 th iteration => 0.0399098654689\n",
      "Loss for  9208 th iteration => 0.0399078517517\n",
      "Loss for  9209 th iteration => 0.0398983956335\n",
      "Loss for  9210 th iteration => 0.0398971464152\n",
      "Loss for  9211 th iteration => 0.0398908275161\n",
      "Loss for  9212 th iteration => 0.0398830269314\n",
      "Loss for  9213 th iteration => 0.0398826218213\n",
      "Loss for  9214 th iteration => 0.0398737749712\n",
      "Loss for  9215 th iteration => 0.0398697998482\n",
      "Loss for  9216 th iteration => 0.0398661831092\n",
      "Loss for  9217 th iteration => 0.0398569954591\n",
      "Loss for  9218 th iteration => 0.039856734608\n",
      "Loss for  9219 th iteration => 0.0398491651196\n",
      "Loss for  9220 th iteration => 0.0398426243011\n",
      "Loss for  9221 th iteration => 0.0398413936426\n",
      "Loss for  9222 th iteration => 0.0398322013124\n",
      "Loss for  9223 th iteration => 0.0398297424696\n",
      "Loss for  9224 th iteration => 0.03982464035\n",
      "Loss for  9225 th iteration => 0.0398161288145\n",
      "Loss for  9226 th iteration => 0.0398160295568\n",
      "Loss for  9227 th iteration => 0.0398075906183\n",
      "Loss for  9228 th iteration => 0.0398026013516\n",
      "Loss for  9229 th iteration => 0.0398000077259\n",
      "Loss for  9230 th iteration => 0.0397906145558\n",
      "Loss for  9231 th iteration => 0.0397880432014\n",
      "Loss for  9232 th iteration => 0.0397834858069\n",
      "Loss for  9233 th iteration => 0.0397765624255\n",
      "Loss for  9234 th iteration => 0.0397730468965\n",
      "Loss for  9235 th iteration => 0.0397681487152\n",
      "Loss for  9236 th iteration => 0.0397617861737\n",
      "Loss for  9237 th iteration => 0.0397579665505\n",
      "Loss for  9238 th iteration => 0.0397514595889\n",
      "Loss for  9239 th iteration => 0.0397475935673\n",
      "Loss for  9240 th iteration => 0.0397426091305\n",
      "Loss for  9241 th iteration => 0.0397357471905\n",
      "Loss for  9242 th iteration => 0.0397323556418\n",
      "Loss for  9243 th iteration => 0.0397270608823\n",
      "Loss for  9244 th iteration => 0.0397214345363\n",
      "Loss for  9245 th iteration => 0.0397171282835\n",
      "Loss for  9246 th iteration => 0.0397103755571\n",
      "Loss for  9247 th iteration => 0.039706939295\n",
      "Loss for  9248 th iteration => 0.0397018339721\n",
      "Loss for  9249 th iteration => 0.0396953145904\n",
      "Loss for  9250 th iteration => 0.0396916916121\n",
      "Loss for  9251 th iteration => 0.0396857110155\n",
      "Loss for  9252 th iteration => 0.0396811935682\n",
      "Loss for  9253 th iteration => 0.0396763757996\n",
      "Loss for  9254 th iteration => 0.0396696785836\n",
      "Loss for  9255 th iteration => 0.0396661359204\n",
      "Loss for  9256 th iteration => 0.0396611166037\n",
      "Loss for  9257 th iteration => 0.0396550749292\n",
      "Loss for  9258 th iteration => 0.0396509883977\n",
      "Loss for  9259 th iteration => 0.0396445532732\n",
      "Loss for  9260 th iteration => 0.0396409555295\n",
      "Loss for  9261 th iteration => 0.0396357096454\n",
      "Loss for  9262 th iteration => 0.0396291044242\n",
      "Loss for  9263 th iteration => 0.0396241714407\n",
      "Loss for  9264 th iteration => 0.0396206852984\n",
      "Loss for  9265 th iteration => 0.0396153418591\n",
      "Loss for  9266 th iteration => 0.0396089595833\n",
      "Loss for  9267 th iteration => 0.0396039806679\n",
      "Loss for  9268 th iteration => 0.0396000711262\n",
      "Loss for  9269 th iteration => 0.0395953436025\n",
      "Loss for  9270 th iteration => 0.0395886781007\n",
      "Loss for  9271 th iteration => 0.0395836557146\n",
      "Loss for  9272 th iteration => 0.0395800209574\n",
      "Loss for  9273 th iteration => 0.0395748051827\n",
      "Loss for  9274 th iteration => 0.0395688484172\n",
      "Loss for  9275 th iteration => 0.0395648927476\n",
      "Loss for  9276 th iteration => 0.0395585979944\n",
      "Loss for  9277 th iteration => 0.0395549097086\n",
      "Loss for  9278 th iteration => 0.0395495678134\n",
      "Loss for  9279 th iteration => 0.0395432425485\n",
      "Loss for  9280 th iteration => 0.0395394645281\n",
      "Loss for  9281 th iteration => 0.0395343001556\n",
      "Loss for  9282 th iteration => 0.0395288730941\n",
      "Loss for  9283 th iteration => 0.0395244030137\n",
      "Loss for  9284 th iteration => 0.0395180373807\n",
      "Loss for  9285 th iteration => 0.0395145013823\n",
      "Loss for  9286 th iteration => 0.0395091368552\n",
      "Loss for  9287 th iteration => 0.0395029745265\n",
      "Loss for  9288 th iteration => 0.0394992608122\n",
      "Loss for  9289 th iteration => 0.0394934119854\n",
      "Loss for  9290 th iteration => 0.0394891070216\n",
      "Loss for  9291 th iteration => 0.039483974542\n",
      "Loss for  9292 th iteration => 0.0394777483599\n",
      "Loss for  9293 th iteration => 0.0394740436113\n",
      "Loss for  9294 th iteration => 0.0394687480634\n",
      "Loss for  9295 th iteration => 0.0394631961936\n",
      "Loss for  9296 th iteration => 0.0394588864997\n",
      "Loss for  9297 th iteration => 0.0394526417276\n",
      "Loss for  9298 th iteration => 0.0394472757383\n",
      "Loss for  9299 th iteration => 0.0394443446183\n",
      "Loss for  9300 th iteration => 0.0394380554217\n",
      "Loss for  9301 th iteration => 0.0394319870566\n",
      "Loss for  9302 th iteration => 0.0394287877724\n",
      "Loss for  9303 th iteration => 0.0394230445997\n",
      "Loss for  9304 th iteration => 0.0394188452566\n",
      "Loss for  9305 th iteration => 0.0394121509354\n",
      "Loss for  9306 th iteration => 0.0394089147273\n",
      "Loss for  9307 th iteration => 0.0394037613412\n",
      "Loss for  9308 th iteration => 0.0393973391201\n",
      "Loss for  9309 th iteration => 0.0393937696667\n",
      "Loss for  9310 th iteration => 0.0393879143227\n",
      "Loss for  9311 th iteration => 0.039383437793\n",
      "Loss for  9312 th iteration => 0.0393786650036\n",
      "Loss for  9313 th iteration => 0.0393720194374\n",
      "Loss for  9314 th iteration => 0.0393686608277\n",
      "Loss for  9315 th iteration => 0.0393636167217\n",
      "Loss for  9316 th iteration => 0.0393577250733\n",
      "Loss for  9317 th iteration => 0.0393536379486\n",
      "Loss for  9318 th iteration => 0.0393472506725\n",
      "Loss for  9319 th iteration => 0.039343820985\n",
      "Loss for  9320 th iteration => 0.0393385710508\n",
      "Loss for  9321 th iteration => 0.0393321077886\n",
      "Loss for  9322 th iteration => 0.0393286166468\n",
      "Loss for  9323 th iteration => 0.0393231912218\n",
      "Loss for  9324 th iteration => 0.039318269919\n",
      "Loss for  9325 th iteration => 0.0393135539635\n",
      "Loss for  9326 th iteration => 0.0393070881237\n",
      "Loss for  9327 th iteration => 0.0393037745592\n",
      "Loss for  9328 th iteration => 0.039298549365\n",
      "Loss for  9329 th iteration => 0.0392926707286\n",
      "Loss for  9330 th iteration => 0.0392886069441\n",
      "Loss for  9331 th iteration => 0.0392825724917\n",
      "Loss for  9332 th iteration => 0.0392788261281\n",
      "Loss for  9333 th iteration => 0.03927358327\n",
      "Loss for  9334 th iteration => 0.0392672157254\n",
      "Loss for  9335 th iteration => 0.0392623099129\n",
      "Loss for  9336 th iteration => 0.0392572070436\n",
      "Loss for  9337 th iteration => 0.0392542335697\n",
      "Loss for  9338 th iteration => 0.039247935686\n",
      "Loss for  9339 th iteration => 0.0392406580631\n",
      "Loss for  9340 th iteration => 0.0392408747664\n",
      "Loss for  9341 th iteration => 0.0392330028416\n",
      "Loss for  9342 th iteration => 0.0392271687947\n",
      "Loss for  9343 th iteration => 0.0392255905242\n",
      "Loss for  9344 th iteration => 0.0392163674645\n",
      "Loss for  9345 th iteration => 0.0392149917811\n",
      "Loss for  9346 th iteration => 0.0392090133558\n",
      "Loss for  9347 th iteration => 0.0392015370642\n",
      "Loss for  9348 th iteration => 0.0392009177409\n",
      "Loss for  9349 th iteration => 0.0391924191006\n",
      "Loss for  9350 th iteration => 0.0391887229432\n",
      "Loss for  9351 th iteration => 0.0391850452712\n",
      "Loss for  9352 th iteration => 0.0391761051781\n",
      "Loss for  9353 th iteration => 0.0391761786173\n",
      "Loss for  9354 th iteration => 0.0391685010464\n",
      "Loss for  9355 th iteration => 0.0391625780363\n",
      "Loss for  9356 th iteration => 0.0391609632825\n",
      "Loss for  9357 th iteration => 0.0391520095315\n",
      "Loss for  9358 th iteration => 0.039150206363\n",
      "Loss for  9359 th iteration => 0.0391446706683\n",
      "Loss for  9360 th iteration => 0.039137014359\n",
      "Loss for  9361 th iteration => 0.0391363928701\n",
      "Loss for  9362 th iteration => 0.0391281051473\n",
      "Loss for  9363 th iteration => 0.0391240716157\n",
      "Loss for  9364 th iteration => 0.0391207474444\n",
      "Loss for  9365 th iteration => 0.0391117728325\n",
      "Loss for  9366 th iteration => 0.039111640686\n",
      "Loss for  9367 th iteration => 0.0391042343375\n",
      "Loss for  9368 th iteration => 0.0390982166879\n",
      "Loss for  9369 th iteration => 0.0390965498309\n",
      "Loss for  9370 th iteration => 0.0390877759876\n",
      "Loss for  9371 th iteration => 0.0390857439161\n",
      "Loss for  9372 th iteration => 0.0390804545982\n",
      "Loss for  9373 th iteration => 0.0390727541969\n",
      "Loss for  9374 th iteration => 0.0390720506166\n",
      "Loss for  9375 th iteration => 0.0390639246752\n",
      "Loss for  9376 th iteration => 0.0390597263485\n",
      "Loss for  9377 th iteration => 0.0390565850849\n",
      "Loss for  9378 th iteration => 0.0390476343446\n",
      "Loss for  9379 th iteration => 0.0390473467273\n",
      "Loss for  9380 th iteration => 0.0390401089904\n",
      "Loss for  9381 th iteration => 0.0390341042881\n",
      "Loss for  9382 th iteration => 0.0390323226748\n",
      "Loss for  9383 th iteration => 0.0390236891677\n",
      "Loss for  9384 th iteration => 0.0390215654858\n",
      "Loss for  9385 th iteration => 0.0390163867812\n",
      "Loss for  9386 th iteration => 0.0390087353849\n",
      "Loss for  9387 th iteration => 0.0390078961832\n",
      "Loss for  9388 th iteration => 0.0389998971575\n",
      "Loss for  9389 th iteration => 0.0389956533092\n",
      "Loss for  9390 th iteration => 0.038992577037\n",
      "Loss for  9391 th iteration => 0.0389836900153\n",
      "Loss for  9392 th iteration => 0.0389832825251\n",
      "Loss for  9393 th iteration => 0.0389761419854\n",
      "Loss for  9394 th iteration => 0.0389702237077\n",
      "Loss for  9395 th iteration => 0.0389682857405\n",
      "Loss for  9396 th iteration => 0.0389597643444\n",
      "Loss for  9397 th iteration => 0.0389576438881\n",
      "Loss for  9398 th iteration => 0.0389524819973\n",
      "Loss for  9399 th iteration => 0.0389449430492\n",
      "Loss for  9400 th iteration => 0.0389439329307\n",
      "Loss for  9401 th iteration => 0.0389360358664\n",
      "Loss for  9402 th iteration => 0.0389318390078\n",
      "Loss for  9403 th iteration => 0.0389273230011\n",
      "Loss for  9404 th iteration => 0.0389204959874\n",
      "Loss for  9405 th iteration => 0.0389197284277\n",
      "Loss for  9406 th iteration => 0.0389111000435\n",
      "Loss for  9407 th iteration => 0.0389080059442\n",
      "Loss for  9408 th iteration => 0.0389036568184\n",
      "Loss for  9409 th iteration => 0.0388959870778\n",
      "Loss for  9410 th iteration => 0.0388947797135\n",
      "Loss for  9411 th iteration => 0.0388873509896\n",
      "Loss for  9412 th iteration => 0.0388823899239\n",
      "Loss for  9413 th iteration => 0.0388800572775\n",
      "Loss for  9414 th iteration => 0.0388717397684\n",
      "Loss for  9415 th iteration => 0.0388696037109\n",
      "Loss for  9416 th iteration => 0.0388636856343\n",
      "Loss for  9417 th iteration => 0.0388569403657\n",
      "Loss for  9418 th iteration => 0.0388560773709\n",
      "Loss for  9419 th iteration => 0.0388475472699\n",
      "Loss for  9420 th iteration => 0.038844421842\n",
      "Loss for  9421 th iteration => 0.0388400832662\n",
      "Loss for  9422 th iteration => 0.0388324784579\n",
      "Loss for  9423 th iteration => 0.0388312409169\n",
      "Loss for  9424 th iteration => 0.0388238214287\n",
      "Loss for  9425 th iteration => 0.0388189400824\n",
      "Loss for  9426 th iteration => 0.0388165484722\n",
      "Loss for  9427 th iteration => 0.0388083041314\n",
      "Loss for  9428 th iteration => 0.0388061497577\n",
      "Loss for  9429 th iteration => 0.0388002218268\n",
      "Loss for  9430 th iteration => 0.0387935942287\n",
      "Loss for  9431 th iteration => 0.0387926227111\n",
      "Loss for  9432 th iteration => 0.0387841842926\n",
      "Loss for  9433 th iteration => 0.038781052214\n",
      "Loss for  9434 th iteration => 0.0387766856057\n",
      "Loss for  9435 th iteration => 0.0387691601923\n",
      "Loss for  9436 th iteration => 0.0387679149591\n",
      "Loss for  9437 th iteration => 0.03876046947\n",
      "Loss for  9438 th iteration => 0.0387557141424\n",
      "Loss for  9439 th iteration => 0.0387532177542\n",
      "Loss for  9440 th iteration => 0.0387450591272\n",
      "Loss for  9441 th iteration => 0.0387429059284\n",
      "Loss for  9442 th iteration => 0.0387369375853\n",
      "Loss for  9443 th iteration => 0.038730452368\n",
      "Loss for  9444 th iteration => 0.0387293636433\n",
      "Loss for  9445 th iteration => 0.0387210121034\n",
      "Loss for  9446 th iteration => 0.0387178904529\n",
      "Loss for  9447 th iteration => 0.0387134690491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  9448 th iteration => 0.0387060330443\n",
      "Loss for  9449 th iteration => 0.0387047956735\n",
      "Loss for  9450 th iteration => 0.0386972997461\n",
      "Loss for  9451 th iteration => 0.0386927025017\n",
      "Loss for  9452 th iteration => 0.0386900695808\n",
      "Loss for  9453 th iteration => 0.0386820053378\n",
      "Loss for  9454 th iteration => 0.0386798667748\n",
      "Loss for  9455 th iteration => 0.0386738368569\n",
      "Loss for  9456 th iteration => 0.0386675153635\n",
      "Loss for  9457 th iteration => 0.0386647253125\n",
      "Loss for  9458 th iteration => 0.0386584560931\n",
      "Loss for  9459 th iteration => 0.0386556620032\n",
      "Loss for  9460 th iteration => 0.0386485653925\n",
      "Loss for  9461 th iteration => 0.0386433242633\n",
      "Loss for  9462 th iteration => 0.0386400998149\n",
      "Loss for  9463 th iteration => 0.0386347588362\n",
      "Loss for  9464 th iteration => 0.0386303638413\n",
      "Loss for  9465 th iteration => 0.0386237168749\n",
      "Loss for  9466 th iteration => 0.0386202117273\n",
      "Loss for  9467 th iteration => 0.0386168046646\n",
      "Loss for  9468 th iteration => 0.0386096472024\n",
      "Loss for  9469 th iteration => 0.0386067392313\n",
      "Loss for  9470 th iteration => 0.0386005760712\n",
      "Loss for  9471 th iteration => 0.0385960520098\n",
      "Loss for  9472 th iteration => 0.0385933765996\n",
      "Loss for  9473 th iteration => 0.0385844919442\n",
      "Loss for  9474 th iteration => 0.0385824844954\n",
      "Loss for  9475 th iteration => 0.0385777932066\n",
      "Loss for  9476 th iteration => 0.0385712702552\n",
      "Loss for  9477 th iteration => 0.0385682160456\n",
      "Loss for  9478 th iteration => 0.0385630960923\n",
      "Loss for  9479 th iteration => 0.0385578612165\n",
      "Loss for  9480 th iteration => 0.0385537557478\n",
      "Loss for  9481 th iteration => 0.038547232324\n",
      "Loss for  9482 th iteration => 0.0385442717888\n",
      "Loss for  9483 th iteration => 0.0385391691502\n",
      "Loss for  9484 th iteration => 0.0385332585065\n",
      "Loss for  9485 th iteration => 0.0385297587627\n",
      "Loss for  9486 th iteration => 0.0385236759196\n",
      "Loss for  9487 th iteration => 0.0385200736715\n",
      "Loss for  9488 th iteration => 0.0385151565044\n",
      "Loss for  9489 th iteration => 0.0385088322914\n",
      "Loss for  9490 th iteration => 0.0385043799791\n",
      "Loss for  9491 th iteration => 0.0385008640567\n",
      "Loss for  9492 th iteration => 0.0384958876171\n",
      "Loss for  9493 th iteration => 0.0384899168449\n",
      "Loss for  9494 th iteration => 0.0384849706994\n",
      "Loss for  9495 th iteration => 0.0384815446263\n",
      "Loss for  9496 th iteration => 0.0384768439401\n",
      "Loss for  9497 th iteration => 0.0384707063925\n",
      "Loss for  9498 th iteration => 0.03846746308\n",
      "Loss for  9499 th iteration => 0.0384613357342\n",
      "Loss for  9500 th iteration => 0.0384575699481\n",
      "Loss for  9501 th iteration => 0.0384529017139\n",
      "Loss for  9502 th iteration => 0.0384466380216\n",
      "Loss for  9503 th iteration => 0.0384431765033\n",
      "Loss for  9504 th iteration => 0.0384381838356\n",
      "Loss for  9505 th iteration => 0.0384331047199\n",
      "Loss for  9506 th iteration => 0.0384290116729\n",
      "Loss for  9507 th iteration => 0.0384227450332\n",
      "Loss for  9508 th iteration => 0.0384192672341\n",
      "Loss for  9509 th iteration => 0.038414512547\n",
      "Loss for  9510 th iteration => 0.0384086597523\n",
      "Loss for  9511 th iteration => 0.0384051606392\n",
      "Loss for  9512 th iteration => 0.0383989925946\n",
      "Loss for  9513 th iteration => 0.038393763871\n",
      "Loss for  9514 th iteration => 0.0383911975673\n",
      "Loss for  9515 th iteration => 0.0383851072088\n",
      "Loss for  9516 th iteration => 0.038379405759\n",
      "Loss for  9517 th iteration => 0.0383761198729\n",
      "Loss for  9518 th iteration => 0.0383708058903\n",
      "Loss for  9519 th iteration => 0.0383670773645\n",
      "Loss for  9520 th iteration => 0.0383603373084\n",
      "Loss for  9521 th iteration => 0.0383573957816\n",
      "Loss for  9522 th iteration => 0.0383526184665\n",
      "Loss for  9523 th iteration => 0.0383464511699\n",
      "Loss for  9524 th iteration => 0.0383432953606\n",
      "Loss for  9525 th iteration => 0.038337014501\n",
      "Loss for  9526 th iteration => 0.0383333975493\n",
      "Loss for  9527 th iteration => 0.0383288212599\n",
      "Loss for  9528 th iteration => 0.0383222524284\n",
      "Loss for  9529 th iteration => 0.038318141189\n",
      "Loss for  9530 th iteration => 0.0383143754995\n",
      "Loss for  9531 th iteration => 0.0383094631734\n",
      "Loss for  9532 th iteration => 0.0383037764338\n",
      "Loss for  9533 th iteration => 0.0382986469845\n",
      "Loss for  9534 th iteration => 0.0382952280556\n",
      "Loss for  9535 th iteration => 0.0382908510255\n",
      "Loss for  9536 th iteration => 0.0382845168721\n",
      "Loss for  9537 th iteration => 0.0382815566419\n",
      "Loss for  9538 th iteration => 0.0382751897284\n",
      "Loss for  9539 th iteration => 0.0382715078807\n",
      "Loss for  9540 th iteration => 0.038267124127\n",
      "Loss for  9541 th iteration => 0.0382606644772\n",
      "Loss for  9542 th iteration => 0.0382575058869\n",
      "Loss for  9543 th iteration => 0.0382522334957\n",
      "Loss for  9544 th iteration => 0.0382472790353\n",
      "Loss for  9545 th iteration => 0.0382434493575\n",
      "Loss for  9546 th iteration => 0.038236991335\n",
      "Loss for  9547 th iteration => 0.0382335208388\n",
      "Loss for  9548 th iteration => 0.0382275152757\n",
      "Loss for  9549 th iteration => 0.0382235884399\n",
      "Loss for  9550 th iteration => 0.0382204381346\n",
      "Loss for  9551 th iteration => 0.0382117712887\n",
      "Loss for  9552 th iteration => 0.0382084075204\n",
      "Loss for  9553 th iteration => 0.0382057950406\n",
      "Loss for  9554 th iteration => 0.0381997734005\n",
      "Loss for  9555 th iteration => 0.0381941768199\n",
      "Loss for  9556 th iteration => 0.0381926842054\n",
      "Loss for  9557 th iteration => 0.0381849231903\n",
      "Loss for  9558 th iteration => 0.0381816066387\n",
      "Loss for  9559 th iteration => 0.0381767422082\n",
      "Loss for  9560 th iteration => 0.038170324651\n",
      "Loss for  9561 th iteration => 0.0381688669162\n",
      "Loss for  9562 th iteration => 0.0381615096186\n",
      "Loss for  9563 th iteration => 0.0381573241357\n",
      "Loss for  9564 th iteration => 0.0381538796047\n",
      "Loss for  9565 th iteration => 0.0381468991467\n",
      "Loss for  9566 th iteration => 0.0381446525748\n",
      "Loss for  9567 th iteration => 0.0381381062042\n",
      "Loss for  9568 th iteration => 0.0381318208402\n",
      "Loss for  9569 th iteration => 0.0381314685691\n",
      "Loss for  9570 th iteration => 0.0381237295049\n",
      "Loss for  9571 th iteration => 0.0381194419579\n",
      "Loss for  9572 th iteration => 0.0381166798103\n",
      "Loss for  9573 th iteration => 0.0381079279024\n",
      "Loss for  9574 th iteration => 0.0381063232207\n",
      "Loss for  9575 th iteration => 0.0381013556962\n",
      "Loss for  9576 th iteration => 0.0380955399154\n",
      "Loss for  9577 th iteration => 0.0380918641724\n",
      "Loss for  9578 th iteration => 0.0380870095394\n",
      "Loss for  9579 th iteration => 0.0380823175032\n",
      "Loss for  9580 th iteration => 0.0380778761024\n",
      "Loss for  9581 th iteration => 0.0380720559613\n",
      "Loss for  9582 th iteration => 0.0380684791138\n",
      "Loss for  9583 th iteration => 0.0380636208619\n",
      "Loss for  9584 th iteration => 0.0380583257324\n",
      "Loss for  9585 th iteration => 0.0380544354722\n",
      "Loss for  9586 th iteration => 0.0380485991181\n",
      "Loss for  9587 th iteration => 0.0380451762786\n",
      "Loss for  9588 th iteration => 0.0380401845054\n",
      "Loss for  9589 th iteration => 0.0380344745137\n",
      "Loss for  9590 th iteration => 0.0380309800564\n",
      "Loss for  9591 th iteration => 0.0380256120879\n",
      "Loss for  9592 th iteration => 0.0380215398561\n",
      "Loss for  9593 th iteration => 0.038016778968\n",
      "Loss for  9594 th iteration => 0.0380110612539\n",
      "Loss for  9595 th iteration => 0.0380073986321\n",
      "Loss for  9596 th iteration => 0.0380025711211\n",
      "Loss for  9597 th iteration => 0.0379975984321\n",
      "Loss for  9598 th iteration => 0.0379934124656\n",
      "Loss for  9599 th iteration => 0.0379876739719\n",
      "Loss for  9600 th iteration => 0.0379841717826\n",
      "Loss for  9601 th iteration => 0.0379792085473\n",
      "Loss for  9602 th iteration => 0.0379737524817\n",
      "Loss for  9603 th iteration => 0.0379700657656\n",
      "Loss for  9604 th iteration => 0.0379644138245\n",
      "Loss for  9605 th iteration => 0.0379591736143\n",
      "Loss for  9606 th iteration => 0.0379564417525\n",
      "Loss for  9607 th iteration => 0.0379507581736\n",
      "Loss for  9608 th iteration => 0.0379449475685\n",
      "Loss for  9609 th iteration => 0.0379420466753\n",
      "Loss for  9610 th iteration => 0.0379366175523\n",
      "Loss for  9611 th iteration => 0.0379328986952\n",
      "Loss for  9612 th iteration => 0.0379265368764\n",
      "Loss for  9613 th iteration => 0.0379234194525\n",
      "Loss for  9614 th iteration => 0.0379188462397\n",
      "Loss for  9615 th iteration => 0.0379128743982\n",
      "Loss for  9616 th iteration => 0.0379095802494\n",
      "Loss for  9617 th iteration => 0.0379036427024\n",
      "Loss for  9618 th iteration => 0.0379000163911\n",
      "Loss for  9619 th iteration => 0.0378955120566\n",
      "Loss for  9620 th iteration => 0.0378892362092\n",
      "Loss for  9621 th iteration => 0.0378849136322\n",
      "Loss for  9622 th iteration => 0.0378815278075\n",
      "Loss for  9623 th iteration => 0.037876563243\n",
      "Loss for  9624 th iteration => 0.0378710858814\n",
      "Loss for  9625 th iteration => 0.0378674687229\n",
      "Loss for  9626 th iteration => 0.0378618214613\n",
      "Loss for  9627 th iteration => 0.0378583723737\n",
      "Loss for  9628 th iteration => 0.0378533208149\n",
      "Loss for  9629 th iteration => 0.0378476877536\n",
      "Loss for  9630 th iteration => 0.0378440786668\n",
      "Loss for  9631 th iteration => 0.0378375836467\n",
      "Loss for  9632 th iteration => 0.0378352308814\n",
      "Loss for  9633 th iteration => 0.0378306450751\n",
      "Loss for  9634 th iteration => 0.0378230083949\n",
      "Loss for  9635 th iteration => 0.0378228535279\n",
      "Loss for  9636 th iteration => 0.0378149371063\n",
      "Loss for  9637 th iteration => 0.0378111256043\n",
      "Loss for  9638 th iteration => 0.0378079877666\n",
      "Loss for  9639 th iteration => 0.0377996715995\n",
      "Loss for  9640 th iteration => 0.0377994721276\n",
      "Loss for  9641 th iteration => 0.0377923442319\n",
      "Loss for  9642 th iteration => 0.0377870281289\n",
      "Loss for  9643 th iteration => 0.0377840205018\n",
      "Loss for  9644 th iteration => 0.0377773094062\n",
      "Loss for  9645 th iteration => 0.0377759625165\n",
      "Loss for  9646 th iteration => 0.0377686681967\n",
      "Loss for  9647 th iteration => 0.0377646831312\n",
      "Loss for  9648 th iteration => 0.0377615057311\n",
      "Loss for  9649 th iteration => 0.0377543179712\n",
      "Loss for  9650 th iteration => 0.0377522998053\n",
      "Loss for  9651 th iteration => 0.0377459242863\n",
      "Loss for  9652 th iteration => 0.0377409193204\n",
      "Loss for  9653 th iteration => 0.0377373037869\n",
      "Loss for  9654 th iteration => 0.0377320894259\n",
      "Loss for  9655 th iteration => 0.0377288965426\n",
      "Loss for  9656 th iteration => 0.0377220504345\n",
      "Loss for  9657 th iteration => 0.0377191115086\n",
      "Loss for  9658 th iteration => 0.0377130053372\n",
      "Loss for  9659 th iteration => 0.0377094855502\n",
      "Loss for  9660 th iteration => 0.0377060906131\n",
      "Loss for  9661 th iteration => 0.037697918305\n",
      "Loss for  9662 th iteration => 0.0376977927444\n",
      "Loss for  9663 th iteration => 0.0376905238409\n",
      "Loss for  9664 th iteration => 0.0376855004343\n",
      "Loss for  9665 th iteration => 0.0376835974324\n",
      "Loss for  9666 th iteration => 0.0376749781275\n",
      "Loss for  9667 th iteration => 0.0376726510184\n",
      "Loss for  9668 th iteration => 0.0376685748804\n",
      "Loss for  9669 th iteration => 0.0376622577829\n",
      "Loss for  9670 th iteration => 0.0376591773934\n",
      "Loss for  9671 th iteration => 0.037654552869\n",
      "Loss for  9672 th iteration => 0.0376492731487\n",
      "Loss for  9673 th iteration => 0.0376455523952\n",
      "Loss for  9674 th iteration => 0.0376393122886\n",
      "Loss for  9675 th iteration => 0.0376361978991\n",
      "Loss for  9676 th iteration => 0.03763169601\n",
      "Loss for  9677 th iteration => 0.0376258966053\n",
      "Loss for  9678 th iteration => 0.0376225670467\n",
      "Loss for  9679 th iteration => 0.0376166264236\n",
      "Loss for  9680 th iteration => 0.0376132337263\n",
      "Loss for  9681 th iteration => 0.0376086960564\n",
      "Loss for  9682 th iteration => 0.0376025870963\n",
      "Loss for  9683 th iteration => 0.0375982490071\n",
      "Loss for  9684 th iteration => 0.0375948858375\n",
      "Loss for  9685 th iteration => 0.0375900420715\n",
      "Loss for  9686 th iteration => 0.0375847186401\n",
      "Loss for  9687 th iteration => 0.0375810802937\n",
      "Loss for  9688 th iteration => 0.0375754000015\n",
      "Loss for  9689 th iteration => 0.0375703506216\n",
      "Loss for  9690 th iteration => 0.03756780838\n",
      "Loss for  9691 th iteration => 0.0375619353981\n",
      "Loss for  9692 th iteration => 0.0375566509598\n",
      "Loss for  9693 th iteration => 0.0375550447839\n",
      "Loss for  9694 th iteration => 0.0375475789404\n",
      "Loss for  9695 th iteration => 0.0375444670681\n",
      "Loss for  9696 th iteration => 0.0375395315578\n",
      "Loss for  9697 th iteration => 0.0375334147956\n",
      "Loss for  9698 th iteration => 0.0375321697682\n",
      "Loss for  9699 th iteration => 0.0375248788178\n",
      "Loss for  9700 th iteration => 0.0375210246294\n",
      "Loss for  9701 th iteration => 0.0375172999368\n",
      "Loss for  9702 th iteration => 0.0375107059867\n",
      "Loss for  9703 th iteration => 0.0375087861396\n",
      "Loss for  9704 th iteration => 0.0375021798107\n",
      "Loss for  9705 th iteration => 0.0374976872071\n",
      "Loss for  9706 th iteration => 0.037495017039\n",
      "Loss for  9707 th iteration => 0.0374880009605\n",
      "Loss for  9708 th iteration => 0.0374855017243\n",
      "Loss for  9709 th iteration => 0.0374795972971\n",
      "Loss for  9710 th iteration => 0.037473132185\n",
      "Loss for  9711 th iteration => 0.0374730741027\n",
      "Loss for  9712 th iteration => 0.037465647893\n",
      "Loss for  9713 th iteration => 0.037461152244\n",
      "Loss for  9714 th iteration => 0.037458796355\n",
      "Loss for  9715 th iteration => 0.0374502669367\n",
      "Loss for  9716 th iteration => 0.0374469649376\n",
      "Loss for  9717 th iteration => 0.0374443633064\n",
      "Loss for  9718 th iteration => 0.0374387567971\n",
      "Loss for  9719 th iteration => 0.0374331540883\n",
      "Loss for  9720 th iteration => 0.0374302371808\n",
      "Loss for  9721 th iteration => 0.0374250970071\n",
      "Loss for  9722 th iteration => 0.0374213437609\n",
      "Loss for  9723 th iteration => 0.0374152323559\n",
      "Loss for  9724 th iteration => 0.0374120904125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  9725 th iteration => 0.0374076396453\n",
      "Loss for  9726 th iteration => 0.0374020037558\n",
      "Loss for  9727 th iteration => 0.0373986169227\n",
      "Loss for  9728 th iteration => 0.037392682894\n",
      "Loss for  9729 th iteration => 0.0373894916983\n",
      "Loss for  9730 th iteration => 0.0373848991142\n",
      "Loss for  9731 th iteration => 0.0373789627611\n",
      "Loss for  9732 th iteration => 0.0373758929497\n",
      "Loss for  9733 th iteration => 0.0373705329744\n",
      "Loss for  9734 th iteration => 0.0373664917167\n",
      "Loss for  9735 th iteration => 0.037362181744\n",
      "Loss for  9736 th iteration => 0.0373561843506\n",
      "Loss for  9737 th iteration => 0.0373530387116\n",
      "Loss for  9738 th iteration => 0.037346662335\n",
      "Loss for  9739 th iteration => 0.0373442236942\n",
      "Loss for  9740 th iteration => 0.0373398720443\n",
      "Loss for  9741 th iteration => 0.0373322303297\n",
      "Loss for  9742 th iteration => 0.0373323790226\n",
      "Loss for  9743 th iteration => 0.037324483982\n",
      "Loss for  9744 th iteration => 0.0373208439466\n",
      "Loss for  9745 th iteration => 0.0373176857209\n",
      "Loss for  9746 th iteration => 0.0373093213807\n",
      "Loss for  9747 th iteration => 0.0373064602348\n",
      "Loss for  9748 th iteration => 0.0373036483038\n",
      "Loss for  9749 th iteration => 0.0372980209399\n",
      "Loss for  9750 th iteration => 0.0372927012645\n",
      "Loss for  9751 th iteration => 0.0372909564953\n",
      "Loss for  9752 th iteration => 0.0372839621443\n",
      "Loss for  9753 th iteration => 0.0372806834459\n",
      "Loss for  9754 th iteration => 0.0372756636836\n",
      "Loss for  9755 th iteration => 0.0372687071224\n",
      "Loss for  9756 th iteration => 0.0372674154192\n",
      "Loss for  9757 th iteration => 0.0372624142111\n",
      "Loss for  9758 th iteration => 0.0372568440738\n",
      "Loss for  9759 th iteration => 0.037252287794\n",
      "Loss for  9760 th iteration => 0.0372491299289\n",
      "Loss for  9761 th iteration => 0.0372446288299\n",
      "Loss for  9762 th iteration => 0.0372389024639\n",
      "Loss for  9763 th iteration => 0.0372356865698\n",
      "Loss for  9764 th iteration => 0.0372301068365\n",
      "Loss for  9765 th iteration => 0.0372265156348\n",
      "Loss for  9766 th iteration => 0.0372220740965\n",
      "Loss for  9767 th iteration => 0.0372161561506\n",
      "Loss for  9768 th iteration => 0.0372118296303\n",
      "Loss for  9769 th iteration => 0.0372085738345\n",
      "Loss for  9770 th iteration => 0.037203948149\n",
      "Loss for  9771 th iteration => 0.0371984521288\n",
      "Loss for  9772 th iteration => 0.0371951651452\n",
      "Loss for  9773 th iteration => 0.0371894702772\n",
      "Loss for  9774 th iteration => 0.0371862007881\n",
      "Loss for  9775 th iteration => 0.0371814723188\n",
      "Loss for  9776 th iteration => 0.0371758337092\n",
      "Loss for  9777 th iteration => 0.0371725437885\n",
      "Loss for  9778 th iteration => 0.0371675546965\n",
      "Loss for  9779 th iteration => 0.0371633803322\n",
      "Loss for  9780 th iteration => 0.0371590442468\n",
      "Loss for  9781 th iteration => 0.037153410962\n",
      "Loss for  9782 th iteration => 0.0371499865994\n",
      "Loss for  9783 th iteration => 0.0371454139537\n",
      "Loss for  9784 th iteration => 0.0371405607825\n",
      "Loss for  9785 th iteration => 0.0371366540739\n",
      "Loss for  9786 th iteration => 0.0371310119171\n",
      "Loss for  9787 th iteration => 0.0371277077439\n",
      "Loss for  9788 th iteration => 0.0371230292424\n",
      "Loss for  9789 th iteration => 0.0371178172628\n",
      "Loss for  9790 th iteration => 0.0371142829893\n",
      "Loss for  9791 th iteration => 0.0371086635056\n",
      "Loss for  9792 th iteration => 0.0371037191546\n",
      "Loss for  9793 th iteration => 0.0371014135871\n",
      "Loss for  9794 th iteration => 0.0370954390119\n",
      "Loss for  9795 th iteration => 0.0370892980425\n",
      "Loss for  9796 th iteration => 0.0370890833909\n",
      "Loss for  9797 th iteration => 0.0370817142971\n",
      "Loss for  9798 th iteration => 0.0370776543293\n",
      "Loss for  9799 th iteration => 0.0370749840091\n",
      "Loss for  9800 th iteration => 0.0370667288437\n",
      "Loss for  9801 th iteration => 0.0370634544782\n",
      "Loss for  9802 th iteration => 0.037061142614\n",
      "Loss for  9803 th iteration => 0.0370552889906\n",
      "Loss for  9804 th iteration => 0.0370502667921\n",
      "Loss for  9805 th iteration => 0.0370485545584\n",
      "Loss for  9806 th iteration => 0.0370412422939\n",
      "Loss for  9807 th iteration => 0.0370384610114\n",
      "Loss for  9808 th iteration => 0.0370333795551\n",
      "Loss for  9809 th iteration => 0.037027453664\n",
      "Loss for  9810 th iteration => 0.0370249346386\n",
      "Loss for  9811 th iteration => 0.0370196221168\n",
      "Loss for  9812 th iteration => 0.0370160901285\n",
      "Loss for  9813 th iteration => 0.0370101729687\n",
      "Loss for  9814 th iteration => 0.037007080677\n",
      "Loss for  9815 th iteration => 0.0370026547824\n",
      "Loss for  9816 th iteration => 0.0369970631655\n",
      "Loss for  9817 th iteration => 0.0369938252033\n",
      "Loss for  9818 th iteration => 0.0369881624911\n",
      "Loss for  9819 th iteration => 0.0369848351988\n",
      "Loss for  9820 th iteration => 0.0369803770919\n",
      "Loss for  9821 th iteration => 0.036974557126\n",
      "Loss for  9822 th iteration => 0.0369702579779\n",
      "Loss for  9823 th iteration => 0.0369670093001\n",
      "Loss for  9824 th iteration => 0.0369624193375\n",
      "Loss for  9825 th iteration => 0.0369571860785\n",
      "Loss for  9826 th iteration => 0.036953746345\n",
      "Loss for  9827 th iteration => 0.0369480754707\n",
      "Loss for  9828 th iteration => 0.0369432079439\n",
      "Loss for  9829 th iteration => 0.0369409859183\n",
      "Loss for  9830 th iteration => 0.0369349678242\n",
      "Loss for  9831 th iteration => 0.0369289752547\n",
      "Loss for  9832 th iteration => 0.0369286929162\n",
      "Loss for  9833 th iteration => 0.036921337143\n",
      "Loss for  9834 th iteration => 0.036917480706\n",
      "Loss for  9835 th iteration => 0.0369146575096\n",
      "Loss for  9836 th iteration => 0.0369065435213\n",
      "Loss for  9837 th iteration => 0.0369065811788\n",
      "Loss for  9838 th iteration => 0.0368994985298\n",
      "Loss for  9839 th iteration => 0.0368948411653\n",
      "Loss for  9840 th iteration => 0.0368914992866\n",
      "Loss for  9841 th iteration => 0.0368850023743\n",
      "Loss for  9842 th iteration => 0.0368829182649\n",
      "Loss for  9843 th iteration => 0.036877194425\n",
      "Loss for  9844 th iteration => 0.0368741423106\n",
      "Loss for  9845 th iteration => 0.0368683558779\n",
      "Loss for  9846 th iteration => 0.0368648003577\n",
      "Loss for  9847 th iteration => 0.0368607968823\n",
      "Loss for  9848 th iteration => 0.0368549182584\n",
      "Loss for  9849 th iteration => 0.0368519707193\n",
      "Loss for  9850 th iteration => 0.0368465573475\n",
      "Loss for  9851 th iteration => 0.0368426545889\n",
      "Loss for  9852 th iteration => 0.0368387009681\n",
      "Loss for  9853 th iteration => 0.0368328903036\n",
      "Loss for  9854 th iteration => 0.0368295434263\n",
      "Loss for  9855 th iteration => 0.0368249495585\n",
      "Loss for  9856 th iteration => 0.0368203127335\n",
      "Loss for  9857 th iteration => 0.0368166289942\n",
      "Loss for  9858 th iteration => 0.0368108818906\n",
      "Loss for  9859 th iteration => 0.0368072518773\n",
      "Loss for  9860 th iteration => 0.0368015918241\n",
      "Loss for  9861 th iteration => 0.036798692812\n",
      "Loss for  9862 th iteration => 0.0367949599523\n",
      "Loss for  9863 th iteration => 0.0367875649104\n",
      "Loss for  9864 th iteration => 0.0367871829277\n",
      "Loss for  9865 th iteration => 0.0367799129989\n",
      "Loss for  9866 th iteration => 0.0367761300042\n",
      "Loss for  9867 th iteration => 0.0367732766835\n",
      "Loss for  9868 th iteration => 0.0367653134969\n",
      "Loss for  9869 th iteration => 0.0367635085513\n",
      "Loss for  9870 th iteration => 0.0367590303723\n",
      "Loss for  9871 th iteration => 0.0367537740535\n",
      "Loss for  9872 th iteration => 0.0367502589241\n",
      "Loss for  9873 th iteration => 0.0367453538267\n",
      "Loss for  9874 th iteration => 0.0367400429708\n",
      "Loss for  9875 th iteration => 0.0367375805474\n",
      "Loss for  9876 th iteration => 0.0367322591928\n",
      "Loss for  9877 th iteration => 0.0367266815632\n",
      "Loss for  9878 th iteration => 0.036723884027\n",
      "Loss for  9879 th iteration => 0.0367189290486\n",
      "Loss for  9880 th iteration => 0.0367155232619\n",
      "Loss for  9881 th iteration => 0.0367093652843\n",
      "Loss for  9882 th iteration => 0.0367064853572\n",
      "Loss for  9883 th iteration => 0.0367021799026\n",
      "Loss for  9884 th iteration => 0.0366967061812\n",
      "Loss for  9885 th iteration => 0.0366936226645\n",
      "Loss for  9886 th iteration => 0.036687472715\n",
      "Loss for  9887 th iteration => 0.0366846828466\n",
      "Loss for  9888 th iteration => 0.0366802861134\n",
      "Loss for  9889 th iteration => 0.0366745471843\n",
      "Loss for  9890 th iteration => 0.0366717412631\n",
      "Loss for  9891 th iteration => 0.0366659355763\n",
      "Loss for  9892 th iteration => 0.0366626519444\n",
      "Loss for  9893 th iteration => 0.0366583961023\n",
      "Loss for  9894 th iteration => 0.0366524799509\n",
      "Loss for  9895 th iteration => 0.0366485730425\n",
      "Loss for  9896 th iteration => 0.0366450030132\n",
      "Loss for  9897 th iteration => 0.0366406824763\n",
      "Loss for  9898 th iteration => 0.0366355452048\n",
      "Loss for  9899 th iteration => 0.0366320226308\n",
      "Loss for  9900 th iteration => 0.0366267661172\n",
      "Loss for  9901 th iteration => 0.0366232616667\n",
      "Loss for  9902 th iteration => 0.0366188388187\n",
      "Loss for  9903 th iteration => 0.0366135315459\n",
      "Loss for  9904 th iteration => 0.0366089072507\n",
      "Loss for  9905 th iteration => 0.036605651301\n",
      "Loss for  9906 th iteration => 0.0366001553928\n",
      "Loss for  9907 th iteration => 0.0365966086425\n",
      "Loss for  9908 th iteration => 0.0365935756929\n",
      "Loss for  9909 th iteration => 0.0365858226164\n",
      "Loss for  9910 th iteration => 0.0365856778352\n",
      "Loss for  9911 th iteration => 0.0365786368462\n",
      "Loss for  9912 th iteration => 0.0365743807645\n",
      "Loss for  9913 th iteration => 0.0365720547098\n",
      "Loss for  9914 th iteration => 0.0365639545493\n",
      "Loss for  9915 th iteration => 0.0365606532339\n",
      "Loss for  9916 th iteration => 0.0365585079342\n",
      "Loss for  9917 th iteration => 0.0365527741333\n",
      "Loss for  9918 th iteration => 0.0365478685965\n",
      "Loss for  9919 th iteration => 0.0365461914534\n",
      "Loss for  9920 th iteration => 0.036538987623\n",
      "Loss for  9921 th iteration => 0.0365364764714\n",
      "Loss for  9922 th iteration => 0.0365313446252\n",
      "Loss for  9923 th iteration => 0.0365256657561\n",
      "Loss for  9924 th iteration => 0.0365214584661\n",
      "Loss for  9925 th iteration => 0.0365187967796\n",
      "Loss for  9926 th iteration => 0.0365149131492\n",
      "Loss for  9927 th iteration => 0.0365075561723\n",
      "Loss for  9928 th iteration => 0.0365075373045\n",
      "Loss for  9929 th iteration => 0.0365000556107\n",
      "Loss for  9930 th iteration => 0.0364966134759\n",
      "Loss for  9931 th iteration => 0.0364935070043\n",
      "Loss for  9932 th iteration => 0.0364856540988\n",
      "Loss for  9933 th iteration => 0.0364858854236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  9934 th iteration => 0.0364786359601\n",
      "Loss for  9935 th iteration => 0.0364745197733\n",
      "Loss for  9936 th iteration => 0.0364720853151\n",
      "Loss for  9937 th iteration => 0.0364639192109\n",
      "Loss for  9938 th iteration => 0.0364626327408\n",
      "Loss for  9939 th iteration => 0.0364579124059\n",
      "Loss for  9940 th iteration => 0.0364525724615\n",
      "Loss for  9941 th iteration => 0.0364481918835\n",
      "Loss for  9942 th iteration => 0.0364449531031\n",
      "Loss for  9943 th iteration => 0.0364408775543\n",
      "Loss for  9944 th iteration => 0.0364353374104\n",
      "Loss for  9945 th iteration => 0.0364323087345\n",
      "Loss for  9946 th iteration => 0.0364266562827\n",
      "Loss for  9947 th iteration => 0.0364218631891\n",
      "Loss for  9948 th iteration => 0.0364198569622\n",
      "Loss for  9949 th iteration => 0.0364138378612\n",
      "Loss for  9950 th iteration => 0.0364081865882\n",
      "Loss for  9951 th iteration => 0.0364077366563\n",
      "Loss for  9952 th iteration => 0.0364004805414\n",
      "Loss for  9953 th iteration => 0.0363971191915\n",
      "Loss for  9954 th iteration => 0.0363939625347\n",
      "Loss for  9955 th iteration => 0.0363864066793\n",
      "Loss for  9956 th iteration => 0.0363861948589\n",
      "Loss for  9957 th iteration => 0.0363791594923\n",
      "Loss for  9958 th iteration => 0.0363751579878\n",
      "Loss for  9959 th iteration => 0.0363726398261\n",
      "Loss for  9960 th iteration => 0.0363647769564\n",
      "Loss for  9961 th iteration => 0.0363645702471\n",
      "Loss for  9962 th iteration => 0.0363579200033\n",
      "Loss for  9963 th iteration => 0.0363532335019\n",
      "Loss for  9964 th iteration => 0.0363512501542\n",
      "Loss for  9965 th iteration => 0.036343351161\n",
      "Loss for  9966 th iteration => 0.0363411255886\n",
      "Loss for  9967 th iteration => 0.0363374027058\n",
      "Loss for  9968 th iteration => 0.036331657851\n",
      "Loss for  9969 th iteration => 0.0363284755083\n",
      "Loss for  9970 th iteration => 0.0363240729735\n",
      "Loss for  9971 th iteration => 0.0363194192235\n",
      "Loss for  9972 th iteration => 0.0363158932726\n",
      "Loss for  9973 th iteration => 0.036310205795\n",
      "Loss for  9974 th iteration => 0.0363068002222\n",
      "Loss for  9975 th iteration => 0.0363012149049\n",
      "Loss for  9976 th iteration => 0.036298437864\n",
      "Loss for  9977 th iteration => 0.0362947336657\n",
      "Loss for  9978 th iteration => 0.0362875344947\n",
      "Loss for  9979 th iteration => 0.0362872495127\n",
      "Loss for  9980 th iteration => 0.0362800152498\n",
      "Loss for  9981 th iteration => 0.0362765710739\n",
      "Loss for  9982 th iteration => 0.0362735320063\n",
      "Loss for  9983 th iteration => 0.0362659134514\n",
      "Loss for  9984 th iteration => 0.0362658351057\n",
      "Loss for  9985 th iteration => 0.0362588039137\n",
      "Loss for  9986 th iteration => 0.0362547843615\n",
      "Loss for  9987 th iteration => 0.0362523197576\n",
      "Loss for  9988 th iteration => 0.0362443710472\n",
      "Loss for  9989 th iteration => 0.036241199078\n",
      "Loss for  9990 th iteration => 0.0362391049516\n",
      "Loss for  9991 th iteration => 0.0362333425882\n",
      "Loss for  9992 th iteration => 0.0362275591275\n",
      "Loss for  9993 th iteration => 0.0362272047777\n",
      "Loss for  9994 th iteration => 0.0362201189385\n",
      "Loss for  9995 th iteration => 0.0362165132372\n",
      "Loss for  9996 th iteration => 0.0362136513869\n",
      "Loss for  9997 th iteration => 0.0362060214985\n",
      "Loss for  9998 th iteration => 0.0362058525934\n",
      "Loss for  9999 th iteration => 0.0361989564399\n",
      "Loss for  10000 th iteration => 0.0361948628626\n",
      "Loss for  10001 th iteration => 0.0361912091347\n",
      "Loss for  10002 th iteration => 0.0361851255298\n",
      "Loss for  10003 th iteration => 0.036181455784\n",
      "Loss for  10004 th iteration => 0.0361784049713\n",
      "Loss for  10005 th iteration => 0.0361750065674\n",
      "Loss for  10006 th iteration => 0.0361678100299\n",
      "Loss for  10007 th iteration => 0.0361673412862\n",
      "Loss for  10008 th iteration => 0.0361603561473\n",
      "Loss for  10009 th iteration => 0.0361567254047\n",
      "Loss for  10010 th iteration => 0.0361539059366\n",
      "Loss for  10011 th iteration => 0.036146349188\n",
      "Loss for  10012 th iteration => 0.0361460531112\n",
      "Loss for  10013 th iteration => 0.0361392488136\n",
      "Loss for  10014 th iteration => 0.0361351854741\n",
      "Loss for  10015 th iteration => 0.0361315220563\n",
      "Loss for  10016 th iteration => 0.03612541672\n",
      "Loss for  10017 th iteration => 0.0361233547213\n",
      "Loss for  10018 th iteration => 0.0361180265801\n",
      "Loss for  10019 th iteration => 0.0361151806369\n",
      "Loss for  10020 th iteration => 0.0361092900936\n",
      "Loss for  10021 th iteration => 0.0361059084305\n",
      "Loss for  10022 th iteration => 0.0361022274292\n",
      "Loss for  10023 th iteration => 0.0360965715823\n",
      "Loss for  10024 th iteration => 0.0360939404719\n",
      "Loss for  10025 th iteration => 0.0360880668538\n",
      "Loss for  10026 th iteration => 0.0360847505467\n",
      "Loss for  10027 th iteration => 0.0360809955546\n",
      "Loss for  10028 th iteration => 0.0360752590472\n",
      "Loss for  10029 th iteration => 0.0360714611325\n",
      "Loss for  10030 th iteration => 0.0360673473657\n",
      "Loss for  10031 th iteration => 0.0360638615873\n",
      "Loss for  10032 th iteration => 0.0360587548173\n",
      "Loss for  10033 th iteration => 0.0360554625081\n",
      "Loss for  10034 th iteration => 0.0360502951458\n",
      "Loss for  10035 th iteration => 0.0360462416609\n",
      "Loss for  10036 th iteration => 0.0360426616605\n",
      "Loss for  10037 th iteration => 0.0360374269739\n",
      "Loss for  10038 th iteration => 0.0360330225423\n",
      "Loss for  10039 th iteration => 0.0360296292889\n",
      "Loss for  10040 th iteration => 0.036025372829\n",
      "Loss for  10041 th iteration => 0.0360205778568\n",
      "Loss for  10042 th iteration => 0.0360176518718\n",
      "Loss for  10043 th iteration => 0.0360118843238\n",
      "Loss for  10044 th iteration => 0.0360082928744\n",
      "Loss for  10045 th iteration => 0.0360044756161\n",
      "Loss for  10046 th iteration => 0.035999280507\n",
      "Loss for  10047 th iteration => 0.035996508041\n",
      "Loss for  10048 th iteration => 0.0359907680539\n",
      "Loss for  10049 th iteration => 0.0359870983095\n",
      "Loss for  10050 th iteration => 0.0359820096738\n",
      "Loss for  10051 th iteration => 0.0359787557331\n",
      "Loss for  10052 th iteration => 0.0359743512285\n",
      "Loss for  10053 th iteration => 0.0359690788749\n",
      "Loss for  10054 th iteration => 0.0359679350562\n",
      "Loss for  10055 th iteration => 0.035960187917\n",
      "Loss for  10056 th iteration => 0.0359588256671\n",
      "Loss for  10057 th iteration => 0.0359534470369\n",
      "Loss for  10058 th iteration => 0.0359476906109\n",
      "Loss for  10059 th iteration => 0.0359470308098\n",
      "Loss for  10060 th iteration => 0.0359391559965\n",
      "Loss for  10061 th iteration => 0.0359375942764\n",
      "Loss for  10062 th iteration => 0.0359325390488\n",
      "Loss for  10063 th iteration => 0.0359264050513\n",
      "Loss for  10064 th iteration => 0.0359245504549\n",
      "Loss for  10065 th iteration => 0.035918982799\n",
      "Loss for  10066 th iteration => 0.0359163798168\n",
      "Loss for  10067 th iteration => 0.0359107092422\n",
      "Loss for  10068 th iteration => 0.0359068402357\n",
      "Loss for  10069 th iteration => 0.0359020373087\n",
      "Loss for  10070 th iteration => 0.0358985757626\n",
      "Loss for  10071 th iteration => 0.0358955325387\n",
      "Loss for  10072 th iteration => 0.0358884553688\n",
      "Loss for  10073 th iteration => 0.0358876791905\n",
      "Loss for  10074 th iteration => 0.0358812601913\n",
      "Loss for  10075 th iteration => 0.0358774145984\n",
      "Loss for  10076 th iteration => 0.0358745294461\n",
      "Loss for  10077 th iteration => 0.0358673303567\n",
      "Loss for  10078 th iteration => 0.0358651309003\n",
      "Loss for  10079 th iteration => 0.0358613569215\n",
      "Loss for  10080 th iteration => 0.0358561983697\n",
      "Loss for  10081 th iteration => 0.0358526403791\n",
      "Loss for  10082 th iteration => 0.0358480099508\n",
      "Loss for  10083 th iteration => 0.0358425721125\n",
      "Loss for  10084 th iteration => 0.0358410503938\n",
      "Loss for  10085 th iteration => 0.0358355226715\n",
      "Loss for  10086 th iteration => 0.0358303276339\n",
      "Loss for  10087 th iteration => 0.0358291483191\n",
      "Loss for  10088 th iteration => 0.0358216714721\n",
      "Loss for  10089 th iteration => 0.0358199455157\n",
      "Loss for  10090 th iteration => 0.0358147506881\n",
      "Loss for  10091 th iteration => 0.0358091264333\n",
      "Loss for  10092 th iteration => 0.0358083769271\n",
      "Loss for  10093 th iteration => 0.0358007864834\n",
      "Loss for  10094 th iteration => 0.0357988859683\n",
      "Loss for  10095 th iteration => 0.035793977398\n",
      "Loss for  10096 th iteration => 0.0357880495191\n",
      "Loss for  10097 th iteration => 0.0357843938858\n",
      "Loss for  10098 th iteration => 0.0357814856451\n",
      "Loss for  10099 th iteration => 0.0357780635674\n",
      "Loss for  10100 th iteration => 0.0357711950569\n",
      "Loss for  10101 th iteration => 0.0357704961058\n",
      "Loss for  10102 th iteration => 0.0357636740227\n",
      "Loss for  10103 th iteration => 0.0357603612776\n",
      "Loss for  10104 th iteration => 0.0357560899092\n",
      "Loss for  10105 th iteration => 0.0357508449208\n",
      "Loss for  10106 th iteration => 0.0357497382112\n",
      "Loss for  10107 th iteration => 0.0357422447313\n",
      "Loss for  10108 th iteration => 0.0357405605038\n",
      "Loss for  10109 th iteration => 0.0357353893268\n",
      "Loss for  10110 th iteration => 0.0357297567456\n",
      "Loss for  10111 th iteration => 0.035729038719\n",
      "Loss for  10112 th iteration => 0.0357214436622\n",
      "Loss for  10113 th iteration => 0.0357196011184\n",
      "Loss for  10114 th iteration => 0.0357146893464\n",
      "Loss for  10115 th iteration => 0.0357087794169\n",
      "Loss for  10116 th iteration => 0.0357051396994\n",
      "Loss for  10117 th iteration => 0.0357022605747\n",
      "Loss for  10118 th iteration => 0.0356988330034\n",
      "Loss for  10119 th iteration => 0.0356920377208\n",
      "Loss for  10120 th iteration => 0.0356912774976\n",
      "Loss for  10121 th iteration => 0.035684495351\n",
      "Loss for  10122 th iteration => 0.0356812500551\n",
      "Loss for  10123 th iteration => 0.0356769393026\n",
      "Loss for  10124 th iteration => 0.0356717723568\n",
      "Loss for  10125 th iteration => 0.035670611789\n",
      "Loss for  10126 th iteration => 0.035663139132\n",
      "Loss for  10127 th iteration => 0.0356615456203\n",
      "Loss for  10128 th iteration => 0.0356563159261\n",
      "Loss for  10129 th iteration => 0.0356507869265\n",
      "Loss for  10130 th iteration => 0.0356499897997\n",
      "Loss for  10131 th iteration => 0.0356424231716\n",
      "Loss for  10132 th iteration => 0.035640680819\n",
      "Loss for  10133 th iteration => 0.0356356942924\n",
      "Loss for  10134 th iteration => 0.035629883441\n",
      "Loss for  10135 th iteration => 0.0356278170477\n",
      "Loss for  10136 th iteration => 0.0356225866493\n",
      "Loss for  10137 th iteration => 0.0356197900109\n",
      "Loss for  10138 th iteration => 0.035614213752\n",
      "Loss for  10139 th iteration => 0.0356106199139\n",
      "Loss for  10140 th iteration => 0.035605616948\n",
      "Loss for  10141 th iteration => 0.0356024897237\n",
      "Loss for  10142 th iteration => 0.0355980902257\n",
      "Loss for  10143 th iteration => 0.0355930756609\n",
      "Loss for  10144 th iteration => 0.0355917877329\n",
      "Loss for  10145 th iteration => 0.0355843953147\n",
      "Loss for  10146 th iteration => 0.0355828457852\n",
      "Loss for  10147 th iteration => 0.0355775471365\n",
      "Loss for  10148 th iteration => 0.0355721869083\n",
      "Loss for  10149 th iteration => 0.0355712462125\n",
      "Loss for  10150 th iteration => 0.0355637646146\n",
      "Loss for  10151 th iteration => 0.035562072479\n",
      "Loss for  10152 th iteration => 0.0355570064941\n",
      "Loss for  10153 th iteration => 0.0355513545497\n",
      "Loss for  10154 th iteration => 0.0355491604725\n",
      "Loss for  10155 th iteration => 0.0355440697235\n",
      "Loss for  10156 th iteration => 0.0355412123081\n",
      "Loss for  10157 th iteration => 0.0355356727463\n",
      "Loss for  10158 th iteration => 0.0355321220817\n",
      "Loss for  10159 th iteration => 0.0355270476458\n",
      "Loss for  10160 th iteration => 0.0355241027191\n",
      "Loss for  10161 th iteration => 0.0355207922786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  10162 th iteration => 0.0355140180248\n",
      "Loss for  10163 th iteration => 0.0355132792685\n",
      "Loss for  10164 th iteration => 0.0355065662882\n",
      "Loss for  10165 th iteration => 0.0355033559116\n",
      "Loss for  10166 th iteration => 0.0354990712336\n",
      "Loss for  10167 th iteration => 0.0354939690844\n",
      "Loss for  10168 th iteration => 0.0354927965444\n",
      "Loss for  10169 th iteration => 0.0354853904462\n",
      "Loss for  10170 th iteration => 0.0354838536796\n",
      "Loss for  10171 th iteration => 0.0354786156225\n",
      "Loss for  10172 th iteration => 0.0354732217678\n",
      "Loss for  10173 th iteration => 0.0354723431534\n",
      "Loss for  10174 th iteration => 0.0354648633033\n",
      "Loss for  10175 th iteration => 0.0354632053152\n",
      "Loss for  10176 th iteration => 0.0354581649376\n",
      "Loss for  10177 th iteration => 0.0354525334307\n",
      "Loss for  10178 th iteration => 0.0354503540273\n",
      "Loss for  10179 th iteration => 0.0354452872994\n",
      "Loss for  10180 th iteration => 0.0354424469217\n",
      "Loss for  10181 th iteration => 0.0354369518344\n",
      "Loss for  10182 th iteration => 0.0354334023862\n",
      "Loss for  10183 th iteration => 0.0354283401584\n",
      "Loss for  10184 th iteration => 0.0354254514601\n",
      "Loss for  10185 th iteration => 0.0354221143402\n",
      "Loss for  10186 th iteration => 0.0354154306596\n",
      "Loss for  10187 th iteration => 0.0354146355633\n",
      "Loss for  10188 th iteration => 0.0354079534543\n",
      "Loss for  10189 th iteration => 0.0354048433748\n",
      "Loss for  10190 th iteration => 0.0354004935152\n",
      "Loss for  10191 th iteration => 0.0353955037119\n",
      "Loss for  10192 th iteration => 0.0353942490246\n",
      "Loss for  10193 th iteration => 0.0353868870166\n",
      "Loss for  10194 th iteration => 0.0353854410356\n",
      "Loss for  10195 th iteration => 0.0353801344459\n",
      "Loss for  10196 th iteration => 0.0353748818841\n",
      "Loss for  10197 th iteration => 0.0353738925102\n",
      "Loss for  10198 th iteration => 0.0353664653165\n",
      "Loss for  10199 th iteration => 0.0353649089627\n",
      "Loss for  10200 th iteration => 0.035359781584\n",
      "Loss for  10201 th iteration => 0.0353543070022\n",
      "Loss for  10202 th iteration => 0.0353535427174\n",
      "Loss for  10203 th iteration => 0.0353460615704\n",
      "Loss for  10204 th iteration => 0.0353444118911\n",
      "Loss for  10205 th iteration => 0.0353394368659\n",
      "Loss for  10206 th iteration => 0.0353338292731\n",
      "Loss for  10207 th iteration => 0.0353300509884\n",
      "Loss for  10208 th iteration => 0.0353274699832\n",
      "Loss for  10209 th iteration => 0.0353238577085\n",
      "Loss for  10210 th iteration => 0.0353174397895\n",
      "Loss for  10211 th iteration => 0.0353164816645\n",
      "Loss for  10212 th iteration => 0.0353097704071\n",
      "Loss for  10213 th iteration => 0.035306953157\n",
      "Loss for  10214 th iteration => 0.0353023491953\n",
      "Loss for  10215 th iteration => 0.0352976809337\n",
      "Loss for  10216 th iteration => 0.0352961374196\n",
      "Loss for  10217 th iteration => 0.0352888784259\n",
      "Loss for  10218 th iteration => 0.0352876079725\n",
      "Loss for  10219 th iteration => 0.0352820961659\n",
      "Loss for  10220 th iteration => 0.0352771680965\n",
      "Loss for  10221 th iteration => 0.0352758869483\n",
      "Loss for  10222 th iteration => 0.0352685642131\n",
      "Loss for  10223 th iteration => 0.0352671832562\n",
      "Loss for  10224 th iteration => 0.0352618493766\n",
      "Loss for  10225 th iteration => 0.0352567017283\n",
      "Loss for  10226 th iteration => 0.0352556432236\n",
      "Loss for  10227 th iteration => 0.0352482678322\n",
      "Loss for  10228 th iteration => 0.0352467932185\n",
      "Loss for  10229 th iteration => 0.0352416107358\n",
      "Loss for  10230 th iteration => 0.0352362787896\n",
      "Loss for  10231 th iteration => 0.0352354080935\n",
      "Loss for  10232 th iteration => 0.0352279897816\n",
      "Loss for  10233 th iteration => 0.0352264362613\n",
      "Loss for  10234 th iteration => 0.0352213819197\n",
      "Loss for  10235 th iteration => 0.0352159459863\n",
      "Loss for  10236 th iteration => 0.0352120493894\n",
      "Loss for  10237 th iteration => 0.0352096378616\n",
      "Loss for  10238 th iteration => 0.0352058926027\n",
      "Loss for  10239 th iteration => 0.0351996447479\n",
      "Loss for  10240 th iteration => 0.0351985944205\n",
      "Loss for  10241 th iteration => 0.0351918864884\n",
      "Loss for  10242 th iteration => 0.0351892691306\n",
      "Loss for  10243 th iteration => 0.0351845086647\n",
      "Loss for  10244 th iteration => 0.0351800589473\n",
      "Loss for  10245 th iteration => 0.035178333956\n",
      "Loss for  10246 th iteration => 0.0351711604564\n",
      "Loss for  10247 th iteration => 0.0351700142107\n",
      "Loss for  10248 th iteration => 0.0351643747313\n",
      "Loss for  10249 th iteration => 0.0351596887788\n",
      "Loss for  10250 th iteration => 0.0351582028393\n",
      "Loss for  10251 th iteration => 0.0351509727343\n",
      "Loss for  10252 th iteration => 0.0351497244605\n",
      "Loss for  10253 th iteration => 0.035144248036\n",
      "Loss for  10254 th iteration => 0.0351393630247\n",
      "Loss for  10255 th iteration => 0.0351380794287\n",
      "Loss for  10256 th iteration => 0.0351308029623\n",
      "Loss for  10257 th iteration => 0.0351294682296\n",
      "Loss for  10258 th iteration => 0.0351241303399\n",
      "Loss for  10259 th iteration => 0.0351190788821\n",
      "Loss for  10260 th iteration => 0.0351179654289\n",
      "Loss for  10261 th iteration => 0.0351106516003\n",
      "Loss for  10262 th iteration => 0.0351092440441\n",
      "Loss for  10263 th iteration => 0.0351040231893\n",
      "Loss for  10264 th iteration => 0.0350988460045\n",
      "Loss for  10265 th iteration => 0.0350963451702\n",
      "Loss for  10266 th iteration => 0.035091599614\n",
      "Loss for  10267 th iteration => 0.0350887384476\n",
      "Loss for  10268 th iteration => 0.0350833494537\n",
      "Loss for  10269 th iteration => 0.0350799452987\n",
      "Loss for  10270 th iteration => 0.0350747041859\n",
      "Loss for  10271 th iteration => 0.0350723603394\n",
      "Loss for  10272 th iteration => 0.035067371253\n",
      "Loss for  10273 th iteration => 0.0350632383772\n",
      "Loss for  10274 th iteration => 0.0350612345895\n",
      "Loss for  10275 th iteration => 0.0350542107994\n",
      "Loss for  10276 th iteration => 0.0350531552273\n",
      "Loss for  10277 th iteration => 0.0350473601191\n",
      "Loss for  10278 th iteration => 0.0350430022988\n",
      "Loss for  10279 th iteration => 0.0350412263744\n",
      "Loss for  10280 th iteration => 0.0350341494509\n",
      "Loss for  10281 th iteration => 0.0350329957377\n",
      "Loss for  10282 th iteration => 0.0350273566327\n",
      "Loss for  10283 th iteration => 0.0350228094992\n",
      "Loss for  10284 th iteration => 0.0350212262545\n",
      "Loss for  10285 th iteration => 0.0350141060134\n",
      "Loss for  10286 th iteration => 0.0350128690857\n",
      "Loss for  10287 th iteration => 0.0350073624782\n",
      "Loss for  10288 th iteration => 0.0350026572979\n",
      "Loss for  10289 th iteration => 0.0350012358602\n",
      "Loss for  10290 th iteration => 0.0349940809264\n",
      "Loss for  10291 th iteration => 0.0349927738615\n",
      "Loss for  10292 th iteration => 0.0349873791347\n",
      "Loss for  10293 th iteration => 0.0349825433418\n",
      "Loss for  10294 th iteration => 0.0349812566232\n",
      "Loss for  10295 th iteration => 0.0349740745751\n",
      "Loss for  10296 th iteration => 0.0349727088266\n",
      "Loss for  10297 th iteration => 0.0349674079002\n",
      "Loss for  10298 th iteration => 0.0349624904191\n",
      "Loss for  10299 th iteration => 0.0349581939055\n",
      "Loss for  10300 th iteration => 0.0349563029306\n",
      "Loss for  10301 th iteration => 0.0349521173222\n",
      "Loss for  10302 th iteration => 0.0349462958006\n",
      "Loss for  10303 th iteration => 0.0349450766374\n",
      "Loss for  10304 th iteration => 0.0349382906688\n",
      "Loss for  10305 th iteration => 0.0349362061791\n",
      "Loss for  10306 th iteration => 0.0349310082799\n",
      "Loss for  10307 th iteration => 0.0349271574581\n",
      "Loss for  10308 th iteration => 0.0349249147013\n",
      "Loss for  10309 th iteration => 0.0349180084355\n",
      "Loss for  10310 th iteration => 0.0349170869901\n",
      "Loss for  10311 th iteration => 0.0349111357927\n",
      "Loss for  10312 th iteration => 0.0349070829355\n",
      "Loss for  10313 th iteration => 0.0349050453754\n",
      "Loss for  10314 th iteration => 0.0348980929381\n",
      "Loss for  10315 th iteration => 0.0348970815125\n",
      "Loss for  10316 th iteration => 0.0348912718934\n",
      "Loss for  10317 th iteration => 0.0348870496593\n",
      "Loss for  10318 th iteration => 0.0348851850471\n",
      "Loss for  10319 th iteration => 0.034878195434\n",
      "Loss for  10320 th iteration => 0.034877107712\n",
      "Loss for  10321 th iteration => 0.0348714181225\n",
      "Loss for  10322 th iteration => 0.0348670551782\n",
      "Loss for  10323 th iteration => 0.0348653352077\n",
      "Loss for  10324 th iteration => 0.0348583163252\n",
      "Loss for  10325 th iteration => 0.0348571642988\n",
      "Loss for  10326 th iteration => 0.0348515758326\n",
      "Loss for  10327 th iteration => 0.034847097339\n",
      "Loss for  10328 th iteration => 0.0348454971659\n",
      "Loss for  10329 th iteration => 0.0348384559631\n",
      "Loss for  10330 th iteration => 0.0348372501388\n",
      "Loss for  10331 th iteration => 0.0348317462105\n",
      "Loss for  10332 th iteration => 0.0348271742496\n",
      "Loss for  10333 th iteration => 0.0348256720696\n",
      "Loss for  10334 th iteration => 0.0348186146545\n",
      "Loss for  10335 th iteration => 0.0348173642344\n",
      "Loss for  10336 th iteration => 0.0348119302964\n",
      "Loss for  10337 th iteration => 0.0348073209782\n",
      "Loss for  10338 th iteration => 0.0348027876481\n",
      "Loss for  10339 th iteration => 0.0348011811556\n",
      "Loss for  10340 th iteration => 0.0347967595667\n",
      "Loss for  10341 th iteration => 0.0347911832865\n",
      "Loss for  10342 th iteration => 0.0347898582334\n",
      "Loss for  10343 th iteration => 0.0347830412902\n",
      "Loss for  10344 th iteration => 0.0347812673217\n",
      "Loss for  10345 th iteration => 0.0347758165822\n",
      "Loss for  10346 th iteration => 0.0347723024795\n",
      "Loss for  10347 th iteration => 0.0347697721181\n",
      "Loss for  10348 th iteration => 0.0347630122987\n",
      "Loss for  10349 th iteration => 0.0347622328636\n",
      "Loss for  10350 th iteration => 0.0347561022612\n",
      "Loss for  10351 th iteration => 0.0347524097078\n",
      "Loss for  10352 th iteration => 0.0347500612021\n",
      "Loss for  10353 th iteration => 0.0347432623759\n",
      "Loss for  10354 th iteration => 0.0347424013519\n",
      "Loss for  10355 th iteration => 0.034736397476\n",
      "Loss for  10356 th iteration => 0.0347325560673\n",
      "Loss for  10357 th iteration => 0.0347303601924\n",
      "Loss for  10358 th iteration => 0.0347235305097\n",
      "Loss for  10359 th iteration => 0.0347226003001\n",
      "Loss for  10360 th iteration => 0.0347167036187\n",
      "Loss for  10361 th iteration => 0.0347127393421\n",
      "Loss for  10362 th iteration => 0.0347106704364\n",
      "Loss for  10363 th iteration => 0.0347038170627\n",
      "Loss for  10364 th iteration => 0.0347028285416\n",
      "Loss for  10365 th iteration => 0.0346970219113\n",
      "Loss for  10366 th iteration => 0.0346929575843\n",
      "Loss for  10367 th iteration => 0.0346909931164\n",
      "Loss for  10368 th iteration => 0.0346841223517\n",
      "Loss for  10369 th iteration => 0.0346830850497\n",
      "Loss for  10370 th iteration => 0.0346773534261\n",
      "Loss for  10371 th iteration => 0.0346732090811\n",
      "Loss for  10372 th iteration => 0.0346713292695\n",
      "Loss for  10373 th iteration => 0.0346644466528\n",
      "Loss for  10374 th iteration => 0.0346633689203\n",
      "Loss for  10375 th iteration => 0.0346576991025\n",
      "Loss for  10376 th iteration => 0.0346534986409\n",
      "Loss for  10377 th iteration => 0.0346502814246\n",
      "Loss for  10378 th iteration => 0.0346449876605\n",
      "Loss for  10379 th iteration => 0.0346433760703\n",
      "Loss for  10380 th iteration => 0.0346382862277\n",
      "Loss for  10381 th iteration => 0.034633707593\n",
      "Loss for  10382 th iteration => 0.0346292164666\n",
      "Loss for  10383 th iteration => 0.0346263631684\n",
      "Loss for  10384 th iteration => 0.0346233407394\n",
      "Loss for  10385 th iteration => 0.0346180974884\n",
      "Loss for  10386 th iteration => 0.0346152033625\n",
      "Loss for  10387 th iteration => 0.0346098393398\n",
      "Loss for  10388 th iteration => 0.0346074662781\n",
      "Loss for  10389 th iteration => 0.0346038570885\n",
      "Loss for  10390 th iteration => 0.0345975415221\n",
      "Loss for  10391 th iteration => 0.0345970851245\n",
      "Loss for  10392 th iteration => 0.0345902326831\n",
      "Loss for  10393 th iteration => 0.034587872918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  10394 th iteration => 0.0345831580022\n",
      "Loss for  10395 th iteration => 0.0345780887454\n",
      "Loss for  10396 th iteration => 0.0345772587181\n",
      "Loss for  10397 th iteration => 0.0345708257369\n",
      "Loss for  10398 th iteration => 0.0345681813005\n",
      "Loss for  10399 th iteration => 0.0345636543676\n",
      "Loss for  10400 th iteration => 0.0345592144345\n",
      "Loss for  10401 th iteration => 0.0345576588442\n",
      "Loss for  10402 th iteration => 0.0345506760174\n",
      "Loss for  10403 th iteration => 0.0345495902096\n",
      "Loss for  10404 th iteration => 0.0345440889439\n",
      "Loss for  10405 th iteration => 0.0345396698138\n",
      "Loss for  10406 th iteration => 0.0345380993175\n",
      "Loss for  10407 th iteration => 0.0345311332422\n",
      "Loss for  10408 th iteration => 0.0345300475781\n",
      "Loss for  10409 th iteration => 0.0345245423836\n",
      "Loss for  10410 th iteration => 0.0345201551768\n",
      "Loss for  10411 th iteration => 0.0345171207884\n",
      "Loss for  10412 th iteration => 0.0345118079191\n",
      "Loss for  10413 th iteration => 0.0345102259572\n",
      "Loss for  10414 th iteration => 0.0345052409491\n",
      "Loss for  10415 th iteration => 0.034500529909\n",
      "Loss for  10416 th iteration => 0.0344962171141\n",
      "Loss for  10417 th iteration => 0.0344943729183\n",
      "Loss for  10418 th iteration => 0.0344902740532\n",
      "Loss for  10419 th iteration => 0.034484541537\n",
      "Loss for  10420 th iteration => 0.0344835109028\n",
      "Loss for  10421 th iteration => 0.0344767399653\n",
      "Loss for  10422 th iteration => 0.0344748807823\n",
      "Loss for  10423 th iteration => 0.0344696648689\n",
      "Loss for  10424 th iteration => 0.0344652004355\n",
      "Loss for  10425 th iteration => 0.0344638129136\n",
      "Loss for  10426 th iteration => 0.0344574600266\n",
      "Loss for  10427 th iteration => 0.0344553021088\n",
      "Loss for  10428 th iteration => 0.0344503349827\n",
      "Loss for  10429 th iteration => 0.0344463950809\n",
      "Loss for  10430 th iteration => 0.0344443782778\n",
      "Loss for  10431 th iteration => 0.0344374013893\n",
      "Loss for  10432 th iteration => 0.0344358428888\n",
      "Loss for  10433 th iteration => 0.0344310079215\n",
      "Loss for  10434 th iteration => 0.0344275929346\n",
      "Loss for  10435 th iteration => 0.0344236156828\n",
      "Loss for  10436 th iteration => 0.0344181807025\n",
      "Loss for  10437 th iteration => 0.0344171451271\n",
      "Loss for  10438 th iteration => 0.0344116889471\n",
      "Loss for  10439 th iteration => 0.0344072588048\n",
      "Loss for  10440 th iteration => 0.0344057369122\n",
      "Loss for  10441 th iteration => 0.0343987734062\n",
      "Loss for  10442 th iteration => 0.0343977629856\n",
      "Loss for  10443 th iteration => 0.0343922617017\n",
      "Loss for  10444 th iteration => 0.0343879133917\n",
      "Loss for  10445 th iteration => 0.0343863163014\n",
      "Loss for  10446 th iteration => 0.0343793871942\n",
      "Loss for  10447 th iteration => 0.0343784007546\n",
      "Loss for  10448 th iteration => 0.0343728560772\n",
      "Loss for  10449 th iteration => 0.0343685873813\n",
      "Loss for  10450 th iteration => 0.0343669172733\n",
      "Loss for  10451 th iteration => 0.0343600219998\n",
      "Loss for  10452 th iteration => 0.0343590584926\n",
      "Loss for  10453 th iteration => 0.0343534719378\n",
      "Loss for  10454 th iteration => 0.0343492808944\n",
      "Loss for  10455 th iteration => 0.0343475396958\n",
      "Loss for  10456 th iteration => 0.0343406777602\n",
      "Loss for  10457 th iteration => 0.0343397362471\n",
      "Loss for  10458 th iteration => 0.0343341091606\n",
      "Loss for  10459 th iteration => 0.0343299940332\n",
      "Loss for  10460 th iteration => 0.0343281834489\n",
      "Loss for  10461 th iteration => 0.0343213544167\n",
      "Loss for  10462 th iteration => 0.0343204340559\n",
      "Loss for  10463 th iteration => 0.0343147676337\n",
      "Loss for  10464 th iteration => 0.0343107268838\n",
      "Loss for  10465 th iteration => 0.0343088484234\n",
      "Loss for  10466 th iteration => 0.0343020519136\n",
      "Loss for  10467 th iteration => 0.0343011519486\n",
      "Loss for  10468 th iteration => 0.0342954472549\n",
      "Loss for  10469 th iteration => 0.0342914795181\n",
      "Loss for  10470 th iteration => 0.0342895345195\n",
      "Loss for  10471 th iteration => 0.0342827701982\n",
      "Loss for  10472 th iteration => 0.0342818899476\n",
      "Loss for  10473 th iteration => 0.0342761479307\n",
      "Loss for  10474 th iteration => 0.0342722519954\n",
      "Loss for  10475 th iteration => 0.0342702416456\n",
      "Loss for  10476 th iteration => 0.0342635092203\n",
      "Loss for  10477 th iteration => 0.0342626480688\n",
      "Loss for  10478 th iteration => 0.0342568695751\n",
      "Loss for  10479 th iteration => 0.0342530443639\n",
      "Loss for  10480 th iteration => 0.0342509697177\n",
      "Loss for  10481 th iteration => 0.0342442689317\n",
      "Loss for  10482 th iteration => 0.0342434263222\n",
      "Loss for  10483 th iteration => 0.0342376235083\n",
      "Loss for  10484 th iteration => 0.034233135643\n",
      "Loss for  10485 th iteration => 0.0342317094657\n",
      "Loss for  10486 th iteration => 0.0342255795714\n",
      "Loss for  10487 th iteration => 0.0342233625806\n",
      "Loss for  10488 th iteration => 0.0342185288025\n",
      "Loss for  10489 th iteration => 0.0342144756403\n",
      "Loss for  10490 th iteration => 0.0342126369007\n",
      "Loss for  10491 th iteration => 0.0342056621445\n",
      "Loss for  10492 th iteration => 0.0342033049779\n",
      "Loss for  10493 th iteration => 0.034199381018\n",
      "Loss for  10494 th iteration => 0.0341958695236\n",
      "Loss for  10495 th iteration => 0.0341923594608\n",
      "Loss for  10496 th iteration => 0.0341865662771\n",
      "Loss for  10497 th iteration => 0.0341847854608\n",
      "Loss for  10498 th iteration => 0.0341810448062\n",
      "Loss for  10499 th iteration => 0.0341758042709\n",
      "Loss for  10500 th iteration => 0.0341724436784\n",
      "Loss for  10501 th iteration => 0.0341695609386\n",
      "Loss for  10502 th iteration => 0.0341652861257\n",
      "Loss for  10503 th iteration => 0.0341613880464\n",
      "Loss for  10504 th iteration => 0.0341577366213\n",
      "Loss for  10505 th iteration => 0.0341533027654\n",
      "Loss for  10506 th iteration => 0.0341504284011\n",
      "Loss for  10507 th iteration => 0.034146143924\n",
      "Loss for  10508 th iteration => 0.0341422693748\n",
      "Loss for  10509 th iteration => 0.0341386141548\n",
      "Loss for  10510 th iteration => 0.0341329346475\n",
      "Loss for  10511 th iteration => 0.0341306284069\n",
      "Loss for  10512 th iteration => 0.0341268757665\n",
      "Loss for  10513 th iteration => 0.0341239982789\n",
      "Loss for  10514 th iteration => 0.0341182506894\n",
      "Loss for  10515 th iteration => 0.0341138891471\n",
      "Loss for  10516 th iteration => 0.0341132628747\n",
      "Loss for  10517 th iteration => 0.0341075520421\n",
      "Loss for  10518 th iteration => 0.0341035234188\n",
      "Loss for  10519 th iteration => 0.0341002409308\n",
      "Loss for  10520 th iteration => 0.0340960367009\n",
      "Loss for  10521 th iteration => 0.0340937062405\n",
      "Loss for  10522 th iteration => 0.0340881197535\n",
      "Loss for  10523 th iteration => 0.0340854580547\n",
      "Loss for  10524 th iteration => 0.0340811863284\n",
      "Loss for  10525 th iteration => 0.0340759412493\n",
      "Loss for  10526 th iteration => 0.0340739545225\n",
      "Loss for  10527 th iteration => 0.0340696159853\n",
      "Loss for  10528 th iteration => 0.0340660308572\n",
      "Loss for  10529 th iteration => 0.0340610089422\n",
      "Loss for  10530 th iteration => 0.0340578934807\n",
      "Loss for  10531 th iteration => 0.0340555087756\n",
      "Loss for  10532 th iteration => 0.0340499657083\n",
      "Loss for  10533 th iteration => 0.0340473536078\n",
      "Loss for  10534 th iteration => 0.034041871053\n",
      "Loss for  10535 th iteration => 0.0340401714088\n",
      "Loss for  10536 th iteration => 0.0340360592041\n",
      "Loss for  10537 th iteration => 0.0340306056871\n",
      "Loss for  10538 th iteration => 0.0340294683081\n",
      "Loss for  10539 th iteration => 0.0340228590494\n",
      "Loss for  10540 th iteration => 0.0340200946219\n",
      "Loss for  10541 th iteration => 0.0340172250328\n",
      "Loss for  10542 th iteration => 0.0340121619897\n",
      "Loss for  10543 th iteration => 0.0340092564083\n",
      "Loss for  10544 th iteration => 0.0340039970527\n",
      "Loss for  10545 th iteration => 0.0340020422326\n",
      "Loss for  10546 th iteration => 0.0339981926949\n",
      "Loss for  10547 th iteration => 0.0339924689238\n",
      "Loss for  10548 th iteration => 0.0339916500452\n",
      "Loss for  10549 th iteration => 0.0339849619146\n",
      "Loss for  10550 th iteration => 0.0339831897351\n",
      "Loss for  10551 th iteration => 0.0339791652956\n",
      "Loss for  10552 th iteration => 0.0339736012907\n",
      "Loss for  10553 th iteration => 0.0339726531002\n",
      "Loss for  10554 th iteration => 0.0339659528947\n",
      "Loss for  10555 th iteration => 0.0339643482995\n",
      "Loss for  10556 th iteration => 0.0339601638096\n",
      "Loss for  10557 th iteration => 0.0339547485785\n",
      "Loss for  10558 th iteration => 0.0339536784029\n",
      "Loss for  10559 th iteration => 0.0339469692332\n",
      "Loss for  10560 th iteration => 0.0339455189684\n",
      "Loss for  10561 th iteration => 0.0339411875013\n",
      "Loss for  10562 th iteration => 0.0339359113293\n",
      "Loss for  10563 th iteration => 0.033934725693\n",
      "Loss for  10564 th iteration => 0.0339280531978\n",
      "Loss for  10565 th iteration => 0.0339256820386\n",
      "Loss for  10566 th iteration => 0.0339223798903\n",
      "Loss for  10567 th iteration => 0.0339175898302\n",
      "Loss for  10568 th iteration => 0.0339146963671\n",
      "Loss for  10569 th iteration => 0.0339092761946\n",
      "Loss for  10570 th iteration => 0.0339076179771\n",
      "Loss for  10571 th iteration => 0.0339035024899\n",
      "Loss for  10572 th iteration => 0.0338980248525\n",
      "Loss for  10573 th iteration => 0.0338964961372\n",
      "Loss for  10574 th iteration => 0.0338903982967\n",
      "Loss for  10575 th iteration => 0.0338883189082\n",
      "Loss for  10576 th iteration => 0.033883876515\n",
      "Loss for  10577 th iteration => 0.0338798407898\n",
      "Loss for  10578 th iteration => 0.0338768597936\n",
      "Loss for  10579 th iteration => 0.0338717664933\n",
      "Loss for  10580 th iteration => 0.0338697310512\n",
      "Loss for  10581 th iteration => 0.0338648864981\n",
      "Loss for  10582 th iteration => 0.0338604205893\n",
      "Loss for  10583 th iteration => 0.033859166468\n",
      "Loss for  10584 th iteration => 0.0338530199341\n",
      "Loss for  10585 th iteration => 0.0338509235848\n",
      "Loss for  10586 th iteration => 0.0338460961777\n",
      "Loss for  10587 th iteration => 0.0338423367742\n",
      "Loss for  10588 th iteration => 0.0338403132606\n",
      "Loss for  10589 th iteration => 0.0338335725605\n",
      "Loss for  10590 th iteration => 0.0338331517733\n",
      "Loss for  10591 th iteration => 0.0338272110956\n",
      "Loss for  10592 th iteration => 0.0338236766093\n",
      "Loss for  10593 th iteration => 0.0338214364532\n",
      "Loss for  10594 th iteration => 0.0338147825538\n",
      "Loss for  10595 th iteration => 0.0338135241463\n",
      "Loss for  10596 th iteration => 0.0338084449289\n",
      "Loss for  10597 th iteration => 0.0338055919571\n",
      "Loss for  10598 th iteration => 0.0338014143301\n",
      "Loss for  10599 th iteration => 0.0337961752605\n",
      "Loss for  10600 th iteration => 0.03379545629\n",
      "Loss for  10601 th iteration => 0.033789715456\n",
      "Loss for  10602 th iteration => 0.033786116104\n",
      "Loss for  10603 th iteration => 0.03378395054\n",
      "Loss for  10604 th iteration => 0.0337774460881\n",
      "Loss for  10605 th iteration => 0.0337757420414\n",
      "Loss for  10606 th iteration => 0.0337710830378\n",
      "Loss for  10607 th iteration => 0.0337673144341\n",
      "Loss for  10608 th iteration => 0.0337640047489\n",
      "Loss for  10609 th iteration => 0.033759320175\n",
      "Loss for  10610 th iteration => 0.0337570093481\n",
      "Loss for  10611 th iteration => 0.0337524233437\n",
      "Loss for  10612 th iteration => 0.0337484667971\n",
      "Loss for  10613 th iteration => 0.0337466642316\n",
      "Loss for  10614 th iteration => 0.0337399186148\n",
      "Loss for  10615 th iteration => 0.0337393791794\n",
      "Loss for  10616 th iteration => 0.0337336125907\n",
      "Loss for  10617 th iteration => 0.0337299408883\n",
      "Loss for  10618 th iteration => 0.0337278625121\n",
      "Loss for  10619 th iteration => 0.0337212106998\n",
      "Loss for  10620 th iteration => 0.0337207730734\n",
      "Loss for  10621 th iteration => 0.0337148323939\n",
      "Loss for  10622 th iteration => 0.0337114187218\n",
      "Loss for  10623 th iteration => 0.0337090909788\n",
      "Loss for  10624 th iteration => 0.0337025536279\n",
      "Loss for  10625 th iteration => 0.0337012580142\n",
      "Loss for  10626 th iteration => 0.0336961952668\n",
      "Loss for  10627 th iteration => 0.0336934418048\n",
      "Loss for  10628 th iteration => 0.0336892046729\n",
      "Loss for  10629 th iteration => 0.0336840524555\n",
      "Loss for  10630 th iteration => 0.033682336624\n",
      "Loss for  10631 th iteration => 0.0336777091965\n",
      "Loss for  10632 th iteration => 0.0336739523875\n",
      "Loss for  10633 th iteration => 0.0336706862517\n",
      "Loss for  10634 th iteration => 0.0336660310494\n",
      "Loss for  10635 th iteration => 0.0336636831964\n",
      "Loss for  10636 th iteration => 0.0336591644831\n",
      "Loss for  10637 th iteration => 0.0336552039089\n",
      "Loss for  10638 th iteration => 0.0336534315233\n",
      "Loss for  10639 th iteration => 0.0336467359736\n",
      "Loss for  10640 th iteration => 0.0336461551112\n",
      "Loss for  10641 th iteration => 0.0336404367967\n",
      "Loss for  10642 th iteration => 0.0336367975584\n",
      "Loss for  10643 th iteration => 0.0336347133164\n",
      "Loss for  10644 th iteration => 0.0336281217775\n",
      "Loss for  10645 th iteration => 0.0336276577008\n",
      "Loss for  10646 th iteration => 0.0336217413109\n",
      "Loss for  10647 th iteration => 0.0336183922345\n",
      "Loss for  10648 th iteration => 0.0336160268799\n",
      "Loss for  10649 th iteration => 0.0336095590062\n",
      "Loss for  10650 th iteration => 0.0336082479494\n",
      "Loss for  10651 th iteration => 0.0336032005921\n",
      "Loss for  10652 th iteration => 0.033599795826\n",
      "Loss for  10653 th iteration => 0.0335963257817\n",
      "Loss for  10654 th iteration => 0.0335916144127\n",
      "Loss for  10655 th iteration => 0.0335885412963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  10656 th iteration => 0.0335849237186\n",
      "Loss for  10657 th iteration => 0.0335816754218\n",
      "Loss for  10658 th iteration => 0.0335778428349\n",
      "Loss for  10659 th iteration => 0.0335725734573\n",
      "Loss for  10660 th iteration => 0.0335718615965\n",
      "Loss for  10661 th iteration => 0.0335663314806\n",
      "Loss for  10662 th iteration => 0.0335625209769\n",
      "Loss for  10663 th iteration => 0.0335606265178\n",
      "Loss for  10664 th iteration => 0.0335540304693\n",
      "Loss for  10665 th iteration => 0.0335534586447\n",
      "Loss for  10666 th iteration => 0.033547693709\n",
      "Loss for  10667 th iteration => 0.0335442223675\n",
      "Loss for  10668 th iteration => 0.0335419984199\n",
      "Loss for  10669 th iteration => 0.0335355114074\n",
      "Loss for  10670 th iteration => 0.0335350635495\n",
      "Loss for  10671 th iteration => 0.0335290888645\n",
      "Loss for  10672 th iteration => 0.033525923391\n",
      "Loss for  10673 th iteration => 0.03352340279\n",
      "Loss for  10674 th iteration => 0.0335170775941\n",
      "Loss for  10675 th iteration => 0.0335156920341\n",
      "Loss for  10676 th iteration => 0.0335107488063\n",
      "Loss for  10677 th iteration => 0.0335072841331\n",
      "Loss for  10678 th iteration => 0.0335039149\n",
      "Loss for  10679 th iteration => 0.0334991267413\n",
      "Loss for  10680 th iteration => 0.0334961666685\n",
      "Loss for  10681 th iteration => 0.0334924898211\n",
      "Loss for  10682 th iteration => 0.0334893781594\n",
      "Loss for  10683 th iteration => 0.0334854256921\n",
      "Loss for  10684 th iteration => 0.0334802472423\n",
      "Loss for  10685 th iteration => 0.033479574187\n",
      "Loss for  10686 th iteration => 0.0334739508369\n",
      "Loss for  10687 th iteration => 0.0334703696859\n",
      "Loss for  10688 th iteration => 0.0334682747272\n",
      "Loss for  10689 th iteration => 0.0334617998269\n",
      "Loss for  10690 th iteration => 0.0334612703119\n",
      "Loss for  10691 th iteration => 0.0334554062185\n",
      "Loss for  10692 th iteration => 0.0334521723019\n",
      "Loss for  10693 th iteration => 0.0334497398447\n",
      "Loss for  10694 th iteration => 0.0334434190036\n",
      "Loss for  10695 th iteration => 0.0334420025981\n",
      "Loss for  10696 th iteration => 0.033437113287\n",
      "Loss for  10697 th iteration => 0.0334336295814\n",
      "Loss for  10698 th iteration => 0.0334303143101\n",
      "Loss for  10699 th iteration => 0.0334255113178\n",
      "Loss for  10700 th iteration => 0.0334225940264\n",
      "Loss for  10701 th iteration => 0.0334188960909\n",
      "Loss for  10702 th iteration => 0.0334158543984\n",
      "Loss for  10703 th iteration => 0.0334118636609\n",
      "Loss for  10704 th iteration => 0.033406754096\n",
      "Loss for  10705 th iteration => 0.0334060454832\n",
      "Loss for  10706 th iteration => 0.0334004591147\n",
      "Loss for  10707 th iteration => 0.0333969079965\n",
      "Loss for  10708 th iteration => 0.0333948039054\n",
      "Loss for  10709 th iteration => 0.0333883801375\n",
      "Loss for  10710 th iteration => 0.0333878265401\n",
      "Loss for  10711 th iteration => 0.0333819811018\n",
      "Loss for  10712 th iteration => 0.0333788017533\n",
      "Loss for  10713 th iteration => 0.0333763359233\n",
      "Loss for  10714 th iteration => 0.0333700752748\n",
      "Loss for  10715 th iteration => 0.0333686182656\n",
      "Loss for  10716 th iteration => 0.0333637953434\n",
      "Loss for  10717 th iteration => 0.033360280066\n",
      "Loss for  10718 th iteration => 0.033357027083\n",
      "Loss for  10719 th iteration => 0.0333521957268\n",
      "Loss for  10720 th iteration => 0.0333503769375\n",
      "Loss for  10721 th iteration => 0.0333454429343\n",
      "Loss for  10722 th iteration => 0.033342150132\n",
      "Loss for  10723 th iteration => 0.0333398064418\n",
      "Loss for  10724 th iteration => 0.0333334372455\n",
      "Loss for  10725 th iteration => 0.0333319961038\n",
      "Loss for  10726 th iteration => 0.0333273375018\n",
      "Loss for  10727 th iteration => 0.0333244657636\n",
      "Loss for  10728 th iteration => 0.0333202672312\n",
      "Loss for  10729 th iteration => 0.0333152552894\n",
      "Loss for  10730 th iteration => 0.0333147090993\n",
      "Loss for  10731 th iteration => 0.0333088424235\n",
      "Loss for  10732 th iteration => 0.0333057644562\n",
      "Loss for  10733 th iteration => 0.0333032189357\n",
      "Loss for  10734 th iteration => 0.0332970291758\n",
      "Loss for  10735 th iteration => 0.0332955545633\n",
      "Loss for  10736 th iteration => 0.0332907727876\n",
      "Loss for  10737 th iteration => 0.0332872508975\n",
      "Loss for  10738 th iteration => 0.0332840368537\n",
      "Loss for  10739 th iteration => 0.0332791698881\n",
      "Loss for  10740 th iteration => 0.0332774416198\n",
      "Loss for  10741 th iteration => 0.0332724430895\n",
      "Loss for  10742 th iteration => 0.033269284269\n",
      "Loss for  10743 th iteration => 0.0332668286583\n",
      "Loss for  10744 th iteration => 0.0332605414927\n",
      "Loss for  10745 th iteration => 0.033259090168\n",
      "Loss for  10746 th iteration => 0.0332544441761\n",
      "Loss for  10747 th iteration => 0.0332516443315\n",
      "Loss for  10748 th iteration => 0.0332473734377\n",
      "Loss for  10749 th iteration => 0.0332424269901\n",
      "Loss for  10750 th iteration => 0.0332419148877\n",
      "Loss for  10751 th iteration => 0.0332359894882\n",
      "Loss for  10752 th iteration => 0.0332322210569\n",
      "Loss for  10753 th iteration => 0.0332305563207\n",
      "Loss for  10754 th iteration => 0.0332245943668\n",
      "Loss for  10755 th iteration => 0.0332219654744\n",
      "Loss for  10756 th iteration => 0.0332181154355\n",
      "Loss for  10757 th iteration => 0.0332152578689\n",
      "Loss for  10758 th iteration => 0.0332111746087\n",
      "Loss for  10759 th iteration => 0.033206024204\n",
      "Loss for  10760 th iteration => 0.0332046245165\n",
      "Loss for  10761 th iteration => 0.0332000147225\n",
      "Loss for  10762 th iteration => 0.0331971276627\n",
      "Loss for  10763 th iteration => 0.0331929728191\n",
      "Loss for  10764 th iteration => 0.03318802615\n",
      "Loss for  10765 th iteration => 0.0331874426047\n",
      "Loss for  10766 th iteration => 0.0331816653795\n",
      "Loss for  10767 th iteration => 0.0331785655273\n",
      "Loss for  10768 th iteration => 0.0331760769234\n",
      "Loss for  10769 th iteration => 0.0331698823396\n",
      "Loss for  10770 th iteration => 0.0331684602605\n",
      "Loss for  10771 th iteration => 0.0331636703631\n",
      "Loss for  10772 th iteration => 0.0331602160547\n",
      "Loss for  10773 th iteration => 0.0331569890188\n",
      "Loss for  10774 th iteration => 0.0331521746565\n",
      "Loss for  10775 th iteration => 0.0331504593054\n",
      "Loss for  10776 th iteration => 0.0331454900967\n",
      "Loss for  10777 th iteration => 0.0331424076391\n",
      "Loss for  10778 th iteration => 0.0331399118867\n",
      "Loss for  10779 th iteration => 0.0331336746281\n",
      "Loss for  10780 th iteration => 0.0331322363685\n",
      "Loss for  10781 th iteration => 0.0331276434702\n",
      "Loss for  10782 th iteration => 0.0331248789641\n",
      "Loss for  10783 th iteration => 0.0331205687089\n",
      "Loss for  10784 th iteration => 0.0331157041093\n",
      "Loss for  10785 th iteration => 0.0331152333138\n",
      "Loss for  10786 th iteration => 0.0331092661124\n",
      "Loss for  10787 th iteration => 0.0331056130547\n",
      "Loss for  10788 th iteration => 0.0331039192313\n",
      "Loss for  10789 th iteration => 0.0330979358137\n",
      "Loss for  10790 th iteration => 0.0330953986893\n",
      "Loss for  10791 th iteration => 0.0330915532551\n",
      "Loss for  10792 th iteration => 0.0330887937017\n",
      "Loss for  10793 th iteration => 0.033084609112\n",
      "Loss for  10794 th iteration => 0.0330795608498\n",
      "Loss for  10795 th iteration => 0.0330781598607\n",
      "Loss for  10796 th iteration => 0.0330736047107\n",
      "Loss for  10797 th iteration => 0.0330707790044\n",
      "Loss for  10798 th iteration => 0.0330665430035\n",
      "Loss for  10799 th iteration => 0.0330616862633\n",
      "Loss for  10800 th iteration => 0.0330611833475\n",
      "Loss for  10801 th iteration => 0.0330553024219\n",
      "Loss for  10802 th iteration => 0.0330524417679\n",
      "Loss for  10803 th iteration => 0.0330497520765\n",
      "Loss for  10804 th iteration => 0.0330436746433\n",
      "Loss for  10805 th iteration => 0.0330422775456\n",
      "Loss for  10806 th iteration => 0.0330375120724\n",
      "Loss for  10807 th iteration => 0.0330340925209\n",
      "Loss for  10808 th iteration => 0.0330308768592\n",
      "Loss for  10809 th iteration => 0.033026010475\n",
      "Loss for  10810 th iteration => 0.0330245299261\n",
      "Loss for  10811 th iteration => 0.0330204860228\n",
      "Loss for  10812 th iteration => 0.0330157216965\n",
      "Loss for  10813 th iteration => 0.0330140962962\n",
      "Loss for  10814 th iteration => 0.0330079744879\n",
      "Loss for  10815 th iteration => 0.0330055225385\n",
      "Loss for  10816 th iteration => 0.0330028551611\n",
      "Loss for  10817 th iteration => 0.0329981078355\n",
      "Loss for  10818 th iteration => 0.0329951439905\n",
      "Loss for  10819 th iteration => 0.0329914541141\n",
      "Loss for  10820 th iteration => 0.0329865789715\n",
      "Loss for  10821 th iteration => 0.0329853238251\n",
      "Loss for  10822 th iteration => 0.0329804444861\n",
      "Loss for  10823 th iteration => 0.0329764282751\n",
      "Loss for  10824 th iteration => 0.0329748997825\n",
      "Loss for  10825 th iteration => 0.0329686117198\n",
      "Loss for  10826 th iteration => 0.0329676119393\n",
      "Loss for  10827 th iteration => 0.0329623149689\n",
      "Loss for  10828 th iteration => 0.0329588820685\n",
      "Loss for  10829 th iteration => 0.032956782979\n",
      "Loss for  10830 th iteration => 0.0329506716909\n",
      "Loss for  10831 th iteration => 0.0329498973742\n",
      "Loss for  10832 th iteration => 0.0329442861569\n",
      "Loss for  10833 th iteration => 0.0329403787095\n",
      "Loss for  10834 th iteration => 0.0329389957206\n",
      "Loss for  10835 th iteration => 0.032932962339\n",
      "Loss for  10836 th iteration => 0.0329302945747\n",
      "Loss for  10837 th iteration => 0.0329266510762\n",
      "Loss for  10838 th iteration => 0.0329238739243\n",
      "Loss for  10839 th iteration => 0.0329196775777\n",
      "Loss for  10840 th iteration => 0.0329148374589\n",
      "Loss for  10841 th iteration => 0.032914361704\n",
      "Loss for  10842 th iteration => 0.0329085147317\n",
      "Loss for  10843 th iteration => 0.0329056943895\n",
      "Loss for  10844 th iteration => 0.0329030056144\n",
      "Loss for  10845 th iteration => 0.0328970082485\n",
      "Loss for  10846 th iteration => 0.0328955864627\n",
      "Loss for  10847 th iteration => 0.0328909176159\n",
      "Loss for  10848 th iteration => 0.0328874698253\n",
      "Loss for  10849 th iteration => 0.0328843229337\n",
      "Loss for  10850 th iteration => 0.0328794464982\n",
      "Loss for  10851 th iteration => 0.0328768854244\n",
      "Loss for  10852 th iteration => 0.0328742823698\n",
      "Loss for  10853 th iteration => 0.0328696202707\n",
      "Loss for  10854 th iteration => 0.0328665465897\n",
      "Loss for  10855 th iteration => 0.0328630985724\n",
      "Loss for  10856 th iteration => 0.0328592471636\n",
      "Loss for  10857 th iteration => 0.0328564650206\n",
      "Loss for  10858 th iteration => 0.0328518196272\n",
      "Loss for  10859 th iteration => 0.0328488986442\n",
      "Loss for  10860 th iteration => 0.0328451884029\n",
      "Loss for  10861 th iteration => 0.0328403638794\n",
      "Loss for  10862 th iteration => 0.0328391881112\n",
      "Loss for  10863 th iteration => 0.0328341932097\n",
      "Loss for  10864 th iteration => 0.03283045748\n",
      "Loss for  10865 th iteration => 0.0328286926718\n",
      "Loss for  10866 th iteration => 0.0328225447459\n",
      "Loss for  10867 th iteration => 0.0328216325357\n",
      "Loss for  10868 th iteration => 0.0328162352638\n",
      "Loss for  10869 th iteration => 0.0328121933085\n",
      "Loss for  10870 th iteration => 0.0328109531256\n",
      "Loss for  10871 th iteration => 0.0328049797777\n",
      "Loss for  10872 th iteration => 0.0328033346731\n",
      "Loss for  10873 th iteration => 0.0327984092286\n",
      "Loss for  10874 th iteration => 0.0327954713907\n",
      "Loss for  10875 th iteration => 0.0327929290385\n",
      "Loss for  10876 th iteration => 0.0327868539225\n",
      "Loss for  10877 th iteration => 0.032785471194\n",
      "Loss for  10878 th iteration => 0.032780909434\n",
      "Loss for  10879 th iteration => 0.0327783116871\n",
      "Loss for  10880 th iteration => 0.0327739588555\n",
      "Loss for  10881 th iteration => 0.0327692442101\n",
      "Loss for  10882 th iteration => 0.0327677989177\n",
      "Loss for  10883 th iteration => 0.0327632025352\n",
      "Loss for  10884 th iteration => 0.0327597414896\n",
      "Loss for  10885 th iteration => 0.0327566576613\n",
      "Loss for  10886 th iteration => 0.0327518065496\n",
      "Loss for  10887 th iteration => 0.0327504067534\n",
      "Loss for  10888 th iteration => 0.0327463590521\n",
      "Loss for  10889 th iteration => 0.0327417340537\n",
      "Loss for  10890 th iteration => 0.0327400868692\n",
      "Loss for  10891 th iteration => 0.0327340432515\n",
      "Loss for  10892 th iteration => 0.0327317355328\n",
      "Loss for  10893 th iteration => 0.0327289927176\n",
      "Loss for  10894 th iteration => 0.0327243959372\n",
      "Loss for  10895 th iteration => 0.0327214564818\n",
      "Loss for  10896 th iteration => 0.0327178097039\n",
      "Loss for  10897 th iteration => 0.0327129815227\n",
      "Loss for  10898 th iteration => 0.0327118654021\n",
      "Loss for  10899 th iteration => 0.0327068692852\n",
      "Loss for  10900 th iteration => 0.0327031923788\n",
      "Loss for  10901 th iteration => 0.0327014046289\n",
      "Loss for  10902 th iteration => 0.0326952882148\n",
      "Loss for  10903 th iteration => 0.0326944523559\n",
      "Loss for  10904 th iteration => 0.0326890294182\n",
      "Loss for  10905 th iteration => 0.0326850864581\n",
      "Loss for  10906 th iteration => 0.0326837819073\n",
      "Loss for  10907 th iteration => 0.0326778431991\n",
      "Loss for  10908 th iteration => 0.0326763072101\n",
      "Loss for  10909 th iteration => 0.0326713156547\n",
      "Loss for  10910 th iteration => 0.032668510476\n",
      "Loss for  10911 th iteration => 0.0326658722276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  10912 th iteration => 0.0326598834484\n",
      "Loss for  10913 th iteration => 0.0326585270885\n",
      "Loss for  10914 th iteration => 0.0326539882027\n",
      "Loss for  10915 th iteration => 0.0326506131501\n",
      "Loss for  10916 th iteration => 0.0326473357015\n",
      "Loss for  10917 th iteration => 0.0326441113465\n",
      "Loss for  10918 th iteration => 0.0326389346811\n",
      "Loss for  10919 th iteration => 0.0326381054695\n",
      "Loss for  10920 th iteration => 0.0326332082004\n",
      "Loss for  10921 th iteration => 0.0326291247148\n",
      "Loss for  10922 th iteration => 0.032627758468\n",
      "Loss for  10923 th iteration => 0.0326213089537\n",
      "Loss for  10924 th iteration => 0.0326207895595\n",
      "Loss for  10925 th iteration => 0.0326153819133\n",
      "Loss for  10926 th iteration => 0.0326120095867\n",
      "Loss for  10927 th iteration => 0.0326099464899\n",
      "Loss for  10928 th iteration => 0.0326037453626\n",
      "Loss for  10929 th iteration => 0.0326023998915\n",
      "Loss for  10930 th iteration => 0.0325978922158\n",
      "Loss for  10931 th iteration => 0.0325952191461\n",
      "Loss for  10932 th iteration => 0.0325910869118\n",
      "Loss for  10933 th iteration => 0.0325863242419\n",
      "Loss for  10934 th iteration => 0.0325858779816\n",
      "Loss for  10935 th iteration => 0.032580097467\n",
      "Loss for  10936 th iteration => 0.0325765686228\n",
      "Loss for  10937 th iteration => 0.0325748933112\n",
      "Loss for  10938 th iteration => 0.0325691004078\n",
      "Loss for  10939 th iteration => 0.0325666571136\n",
      "Loss for  10940 th iteration => 0.0325629155532\n",
      "Loss for  10941 th iteration => 0.0325602766804\n",
      "Loss for  10942 th iteration => 0.0325561508575\n",
      "Loss for  10943 th iteration => 0.0325513139122\n",
      "Loss for  10944 th iteration => 0.0325499238862\n",
      "Loss for  10945 th iteration => 0.0325454673946\n",
      "Loss for  10946 th iteration => 0.0325428778967\n",
      "Loss for  10947 th iteration => 0.0325386042166\n",
      "Loss for  10948 th iteration => 0.0325353498493\n",
      "Loss for  10949 th iteration => 0.0325313296163\n",
      "Loss for  10950 th iteration => 0.0325285522146\n",
      "Loss for  10951 th iteration => 0.0325248246989\n",
      "Loss for  10952 th iteration => 0.0325207041405\n",
      "Loss for  10953 th iteration => 0.032519404206\n",
      "Loss for  10954 th iteration => 0.0325129740017\n",
      "Loss for  10955 th iteration => 0.032512449506\n",
      "Loss for  10956 th iteration => 0.0325070914797\n",
      "Loss for  10957 th iteration => 0.0325037240034\n",
      "Loss for  10958 th iteration => 0.0325016858257\n",
      "Loss for  10959 th iteration => 0.0324955209937\n",
      "Loss for  10960 th iteration => 0.0324941647571\n",
      "Loss for  10961 th iteration => 0.0324897219149\n",
      "Loss for  10962 th iteration => 0.0324870344548\n",
      "Loss for  10963 th iteration => 0.032482938293\n",
      "Loss for  10964 th iteration => 0.0324782000641\n",
      "Loss for  10965 th iteration => 0.0324777829736\n",
      "Loss for  10966 th iteration => 0.0324720007896\n",
      "Loss for  10967 th iteration => 0.0324685357249\n",
      "Loss for  10968 th iteration => 0.0324668409466\n",
      "Loss for  10969 th iteration => 0.0324610738499\n",
      "Loss for  10970 th iteration => 0.0324575676898\n",
      "Loss for  10971 th iteration => 0.0324553071011\n",
      "Loss for  10972 th iteration => 0.0324526945745\n",
      "Loss for  10973 th iteration => 0.0324471491439\n",
      "Loss for  10974 th iteration => 0.0324447714983\n",
      "Loss for  10975 th iteration => 0.0324419757246\n",
      "Loss for  10976 th iteration => 0.0324377325111\n",
      "Loss for  10977 th iteration => 0.0324340694096\n",
      "Loss for  10978 th iteration => 0.0324312803203\n",
      "Loss for  10979 th iteration => 0.0324275885735\n",
      "Loss for  10980 th iteration => 0.0324238589733\n",
      "Loss for  10981 th iteration => 0.0324215576243\n",
      "Loss for  10982 th iteration => 0.0324167262036\n",
      "Loss for  10983 th iteration => 0.0324140867265\n",
      "Loss for  10984 th iteration => 0.0324101338416\n",
      "Loss for  10985 th iteration => 0.0324053160351\n",
      "Loss for  10986 th iteration => 0.0324038904234\n",
      "Loss for  10987 th iteration => 0.0323996691578\n",
      "Loss for  10988 th iteration => 0.0323965367471\n",
      "Loss for  10989 th iteration => 0.0323930437064\n",
      "Loss for  10990 th iteration => 0.0323880840801\n",
      "Loss for  10991 th iteration => 0.0323875147628\n",
      "Loss for  10992 th iteration => 0.0323820313878\n",
      "Loss for  10993 th iteration => 0.0323790640341\n",
      "Loss for  10994 th iteration => 0.0323766651923\n",
      "Loss for  10995 th iteration => 0.0323707512037\n",
      "Loss for  10996 th iteration => 0.0323694114013\n",
      "Loss for  10997 th iteration => 0.0323648542545\n",
      "Loss for  10998 th iteration => 0.0323615367607\n",
      "Loss for  10999 th iteration => 0.0323584683476\n",
      "Loss for  11000 th iteration => 0.0323537401539\n",
      "Loss for  11001 th iteration => 0.0323512613549\n",
      "Loss for  11002 th iteration => 0.0323487474919\n",
      "Loss for  11003 th iteration => 0.0323441927928\n",
      "Loss for  11004 th iteration => 0.0323412933146\n",
      "Loss for  11005 th iteration => 0.0323378063017\n",
      "Loss for  11006 th iteration => 0.0323330706367\n",
      "Loss for  11007 th iteration => 0.03233188973\n",
      "Loss for  11008 th iteration => 0.0323271318583\n",
      "Loss for  11009 th iteration => 0.0323233914043\n",
      "Loss for  11010 th iteration => 0.0323217704772\n",
      "Loss for  11011 th iteration => 0.0323157455699\n",
      "Loss for  11012 th iteration => 0.0323149081019\n",
      "Loss for  11013 th iteration => 0.0323096195346\n",
      "Loss for  11014 th iteration => 0.0323057629607\n",
      "Loss for  11015 th iteration => 0.0323044901058\n",
      "Loss for  11016 th iteration => 0.0322986621903\n",
      "Loss for  11017 th iteration => 0.0322960962064\n",
      "Loss for  11018 th iteration => 0.0322925624986\n",
      "Loss for  11019 th iteration => 0.0322899332333\n",
      "Loss for  11020 th iteration => 0.0322858287764\n",
      "Loss for  11021 th iteration => 0.0322811547114\n",
      "Loss for  11022 th iteration => 0.0322808141686\n",
      "Loss for  11023 th iteration => 0.0322750043615\n",
      "Loss for  11024 th iteration => 0.0322716795892\n",
      "Loss for  11025 th iteration => 0.0322699259175\n",
      "Loss for  11026 th iteration => 0.0322642153166\n",
      "Loss for  11027 th iteration => 0.0322607474554\n",
      "Loss for  11028 th iteration => 0.032259674214\n",
      "Loss for  11029 th iteration => 0.0322550847294\n",
      "Loss for  11030 th iteration => 0.0322509741041\n",
      "Loss for  11031 th iteration => 0.0322497380727\n",
      "Loss for  11032 th iteration => 0.0322434872011\n",
      "Loss for  11033 th iteration => 0.0322427862903\n",
      "Loss for  11034 th iteration => 0.0322375869172\n",
      "Loss for  11035 th iteration => 0.0322343208491\n",
      "Loss for  11036 th iteration => 0.0322322562406\n",
      "Loss for  11037 th iteration => 0.032226257747\n",
      "Loss for  11038 th iteration => 0.0322258849708\n",
      "Loss for  11039 th iteration => 0.032220157119\n",
      "Loss for  11040 th iteration => 0.0322167836214\n",
      "Loss for  11041 th iteration => 0.0322150645773\n",
      "Loss for  11042 th iteration => 0.032209403436\n",
      "Loss for  11043 th iteration => 0.0322059159691\n",
      "Loss for  11044 th iteration => 0.0322037706954\n",
      "Loss for  11045 th iteration => 0.0322011162309\n",
      "Loss for  11046 th iteration => 0.0321957419134\n",
      "Loss for  11047 th iteration => 0.0321947229249\n",
      "Loss for  11048 th iteration => 0.0321894172153\n",
      "Loss for  11049 th iteration => 0.0321869932978\n",
      "Loss for  11050 th iteration => 0.0321830277475\n",
      "Loss for  11051 th iteration => 0.0321792968222\n",
      "Loss for  11052 th iteration => 0.0321777057953\n",
      "Loss for  11053 th iteration => 0.0321713097182\n",
      "Loss for  11054 th iteration => 0.0321713125831\n",
      "Loss for  11055 th iteration => 0.0321656117982\n",
      "Loss for  11056 th iteration => 0.0321627032493\n",
      "Loss for  11057 th iteration => 0.0321603054654\n",
      "Loss for  11058 th iteration => 0.032154343677\n",
      "Loss for  11059 th iteration => 0.0321521808457\n",
      "Loss for  11060 th iteration => 0.032149163243\n",
      "Loss for  11061 th iteration => 0.0321464351882\n",
      "Loss for  11062 th iteration => 0.0321411623041\n",
      "Loss for  11063 th iteration => 0.0321400626358\n",
      "Loss for  11064 th iteration => 0.0321347906141\n",
      "Loss for  11065 th iteration => 0.0321324465067\n",
      "Loss for  11066 th iteration => 0.0321284196896\n",
      "Loss for  11067 th iteration => 0.0321247586163\n",
      "Loss for  11068 th iteration => 0.0321231135662\n",
      "Loss for  11069 th iteration => 0.0321167545871\n",
      "Loss for  11070 th iteration => 0.032115844545\n",
      "Loss for  11071 th iteration => 0.0321113317937\n",
      "Loss for  11072 th iteration => 0.0321085153894\n",
      "Loss for  11073 th iteration => 0.0321047898277\n",
      "Loss for  11074 th iteration => 0.0320998524382\n",
      "Loss for  11075 th iteration => 0.0320986748836\n",
      "Loss for  11076 th iteration => 0.0320943051233\n",
      "Loss for  11077 th iteration => 0.0320916477632\n",
      "Loss for  11078 th iteration => 0.0320876266027\n",
      "Loss for  11079 th iteration => 0.0320843436112\n",
      "Loss for  11080 th iteration => 0.0320805468283\n",
      "Loss for  11081 th iteration => 0.0320777892526\n",
      "Loss for  11082 th iteration => 0.0320741858598\n",
      "Loss for  11083 th iteration => 0.0320701305627\n",
      "Loss for  11084 th iteration => 0.0320688892336\n",
      "Loss for  11085 th iteration => 0.0320625175766\n",
      "Loss for  11086 th iteration => 0.0320622249601\n",
      "Loss for  11087 th iteration => 0.0320568487676\n",
      "Loss for  11088 th iteration => 0.0320536896202\n",
      "Loss for  11089 th iteration => 0.0320515687525\n",
      "Loss for  11090 th iteration => 0.0320455679468\n",
      "Loss for  11091 th iteration => 0.0320443456953\n",
      "Loss for  11092 th iteration => 0.0320400025128\n",
      "Loss for  11093 th iteration => 0.0320373860706\n",
      "Loss for  11094 th iteration => 0.0320333457607\n",
      "Loss for  11095 th iteration => 0.0320301018131\n",
      "Loss for  11096 th iteration => 0.0320262354764\n",
      "Loss for  11097 th iteration => 0.0320236157225\n",
      "Loss for  11098 th iteration => 0.0320198944863\n",
      "Loss for  11099 th iteration => 0.0320159920122\n",
      "Loss for  11100 th iteration => 0.0320146146744\n",
      "Loss for  11101 th iteration => 0.0320083097623\n",
      "Loss for  11102 th iteration => 0.0320080664938\n",
      "Loss for  11103 th iteration => 0.0320026119395\n",
      "Loss for  11104 th iteration => 0.0319996055108\n",
      "Loss for  11105 th iteration => 0.0319973487026\n",
      "Loss for  11106 th iteration => 0.0319914589683\n",
      "Loss for  11107 th iteration => 0.0319901904163\n",
      "Loss for  11108 th iteration => 0.0319858740631\n",
      "Loss for  11109 th iteration => 0.0319833011835\n",
      "Loss for  11110 th iteration => 0.0319792389561\n",
      "Loss for  11111 th iteration => 0.0319760349852\n",
      "Loss for  11112 th iteration => 0.0319720943387\n",
      "Loss for  11113 th iteration => 0.0319696210415\n",
      "Loss for  11114 th iteration => 0.0319657734212\n",
      "Loss for  11115 th iteration => 0.0319620330581\n",
      "Loss for  11116 th iteration => 0.0319605104804\n",
      "Loss for  11117 th iteration => 0.0319542916057\n",
      "Loss for  11118 th iteration => 0.0319531072108\n",
      "Loss for  11119 th iteration => 0.0319487923786\n",
      "Loss for  11120 th iteration => 0.0319460516467\n",
      "Loss for  11121 th iteration => 0.0319422593384\n",
      "Loss for  11122 th iteration => 0.0319375251411\n",
      "Loss for  11123 th iteration => 0.0319363060646\n",
      "Loss for  11124 th iteration => 0.0319318220926\n",
      "Loss for  11125 th iteration => 0.0319286273907\n",
      "Loss for  11126 th iteration => 0.0319255706211\n",
      "Loss for  11127 th iteration => 0.031921066337\n",
      "Loss for  11128 th iteration => 0.031918553925\n",
      "Loss for  11129 th iteration => 0.0319151086487\n",
      "Loss for  11130 th iteration => 0.031912515184\n",
      "Loss for  11131 th iteration => 0.0319085026604\n",
      "Loss for  11132 th iteration => 0.0319052915186\n",
      "Loss for  11133 th iteration => 0.0319014804929\n",
      "Loss for  11134 th iteration => 0.0318988122432\n",
      "Loss for  11135 th iteration => 0.0318951786165\n",
      "Loss for  11136 th iteration => 0.0318912673939\n",
      "Loss for  11137 th iteration => 0.031889932446\n",
      "Loss for  11138 th iteration => 0.0318836768087\n",
      "Loss for  11139 th iteration => 0.0318834048051\n",
      "Loss for  11140 th iteration => 0.0318780034517\n",
      "Loss for  11141 th iteration => 0.0318750302521\n",
      "Loss for  11142 th iteration => 0.0318727743841\n",
      "Loss for  11143 th iteration => 0.0318669322312\n",
      "Loss for  11144 th iteration => 0.0318656729978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  11145 th iteration => 0.0318613758548\n",
      "Loss for  11146 th iteration => 0.0318588647908\n",
      "Loss for  11147 th iteration => 0.0318547910068\n",
      "Loss for  11148 th iteration => 0.0318516336633\n",
      "Loss for  11149 th iteration => 0.0318476813449\n",
      "Loss for  11150 th iteration => 0.0318452826425\n",
      "Loss for  11151 th iteration => 0.0318414018593\n",
      "Loss for  11152 th iteration => 0.0318377868642\n",
      "Loss for  11153 th iteration => 0.0318361742126\n",
      "Loss for  11154 th iteration => 0.0318300465838\n",
      "Loss for  11155 th iteration => 0.03182885942\n",
      "Loss for  11156 th iteration => 0.0318245662066\n",
      "Loss for  11157 th iteration => 0.0318219076129\n",
      "Loss for  11158 th iteration => 0.0318180522579\n",
      "Loss for  11159 th iteration => 0.0318134113236\n",
      "Loss for  11160 th iteration => 0.03181217246\n",
      "Loss for  11161 th iteration => 0.0318077510743\n",
      "Loss for  11162 th iteration => 0.0318045500757\n",
      "Loss for  11163 th iteration => 0.0318015476832\n",
      "Loss for  11164 th iteration => 0.0317970083168\n",
      "Loss for  11165 th iteration => 0.0317945841238\n",
      "Loss for  11166 th iteration => 0.0317921756499\n",
      "Loss for  11167 th iteration => 0.0317877778931\n",
      "Loss for  11168 th iteration => 0.0317849579758\n",
      "Loss for  11169 th iteration => 0.0317815674429\n",
      "Loss for  11170 th iteration => 0.0317769055585\n",
      "Loss for  11171 th iteration => 0.0317759885684\n",
      "Loss for  11172 th iteration => 0.0317711175399\n",
      "Loss for  11173 th iteration => 0.0317678296379\n",
      "Loss for  11174 th iteration => 0.0317659131833\n",
      "Loss for  11175 th iteration => 0.0317601226554\n",
      "Loss for  11176 th iteration => 0.0317596061252\n",
      "Loss for  11177 th iteration => 0.0317541709235\n",
      "Loss for  11178 th iteration => 0.0317507714827\n",
      "Loss for  11179 th iteration => 0.0317492325491\n",
      "Loss for  11180 th iteration => 0.0317436228123\n",
      "Loss for  11181 th iteration => 0.0317403120824\n",
      "Loss for  11182 th iteration => 0.031739186316\n",
      "Loss for  11183 th iteration => 0.031734700852\n",
      "Loss for  11184 th iteration => 0.031730894212\n",
      "Loss for  11185 th iteration => 0.031729499445\n",
      "Loss for  11186 th iteration => 0.0317235553721\n",
      "Loss for  11187 th iteration => 0.031722863494\n",
      "Loss for  11188 th iteration => 0.0317176864113\n",
      "Loss for  11189 th iteration => 0.0317140562217\n",
      "Loss for  11190 th iteration => 0.0317126865736\n",
      "Loss for  11191 th iteration => 0.0317071296269\n",
      "Loss for  11192 th iteration => 0.0317046841415\n",
      "Loss for  11193 th iteration => 0.0317012478586\n",
      "Loss for  11194 th iteration => 0.031698733219\n",
      "Loss for  11195 th iteration => 0.0316947277374\n",
      "Loss for  11196 th iteration => 0.0316915867055\n",
      "Loss for  11197 th iteration => 0.0316877422446\n",
      "Loss for  11198 th iteration => 0.0316852551594\n",
      "Loss for  11199 th iteration => 0.0316815117384\n",
      "Loss for  11200 th iteration => 0.0316778327451\n",
      "Loss for  11201 th iteration => 0.0316763263682\n",
      "Loss for  11202 th iteration => 0.0316701944236\n",
      "Loss for  11203 th iteration => 0.0316700340721\n",
      "Loss for  11204 th iteration => 0.0316645321838\n",
      "Loss for  11205 th iteration => 0.0316618327476\n",
      "Loss for  11206 th iteration => 0.0316593643819\n",
      "Loss for  11207 th iteration => 0.0316537625282\n",
      "Loss for  11208 th iteration => 0.0316514457594\n",
      "Loss for  11209 th iteration => 0.0316486374323\n",
      "Loss for  11210 th iteration => 0.0316452202823\n",
      "Loss for  11211 th iteration => 0.0316411964384\n",
      "Loss for  11212 th iteration => 0.0316400400964\n",
      "Loss for  11213 th iteration => 0.0316338244766\n",
      "Loss for  11214 th iteration => 0.0316335181744\n",
      "Loss for  11215 th iteration => 0.0316282554963\n",
      "Loss for  11216 th iteration => 0.0316252620188\n",
      "Loss for  11217 th iteration => 0.0316230935977\n",
      "Loss for  11218 th iteration => 0.031617321899\n",
      "Loss for  11219 th iteration => 0.0316151821867\n",
      "Loss for  11220 th iteration => 0.0316121524022\n",
      "Loss for  11221 th iteration => 0.0316089637163\n",
      "Loss for  11222 th iteration => 0.031604714457\n",
      "Loss for  11223 th iteration => 0.0316024879067\n",
      "Loss for  11224 th iteration => 0.0315986021131\n",
      "Loss for  11225 th iteration => 0.0315966055975\n",
      "Loss for  11226 th iteration => 0.0315918785594\n",
      "Loss for  11227 th iteration => 0.0315894717449\n",
      "Loss for  11228 th iteration => 0.0315857210332\n",
      "Loss for  11229 th iteration => 0.0315811564708\n",
      "Loss for  11230 th iteration => 0.0315796047699\n",
      "Loss for  11231 th iteration => 0.0315757043264\n",
      "Loss for  11232 th iteration => 0.0315728318192\n",
      "Loss for  11233 th iteration => 0.0315692669963\n",
      "Loss for  11234 th iteration => 0.0315646869359\n",
      "Loss for  11235 th iteration => 0.0315642109032\n",
      "Loss for  11236 th iteration => 0.0315588012366\n",
      "Loss for  11237 th iteration => 0.0315554861066\n",
      "Loss for  11238 th iteration => 0.0315539231583\n",
      "Loss for  11239 th iteration => 0.0315483919491\n",
      "Loss for  11240 th iteration => 0.0315450999196\n",
      "Loss for  11241 th iteration => 0.0315440536528\n",
      "Loss for  11242 th iteration => 0.0315395300581\n",
      "Loss for  11243 th iteration => 0.0315358896053\n",
      "Loss for  11244 th iteration => 0.0315343833815\n",
      "Loss for  11245 th iteration => 0.0315285314799\n",
      "Loss for  11246 th iteration => 0.0315279377955\n",
      "Loss for  11247 th iteration => 0.0315227108571\n",
      "Loss for  11248 th iteration => 0.0315192380119\n",
      "Loss for  11249 th iteration => 0.0315177892923\n",
      "Loss for  11250 th iteration => 0.0315122809863\n",
      "Loss for  11251 th iteration => 0.0315099339112\n",
      "Loss for  11252 th iteration => 0.0315065029645\n",
      "Loss for  11253 th iteration => 0.0315040833385\n",
      "Loss for  11254 th iteration => 0.0315000603193\n",
      "Loss for  11255 th iteration => 0.0314969768821\n",
      "Loss for  11256 th iteration => 0.0314930614981\n",
      "Loss for  11257 th iteration => 0.0314908318872\n",
      "Loss for  11258 th iteration => 0.0314868977003\n",
      "Loss for  11259 th iteration => 0.0314835370984\n",
      "Loss for  11260 th iteration => 0.031481768874\n",
      "Loss for  11261 th iteration => 0.0314758280004\n",
      "Loss for  11262 th iteration => 0.031474706114\n",
      "Loss for  11263 th iteration => 0.0314704622797\n",
      "Loss for  11264 th iteration => 0.0314679590337\n",
      "Loss for  11265 th iteration => 0.0314640344452\n",
      "Loss for  11266 th iteration => 0.0314609265003\n",
      "Loss for  11267 th iteration => 0.0314571489501\n",
      "Loss for  11268 th iteration => 0.0314546868163\n",
      "Loss for  11269 th iteration => 0.0314509917676\n",
      "Loss for  11270 th iteration => 0.0314473806203\n",
      "Loss for  11271 th iteration => 0.0314458692497\n",
      "Loss for  11272 th iteration => 0.0314398278295\n",
      "Loss for  11273 th iteration => 0.0314387277532\n",
      "Loss for  11274 th iteration => 0.0314344967334\n",
      "Loss for  11275 th iteration => 0.0314319347459\n",
      "Loss for  11276 th iteration => 0.0314281168552\n",
      "Loss for  11277 th iteration => 0.0314236045352\n",
      "Loss for  11278 th iteration => 0.0314223902726\n",
      "Loss for  11279 th iteration => 0.0314181101206\n",
      "Loss for  11280 th iteration => 0.0314149425434\n",
      "Loss for  11281 th iteration => 0.0314120634694\n",
      "Loss for  11282 th iteration => 0.0314088029068\n",
      "Loss for  11283 th iteration => 0.0314041319231\n",
      "Loss for  11284 th iteration => 0.0314034443271\n",
      "Loss for  11285 th iteration => 0.0313986224543\n",
      "Loss for  11286 th iteration => 0.03139533707\n",
      "Loss for  11287 th iteration => 0.0313935182913\n",
      "Loss for  11288 th iteration => 0.0313877348324\n",
      "Loss for  11289 th iteration => 0.0313865056616\n",
      "Loss for  11290 th iteration => 0.031382243388\n",
      "Loss for  11291 th iteration => 0.0313790735961\n",
      "Loss for  11292 th iteration => 0.0313761889583\n",
      "Loss for  11293 th iteration => 0.0313729942353\n",
      "Loss for  11294 th iteration => 0.0313683339146\n",
      "Loss for  11295 th iteration => 0.0313675446949\n",
      "Loss for  11296 th iteration => 0.0313628709101\n",
      "Loss for  11297 th iteration => 0.0313594256739\n",
      "Loss for  11298 th iteration => 0.0313577742059\n",
      "Loss for  11299 th iteration => 0.0313519475138\n",
      "Loss for  11300 th iteration => 0.031350696765\n",
      "Loss for  11301 th iteration => 0.0313464611268\n",
      "Loss for  11302 th iteration => 0.0313432804462\n",
      "Loss for  11303 th iteration => 0.0313404252427\n",
      "Loss for  11304 th iteration => 0.0313359765076\n",
      "Loss for  11305 th iteration => 0.0313336637162\n",
      "Loss for  11306 th iteration => 0.0313302207635\n",
      "Loss for  11307 th iteration => 0.0313278904012\n",
      "Loss for  11308 th iteration => 0.0313238486342\n",
      "Loss for  11309 th iteration => 0.0313208640311\n",
      "Loss for  11310 th iteration => 0.0313169948839\n",
      "Loss for  11311 th iteration => 0.0313147054461\n",
      "Loss for  11312 th iteration => 0.0313108859204\n",
      "Loss for  11313 th iteration => 0.0313075226462\n",
      "Loss for  11314 th iteration => 0.0313058041602\n",
      "Loss for  11315 th iteration => 0.0312999101226\n",
      "Loss for  11316 th iteration => 0.0312988171929\n",
      "Loss for  11317 th iteration => 0.0312945585934\n",
      "Loss for  11318 th iteration => 0.0312921825942\n",
      "Loss for  11319 th iteration => 0.031288200786\n",
      "Loss for  11320 th iteration => 0.031285204803\n",
      "Loss for  11321 th iteration => 0.0312814067122\n",
      "Loss for  11322 th iteration => 0.0312790080369\n",
      "Loss for  11323 th iteration => 0.0312753066921\n",
      "Loss for  11324 th iteration => 0.0312718271893\n",
      "Loss for  11325 th iteration => 0.0312702329026\n",
      "Loss for  11326 th iteration => 0.0312643019606\n",
      "Loss for  11327 th iteration => 0.0312632130448\n",
      "Loss for  11328 th iteration => 0.0312589714803\n",
      "Loss for  11329 th iteration => 0.0312565655993\n",
      "Loss for  11330 th iteration => 0.0312526423853\n",
      "Loss for  11331 th iteration => 0.0312482824149\n",
      "Loss for  11332 th iteration => 0.0312469939774\n",
      "Loss for  11333 th iteration => 0.0312428455504\n",
      "Loss for  11334 th iteration => 0.0312396246917\n",
      "Loss for  11335 th iteration => 0.0312369160257\n",
      "Loss for  11336 th iteration => 0.0312335238826\n",
      "Loss for  11337 th iteration => 0.0312289836988\n",
      "Loss for  11338 th iteration => 0.0312283373752\n",
      "Loss for  11339 th iteration => 0.0312234332229\n",
      "Loss for  11340 th iteration => 0.0312204188793\n",
      "Loss for  11341 th iteration => 0.0312183794535\n",
      "Loss for  11342 th iteration => 0.0312127978252\n",
      "Loss for  11343 th iteration => 0.0312114879079\n",
      "Loss for  11344 th iteration => 0.0312073657432\n",
      "Loss for  11345 th iteration => 0.0312041341901\n",
      "Loss for  11346 th iteration => 0.0312014376527\n",
      "Loss for  11347 th iteration => 0.0311980793161\n",
      "Loss for  11348 th iteration => 0.0311935549873\n",
      "Loss for  11349 th iteration => 0.0311928414128\n",
      "Loss for  11350 th iteration => 0.0311880306773\n",
      "Loss for  11351 th iteration => 0.0311849294127\n",
      "Loss for  11352 th iteration => 0.0311829852088\n",
      "Loss for  11353 th iteration => 0.0311773930914\n",
      "Loss for  11354 th iteration => 0.0311760570402\n",
      "Loss for  11355 th iteration => 0.0311719672912\n",
      "Loss for  11356 th iteration => 0.0311687189403\n",
      "Loss for  11357 th iteration => 0.0311660466821\n",
      "Loss for  11358 th iteration => 0.0311627011804\n",
      "Loss for  11359 th iteration => 0.031158196628\n",
      "Loss for  11360 th iteration => 0.0311574370509\n",
      "Loss for  11361 th iteration => 0.0311526859407\n",
      "Loss for  11362 th iteration => 0.0311495430551\n",
      "Loss for  11363 th iteration => 0.0311476493515\n",
      "Loss for  11364 th iteration => 0.0311420671795\n",
      "Loss for  11365 th iteration => 0.0311407015731\n",
      "Loss for  11366 th iteration => 0.031136648802\n",
      "Loss for  11367 th iteration => 0.0311333791196\n",
      "Loss for  11368 th iteration => 0.0311307403479\n",
      "Loss for  11369 th iteration => 0.0311273915411\n",
      "Loss for  11370 th iteration => 0.0311229096823\n",
      "Loss for  11371 th iteration => 0.0311221207667\n",
      "Loss for  11372 th iteration => 0.031117402873\n",
      "Loss for  11373 th iteration => 0.0311142536939\n",
      "Loss for  11374 th iteration => 0.0311123756087\n",
      "Loss for  11375 th iteration => 0.0311068192394\n",
      "Loss for  11376 th iteration => 0.0311054216135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  11377 th iteration => 0.0311014091448\n",
      "Loss for  11378 th iteration => 0.0310981148179\n",
      "Loss for  11379 th iteration => 0.0310955164465\n",
      "Loss for  11380 th iteration => 0.0310921519646\n",
      "Loss for  11381 th iteration => 0.0310876949348\n",
      "Loss for  11382 th iteration => 0.0310868897605\n",
      "Loss for  11383 th iteration => 0.0310821844438\n",
      "Loss for  11384 th iteration => 0.031079056508\n",
      "Loss for  11385 th iteration => 0.0310771668461\n",
      "Loss for  11386 th iteration => 0.0310716485621\n",
      "Loss for  11387 th iteration => 0.0310702171947\n",
      "Loss for  11388 th iteration => 0.0310662473911\n",
      "Loss for  11389 th iteration => 0.0310629260548\n",
      "Loss for  11390 th iteration => 0.031060373212\n",
      "Loss for  11391 th iteration => 0.031056983625\n",
      "Loss for  11392 th iteration => 0.0310525529521\n",
      "Loss for  11393 th iteration => 0.0310517417954\n",
      "Loss for  11394 th iteration => 0.0310470329254\n",
      "Loss for  11395 th iteration => 0.0310439476825\n",
      "Loss for  11396 th iteration => 0.0310420252545\n",
      "Loss for  11397 th iteration => 0.0310365545471\n",
      "Loss for  11398 th iteration => 0.0310350882908\n",
      "Loss for  11399 th iteration => 0.0310311627688\n",
      "Loss for  11400 th iteration => 0.0310278127936\n",
      "Loss for  11401 th iteration => 0.0310253092187\n",
      "Loss for  11402 th iteration => 0.0310218873889\n",
      "Loss for  11403 th iteration => 0.0310174841291\n",
      "Loss for  11404 th iteration => 0.0310166750733\n",
      "Loss for  11405 th iteration => 0.0310119500444\n",
      "Loss for  11406 th iteration => 0.0310089241879\n",
      "Loss for  11407 th iteration => 0.0310069524963\n",
      "Loss for  11408 th iteration => 0.0310015366779\n",
      "Loss for  11409 th iteration => 0.0310000348289\n",
      "Loss for  11410 th iteration => 0.0309961546273\n",
      "Loss for  11411 th iteration => 0.0309927749529\n",
      "Loss for  11412 th iteration => 0.0309903233057\n",
      "Loss for  11413 th iteration => 0.0309868638821\n",
      "Loss for  11414 th iteration => 0.0309824887261\n",
      "Loss for  11415 th iteration => 0.0309816881379\n",
      "Loss for  11416 th iteration => 0.0309769370999\n",
      "Loss for  11417 th iteration => 0.0309739836073\n",
      "Loss for  11418 th iteration => 0.0309719498208\n",
      "Loss for  11419 th iteration => 0.0309665945029\n",
      "Loss for  11420 th iteration => 0.0309650566994\n",
      "Loss for  11421 th iteration => 0.0309612224105\n",
      "Loss for  11422 th iteration => 0.0309578124164\n",
      "Loss for  11423 th iteration => 0.0309554145189\n",
      "Loss for  11424 th iteration => 0.0309519135419\n",
      "Loss for  11425 th iteration => 0.0309475668981\n",
      "Loss for  11426 th iteration => 0.0309467798\n",
      "Loss for  11427 th iteration => 0.0309419950575\n",
      "Loss for  11428 th iteration => 0.0309391240017\n",
      "Loss for  11429 th iteration => 0.0309370181544\n",
      "Loss for  11430 th iteration => 0.0309317276212\n",
      "Loss for  11431 th iteration => 0.0309301537634\n",
      "Loss for  11432 th iteration => 0.0309263656356\n",
      "Loss for  11433 th iteration => 0.0309229250403\n",
      "Loss for  11434 th iteration => 0.0309205820653\n",
      "Loss for  11435 th iteration => 0.0309170366583\n",
      "Loss for  11436 th iteration => 0.0309127187171\n",
      "Loss for  11437 th iteration => 0.0309119490789\n",
      "Loss for  11438 th iteration => 0.0309071369983\n",
      "Loss for  11439 th iteration => 0.0309035962129\n",
      "Loss for  11440 th iteration => 0.0309023665873\n",
      "Loss for  11441 th iteration => 0.0308971105037\n",
      "Loss for  11442 th iteration => 0.0308946798239\n",
      "Loss for  11443 th iteration => 0.0308915093309\n",
      "Loss for  11444 th iteration => 0.0308889902091\n",
      "Loss for  11445 th iteration => 0.0308853882526\n",
      "Loss for  11446 th iteration => 0.0308821758043\n",
      "Loss for  11447 th iteration => 0.0308784373962\n",
      "Loss for  11448 th iteration => 0.0308764006037\n",
      "Loss for  11449 th iteration => 0.0308724707244\n",
      "Loss for  11450 th iteration => 0.0308694529536\n",
      "Loss for  11451 th iteration => 0.0308675102769\n",
      "Loss for  11452 th iteration => 0.0308620168163\n",
      "Loss for  11453 th iteration => 0.0308607318459\n",
      "Loss for  11454 th iteration => 0.0308567869553\n",
      "Loss for  11455 th iteration => 0.0308535324686\n",
      "Loss for  11456 th iteration => 0.0308509466589\n",
      "Loss for  11457 th iteration => 0.0308476444758\n",
      "Loss for  11458 th iteration => 0.0308430447786\n",
      "Loss for  11459 th iteration => 0.0308418445895\n",
      "Loss for  11460 th iteration => 0.0308379344337\n",
      "Loss for  11461 th iteration => 0.0308351484167\n",
      "Loss for  11462 th iteration => 0.0308318865877\n",
      "Loss for  11463 th iteration => 0.0308273096476\n",
      "Loss for  11464 th iteration => 0.0308270663333\n",
      "Loss for  11465 th iteration => 0.0308216981403\n",
      "Loss for  11466 th iteration => 0.030818749798\n",
      "Loss for  11467 th iteration => 0.0308170162834\n",
      "Loss for  11468 th iteration => 0.0308119401851\n",
      "Loss for  11469 th iteration => 0.0308085553061\n",
      "Loss for  11470 th iteration => 0.0308078849271\n",
      "Loss for  11471 th iteration => 0.0308032184198\n",
      "Loss for  11472 th iteration => 0.0308001192709\n",
      "Loss for  11473 th iteration => 0.0307982751864\n",
      "Loss for  11474 th iteration => 0.0307927247225\n",
      "Loss for  11475 th iteration => 0.0307906879705\n",
      "Loss for  11476 th iteration => 0.0307877380011\n",
      "Loss for  11477 th iteration => 0.0307847312304\n",
      "Loss for  11478 th iteration => 0.030780781488\n",
      "Loss for  11479 th iteration => 0.0307784445493\n",
      "Loss for  11480 th iteration => 0.0307751505689\n",
      "Loss for  11481 th iteration => 0.0307728381936\n",
      "Loss for  11482 th iteration => 0.0307684958887\n",
      "Loss for  11483 th iteration => 0.0307663466195\n",
      "Loss for  11484 th iteration => 0.0307625207618\n",
      "Loss for  11485 th iteration => 0.0307583748391\n",
      "Loss for  11486 th iteration => 0.0307568729984\n",
      "Loss for  11487 th iteration => 0.0307530723925\n",
      "Loss for  11488 th iteration => 0.0307497193019\n",
      "Loss for  11489 th iteration => 0.030747334784\n",
      "Loss for  11490 th iteration => 0.030743922303\n",
      "Loss for  11491 th iteration => 0.0307396472584\n",
      "Loss for  11492 th iteration => 0.0307387769166\n",
      "Loss for  11493 th iteration => 0.0307341793982\n",
      "Loss for  11494 th iteration => 0.0307311986057\n",
      "Loss for  11495 th iteration => 0.0307292559528\n",
      "Loss for  11496 th iteration => 0.0307239661341\n",
      "Loss for  11497 th iteration => 0.0307224079093\n",
      "Loss for  11498 th iteration => 0.0307186789879\n",
      "Loss for  11499 th iteration => 0.0307152699156\n",
      "Loss for  11500 th iteration => 0.030712988977\n",
      "Loss for  11501 th iteration => 0.0307094505225\n",
      "Loss for  11502 th iteration => 0.0307052202787\n",
      "Loss for  11503 th iteration => 0.0307044468241\n",
      "Loss for  11504 th iteration => 0.0306996861958\n",
      "Loss for  11505 th iteration => 0.0306962075776\n",
      "Loss for  11506 th iteration => 0.0306949984356\n",
      "Loss for  11507 th iteration => 0.030689775699\n",
      "Loss for  11508 th iteration => 0.0306873999048\n",
      "Loss for  11509 th iteration => 0.0306852729834\n",
      "Loss for  11510 th iteration => 0.0306809223369\n",
      "Loss for  11511 th iteration => 0.0306786868059\n",
      "Loss for  11512 th iteration => 0.0306750686376\n",
      "Loss for  11513 th iteration => 0.0306708475409\n",
      "Loss for  11514 th iteration => 0.03067020764\n",
      "Loss for  11515 th iteration => 0.0306653013705\n",
      "Loss for  11516 th iteration => 0.0306619835257\n",
      "Loss for  11517 th iteration => 0.0306606488382\n",
      "Loss for  11518 th iteration => 0.0306554270137\n",
      "Loss for  11519 th iteration => 0.0306531711103\n",
      "Loss for  11520 th iteration => 0.030650966338\n",
      "Loss for  11521 th iteration => 0.0306466484336\n",
      "Loss for  11522 th iteration => 0.0306444513254\n",
      "Loss for  11523 th iteration => 0.030640766313\n",
      "Loss for  11524 th iteration => 0.0306365857707\n",
      "Loss for  11525 th iteration => 0.0306351119258\n",
      "Loss for  11526 th iteration => 0.0306313210213\n",
      "Loss for  11527 th iteration => 0.0306280107746\n",
      "Loss for  11528 th iteration => 0.0306256003439\n",
      "Loss for  11529 th iteration => 0.0306222926927\n",
      "Loss for  11530 th iteration => 0.030618018968\n",
      "Loss for  11531 th iteration => 0.0306171080037\n",
      "Loss for  11532 th iteration => 0.0306126264992\n",
      "Loss for  11533 th iteration => 0.0306095602452\n",
      "Loss for  11534 th iteration => 0.0306077339805\n",
      "Loss for  11535 th iteration => 0.0306024439222\n",
      "Loss for  11536 th iteration => 0.0306009011728\n",
      "Loss for  11537 th iteration => 0.0305971976391\n",
      "Loss for  11538 th iteration => 0.0305938158339\n",
      "Loss for  11539 th iteration => 0.0305915388217\n",
      "Loss for  11540 th iteration => 0.0305880554559\n",
      "Loss for  11541 th iteration => 0.0305838366602\n",
      "Loss for  11542 th iteration => 0.0305830703457\n",
      "Loss for  11543 th iteration => 0.0305783535059\n",
      "Loss for  11544 th iteration => 0.0305748973032\n",
      "Loss for  11545 th iteration => 0.0305736907312\n",
      "Loss for  11546 th iteration => 0.0305685153631\n",
      "Loss for  11547 th iteration => 0.0305661561612\n",
      "Loss for  11548 th iteration => 0.0305640418295\n",
      "Loss for  11549 th iteration => 0.0305597226668\n",
      "Loss for  11550 th iteration => 0.0305575148654\n",
      "Loss for  11551 th iteration => 0.0305539125804\n",
      "Loss for  11552 th iteration => 0.0305497221319\n",
      "Loss for  11553 th iteration => 0.0305482476628\n",
      "Loss for  11554 th iteration => 0.0305444825304\n",
      "Loss for  11555 th iteration => 0.0305411839384\n",
      "Loss for  11556 th iteration => 0.0305387783034\n",
      "Loss for  11557 th iteration => 0.0305343248183\n",
      "Loss for  11558 th iteration => 0.0305323519706\n",
      "Loss for  11559 th iteration => 0.0305297883499\n",
      "Loss for  11560 th iteration => 0.0305258355506\n",
      "Loss for  11561 th iteration => 0.0305233744244\n",
      "Loss for  11562 th iteration => 0.0305199909746\n",
      "Loss for  11563 th iteration => 0.0305158305821\n",
      "Loss for  11564 th iteration => 0.0305150088362\n",
      "Loss for  11565 th iteration => 0.030510292406\n",
      "Loss for  11566 th iteration => 0.0305068693313\n",
      "Loss for  11567 th iteration => 0.0305057279218\n",
      "Loss for  11568 th iteration => 0.0305003501429\n",
      "Loss for  11569 th iteration => 0.0304974108798\n",
      "Loss for  11570 th iteration => 0.0304962543933\n",
      "Loss for  11571 th iteration => 0.0304920143836\n",
      "Loss for  11572 th iteration => 0.0304888158062\n",
      "Loss for  11573 th iteration => 0.0304871517819\n",
      "Loss for  11574 th iteration => 0.0304818274194\n",
      "Loss for  11575 th iteration => 0.0304812424196\n",
      "Loss for  11576 th iteration => 0.0304762534794\n",
      "Loss for  11577 th iteration => 0.0304731148553\n",
      "Loss for  11578 th iteration => 0.0304717488331\n",
      "Loss for  11579 th iteration => 0.0304664626672\n",
      "Loss for  11580 th iteration => 0.0304634537628\n",
      "Loss for  11581 th iteration => 0.0304625122266\n",
      "Loss for  11582 th iteration => 0.030458003036\n",
      "Loss for  11583 th iteration => 0.0304544067076\n",
      "Loss for  11584 th iteration => 0.0304533814206\n",
      "Loss for  11585 th iteration => 0.0304480285875\n",
      "Loss for  11586 th iteration => 0.0304460253323\n",
      "Loss for  11587 th iteration => 0.0304435148175\n",
      "Loss for  11588 th iteration => 0.0304395952034\n",
      "Loss for  11589 th iteration => 0.0304370885648\n",
      "Loss for  11590 th iteration => 0.0304337733125\n",
      "Loss for  11591 th iteration => 0.0304295682534\n",
      "Loss for  11592 th iteration => 0.0304288461326\n",
      "Loss for  11593 th iteration => 0.030424081964\n",
      "Loss for  11594 th iteration => 0.0304207527645\n",
      "Loss for  11595 th iteration => 0.0304195197719\n",
      "Loss for  11596 th iteration => 0.0304142492527\n",
      "Loss for  11597 th iteration => 0.0304112447984\n",
      "Loss for  11598 th iteration => 0.0304102015008\n",
      "Loss for  11599 th iteration => 0.0304059060544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  11600 th iteration => 0.0304027772318\n",
      "Loss for  11601 th iteration => 0.0304010669799\n",
      "Loss for  11602 th iteration => 0.0303957421605\n",
      "Loss for  11603 th iteration => 0.0303952765872\n",
      "Loss for  11604 th iteration => 0.030390212802\n",
      "Loss for  11605 th iteration => 0.0303871943361\n",
      "Loss for  11606 th iteration => 0.0303857178957\n",
      "Loss for  11607 th iteration => 0.0303805419649\n",
      "Loss for  11608 th iteration => 0.0303774653697\n",
      "Loss for  11609 th iteration => 0.0303766537774\n",
      "Loss for  11610 th iteration => 0.0303720549319\n",
      "Loss for  11611 th iteration => 0.0303685930564\n",
      "Loss for  11612 th iteration => 0.0303674474046\n",
      "Loss for  11613 th iteration => 0.0303622052551\n",
      "Loss for  11614 th iteration => 0.0303591936051\n",
      "Loss for  11615 th iteration => 0.0303580900665\n",
      "Loss for  11616 th iteration => 0.0303539816606\n",
      "Loss for  11617 th iteration => 0.0303505884719\n",
      "Loss for  11618 th iteration => 0.0303491526397\n",
      "Loss for  11619 th iteration => 0.0303437343322\n",
      "Loss for  11620 th iteration => 0.0303432285049\n",
      "Loss for  11621 th iteration => 0.0303382672781\n",
      "Loss for  11622 th iteration => 0.0303351773527\n",
      "Loss for  11623 th iteration => 0.0303337411874\n",
      "Loss for  11624 th iteration => 0.0303286052227\n",
      "Loss for  11625 th iteration => 0.0303255110803\n",
      "Loss for  11626 th iteration => 0.0303246819299\n",
      "Loss for  11627 th iteration => 0.0303201671412\n",
      "Loss for  11628 th iteration => 0.0303173300103\n",
      "Loss for  11629 th iteration => 0.0303153544178\n",
      "Loss for  11630 th iteration => 0.0303101405971\n",
      "Loss for  11631 th iteration => 0.0303079626789\n",
      "Loss for  11632 th iteration => 0.0303053190484\n",
      "Loss for  11633 th iteration => 0.030302162247\n",
      "Loss for  11634 th iteration => 0.0302986594721\n",
      "Loss for  11635 th iteration => 0.0302973453045\n",
      "Loss for  11636 th iteration => 0.0302918952519\n",
      "Loss for  11637 th iteration => 0.0302913764696\n",
      "Loss for  11638 th iteration => 0.0302864640697\n",
      "Loss for  11639 th iteration => 0.0302833544746\n",
      "Loss for  11640 th iteration => 0.0302819337421\n",
      "Loss for  11641 th iteration => 0.0302768250576\n",
      "Loss for  11642 th iteration => 0.0302737274399\n",
      "Loss for  11643 th iteration => 0.0302729002824\n",
      "Loss for  11644 th iteration => 0.030268432082\n",
      "Loss for  11645 th iteration => 0.0302655446195\n",
      "Loss for  11646 th iteration => 0.0302636322085\n",
      "Loss for  11647 th iteration => 0.0302583920789\n",
      "Loss for  11648 th iteration => 0.0302562594397\n",
      "Loss for  11649 th iteration => 0.0302535949083\n",
      "Loss for  11650 th iteration => 0.0302504743836\n",
      "Loss for  11651 th iteration => 0.0302469510379\n",
      "Loss for  11652 th iteration => 0.0302456706651\n",
      "Loss for  11653 th iteration => 0.0302402143719\n",
      "Loss for  11654 th iteration => 0.0302397182298\n",
      "Loss for  11655 th iteration => 0.0302348086619\n",
      "Loss for  11656 th iteration => 0.0302317241462\n",
      "Loss for  11657 th iteration => 0.0302302848317\n",
      "Loss for  11658 th iteration => 0.0302252114064\n",
      "Loss for  11659 th iteration => 0.0302221031891\n",
      "Loss for  11660 th iteration => 0.0302213087081\n",
      "Loss for  11661 th iteration => 0.0302168362532\n",
      "Loss for  11662 th iteration => 0.0302139678901\n",
      "Loss for  11663 th iteration => 0.0302120501046\n",
      "Loss for  11664 th iteration => 0.030206828846\n",
      "Loss for  11665 th iteration => 0.030204697928\n",
      "Loss for  11666 th iteration => 0.0302020672753\n",
      "Loss for  11667 th iteration => 0.0301989292952\n",
      "Loss for  11668 th iteration => 0.0301954454674\n",
      "Loss for  11669 th iteration => 0.0301941395561\n",
      "Loss for  11670 th iteration => 0.0301886946715\n",
      "Loss for  11671 th iteration => 0.0301882442939\n",
      "Loss for  11672 th iteration => 0.0301833072434\n",
      "Loss for  11673 th iteration => 0.0301802772447\n",
      "Loss for  11674 th iteration => 0.0301787972774\n",
      "Loss for  11675 th iteration => 0.030173763966\n",
      "Loss for  11676 th iteration => 0.0301706408297\n",
      "Loss for  11677 th iteration => 0.0301698988429\n",
      "Loss for  11678 th iteration => 0.0301653882454\n",
      "Loss for  11679 th iteration => 0.0301625858157\n",
      "Loss for  11680 th iteration => 0.0301606163928\n",
      "Loss for  11681 th iteration => 0.0301554448425\n",
      "Loss for  11682 th iteration => 0.0301532858711\n",
      "Loss for  11683 th iteration => 0.0301507258967\n",
      "Loss for  11684 th iteration => 0.030147534345\n",
      "Loss for  11685 th iteration => 0.0301441305393\n",
      "Loss for  11686 th iteration => 0.030142759075\n",
      "Loss for  11687 th iteration => 0.0301373379123\n",
      "Loss for  11688 th iteration => 0.0301369480109\n",
      "Loss for  11689 th iteration => 0.0301319637549\n",
      "Loss for  11690 th iteration => 0.0301290073679\n",
      "Loss for  11691 th iteration => 0.0301274727321\n",
      "Loss for  11692 th iteration => 0.0301224822954\n",
      "Loss for  11693 th iteration => 0.0301193417982\n",
      "Loss for  11694 th iteration => 0.0301186647961\n",
      "Loss for  11695 th iteration => 0.0301140936336\n",
      "Loss for  11696 th iteration => 0.0301113886705\n",
      "Loss for  11697 th iteration => 0.0301093364416\n",
      "Loss for  11698 th iteration => 0.0301042357146\n",
      "Loss for  11699 th iteration => 0.0301020282581\n",
      "Loss for  11700 th iteration => 0.0300995635919\n",
      "Loss for  11701 th iteration => 0.0300962942754\n",
      "Loss for  11702 th iteration => 0.0300929977339\n",
      "Loss for  11703 th iteration => 0.0300915337837\n",
      "Loss for  11704 th iteration => 0.0300861450271\n",
      "Loss for  11705 th iteration => 0.0300858246465\n",
      "Loss for  11706 th iteration => 0.0300807806109\n",
      "Loss for  11707 th iteration => 0.0300779099493\n",
      "Loss for  11708 th iteration => 0.0300763120545\n",
      "Loss for  11709 th iteration => 0.0300713658549\n",
      "Loss for  11710 th iteration => 0.0300682068069\n",
      "Loss for  11711 th iteration => 0.0300676023465\n",
      "Loss for  11712 th iteration => 0.0300629559479\n",
      "Loss for  11713 th iteration => 0.0300603696214\n",
      "Loss for  11714 th iteration => 0.0300582136411\n",
      "Loss for  11715 th iteration => 0.0300531982575\n",
      "Loss for  11716 th iteration => 0.0300509282222\n",
      "Loss for  11717 th iteration => 0.0300485752566\n",
      "Loss for  11718 th iteration => 0.0300452120523\n",
      "Loss for  11719 th iteration => 0.0300420410338\n",
      "Loss for  11720 th iteration => 0.030040466526\n",
      "Loss for  11721 th iteration => 0.0300351163866\n",
      "Loss for  11722 th iteration => 0.0300348707611\n",
      "Loss for  11723 th iteration => 0.0300297591916\n",
      "Loss for  11724 th iteration => 0.0300269816629\n",
      "Loss for  11725 th iteration => 0.0300253155653\n",
      "Loss for  11726 th iteration => 0.0300204140368\n",
      "Loss for  11727 th iteration => 0.0300172360775\n",
      "Loss for  11728 th iteration => 0.0300167084018\n",
      "Loss for  11729 th iteration => 0.0300119773333\n",
      "Loss for  11730 th iteration => 0.0300095237924\n",
      "Loss for  11731 th iteration => 0.0300072500414\n",
      "Loss for  11732 th iteration => 0.0300023300437\n",
      "Loss for  11733 th iteration => 0.0299999876401\n",
      "Loss for  11734 th iteration => 0.0299977571912\n",
      "Loss for  11735 th iteration => 0.0299942894388\n",
      "Loss for  11736 th iteration => 0.0299912561152\n",
      "Loss for  11737 th iteration => 0.029989558982\n",
      "Loss for  11738 th iteration => 0.029984259325\n",
      "Loss for  11739 th iteration => 0.0299831827837\n",
      "Loss for  11740 th iteration => 0.0299792416777\n",
      "Loss for  11741 th iteration => 0.0299763615353\n",
      "Loss for  11742 th iteration => 0.0299737898781\n",
      "Loss for  11743 th iteration => 0.0299706968549\n",
      "Loss for  11744 th iteration => 0.0299664408261\n",
      "Loss for  11745 th iteration => 0.0299659816737\n",
      "Loss for  11746 th iteration => 0.0299611673296\n",
      "Loss for  11747 th iteration => 0.0299581359227\n",
      "Loss for  11748 th iteration => 0.029956679023\n",
      "Loss for  11749 th iteration => 0.0299517436531\n",
      "Loss for  11750 th iteration => 0.0299486262443\n",
      "Loss for  11751 th iteration => 0.0299479271548\n",
      "Loss for  11752 th iteration => 0.0299435081902\n",
      "Loss for  11753 th iteration => 0.0299406666915\n",
      "Loss for  11754 th iteration => 0.0299387934777\n",
      "Loss for  11755 th iteration => 0.0299336286399\n",
      "Loss for  11756 th iteration => 0.0299315481906\n",
      "Loss for  11757 th iteration => 0.0299290248303\n",
      "Loss for  11758 th iteration => 0.0299258650813\n",
      "Loss for  11759 th iteration => 0.0299225245396\n",
      "Loss for  11760 th iteration => 0.0299211482349\n",
      "Loss for  11761 th iteration => 0.0299157663545\n",
      "Loss for  11762 th iteration => 0.0299154880503\n",
      "Loss for  11763 th iteration => 0.0299104705631\n",
      "Loss for  11764 th iteration => 0.0299076644279\n",
      "Loss for  11765 th iteration => 0.0299060378417\n",
      "Loss for  11766 th iteration => 0.0299011722213\n",
      "Loss for  11767 th iteration => 0.0298980133943\n",
      "Loss for  11768 th iteration => 0.0298974830426\n",
      "Loss for  11769 th iteration => 0.029892830401\n",
      "Loss for  11770 th iteration => 0.0298903234197\n",
      "Loss for  11771 th iteration => 0.0298881332897\n",
      "Loss for  11772 th iteration => 0.0298831942265\n",
      "Loss for  11773 th iteration => 0.0298809157752\n",
      "Loss for  11774 th iteration => 0.0298786748271\n",
      "Loss for  11775 th iteration => 0.0298752537197\n",
      "Loss for  11776 th iteration => 0.0298722257865\n",
      "Loss for  11777 th iteration => 0.0298705541806\n",
      "Loss for  11778 th iteration => 0.029865279004\n",
      "Loss for  11779 th iteration => 0.029864218383\n",
      "Loss for  11780 th iteration => 0.0298603184139\n",
      "Loss for  11781 th iteration => 0.0298574477806\n",
      "Loss for  11782 th iteration => 0.0298549107655\n",
      "Loss for  11783 th iteration => 0.0298518158586\n",
      "Loss for  11784 th iteration => 0.0298475834154\n",
      "Loss for  11785 th iteration => 0.0298471831248\n",
      "Loss for  11786 th iteration => 0.0298423448966\n",
      "Loss for  11787 th iteration => 0.0298393996105\n",
      "Loss for  11788 th iteration => 0.0298378947197\n",
      "Loss for  11789 th iteration => 0.0298330163627\n",
      "Loss for  11790 th iteration => 0.029829899114\n",
      "Loss for  11791 th iteration => 0.0298292763175\n",
      "Loss for  11792 th iteration => 0.0298248052964\n",
      "Loss for  11793 th iteration => 0.0298220882637\n",
      "Loss for  11794 th iteration => 0.029820122975\n",
      "Loss for  11795 th iteration => 0.0298150551651\n",
      "Loss for  11796 th iteration => 0.0298129266755\n",
      "Loss for  11797 th iteration => 0.0298105334455\n",
      "Loss for  11798 th iteration => 0.0298072823559\n",
      "Loss for  11799 th iteration => 0.0298041011958\n",
      "Loss for  11800 th iteration => 0.0298025983304\n",
      "Loss for  11801 th iteration => 0.0297972797199\n",
      "Loss for  11802 th iteration => 0.0297970985738\n",
      "Loss for  11803 th iteration => 0.0297920067899\n",
      "Loss for  11804 th iteration => 0.0297893356843\n",
      "Loss for  11805 th iteration => 0.0297876245512\n",
      "Loss for  11806 th iteration => 0.029782823998\n",
      "Loss for  11807 th iteration => 0.0297796581038\n",
      "Loss for  11808 th iteration => 0.0297792368677\n",
      "Loss for  11809 th iteration => 0.0297744753444\n",
      "Loss for  11810 th iteration => 0.0297721717297\n",
      "Loss for  11811 th iteration => 0.0297698115467\n",
      "Loss for  11812 th iteration => 0.0297650219311\n",
      "Loss for  11813 th iteration => 0.0297626447327\n",
      "Loss for  11814 th iteration => 0.0297605959946\n",
      "Loss for  11815 th iteration => 0.0297570225674\n",
      "Loss for  11816 th iteration => 0.0297542223782\n",
      "Loss for  11817 th iteration => 0.0297523566342\n",
      "Loss for  11818 th iteration => 0.029747239509\n",
      "Loss for  11819 th iteration => 0.0297451845184\n",
      "Loss for  11820 th iteration => 0.0297427424692\n",
      "Loss for  11821 th iteration => 0.0297395598387\n",
      "Loss for  11822 th iteration => 0.029736335417\n",
      "Loss for  11823 th iteration => 0.0297348927073\n",
      "Loss for  11824 th iteration => 0.0297295696622\n",
      "Loss for  11825 th iteration => 0.0297293940775\n",
      "Loss for  11826 th iteration => 0.0297243246847\n",
      "Loss for  11827 th iteration => 0.0297216674075\n",
      "Loss for  11828 th iteration => 0.0297199555356\n",
      "Loss for  11829 th iteration => 0.0297151788681\n",
      "Loss for  11830 th iteration => 0.029712020732\n",
      "Loss for  11831 th iteration => 0.0297116189251\n",
      "Loss for  11832 th iteration => 0.0297068705653\n",
      "Loss for  11833 th iteration => 0.0297045771642\n",
      "Loss for  11834 th iteration => 0.0297022243421\n",
      "Loss for  11835 th iteration => 0.0296974460576\n",
      "Loss for  11836 th iteration => 0.0296950838831\n",
      "Loss for  11837 th iteration => 0.0296930602992\n",
      "Loss for  11838 th iteration => 0.0296894827689\n",
      "Loss for  11839 th iteration => 0.0296867203504\n",
      "Loss for  11840 th iteration => 0.0296848347533\n",
      "Loss for  11841 th iteration => 0.0296797469385\n",
      "Loss for  11842 th iteration => 0.0296776896351\n",
      "Loss for  11843 th iteration => 0.0296752945341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  11844 th iteration => 0.0296720864189\n",
      "Loss for  11845 th iteration => 0.0296689235651\n",
      "Loss for  11846 th iteration => 0.0296674375009\n",
      "Loss for  11847 th iteration => 0.0296621403567\n",
      "Loss for  11848 th iteration => 0.0296620116221\n",
      "Loss for  11849 th iteration => 0.0296569110479\n",
      "Loss for  11850 th iteration => 0.029654319668\n",
      "Loss for  11851 th iteration => 0.0296525676064\n",
      "Loss for  11852 th iteration => 0.0296478241169\n",
      "Loss for  11853 th iteration => 0.0296446653862\n",
      "Loss for  11854 th iteration => 0.0296443185705\n",
      "Loss for  11855 th iteration => 0.0296395236713\n",
      "Loss for  11856 th iteration => 0.0296373224227\n",
      "Loss for  11857 th iteration => 0.0296348960203\n",
      "Loss for  11858 th iteration => 0.0296301829364\n",
      "Loss for  11859 th iteration => 0.0296277837635\n",
      "Loss for  11860 th iteration => 0.0296258500422\n",
      "Loss for  11861 th iteration => 0.0296222048915\n",
      "Loss for  11862 th iteration => 0.029619551235\n",
      "Loss for  11863 th iteration => 0.0296175756493\n",
      "Loss for  11864 th iteration => 0.0296125640148\n",
      "Loss for  11865 th iteration => 0.0296104591118\n",
      "Loss for  11866 th iteration => 0.029608166994\n",
      "Loss for  11867 th iteration => 0.0296048783786\n",
      "Loss for  11868 th iteration => 0.0296018385722\n",
      "Loss for  11869 th iteration => 0.0296002484052\n",
      "Loss for  11870 th iteration => 0.0295950046547\n",
      "Loss for  11871 th iteration => 0.0295940081959\n",
      "Loss for  11872 th iteration => 0.0295901657404\n",
      "Loss for  11873 th iteration => 0.0295873517941\n",
      "Loss for  11874 th iteration => 0.0295848609914\n",
      "Loss for  11875 th iteration => 0.0295817845753\n",
      "Loss for  11876 th iteration => 0.0295776050922\n",
      "Loss for  11877 th iteration => 0.0295773248104\n",
      "Loss for  11878 th iteration => 0.0295724422703\n",
      "Loss for  11879 th iteration => 0.0295696820329\n",
      "Loss for  11880 th iteration => 0.0295680812725\n",
      "Loss for  11881 th iteration => 0.0295633137944\n",
      "Loss for  11882 th iteration => 0.0295602146402\n",
      "Loss for  11883 th iteration => 0.0295597520114\n",
      "Loss for  11884 th iteration => 0.029555179658\n",
      "Loss for  11885 th iteration => 0.0295527236123\n",
      "Loss for  11886 th iteration => 0.0295505702879\n",
      "Loss for  11887 th iteration => 0.0295456960276\n",
      "Loss for  11888 th iteration => 0.029543484338\n",
      "Loss for  11889 th iteration => 0.0295413624999\n",
      "Loss for  11890 th iteration => 0.0295379274026\n",
      "Loss for  11891 th iteration => 0.0295350876878\n",
      "Loss for  11892 th iteration => 0.0295333173177\n",
      "Loss for  11893 th iteration => 0.0295281889033\n",
      "Loss for  11894 th iteration => 0.0295262287148\n",
      "Loss for  11895 th iteration => 0.029523802948\n",
      "Loss for  11896 th iteration => 0.0295206709719\n",
      "Loss for  11897 th iteration => 0.0295175047148\n",
      "Loss for  11898 th iteration => 0.0295160609189\n",
      "Loss for  11899 th iteration => 0.0295107910223\n",
      "Loss for  11900 th iteration => 0.0295106908183\n",
      "Loss for  11901 th iteration => 0.0295056060651\n",
      "Loss for  11902 th iteration => 0.0295030782804\n",
      "Loss for  11903 th iteration => 0.0295013099184\n",
      "Loss for  11904 th iteration => 0.0294966068551\n",
      "Loss for  11905 th iteration => 0.0294934790199\n",
      "Loss for  11906 th iteration => 0.0294931867377\n",
      "Loss for  11907 th iteration => 0.0294883787519\n",
      "Loss for  11908 th iteration => 0.0294862720968\n",
      "Loss for  11909 th iteration => 0.0294837911864\n",
      "Loss for  11910 th iteration => 0.0294791495915\n",
      "Loss for  11911 th iteration => 0.0294767393772\n",
      "Loss for  11912 th iteration => 0.0294749112601\n",
      "Loss for  11913 th iteration => 0.0294712085359\n",
      "Loss for  11914 th iteration => 0.0294687007916\n",
      "Loss for  11915 th iteration => 0.0294666200124\n",
      "Loss for  11916 th iteration => 0.0294617134877\n",
      "Loss for  11917 th iteration => 0.0294595651068\n",
      "Loss for  11918 th iteration => 0.0294574192059\n",
      "Loss for  11919 th iteration => 0.0294540331634\n",
      "Loss for  11920 th iteration => 0.0294511835756\n",
      "Loss for  11921 th iteration => 0.0294494444529\n",
      "Loss for  11922 th iteration => 0.0294443142778\n",
      "Loss for  11923 th iteration => 0.0294433214158\n",
      "Loss for  11924 th iteration => 0.0294395449264\n",
      "Loss for  11925 th iteration => 0.0294367287784\n",
      "Loss for  11926 th iteration => 0.0294343234619\n",
      "Loss for  11927 th iteration => 0.0294311699475\n",
      "Loss for  11928 th iteration => 0.0294270485567\n",
      "Loss for  11929 th iteration => 0.0294268905027\n",
      "Loss for  11930 th iteration => 0.0294219025047\n",
      "Loss for  11931 th iteration => 0.0294193236195\n",
      "Loss for  11932 th iteration => 0.0294176170184\n",
      "Loss for  11933 th iteration => 0.0294129117804\n",
      "Loss for  11934 th iteration => 0.0294098237376\n",
      "Loss for  11935 th iteration => 0.0294094963137\n",
      "Loss for  11936 th iteration => 0.0294047755414\n",
      "Loss for  11937 th iteration => 0.0294025995811\n",
      "Loss for  11938 th iteration => 0.0294002084658\n",
      "Loss for  11939 th iteration => 0.0293955282595\n",
      "Loss for  11940 th iteration => 0.029393187065\n",
      "Loss for  11941 th iteration => 0.0293913175699\n",
      "Loss for  11942 th iteration => 0.0293876807949\n",
      "Loss for  11943 th iteration => 0.0293851475888\n",
      "Loss for  11944 th iteration => 0.0293831133135\n",
      "Loss for  11945 th iteration => 0.0293781966515\n",
      "Loss for  11946 th iteration => 0.0293760898417\n",
      "Loss for  11947 th iteration => 0.0293739373078\n",
      "Loss for  11948 th iteration => 0.0293705831319\n",
      "Loss for  11949 th iteration => 0.0293677460633\n",
      "Loss for  11950 th iteration => 0.0293660159417\n",
      "Loss for  11951 th iteration => 0.0293609107474\n",
      "Loss for  11952 th iteration => 0.0293599167205\n",
      "Loss for  11953 th iteration => 0.0293561762296\n",
      "Loss for  11954 th iteration => 0.0293533590945\n",
      "Loss for  11955 th iteration => 0.0293509936748\n",
      "Loss for  11956 th iteration => 0.0293478207549\n",
      "Loss for  11957 th iteration => 0.0293437337281\n",
      "Loss for  11958 th iteration => 0.029343600213\n",
      "Loss for  11959 th iteration => 0.0293386033617\n",
      "Loss for  11960 th iteration => 0.0293360761914\n",
      "Loss for  11961 th iteration => 0.0293343525444\n",
      "Loss for  11962 th iteration => 0.029329664749\n",
      "Loss for  11963 th iteration => 0.0293265987931\n",
      "Loss for  11964 th iteration => 0.0293263075868\n",
      "Loss for  11965 th iteration => 0.0293215572239\n",
      "Loss for  11966 th iteration => 0.0293194707182\n",
      "Loss for  11967 th iteration => 0.0293170126142\n",
      "Loss for  11968 th iteration => 0.0293123973256\n",
      "Loss for  11969 th iteration => 0.0293100252562\n",
      "Loss for  11970 th iteration => 0.029308240328\n",
      "Loss for  11971 th iteration => 0.0293045459117\n",
      "Loss for  11972 th iteration => 0.0293021250034\n",
      "Loss for  11973 th iteration => 0.0293000011743\n",
      "Loss for  11974 th iteration => 0.0292951644765\n",
      "Loss for  11975 th iteration => 0.0292930122638\n",
      "Loss for  11976 th iteration => 0.0292909623926\n",
      "Loss for  11977 th iteration => 0.0292875328118\n",
      "Loss for  11978 th iteration => 0.0292848277601\n",
      "Loss for  11979 th iteration => 0.0292829886032\n",
      "Loss for  11980 th iteration => 0.0292779640557\n",
      "Loss for  11981 th iteration => 0.0292769569747\n",
      "Loss for  11982 th iteration => 0.0292732664301\n",
      "Loss for  11983 th iteration => 0.0292704343511\n",
      "Loss for  11984 th iteration => 0.0292681352781\n",
      "Loss for  11985 th iteration => 0.0292648992151\n",
      "Loss for  11986 th iteration => 0.0292608554505\n",
      "Loss for  11987 th iteration => 0.0292607880194\n",
      "Loss for  11988 th iteration => 0.0292557285557\n",
      "Loss for  11989 th iteration => 0.0292533052538\n",
      "Loss for  11990 th iteration => 0.0292515248478\n",
      "Loss for  11991 th iteration => 0.0292468707662\n",
      "Loss for  11992 th iteration => 0.0292449528544\n",
      "Loss for  11993 th iteration => 0.0292426577579\n",
      "Loss for  11994 th iteration => 0.0292392082496\n",
      "Loss for  11995 th iteration => 0.0292361494575\n",
      "Loss for  11996 th iteration => 0.0292340012872\n",
      "Loss for  11997 th iteration => 0.0292307346355\n",
      "Loss for  11998 th iteration => 0.0292267147726\n",
      "Loss for  11999 th iteration => 0.0292255956565\n",
      "Loss for  12000 th iteration => 0.0292221699465\n",
      "Loss for  12001 th iteration => 0.0292197247177\n",
      "Loss for  12002 th iteration => 0.0292166698623\n",
      "Loss for  12003 th iteration => 0.0292135855229\n",
      "Loss for  12004 th iteration => 0.0292102007595\n",
      "Loss for  12005 th iteration => 0.029207724866\n",
      "Loss for  12006 th iteration => 0.0292051393449\n",
      "Loss for  12007 th iteration => 0.0292026171411\n",
      "Loss for  12008 th iteration => 0.0291996454434\n",
      "Loss for  12009 th iteration => 0.0291965343961\n",
      "Loss for  12010 th iteration => 0.02919328889\n",
      "Loss for  12011 th iteration => 0.0291914988902\n",
      "Loss for  12012 th iteration => 0.0291878439937\n",
      "Loss for  12013 th iteration => 0.0291854029898\n",
      "Loss for  12014 th iteration => 0.0291833287721\n",
      "Loss for  12015 th iteration => 0.0291785099995\n",
      "Loss for  12016 th iteration => 0.0291773039444\n",
      "Loss for  12017 th iteration => 0.029173951474\n",
      "Loss for  12018 th iteration => 0.0291708244084\n",
      "Loss for  12019 th iteration => 0.0291688519472\n",
      "Loss for  12020 th iteration => 0.0291653270856\n",
      "Loss for  12021 th iteration => 0.0291614982877\n",
      "Loss for  12022 th iteration => 0.0291603176536\n",
      "Loss for  12023 th iteration => 0.0291569355042\n",
      "Loss for  12024 th iteration => 0.0291538443331\n",
      "Loss for  12025 th iteration => 0.0291518042924\n",
      "Loss for  12026 th iteration => 0.0291484125525\n",
      "Loss for  12027 th iteration => 0.0291445087805\n",
      "Loss for  12028 th iteration => 0.0291433478366\n",
      "Loss for  12029 th iteration => 0.0291399439734\n",
      "Loss for  12030 th iteration => 0.0291376001014\n",
      "Loss for  12031 th iteration => 0.0291344736799\n",
      "Loss for  12032 th iteration => 0.0291314474235\n",
      "Loss for  12033 th iteration => 0.0291279917092\n",
      "Loss for  12034 th iteration => 0.0291256165496\n",
      "Loss for  12035 th iteration => 0.0291230053084\n",
      "Loss for  12036 th iteration => 0.0291205836741\n",
      "Loss for  12037 th iteration => 0.0291175411444\n",
      "Loss for  12038 th iteration => 0.0291144873748\n",
      "Loss for  12039 th iteration => 0.0291111278634\n",
      "Loss for  12040 th iteration => 0.0291095671743\n",
      "Loss for  12041 th iteration => 0.029105712538\n",
      "Loss for  12042 th iteration => 0.029103550214\n",
      "Loss for  12043 th iteration => 0.029101221933\n",
      "Loss for  12044 th iteration => 0.0290965712448\n",
      "Loss for  12045 th iteration => 0.0290943152509\n",
      "Loss for  12046 th iteration => 0.0290925330395\n",
      "Loss for  12047 th iteration => 0.0290889006145\n",
      "Loss for  12048 th iteration => 0.0290865015717\n",
      "Loss for  12049 th iteration => 0.0290844111087\n",
      "Loss for  12050 th iteration => 0.0290796424052\n",
      "Loss for  12051 th iteration => 0.0290784224494\n",
      "Loss for  12052 th iteration => 0.0290751096409\n",
      "Loss for  12053 th iteration => 0.0290719841386\n",
      "Loss for  12054 th iteration => 0.0290700681563\n",
      "Loss for  12055 th iteration => 0.029066506517\n",
      "Loss for  12056 th iteration => 0.0290627423274\n",
      "Loss for  12057 th iteration => 0.0290615449567\n",
      "Loss for  12058 th iteration => 0.0290582060875\n",
      "Loss for  12059 th iteration => 0.0290551128678\n",
      "Loss for  12060 th iteration => 0.0290531366095\n",
      "Loss for  12061 th iteration => 0.0290496952833\n",
      "Loss for  12062 th iteration => 0.0290458639692\n",
      "Loss for  12063 th iteration => 0.0290446839452\n",
      "Loss for  12064 th iteration => 0.0290413254304\n",
      "Loss for  12065 th iteration => 0.0290382581695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  12066 th iteration => 0.0290362333309\n",
      "Loss for  12067 th iteration => 0.0290328936309\n",
      "Loss for  12068 th iteration => 0.0290290069017\n",
      "Loss for  12069 th iteration => 0.0290278396437\n",
      "Loss for  12070 th iteration => 0.029024488839\n",
      "Loss for  12071 th iteration => 0.0290221185544\n",
      "Loss for  12072 th iteration => 0.0290190601348\n",
      "Loss for  12073 th iteration => 0.0290160328397\n",
      "Loss for  12074 th iteration => 0.0290126261048\n",
      "Loss for  12075 th iteration => 0.0290102247657\n",
      "Loss for  12076 th iteration => 0.0290076739976\n",
      "Loss for  12077 th iteration => 0.0290052448243\n",
      "Loss for  12078 th iteration => 0.0290022513519\n",
      "Loss for  12079 th iteration => 0.0289992048598\n",
      "Loss for  12080 th iteration => 0.0289958982191\n",
      "Loss for  12081 th iteration => 0.0289942912359\n",
      "Loss for  12082 th iteration => 0.0289905170132\n",
      "Loss for  12083 th iteration => 0.0289883459561\n",
      "Loss for  12084 th iteration => 0.0289860555156\n",
      "Loss for  12085 th iteration => 0.0289814358878\n",
      "Loss for  12086 th iteration => 0.0289812892442\n",
      "Loss for  12087 th iteration => 0.0289759743353\n",
      "Loss for  12088 th iteration => 0.0289742761194\n",
      "Loss for  12089 th iteration => 0.0289719768186\n",
      "Loss for  12090 th iteration => 0.0289678567536\n",
      "Loss for  12091 th iteration => 0.0289667374552\n",
      "Loss for  12092 th iteration => 0.0289623089144\n",
      "Loss for  12093 th iteration => 0.0289598313051\n",
      "Loss for  12094 th iteration => 0.0289573995789\n",
      "Loss for  12095 th iteration => 0.0289547933006\n",
      "Loss for  12096 th iteration => 0.0289519943922\n",
      "Loss for  12097 th iteration => 0.0289487481504\n",
      "Loss for  12098 th iteration => 0.0289455980745\n",
      "Loss for  12099 th iteration => 0.0289430623202\n",
      "Loss for  12100 th iteration => 0.028940653627\n",
      "Loss for  12101 th iteration => 0.0289380078817\n",
      "Loss for  12102 th iteration => 0.0289352544725\n",
      "Loss for  12103 th iteration => 0.0289319991987\n",
      "Loss for  12104 th iteration => 0.0289288981779\n",
      "Loss for  12105 th iteration => 0.0289263155997\n",
      "Loss for  12106 th iteration => 0.028923923876\n",
      "Loss for  12107 th iteration => 0.0289212485855\n",
      "Loss for  12108 th iteration => 0.0289185307476\n",
      "Loss for  12109 th iteration => 0.028915270837\n",
      "Loss for  12110 th iteration => 0.0289122348036\n",
      "Loss for  12111 th iteration => 0.0289104572835\n",
      "Loss for  12112 th iteration => 0.0289068736748\n",
      "Loss for  12113 th iteration => 0.0289044467566\n",
      "Loss for  12114 th iteration => 0.0289024299892\n",
      "Loss for  12115 th iteration => 0.028897599056\n",
      "Loss for  12116 th iteration => 0.0288955927609\n",
      "Loss for  12117 th iteration => 0.028893696935\n",
      "Loss for  12118 th iteration => 0.0288909684864\n",
      "Loss for  12119 th iteration => 0.0288873013307\n",
      "Loss for  12120 th iteration => 0.0288857592815\n",
      "Loss for  12121 th iteration => 0.0288812736674\n",
      "Loss for  12122 th iteration => 0.0288791634669\n",
      "Loss for  12123 th iteration => 0.0288774379003\n",
      "Loss for  12124 th iteration => 0.0288734736091\n",
      "Loss for  12125 th iteration => 0.0288715602455\n",
      "Loss for  12126 th iteration => 0.0288680405815\n",
      "Loss for  12127 th iteration => 0.0288643251993\n",
      "Loss for  12128 th iteration => 0.0288631690031\n",
      "Loss for  12129 th iteration => 0.0288598516577\n",
      "Loss for  12130 th iteration => 0.028856818974\n",
      "Loss for  12131 th iteration => 0.0288548618172\n",
      "Loss for  12132 th iteration => 0.0288514352084\n",
      "Loss for  12133 th iteration => 0.0288476696596\n",
      "Loss for  12134 th iteration => 0.0288465246496\n",
      "Loss for  12135 th iteration => 0.0288431952185\n",
      "Loss for  12136 th iteration => 0.0288401809431\n",
      "Loss for  12137 th iteration => 0.0288381895067\n",
      "Loss for  12138 th iteration => 0.0288348406807\n",
      "Loss for  12139 th iteration => 0.0288310345259\n",
      "Loss for  12140 th iteration => 0.0288298970152\n",
      "Loss for  12141 th iteration => 0.0288265600399\n",
      "Loss for  12142 th iteration => 0.0288235596988\n",
      "Loss for  12143 th iteration => 0.0288215423515\n",
      "Loss for  12144 th iteration => 0.0288182578155\n",
      "Loss for  12145 th iteration => 0.0288144194935\n",
      "Loss for  12146 th iteration => 0.02881328627\n",
      "Loss for  12147 th iteration => 0.0288099491098\n",
      "Loss for  12148 th iteration => 0.028807670033\n",
      "Loss for  12149 th iteration => 0.0288045974142\n",
      "Loss for  12150 th iteration => 0.0288016503648\n",
      "Loss for  12151 th iteration => 0.0287982386787\n",
      "Loss for  12152 th iteration => 0.0287959397238\n",
      "Loss for  12153 th iteration => 0.0287933669646\n",
      "Loss for  12154 th iteration => 0.0287910489046\n",
      "Loss for  12155 th iteration => 0.0287880211993\n",
      "Loss for  12156 th iteration => 0.028785062908\n",
      "Loss for  12157 th iteration => 0.0287816982497\n",
      "Loss for  12158 th iteration => 0.0287793551487\n",
      "Loss for  12159 th iteration => 0.0287768009083\n",
      "Loss for  12160 th iteration => 0.0287744530319\n",
      "Loss for  12161 th iteration => 0.0287714610678\n",
      "Loss for  12162 th iteration => 0.0287684956204\n",
      "Loss for  12163 th iteration => 0.0287652006169\n",
      "Loss for  12164 th iteration => 0.0287636758596\n",
      "Loss for  12165 th iteration => 0.0287598900336\n",
      "Loss for  12166 th iteration => 0.0287578411195\n",
      "Loss for  12167 th iteration => 0.0287554885327\n",
      "Loss for  12168 th iteration => 0.0287510014241\n",
      "Loss for  12169 th iteration => 0.0287498386911\n",
      "Loss for  12170 th iteration => 0.0287461097197\n",
      "Loss for  12171 th iteration => 0.0287439015205\n",
      "Loss for  12172 th iteration => 0.0287412019203\n",
      "Loss for  12173 th iteration => 0.0287385722515\n",
      "Loss for  12174 th iteration => 0.0287352797561\n",
      "Loss for  12175 th iteration => 0.0287332356734\n",
      "Loss for  12176 th iteration => 0.0287290734359\n",
      "Loss for  12177 th iteration => 0.0287274338815\n",
      "Loss for  12178 th iteration => 0.0287251391935\n",
      "Loss for  12179 th iteration => 0.0287211288945\n",
      "Loss for  12180 th iteration => 0.0287199835102\n",
      "Loss for  12181 th iteration => 0.0287155976875\n",
      "Loss for  12182 th iteration => 0.0287132586807\n",
      "Loss for  12183 th iteration => 0.0287107627784\n",
      "Loss for  12184 th iteration => 0.0287083210178\n",
      "Loss for  12185 th iteration => 0.0287054460346\n",
      "Loss for  12186 th iteration => 0.0287023280904\n",
      "Loss for  12187 th iteration => 0.0286991401686\n",
      "Loss for  12188 th iteration => 0.0286967690966\n",
      "Loss for  12189 th iteration => 0.0286942841978\n",
      "Loss for  12190 th iteration => 0.0286918241355\n",
      "Loss for  12191 th iteration => 0.0286889733427\n",
      "Loss for  12192 th iteration => 0.0286858543969\n",
      "Loss for  12193 th iteration => 0.0286826954213\n",
      "Loss for  12194 th iteration => 0.0286803001307\n",
      "Loss for  12195 th iteration => 0.0286778221041\n",
      "Loss for  12196 th iteration => 0.028675350561\n",
      "Loss for  12197 th iteration => 0.0286725171325\n",
      "Loss for  12198 th iteration => 0.0286694001604\n",
      "Loss for  12199 th iteration => 0.0286662640286\n",
      "Loss for  12200 th iteration => 0.0286638514292\n",
      "Loss for  12201 th iteration => 0.0286613766743\n",
      "Loss for  12202 th iteration => 0.0286588996262\n",
      "Loss for  12203 th iteration => 0.0286560775811\n",
      "Loss for  12204 th iteration => 0.0286529651632\n",
      "Loss for  12205 th iteration => 0.0286498587906\n",
      "Loss for  12206 th iteration => 0.0286483058499\n",
      "Loss for  12207 th iteration => 0.0286445805458\n",
      "Loss for  12208 th iteration => 0.0286424426142\n",
      "Loss for  12209 th iteration => 0.0286402069291\n",
      "Loss for  12210 th iteration => 0.0286356274324\n",
      "Loss for  12211 th iteration => 0.0286345924235\n",
      "Loss for  12212 th iteration => 0.028630873658\n",
      "Loss for  12213 th iteration => 0.0286287197371\n",
      "Loss for  12214 th iteration => 0.0286259310583\n",
      "Loss for  12215 th iteration => 0.0286234311387\n",
      "Loss for  12216 th iteration => 0.0286200094726\n",
      "Loss for  12217 th iteration => 0.0286171195583\n",
      "Loss for  12218 th iteration => 0.0286144820943\n",
      "Loss for  12219 th iteration => 0.0286123245303\n",
      "Loss for  12220 th iteration => 0.0286095383682\n",
      "Loss for  12221 th iteration => 0.028607041804\n",
      "Loss for  12222 th iteration => 0.0286036311164\n",
      "Loss for  12223 th iteration => 0.0286007532281\n",
      "Loss for  12224 th iteration => 0.0285981102692\n",
      "Loss for  12225 th iteration => 0.0285959461666\n",
      "Loss for  12226 th iteration => 0.0285931674278\n",
      "Loss for  12227 th iteration => 0.0285906693065\n",
      "Loss for  12228 th iteration => 0.0285872716315\n",
      "Loss for  12229 th iteration => 0.0285844012984\n",
      "Loss for  12230 th iteration => 0.0285817579124\n",
      "Loss for  12231 th iteration => 0.0285795847743\n",
      "Loss for  12232 th iteration => 0.0285768177283\n",
      "Loss for  12233 th iteration => 0.0285743137744\n",
      "Loss for  12234 th iteration => 0.0285709308501\n",
      "Loss for  12235 th iteration => 0.0285680641561\n",
      "Loss for  12236 th iteration => 0.0285654247821\n",
      "Loss for  12237 th iteration => 0.0285632404644\n",
      "Loss for  12238 th iteration => 0.0285604888166\n",
      "Loss for  12239 th iteration => 0.0285579753184\n",
      "Loss for  12240 th iteration => 0.0285546086217\n",
      "Loss for  12241 th iteration => 0.0285517421416\n",
      "Loss for  12242 th iteration => 0.0285491178677\n",
      "Loss for  12243 th iteration => 0.0285476763342\n",
      "Loss for  12244 th iteration => 0.0285437982351\n",
      "Loss for  12245 th iteration => 0.0285416798283\n",
      "Loss for  12246 th iteration => 0.0285386545072\n",
      "Loss for  12247 th iteration => 0.0285347580439\n",
      "Loss for  12248 th iteration => 0.0285337100535\n",
      "Loss for  12249 th iteration => 0.0285305130764\n",
      "Loss for  12250 th iteration => 0.0285280090851\n",
      "Loss for  12251 th iteration => 0.0285252601431\n",
      "Loss for  12252 th iteration => 0.0285221469518\n",
      "Loss for  12253 th iteration => 0.0285190499391\n",
      "Loss for  12254 th iteration => 0.0285165723554\n",
      "Loss for  12255 th iteration => 0.0285142244202\n",
      "Loss for  12256 th iteration => 0.0285117318083\n",
      "Loss for  12257 th iteration => 0.0285089772981\n",
      "Loss for  12258 th iteration => 0.0285058763983\n",
      "Loss for  12259 th iteration => 0.0285027819485\n",
      "Loss for  12260 th iteration => 0.028500310079\n",
      "Loss for  12261 th iteration => 0.0284979528424\n",
      "Loss for  12262 th iteration => 0.02849547478\n",
      "Loss for  12263 th iteration => 0.0284927115251\n",
      "Loss for  12264 th iteration => 0.0284896241825\n",
      "Loss for  12265 th iteration => 0.0284865293599\n",
      "Loss for  12266 th iteration => 0.0284840665279\n",
      "Loss for  12267 th iteration => 0.0284816984215\n",
      "Loss for  12268 th iteration => 0.0284792376509\n",
      "Loss for  12269 th iteration => 0.0284764629019\n",
      "Loss for  12270 th iteration => 0.0284733909035\n",
      "Loss for  12271 th iteration => 0.028471338354\n",
      "Loss for  12272 th iteration => 0.0284672865898\n",
      "Loss for  12273 th iteration => 0.028465567975\n",
      "Loss for  12274 th iteration => 0.0284634164801\n",
      "Loss for  12275 th iteration => 0.0284593862614\n",
      "Loss for  12276 th iteration => 0.0284583480656\n",
      "Loss for  12277 th iteration => 0.0284539338224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  12278 th iteration => 0.0284518100444\n",
      "Loss for  12279 th iteration => 0.0284491196512\n",
      "Loss for  12280 th iteration => 0.0284469798993\n",
      "Loss for  12281 th iteration => 0.0284438964147\n",
      "Loss for  12282 th iteration => 0.0284410236892\n",
      "Loss for  12283 th iteration => 0.0284377468745\n",
      "Loss for  12284 th iteration => 0.0284365045719\n",
      "Loss for  12285 th iteration => 0.0284325380609\n",
      "Loss for  12286 th iteration => 0.0284307837244\n",
      "Loss for  12287 th iteration => 0.0284282227995\n",
      "Loss for  12288 th iteration => 0.0284239570269\n",
      "Loss for  12289 th iteration => 0.0284226829559\n",
      "Loss for  12290 th iteration => 0.0284193025763\n",
      "Loss for  12291 th iteration => 0.0284169199652\n",
      "Loss for  12292 th iteration => 0.0284144894944\n",
      "Loss for  12293 th iteration => 0.0284117070171\n",
      "Loss for  12294 th iteration => 0.0284085347807\n",
      "Loss for  12295 th iteration => 0.028405459326\n",
      "Loss for  12296 th iteration => 0.0284031432351\n",
      "Loss for  12297 th iteration => 0.0284007560411\n",
      "Loss for  12298 th iteration => 0.0283983323413\n",
      "Loss for  12299 th iteration => 0.0283955488346\n",
      "Loss for  12300 th iteration => 0.0283923871158\n",
      "Loss for  12301 th iteration => 0.0283893179269\n",
      "Loss for  12302 th iteration => 0.0283870027916\n",
      "Loss for  12303 th iteration => 0.0283846088751\n",
      "Loss for  12304 th iteration => 0.0283821955982\n",
      "Loss for  12305 th iteration => 0.0283794074041\n",
      "Loss for  12306 th iteration => 0.0283762576971\n",
      "Loss for  12307 th iteration => 0.0283731914229\n",
      "Loss for  12308 th iteration => 0.0283708810376\n",
      "Loss for  12309 th iteration => 0.0283684785589\n",
      "Loss for  12310 th iteration => 0.028366078877\n",
      "Loss for  12311 th iteration => 0.028363282817\n",
      "Loss for  12312 th iteration => 0.0283601463937\n",
      "Loss for  12313 th iteration => 0.0283570801011\n",
      "Loss for  12314 th iteration => 0.0283547777866\n",
      "Loss for  12315 th iteration => 0.0283523651707\n",
      "Loss for  12316 th iteration => 0.028349981831\n",
      "Loss for  12317 th iteration => 0.0283471751512\n",
      "Loss for  12318 th iteration => 0.0283440530872\n",
      "Loss for  12319 th iteration => 0.0283409842133\n",
      "Loss for  12320 th iteration => 0.0283386928705\n",
      "Loss for  12321 th iteration => 0.0283362687768\n",
      "Loss for  12322 th iteration => 0.0283339041506\n",
      "Loss for  12323 th iteration => 0.0283310844727\n",
      "Loss for  12324 th iteration => 0.0283279776701\n",
      "Loss for  12325 th iteration => 0.0283249039802\n",
      "Loss for  12326 th iteration => 0.028322626138\n",
      "Loss for  12327 th iteration => 0.0283201894327\n",
      "Loss for  12328 th iteration => 0.0283178455587\n",
      "Loss for  12329 th iteration => 0.0283150108366\n",
      "Loss for  12330 th iteration => 0.0283119200449\n",
      "Loss for  12331 th iteration => 0.0283088395947\n",
      "Loss for  12332 th iteration => 0.0283065774527\n",
      "Loss for  12333 th iteration => 0.0283041271842\n",
      "Loss for  12334 th iteration => 0.028301805807\n",
      "Loss for  12335 th iteration => 0.0282989542884\n",
      "Loss for  12336 th iteration => 0.0282958801226\n",
      "Loss for  12337 th iteration => 0.0282927912248\n",
      "Loss for  12338 th iteration => 0.0282905466908\n",
      "Loss for  12339 th iteration => 0.0282880820687\n",
      "Loss for  12340 th iteration => 0.028285784673\n",
      "Loss for  12341 th iteration => 0.0282829148653\n",
      "Loss for  12342 th iteration => 0.0282798578214\n",
      "Loss for  12343 th iteration => 0.0282767590167\n",
      "Loss for  12344 th iteration => 0.0282745337402\n",
      "Loss for  12345 th iteration => 0.0282720541159\n",
      "Loss for  12346 th iteration => 0.0282697819568\n",
      "Loss for  12347 th iteration => 0.0282668925967\n",
      "Loss for  12348 th iteration => 0.0282638593235\n",
      "Loss for  12349 th iteration => 0.0282619130675\n",
      "Loss for  12350 th iteration => 0.0282576098816\n",
      "Loss for  12351 th iteration => 0.0282565311098\n",
      "Loss for  12352 th iteration => 0.0282538239882\n",
      "Loss for  12353 th iteration => 0.0282504355368\n",
      "Loss for  12354 th iteration => 0.0282488279869\n",
      "Loss for  12355 th iteration => 0.0282446609688\n",
      "Loss for  12356 th iteration => 0.0282415985235\n",
      "Loss for  12357 th iteration => 0.0282405834608\n",
      "Loss for  12358 th iteration => 0.0282378169548\n",
      "Loss for  12359 th iteration => 0.0282344942628\n",
      "Loss for  12360 th iteration => 0.0282328265232\n",
      "Loss for  12361 th iteration => 0.0282286923763\n",
      "Loss for  12362 th iteration => 0.0282256049197\n",
      "Loss for  12363 th iteration => 0.0282246523414\n",
      "Loss for  12364 th iteration => 0.0282218278071\n",
      "Loss for  12365 th iteration => 0.0282185695203\n",
      "Loss for  12366 th iteration => 0.0282168429316\n",
      "Loss for  12367 th iteration => 0.0282127408464\n",
      "Loss for  12368 th iteration => 0.028209629017\n",
      "Loss for  12369 th iteration => 0.0282087377581\n",
      "Loss for  12370 th iteration => 0.0282058564962\n",
      "Loss for  12371 th iteration => 0.0282026613149\n",
      "Loss for  12372 th iteration => 0.0282008771637\n",
      "Loss for  12373 th iteration => 0.0281968063548\n",
      "Loss for  12374 th iteration => 0.0281936707642\n",
      "Loss for  12375 th iteration => 0.0281928397145\n",
      "Loss for  12376 th iteration => 0.028189902975\n",
      "Loss for  12377 th iteration => 0.0281867696489\n",
      "Loss for  12378 th iteration => 0.0281849291725\n",
      "Loss for  12379 th iteration => 0.0281808888777\n",
      "Loss for  12380 th iteration => 0.028177730113\n",
      "Loss for  12381 th iteration => 0.0281769582114\n",
      "Loss for  12382 th iteration => 0.0281739671979\n",
      "Loss for  12383 th iteration => 0.0281708945225\n",
      "Loss for  12384 th iteration => 0.0281689989127\n",
      "Loss for  12385 th iteration => 0.028164988391\n",
      "Loss for  12386 th iteration => 0.0281618070167\n",
      "Loss for  12387 th iteration => 0.0281610932474\n",
      "Loss for  12388 th iteration => 0.0281580491206\n",
      "Loss for  12389 th iteration => 0.0281550359333\n",
      "Loss for  12390 th iteration => 0.0281530863401\n",
      "Loss for  12391 th iteration => 0.028149104871\n",
      "Loss for  12392 th iteration => 0.028145901431\n",
      "Loss for  12393 th iteration => 0.0281452448189\n",
      "Loss for  12394 th iteration => 0.0281421486998\n",
      "Loss for  12395 th iteration => 0.0281391938771\n",
      "Loss for  12396 th iteration => 0.0281371914119\n",
      "Loss for  12397 th iteration => 0.028133238294\n",
      "Loss for  12398 th iteration => 0.0281300133127\n",
      "Loss for  12399 th iteration => 0.0281294129205\n",
      "Loss for  12400 th iteration => 0.0281262658936\n",
      "Loss for  12401 th iteration => 0.0281233683477\n",
      "Loss for  12402 th iteration => 0.0281213140862\n",
      "Loss for  12403 th iteration => 0.0281173925415\n",
      "Loss for  12404 th iteration => 0.0281152947338\n",
      "Loss for  12405 th iteration => 0.0281127121347\n",
      "Loss for  12406 th iteration => 0.0281108420311\n",
      "Loss for  12407 th iteration => 0.0281076069678\n",
      "Loss for  12408 th iteration => 0.028104976964\n",
      "Loss for  12409 th iteration => 0.0281025165081\n",
      "Loss for  12410 th iteration => 0.0280982015324\n",
      "Loss for  12411 th iteration => 0.0280968690224\n",
      "Loss for  12412 th iteration => 0.0280950426957\n",
      "Loss for  12413 th iteration => 0.0280917694653\n",
      "Loss for  12414 th iteration => 0.0280891835928\n",
      "Loss for  12415 th iteration => 0.0280866748278\n",
      "Loss for  12416 th iteration => 0.0280823899196\n",
      "Loss for  12417 th iteration => 0.0280819149872\n",
      "Loss for  12418 th iteration => 0.0280786697319\n",
      "Loss for  12419 th iteration => 0.0280762088835\n",
      "Loss for  12420 th iteration => 0.028073516583\n",
      "Loss for  12421 th iteration => 0.0280704276772\n",
      "Loss for  12422 th iteration => 0.0280684596113\n",
      "Loss for  12423 th iteration => 0.0280641920687\n",
      "Loss for  12424 th iteration => 0.0280641984451\n",
      "Loss for  12425 th iteration => 0.0280599918398\n",
      "Loss for  12426 th iteration => 0.0280574868043\n",
      "Loss for  12427 th iteration => 0.0280557593339\n",
      "Loss for  12428 th iteration => 0.0280509838419\n",
      "Loss for  12429 th iteration => 0.0280492391176\n",
      "Loss for  12430 th iteration => 0.0280475591945\n",
      "Loss for  12431 th iteration => 0.0280447370073\n",
      "Loss for  12432 th iteration => 0.0280415516488\n",
      "Loss for  12433 th iteration => 0.0280398114173\n",
      "Loss for  12434 th iteration => 0.0280356739724\n",
      "Loss for  12435 th iteration => 0.0280326458992\n",
      "Loss for  12436 th iteration => 0.0280327044205\n",
      "Loss for  12437 th iteration => 0.0280282256728\n",
      "Loss for  12438 th iteration => 0.0280262770893\n",
      "Loss for  12439 th iteration => 0.0280240085368\n",
      "Loss for  12440 th iteration => 0.0280196720352\n",
      "Loss for  12441 th iteration => 0.0280186665774\n",
      "Loss for  12442 th iteration => 0.0280152512152\n",
      "Loss for  12443 th iteration => 0.0280128616295\n",
      "Loss for  12444 th iteration => 0.0280106368335\n",
      "Loss for  12445 th iteration => 0.0280077323664\n",
      "Loss for  12446 th iteration => 0.0280048900928\n",
      "Loss for  12447 th iteration => 0.028001655548\n",
      "Loss for  12448 th iteration => 0.0279995395813\n",
      "Loss for  12449 th iteration => 0.0279971292028\n",
      "Loss for  12450 th iteration => 0.0279949448164\n",
      "Loss for  12451 th iteration => 0.0279920055153\n",
      "Loss for  12452 th iteration => 0.0279891950311\n",
      "Loss for  12453 th iteration => 0.027986998484\n",
      "Loss for  12454 th iteration => 0.0279829134575\n",
      "Loss for  12455 th iteration => 0.0279819437318\n",
      "Loss for  12456 th iteration => 0.0279792089782\n",
      "Loss for  12457 th iteration => 0.0279759686811\n",
      "Loss for  12458 th iteration => 0.0279743044202\n",
      "Loss for  12459 th iteration => 0.0279702061214\n",
      "Loss for  12460 th iteration => 0.0279671805138\n",
      "Loss for  12461 th iteration => 0.0279663102831\n",
      "Loss for  12462 th iteration => 0.0279634895811\n",
      "Loss for  12463 th iteration => 0.0279603404641\n",
      "Loss for  12464 th iteration => 0.0279585906404\n",
      "Loss for  12465 th iteration => 0.0279545305585\n",
      "Loss for  12466 th iteration => 0.0279514670456\n",
      "Loss for  12467 th iteration => 0.0279506904022\n",
      "Loss for  12468 th iteration => 0.027947788828\n",
      "Loss for  12469 th iteration => 0.0279447259294\n",
      "Loss for  12470 th iteration => 0.0279428954667\n",
      "Loss for  12471 th iteration => 0.0279388717724\n",
      "Loss for  12472 th iteration => 0.0279357742731\n",
      "Loss for  12473 th iteration => 0.0279359433577\n",
      "Loss for  12474 th iteration => 0.0279314094576\n",
      "Loss for  12475 th iteration => 0.0279295347948\n",
      "Loss for  12476 th iteration => 0.0279272173183\n",
      "Loss for  12477 th iteration => 0.0279229587065\n",
      "Loss for  12478 th iteration => 0.0279219067616\n",
      "Loss for  12479 th iteration => 0.0279185697103\n",
      "Loss for  12480 th iteration => 0.027916175484\n",
      "Loss for  12481 th iteration => 0.0279139715982\n",
      "Loss for  12482 th iteration => 0.0279110803444\n",
      "Loss for  12483 th iteration => 0.0279082562161\n",
      "Loss for  12484 th iteration => 0.0279061030874\n",
      "Loss for  12485 th iteration => 0.0279020023266\n",
      "Loss for  12486 th iteration => 0.0279011088593\n",
      "Loss for  12487 th iteration => 0.0278982905941\n",
      "Loss for  12488 th iteration => 0.0278951694898\n",
      "Loss for  12489 th iteration => 0.0278934129478\n",
      "Loss for  12490 th iteration => 0.0278894302058\n",
      "Loss for  12491 th iteration => 0.0278863496814\n",
      "Loss for  12492 th iteration => 0.0278855684161\n",
      "Loss for  12493 th iteration => 0.0278826553945\n",
      "Loss for  12494 th iteration => 0.0278796338694\n",
      "Loss for  12495 th iteration => 0.0278777833935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  12496 th iteration => 0.027873839753\n",
      "Loss for  12497 th iteration => 0.0278707171996\n",
      "Loss for  12498 th iteration => 0.0278700404815\n",
      "Loss for  12499 th iteration => 0.0278670391575\n",
      "Loss for  12500 th iteration => 0.0278641109135\n",
      "Loss for  12501 th iteration => 0.0278621727548\n",
      "Loss for  12502 th iteration => 0.0278582660206\n",
      "Loss for  12503 th iteration => 0.0278551044773\n",
      "Loss for  12504 th iteration => 0.0278545254619\n",
      "Loss for  12505 th iteration => 0.027851441632\n",
      "Loss for  12506 th iteration => 0.0278486010098\n",
      "Loss for  12507 th iteration => 0.027846580785\n",
      "Loss for  12508 th iteration => 0.0278427241167\n",
      "Loss for  12509 th iteration => 0.0278406449747\n",
      "Loss for  12510 th iteration => 0.0278380605417\n",
      "Loss for  12511 th iteration => 0.0278363923181\n",
      "Loss for  12512 th iteration => 0.0278330510976\n",
      "Loss for  12513 th iteration => 0.0278306409807\n",
      "Loss for  12514 th iteration => 0.027828042178\n",
      "Loss for  12515 th iteration => 0.0278250247066\n",
      "Loss for  12516 th iteration => 0.0278223891431\n",
      "Loss for  12517 th iteration => 0.0278207522675\n",
      "Loss for  12518 th iteration => 0.0278177888651\n",
      "Loss for  12519 th iteration => 0.0278148477296\n",
      "Loss for  12520 th iteration => 0.027812938135\n",
      "Loss for  12521 th iteration => 0.0278091130399\n",
      "Loss for  12522 th iteration => 0.0278059454459\n",
      "Loss for  12523 th iteration => 0.0278053025968\n",
      "Loss for  12524 th iteration => 0.0278022381255\n",
      "Loss for  12525 th iteration => 0.0277994025281\n",
      "Loss for  12526 th iteration => 0.0277973930565\n",
      "Loss for  12527 th iteration => 0.0277936068418\n",
      "Loss for  12528 th iteration => 0.0277903943157\n",
      "Loss for  12529 th iteration => 0.0277898645201\n",
      "Loss for  12530 th iteration => 0.0277867065954\n",
      "Loss for  12531 th iteration => 0.0277839691118\n",
      "Loss for  12532 th iteration => 0.0277818671322\n",
      "Loss for  12533 th iteration => 0.0277781291807\n",
      "Loss for  12534 th iteration => 0.0277759934589\n",
      "Loss for  12535 th iteration => 0.0277734501016\n",
      "Loss for  12536 th iteration => 0.0277717521231\n",
      "Loss for  12537 th iteration => 0.0277684628719\n",
      "Loss for  12538 th iteration => 0.0277660286535\n",
      "Loss for  12539 th iteration => 0.0277634761186\n",
      "Loss for  12540 th iteration => 0.0277604476812\n",
      "Loss for  12541 th iteration => 0.0277569592143\n",
      "Loss for  12542 th iteration => 0.0277569737586\n",
      "Loss for  12543 th iteration => 0.0277528352396\n",
      "Loss for  12544 th iteration => 0.0277504234268\n",
      "Loss for  12545 th iteration => 0.0277486800714\n",
      "Loss for  12546 th iteration => 0.027744069588\n",
      "Loss for  12547 th iteration => 0.0277422762752\n",
      "Loss for  12548 th iteration => 0.027740873107\n",
      "Loss for  12549 th iteration => 0.0277377272234\n",
      "Loss for  12550 th iteration => 0.0277350004212\n",
      "Loss for  12551 th iteration => 0.0277329036524\n",
      "Loss for  12552 th iteration => 0.027729217406\n",
      "Loss for  12553 th iteration => 0.0277259474595\n",
      "Loss for  12554 th iteration => 0.027725506503\n",
      "Loss for  12555 th iteration => 0.0277222635319\n",
      "Loss for  12556 th iteration => 0.027719638259\n",
      "Loss for  12557 th iteration => 0.0277174455671\n",
      "Loss for  12558 th iteration => 0.0277138193537\n",
      "Loss for  12559 th iteration => 0.0277116171206\n",
      "Loss for  12560 th iteration => 0.0277082562591\n",
      "Loss for  12561 th iteration => 0.0277078860538\n",
      "Loss for  12562 th iteration => 0.0277041392369\n",
      "Loss for  12563 th iteration => 0.0277013712908\n",
      "Loss for  12564 th iteration => 0.0276999912566\n",
      "Loss for  12565 th iteration => 0.0276951865082\n",
      "Loss for  12566 th iteration => 0.0276927385828\n",
      "Loss for  12567 th iteration => 0.0276926568738\n",
      "Loss for  12568 th iteration => 0.0276886291632\n",
      "Loss for  12569 th iteration => 0.0276861426156\n",
      "Loss for  12570 th iteration => 0.0276844887911\n",
      "Loss for  12571 th iteration => 0.0276798612095\n",
      "Loss for  12572 th iteration => 0.0276781069077\n",
      "Loss for  12573 th iteration => 0.0276766103438\n",
      "Loss for  12574 th iteration => 0.0276736116556\n",
      "Loss for  12575 th iteration => 0.0276707676206\n",
      "Loss for  12576 th iteration => 0.0276688084983\n",
      "Loss for  12577 th iteration => 0.0276650680721\n",
      "Loss for  12578 th iteration => 0.0276629954238\n",
      "Loss for  12579 th iteration => 0.0276602276538\n",
      "Loss for  12580 th iteration => 0.0276588797529\n",
      "Loss for  12581 th iteration => 0.0276552804421\n",
      "Loss for  12582 th iteration => 0.0276532039252\n",
      "Loss for  12583 th iteration => 0.0276504021955\n",
      "Loss for  12584 th iteration => 0.0276466065379\n",
      "Loss for  12585 th iteration => 0.0276448642699\n",
      "Loss for  12586 th iteration => 0.0276435865124\n",
      "Loss for  12587 th iteration => 0.027639922445\n",
      "Loss for  12588 th iteration => 0.0276379149667\n",
      "Loss for  12589 th iteration => 0.0276350187544\n",
      "Loss for  12590 th iteration => 0.0276312650976\n",
      "Loss for  12591 th iteration => 0.0276303679767\n",
      "Loss for  12592 th iteration => 0.0276275218738\n",
      "Loss for  12593 th iteration => 0.0276250487486\n",
      "Loss for  12594 th iteration => 0.0276225268479\n",
      "Loss for  12595 th iteration => 0.0276194525769\n",
      "Loss for  12596 th iteration => 0.0276175419814\n",
      "Loss for  12597 th iteration => 0.0276133880791\n",
      "Loss for  12598 th iteration => 0.0276118123787\n",
      "Loss for  12599 th iteration => 0.027610155461\n",
      "Loss for  12600 th iteration => 0.027606881023\n",
      "Loss for  12601 th iteration => 0.0276045055107\n",
      "Loss for  12602 th iteration => 0.0276020488849\n",
      "Loss for  12603 th iteration => 0.0275980737048\n",
      "Loss for  12604 th iteration => 0.0275964954969\n",
      "Loss for  12605 th iteration => 0.027594293682\n",
      "Loss for  12606 th iteration => 0.0275918583838\n",
      "Loss for  12607 th iteration => 0.0275893112365\n",
      "Loss for  12608 th iteration => 0.0275863327376\n",
      "Loss for  12609 th iteration => 0.0275834393485\n",
      "Loss for  12610 th iteration => 0.0275812463889\n",
      "Loss for  12611 th iteration => 0.0275789962902\n",
      "Loss for  12612 th iteration => 0.0275766521297\n",
      "Loss for  12613 th iteration => 0.0275740191514\n",
      "Loss for  12614 th iteration => 0.0275710814817\n",
      "Loss for  12615 th iteration => 0.0275681106449\n",
      "Loss for  12616 th iteration => 0.0275660109521\n",
      "Loss for  12617 th iteration => 0.0275637169954\n",
      "Loss for  12618 th iteration => 0.0275614562318\n",
      "Loss for  12619 th iteration => 0.0275587451543\n",
      "Loss for  12620 th iteration => 0.0275558882861\n",
      "Loss for  12621 th iteration => 0.0275538564389\n",
      "Loss for  12622 th iteration => 0.0275497193118\n",
      "Loss for  12623 th iteration => 0.0275491412915\n",
      "Loss for  12624 th iteration => 0.0275460334391\n",
      "Loss for  12625 th iteration => 0.0275433551148\n",
      "Loss for  12626 th iteration => 0.0275412720656\n",
      "Loss for  12627 th iteration => 0.0275376682418\n",
      "Loss for  12628 th iteration => 0.0275344167311\n",
      "Loss for  12629 th iteration => 0.0275339999631\n",
      "Loss for  12630 th iteration => 0.0275307630492\n",
      "Loss for  12631 th iteration => 0.0275282170381\n",
      "Loss for  12632 th iteration => 0.0275260074174\n",
      "Loss for  12633 th iteration => 0.0275224701422\n",
      "Loss for  12634 th iteration => 0.0275202852528\n",
      "Loss for  12635 th iteration => 0.0275169365431\n",
      "Loss for  12636 th iteration => 0.02751663255\n",
      "Loss for  12637 th iteration => 0.0275128673947\n",
      "Loss for  12638 th iteration => 0.0275102153802\n",
      "Loss for  12639 th iteration => 0.0275087679451\n",
      "Loss for  12640 th iteration => 0.0275040879202\n",
      "Loss for  12641 th iteration => 0.0275015978574\n",
      "Loss for  12642 th iteration => 0.0275016374191\n",
      "Loss for  12643 th iteration => 0.027497536972\n",
      "Loss for  12644 th iteration => 0.0274952192049\n",
      "Loss for  12645 th iteration => 0.0274934457794\n",
      "Loss for  12646 th iteration => 0.0274889803225\n",
      "Loss for  12647 th iteration => 0.0274882610597\n",
      "Loss for  12648 th iteration => 0.0274847336869\n",
      "Loss for  12649 th iteration => 0.0274833804109\n",
      "Loss for  12650 th iteration => 0.0274798465138\n",
      "Loss for  12651 th iteration => 0.0274777792388\n",
      "Loss for  12652 th iteration => 0.02747502359\n",
      "Loss for  12653 th iteration => 0.0274712706645\n",
      "Loss for  12654 th iteration => 0.0274695514204\n",
      "Loss for  12655 th iteration => 0.027468286578\n",
      "Loss for  12656 th iteration => 0.0274646695781\n",
      "Loss for  12657 th iteration => 0.0274626889002\n",
      "Loss for  12658 th iteration => 0.0274598122027\n",
      "Loss for  12659 th iteration => 0.0274561169272\n",
      "Loss for  12660 th iteration => 0.027455251062\n",
      "Loss for  12661 th iteration => 0.0274515308001\n",
      "Loss for  12662 th iteration => 0.0274504628826\n",
      "Loss for  12663 th iteration => 0.0274474675957\n",
      "Loss for  12664 th iteration => 0.0274440993488\n",
      "Loss for  12665 th iteration => 0.027443374456\n",
      "Loss for  12666 th iteration => 0.0274384390304\n",
      "Loss for  12667 th iteration => 0.0274368909774\n",
      "Loss for  12668 th iteration => 0.0274346844474\n",
      "Loss for  12669 th iteration => 0.0274323027199\n",
      "Loss for  12670 th iteration => 0.0274297575832\n",
      "Loss for  12671 th iteration => 0.0274268323226\n",
      "Loss for  12672 th iteration => 0.0274239464127\n",
      "Loss for  12673 th iteration => 0.0274218137515\n",
      "Loss for  12674 th iteration => 0.0274195534755\n",
      "Loss for  12675 th iteration => 0.0274172739438\n",
      "Loss for  12676 th iteration => 0.0274146318259\n",
      "Loss for  12677 th iteration => 0.0274117673344\n",
      "Loss for  12678 th iteration => 0.0274098216195\n",
      "Loss for  12679 th iteration => 0.027405711806\n",
      "Loss for  12680 th iteration => 0.0274051007899\n",
      "Loss for  12681 th iteration => 0.0274020338886\n",
      "Loss for  12682 th iteration => 0.0273993792424\n",
      "Loss for  12683 th iteration => 0.0273973189346\n",
      "Loss for  12684 th iteration => 0.0273937895352\n",
      "Loss for  12685 th iteration => 0.0273905515044\n",
      "Loss for  12686 th iteration => 0.0273893509529\n",
      "Loss for  12687 th iteration => 0.0273872589397\n",
      "Loss for  12688 th iteration => 0.0273844395931\n",
      "Loss for  12689 th iteration => 0.027381757788\n",
      "Loss for  12690 th iteration => 0.0273795304928\n",
      "Loss for  12691 th iteration => 0.0273765727558\n",
      "Loss for  12692 th iteration => 0.0273731920852\n",
      "Loss for  12693 th iteration => 0.0273729811229\n",
      "Loss for  12694 th iteration => 0.027369159363\n",
      "Loss for  12695 th iteration => 0.027366635564\n",
      "Loss for  12696 th iteration => 0.0273650968212\n",
      "Loss for  12697 th iteration => 0.0273605657243\n",
      "Loss for  12698 th iteration => 0.0273588317316\n",
      "Loss for  12699 th iteration => 0.0273573706924\n",
      "Loss for  12700 th iteration => 0.0273544120556\n",
      "Loss for  12701 th iteration => 0.0273516692021\n",
      "Loss for  12702 th iteration => 0.0273497123766\n",
      "Loss for  12703 th iteration => 0.0273461000979\n",
      "Loss for  12704 th iteration => 0.0273440309594\n",
      "Loss for  12705 th iteration => 0.0273413576575\n",
      "Loss for  12706 th iteration => 0.0273400159506\n",
      "Loss for  12707 th iteration => 0.0273365192647\n",
      "Loss for  12708 th iteration => 0.0273344740524\n",
      "Loss for  12709 th iteration => 0.0273317249735\n",
      "Loss for  12710 th iteration => 0.0273280216243\n",
      "Loss for  12711 th iteration => 0.0273263704527\n",
      "Loss for  12712 th iteration => 0.0273236130082\n",
      "Loss for  12713 th iteration => 0.0273222236711\n",
      "Loss for  12714 th iteration => 0.0273195767853\n",
      "Loss for  12715 th iteration => 0.0273159341067\n",
      "Loss for  12716 th iteration => 0.0273155111336\n",
      "Loss for  12717 th iteration => 0.027310452755\n",
      "Loss for  12718 th iteration => 0.0273090738307\n",
      "Loss for  12719 th iteration => 0.0273072397724\n",
      "Loss for  12720 th iteration => 0.0273042450859\n",
      "Loss for  12721 th iteration => 0.0273017179572\n",
      "Loss for  12722 th iteration => 0.0272994644094\n",
      "Loss for  12723 th iteration => 0.0272954700263\n",
      "Loss for  12724 th iteration => 0.0272932695903\n",
      "Loss for  12725 th iteration => 0.02729277473\n",
      "Loss for  12726 th iteration => 0.0272892526224\n",
      "Loss for  12727 th iteration => 0.0272864775363\n",
      "Loss for  12728 th iteration => 0.0272852061617\n",
      "Loss for  12729 th iteration => 0.0272805542658\n",
      "Loss for  12730 th iteration => 0.0272781247371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  12731 th iteration => 0.0272780701446\n",
      "Loss for  12732 th iteration => 0.0272741172498\n",
      "Loss for  12733 th iteration => 0.0272717692937\n",
      "Loss for  12734 th iteration => 0.0272700802423\n",
      "Loss for  12735 th iteration => 0.0272656737999\n",
      "Loss for  12736 th iteration => 0.0272638828198\n",
      "Loss for  12737 th iteration => 0.0272616273314\n",
      "Loss for  12738 th iteration => 0.027259958466\n",
      "Loss for  12739 th iteration => 0.0272568151517\n",
      "Loss for  12740 th iteration => 0.027254456481\n",
      "Loss for  12741 th iteration => 0.027252076482\n",
      "Loss for  12742 th iteration => 0.0272481747411\n",
      "Loss for  12743 th iteration => 0.0272466822558\n",
      "Loss for  12744 th iteration => 0.027244485601\n",
      "Loss for  12745 th iteration => 0.0272421716546\n",
      "Loss for  12746 th iteration => 0.0272396245632\n",
      "Loss for  12747 th iteration => 0.0272367471075\n",
      "Loss for  12748 th iteration => 0.0272338683646\n",
      "Loss for  12749 th iteration => 0.0272309792589\n",
      "Loss for  12750 th iteration => 0.0272299843499\n",
      "Loss for  12751 th iteration => 0.0272273107458\n",
      "Loss for  12752 th iteration => 0.0272243471747\n",
      "Loss for  12753 th iteration => 0.0272226500392\n",
      "Loss for  12754 th iteration => 0.0272189601415\n",
      "Loss for  12755 th iteration => 0.0272159636983\n",
      "Loss for  12756 th iteration => 0.0272152310415\n",
      "Loss for  12757 th iteration => 0.0272123494576\n",
      "Loss for  12758 th iteration => 0.027209594544\n",
      "Loss for  12759 th iteration => 0.0272076949092\n",
      "Loss for  12760 th iteration => 0.0272040869332\n",
      "Loss for  12761 th iteration => 0.0272020723799\n",
      "Loss for  12762 th iteration => 0.0271994259503\n",
      "Loss for  12763 th iteration => 0.0271980610809\n",
      "Loss for  12764 th iteration => 0.0271946355689\n",
      "Loss for  12765 th iteration => 0.0271925801766\n",
      "Loss for  12766 th iteration => 0.0271898768436\n",
      "Loss for  12767 th iteration => 0.0271861704858\n",
      "Loss for  12768 th iteration => 0.0271845358812\n",
      "Loss for  12769 th iteration => 0.0271832882885\n",
      "Loss for  12770 th iteration => 0.0271797507532\n",
      "Loss for  12771 th iteration => 0.0271778096673\n",
      "Loss for  12772 th iteration => 0.0271749657653\n",
      "Loss for  12773 th iteration => 0.0271723585335\n",
      "Loss for  12774 th iteration => 0.0271706259173\n",
      "Loss for  12775 th iteration => 0.0271672196522\n",
      "Loss for  12776 th iteration => 0.027165911778\n",
      "Loss for  12777 th iteration => 0.0271624402743\n",
      "Loss for  12778 th iteration => 0.0271604427567\n",
      "Loss for  12779 th iteration => 0.0271576949231\n",
      "Loss for  12780 th iteration => 0.0271540435392\n",
      "Loss for  12781 th iteration => 0.0271524298986\n",
      "Loss for  12782 th iteration => 0.0271496666281\n",
      "Loss for  12783 th iteration => 0.0271483737246\n",
      "Loss for  12784 th iteration => 0.027145674011\n",
      "Loss for  12785 th iteration => 0.0271421703373\n",
      "Loss for  12786 th iteration => 0.0271416525477\n",
      "Loss for  12787 th iteration => 0.0271366844142\n",
      "Loss for  12788 th iteration => 0.0271353200925\n",
      "Loss for  12789 th iteration => 0.0271335600118\n",
      "Loss for  12790 th iteration => 0.0271305502303\n",
      "Loss for  12791 th iteration => 0.0271281093974\n",
      "Loss for  12792 th iteration => 0.0271258007896\n",
      "Loss for  12793 th iteration => 0.0271219042417\n",
      "Loss for  12794 th iteration => 0.0271196504677\n",
      "Loss for  12795 th iteration => 0.0271193359568\n",
      "Loss for  12796 th iteration => 0.0271156780948\n",
      "Loss for  12797 th iteration => 0.0271131226366\n",
      "Loss for  12798 th iteration => 0.0271116767912\n",
      "Loss for  12799 th iteration => 0.0271071741464\n",
      "Loss for  12800 th iteration => 0.0271046765724\n",
      "Loss for  12801 th iteration => 0.0271039885535\n",
      "Loss for  12802 th iteration => 0.0271010810791\n",
      "Loss for  12803 th iteration => 0.0270985966071\n",
      "Loss for  12804 th iteration => 0.0270962693157\n",
      "Loss for  12805 th iteration => 0.0270933245722\n",
      "Loss for  12806 th iteration => 0.0270906448377\n",
      "Loss for  12807 th iteration => 0.0270884469743\n",
      "Loss for  12808 th iteration => 0.02708687262\n",
      "Loss for  12809 th iteration => 0.0270836934123\n",
      "Loss for  12810 th iteration => 0.0270814401861\n",
      "Loss for  12811 th iteration => 0.0270789647408\n",
      "Loss for  12812 th iteration => 0.0270751478894\n",
      "Loss for  12813 th iteration => 0.0270737116745\n",
      "Loss for  12814 th iteration => 0.0270715316136\n",
      "Loss for  12815 th iteration => 0.0270692889121\n",
      "Loss for  12816 th iteration => 0.0270667298809\n",
      "Loss for  12817 th iteration => 0.0270639251225\n",
      "Loss for  12818 th iteration => 0.0270619995274\n",
      "Loss for  12819 th iteration => 0.0270579502964\n",
      "Loss for  12820 th iteration => 0.0270574832155\n",
      "Loss for  12821 th iteration => 0.027054371704\n",
      "Loss for  12822 th iteration => 0.0270519114692\n",
      "Loss for  12823 th iteration => 0.0270497688801\n",
      "Loss for  12824 th iteration => 0.027046380147\n",
      "Loss for  12825 th iteration => 0.0270442559764\n",
      "Loss for  12826 th iteration => 0.0270409898393\n",
      "Loss for  12827 th iteration => 0.0270407407361\n",
      "Loss for  12828 th iteration => 0.0270370374002\n",
      "Loss for  12829 th iteration => 0.0270345656204\n",
      "Loss for  12830 th iteration => 0.0270330562861\n",
      "Loss for  12831 th iteration => 0.0270286183331\n",
      "Loss for  12832 th iteration => 0.0270269299901\n",
      "Loss for  12833 th iteration => 0.0270246913028\n",
      "Loss for  12834 th iteration => 0.0270230175175\n",
      "Loss for  12835 th iteration => 0.0270199589444\n",
      "Loss for  12836 th iteration => 0.0270176152157\n",
      "Loss for  12837 th iteration => 0.0270152987436\n",
      "Loss for  12838 th iteration => 0.0270114458174\n",
      "Loss for  12839 th iteration => 0.0270099769997\n",
      "Loss for  12840 th iteration => 0.0270078360428\n",
      "Loss for  12841 th iteration => 0.0270055398205\n",
      "Loss for  12842 th iteration => 0.0270030561629\n",
      "Loss for  12843 th iteration => 0.0270002046192\n",
      "Loss for  12844 th iteration => 0.0269983900295\n",
      "Loss for  12845 th iteration => 0.0269943634302\n",
      "Loss for  12846 th iteration => 0.0269937773881\n",
      "Loss for  12847 th iteration => 0.0269907676916\n",
      "Loss for  12848 th iteration => 0.0269882361762\n",
      "Loss for  12849 th iteration => 0.0269861844801\n",
      "Loss for  12850 th iteration => 0.0269827831946\n",
      "Loss for  12851 th iteration => 0.0269796094709\n",
      "Loss for  12852 th iteration => 0.0269784769881\n",
      "Loss for  12853 th iteration => 0.0269764439514\n",
      "Loss for  12854 th iteration => 0.0269737069951\n",
      "Loss for  12855 th iteration => 0.0269711108709\n",
      "Loss for  12856 th iteration => 0.0269689389127\n",
      "Loss for  12857 th iteration => 0.0269660237421\n",
      "Loss for  12858 th iteration => 0.026962675041\n",
      "Loss for  12859 th iteration => 0.0269627005326\n",
      "Loss for  12860 th iteration => 0.0269587451683\n",
      "Loss for  12861 th iteration => 0.0269565579461\n",
      "Loss for  12862 th iteration => 0.0269547868348\n",
      "Loss for  12863 th iteration => 0.026950566207\n",
      "Loss for  12864 th iteration => 0.0269497688428\n",
      "Loss for  12865 th iteration => 0.0269464319502\n",
      "Loss for  12866 th iteration => 0.0269451508188\n",
      "Loss for  12867 th iteration => 0.0269417268172\n",
      "Loss for  12868 th iteration => 0.0269397730466\n",
      "Loss for  12869 th iteration => 0.0269370366509\n",
      "Loss for  12870 th iteration => 0.0269334582871\n",
      "Loss for  12871 th iteration => 0.0269311088737\n",
      "Loss for  12872 th iteration => 0.0269294874227\n",
      "Loss for  12873 th iteration => 0.0269278969462\n",
      "Loss for  12874 th iteration => 0.026924787267\n",
      "Loss for  12875 th iteration => 0.0269225378253\n",
      "Loss for  12876 th iteration => 0.026920248406\n",
      "Loss for  12877 th iteration => 0.0269164886387\n",
      "Loss for  12878 th iteration => 0.0269148621758\n",
      "Loss for  12879 th iteration => 0.0269134474928\n",
      "Loss for  12880 th iteration => 0.0269101673224\n",
      "Loss for  12881 th iteration => 0.0269080885417\n",
      "Loss for  12882 th iteration => 0.0269055536615\n",
      "Loss for  12883 th iteration => 0.0269018871551\n",
      "Loss for  12884 th iteration => 0.0269002577271\n",
      "Loss for  12885 th iteration => 0.0268989996843\n",
      "Loss for  12886 th iteration => 0.0268955681023\n",
      "Loss for  12887 th iteration => 0.0268936414787\n",
      "Loss for  12888 th iteration => 0.0268908870819\n",
      "Loss for  12889 th iteration => 0.026887319022\n",
      "Loss for  12890 th iteration => 0.0268865261725\n",
      "Loss for  12891 th iteration => 0.0268828805182\n",
      "Loss for  12892 th iteration => 0.0268819886504\n",
      "Loss for  12893 th iteration => 0.0268789577948\n",
      "Loss for  12894 th iteration => 0.0268759096998\n",
      "Loss for  12895 th iteration => 0.0268750070565\n",
      "Loss for  12896 th iteration => 0.0268703165846\n",
      "Loss for  12897 th iteration => 0.0268688842526\n",
      "Loss for  12898 th iteration => 0.0268667340741\n",
      "Loss for  12899 th iteration => 0.0268645258582\n",
      "Loss for  12900 th iteration => 0.0268620022975\n",
      "Loss for  12901 th iteration => 0.0268592456102\n",
      "Loss for  12902 th iteration => 0.0268573450002\n",
      "Loss for  12903 th iteration => 0.0268533506336\n",
      "Loss for  12904 th iteration => 0.0268528924626\n",
      "Loss for  12905 th iteration => 0.0268498261476\n",
      "Loss for  12906 th iteration => 0.0268474087313\n",
      "Loss for  12907 th iteration => 0.0268452884074\n",
      "Loss for  12908 th iteration => 0.0268419586183\n",
      "Loss for  12909 th iteration => 0.026839859737\n",
      "Loss for  12910 th iteration => 0.0268365881356\n",
      "Loss for  12911 th iteration => 0.0268364560664\n",
      "Loss for  12912 th iteration => 0.0268326866415\n",
      "Loss for  12913 th iteration => 0.0268303809329\n",
      "Loss for  12914 th iteration => 0.0268287571407\n",
      "Loss for  12915 th iteration => 0.0268244949559\n",
      "Loss for  12916 th iteration => 0.0268227466379\n",
      "Loss for  12917 th iteration => 0.0268205512703\n",
      "Loss for  12918 th iteration => 0.0268189691604\n",
      "Loss for  12919 th iteration => 0.0268158873755\n",
      "Loss for  12920 th iteration => 0.0268136491863\n",
      "Loss for  12921 th iteration => 0.0268112696189\n",
      "Loss for  12922 th iteration => 0.026807514379\n",
      "Loss for  12923 th iteration => 0.0268060896594\n",
      "Loss for  12924 th iteration => 0.0268039427296\n",
      "Loss for  12925 th iteration => 0.026801748665\n",
      "Loss for  12926 th iteration => 0.0267992322975\n",
      "Loss for  12927 th iteration => 0.0267964937696\n",
      "Loss for  12928 th iteration => 0.0267945969282\n",
      "Loss for  12929 th iteration => 0.0267906158703\n",
      "Loss for  12930 th iteration => 0.0267901737475\n",
      "Loss for  12931 th iteration => 0.0267871056661\n",
      "Loss for  12932 th iteration => 0.0267847166622\n",
      "Loss for  12933 th iteration => 0.0267825878571\n",
      "Loss for  12934 th iteration => 0.0267792912785\n",
      "Loss for  12935 th iteration => 0.026778124871\n",
      "Loss for  12936 th iteration => 0.0267737677699\n",
      "Loss for  12937 th iteration => 0.0267733033184\n",
      "Loss for  12938 th iteration => 0.0267701642618\n",
      "Loss for  12939 th iteration => 0.026767994201\n",
      "Loss for  12940 th iteration => 0.0267655155621\n",
      "Loss for  12941 th iteration => 0.0267627157059\n",
      "Loss for  12942 th iteration => 0.0267601117058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  12943 th iteration => 0.0267572330529\n",
      "Loss for  12944 th iteration => 0.0267568113137\n",
      "Loss for  12945 th iteration => 0.0267533468776\n",
      "Loss for  12946 th iteration => 0.026750783234\n",
      "Loss for  12947 th iteration => 0.0267494330175\n",
      "Loss for  12948 th iteration => 0.0267450725441\n",
      "Loss for  12949 th iteration => 0.0267433916836\n",
      "Loss for  12950 th iteration => 0.0267419353144\n",
      "Loss for  12951 th iteration => 0.0267386689886\n",
      "Loss for  12952 th iteration => 0.0267367608934\n",
      "Loss for  12953 th iteration => 0.0267347674959\n",
      "Loss for  12954 th iteration => 0.0267308307042\n",
      "Loss for  12955 th iteration => 0.0267287990239\n",
      "Loss for  12956 th iteration => 0.0267261350934\n",
      "Loss for  12957 th iteration => 0.0267249062957\n",
      "Loss for  12958 th iteration => 0.0267225562431\n",
      "Loss for  12959 th iteration => 0.0267194910031\n",
      "Loss for  12960 th iteration => 0.0267180549676\n",
      "Loss for  12961 th iteration => 0.0267144071397\n",
      "Loss for  12962 th iteration => 0.0267115911178\n",
      "Loss for  12963 th iteration => 0.0267107312532\n",
      "Loss for  12964 th iteration => 0.0267080912655\n",
      "Loss for  12965 th iteration => 0.0267053138917\n",
      "Loss for  12966 th iteration => 0.0267035965573\n",
      "Loss for  12967 th iteration => 0.0267000321031\n",
      "Loss for  12968 th iteration => 0.0266970805437\n",
      "Loss for  12969 th iteration => 0.0266965633399\n",
      "Loss for  12970 th iteration => 0.0266930705014\n",
      "Loss for  12971 th iteration => 0.0266914398992\n",
      "Loss for  12972 th iteration => 0.0266891836334\n",
      "Loss for  12973 th iteration => 0.0266854744585\n",
      "Loss for  12974 th iteration => 0.0266852695441\n",
      "Loss for  12975 th iteration => 0.026680304561\n",
      "Loss for  12976 th iteration => 0.026679189193\n",
      "Loss for  12977 th iteration => 0.0266771327978\n",
      "Loss for  12978 th iteration => 0.0266745208455\n",
      "Loss for  12979 th iteration => 0.0266719223689\n",
      "Loss for  12980 th iteration => 0.026669854138\n",
      "Loss for  12981 th iteration => 0.0266670008201\n",
      "Loss for  12982 th iteration => 0.0266637002581\n",
      "Loss for  12983 th iteration => 0.0266637218871\n",
      "Loss for  12984 th iteration => 0.026659842399\n",
      "Loss for  12985 th iteration => 0.0266577291915\n",
      "Loss for  12986 th iteration => 0.0266559570813\n",
      "Loss for  12987 th iteration => 0.0266518858943\n",
      "Loss for  12988 th iteration => 0.0266510307428\n",
      "Loss for  12989 th iteration => 0.0266478267208\n",
      "Loss for  12990 th iteration => 0.0266452267554\n",
      "Loss for  12991 th iteration => 0.0266438479283\n",
      "Loss for  12992 th iteration => 0.0266413547939\n",
      "Loss for  12993 th iteration => 0.0266378998021\n",
      "Loss for  12994 th iteration => 0.0266374557179\n",
      "Loss for  12995 th iteration => 0.0266326765303\n",
      "Loss for  12996 th iteration => 0.0266306199275\n",
      "Loss for  12997 th iteration => 0.0266299908938\n",
      "Loss for  12998 th iteration => 0.0266267614403\n",
      "Loss for  12999 th iteration => 0.0266240310847\n",
      "Loss for  13000 th iteration => 0.0266228757327\n",
      "Loss for  13001 th iteration => 0.026618473783\n",
      "Loss for  13002 th iteration => 0.0266160682\n",
      "Loss for  13003 th iteration => 0.0266153286462\n",
      "Loss for  13004 th iteration => 0.0266125453056\n",
      "Loss for  13005 th iteration => 0.026610136661\n",
      "Loss for  13006 th iteration => 0.0266078995695\n",
      "Loss for  13007 th iteration => 0.0266050700662\n",
      "Loss for  13008 th iteration => 0.0266024818027\n",
      "Loss for  13009 th iteration => 0.0266003615881\n",
      "Loss for  13010 th iteration => 0.0265982497525\n",
      "Loss for  13011 th iteration => 0.02659602584\n",
      "Loss for  13012 th iteration => 0.0265936088884\n",
      "Loss for  13013 th iteration => 0.0265908562052\n",
      "Loss for  13014 th iteration => 0.026589095941\n",
      "Loss for  13015 th iteration => 0.0265851510997\n",
      "Loss for  13016 th iteration => 0.026584651152\n",
      "Loss for  13017 th iteration => 0.0265816366028\n",
      "Loss for  13018 th iteration => 0.0265792839565\n",
      "Loss for  13019 th iteration => 0.026577183157\n",
      "Loss for  13020 th iteration => 0.0265739777212\n",
      "Loss for  13021 th iteration => 0.0265709204377\n",
      "Loss for  13022 th iteration => 0.0265688346301\n",
      "Loss for  13023 th iteration => 0.0265683375301\n",
      "Loss for  13024 th iteration => 0.0265649926649\n",
      "Loss for  13025 th iteration => 0.026562405063\n",
      "Loss for  13026 th iteration => 0.0265611236587\n",
      "Loss for  13027 th iteration => 0.0265567814416\n",
      "Loss for  13028 th iteration => 0.0265543953162\n",
      "Loss for  13029 th iteration => 0.0265538130173\n",
      "Loss for  13030 th iteration => 0.0265508686883\n",
      "Loss for  13031 th iteration => 0.0265484615867\n",
      "Loss for  13032 th iteration => 0.0265464242934\n",
      "Loss for  13033 th iteration => 0.0265431893675\n",
      "Loss for  13034 th iteration => 0.026540091552\n",
      "Loss for  13035 th iteration => 0.0265381512797\n",
      "Loss for  13036 th iteration => 0.0265374881441\n",
      "Loss for  13037 th iteration => 0.0265343144125\n",
      "Loss for  13038 th iteration => 0.0265315751105\n",
      "Loss for  13039 th iteration => 0.0265304506401\n",
      "Loss for  13040 th iteration => 0.0265259872821\n",
      "Loss for  13041 th iteration => 0.0265236893171\n",
      "Loss for  13042 th iteration => 0.0265230632799\n",
      "Loss for  13043 th iteration => 0.0265201594048\n",
      "Loss for  13044 th iteration => 0.0265177266575\n",
      "Loss for  13045 th iteration => 0.0265157242439\n",
      "Loss for  13046 th iteration => 0.0265124671151\n",
      "Loss for  13047 th iteration => 0.0265093709154\n",
      "Loss for  13048 th iteration => 0.0265083011988\n",
      "Loss for  13049 th iteration => 0.0265063142692\n",
      "Loss for  13050 th iteration => 0.0265036904814\n",
      "Loss for  13051 th iteration => 0.026501171558\n",
      "Loss for  13052 th iteration => 0.0264990812851\n",
      "Loss for  13053 th iteration => 0.0264962274445\n",
      "Loss for  13054 th iteration => 0.0264929624041\n",
      "Loss for  13055 th iteration => 0.0264931147603\n",
      "Loss for  13056 th iteration => 0.0264891470867\n",
      "Loss for  13057 th iteration => 0.0264872035294\n",
      "Loss for  13058 th iteration => 0.0264853048507\n",
      "Loss for  13059 th iteration => 0.0264813718613\n",
      "Loss for  13060 th iteration => 0.0264804328557\n",
      "Loss for  13061 th iteration => 0.0264773532532\n",
      "Loss for  13062 th iteration => 0.0264746926991\n",
      "Loss for  13063 th iteration => 0.0264735138044\n",
      "Loss for  13064 th iteration => 0.026470864155\n",
      "Loss for  13065 th iteration => 0.0264676450224\n",
      "Loss for  13066 th iteration => 0.0264670090239\n",
      "Loss for  13067 th iteration => 0.0264623231857\n",
      "Loss for  13068 th iteration => 0.0264602495981\n",
      "Loss for  13069 th iteration => 0.0264598445538\n",
      "Loss for  13070 th iteration => 0.0264564347733\n",
      "Loss for  13071 th iteration => 0.0264539634617\n",
      "Loss for  13072 th iteration => 0.0264525932485\n",
      "Loss for  13073 th iteration => 0.026448297677\n",
      "Loss for  13074 th iteration => 0.0264458792395\n",
      "Loss for  13075 th iteration => 0.0264452971391\n",
      "Loss for  13076 th iteration => 0.0264424630371\n",
      "Loss for  13077 th iteration => 0.026440170931\n",
      "Loss for  13078 th iteration => 0.0264378744652\n",
      "Loss for  13079 th iteration => 0.0264350844162\n",
      "Loss for  13080 th iteration => 0.0264324578994\n",
      "Loss for  13081 th iteration => 0.0264296794964\n",
      "Loss for  13082 th iteration => 0.0264287755137\n",
      "Loss for  13083 th iteration => 0.0264261569356\n",
      "Loss for  13084 th iteration => 0.0264234830788\n",
      "Loss for  13085 th iteration => 0.0264217498323\n",
      "Loss for  13086 th iteration => 0.0264183648617\n",
      "Loss for  13087 th iteration => 0.0264154213343\n",
      "Loss for  13088 th iteration => 0.0264149200414\n",
      "Loss for  13089 th iteration => 0.0264119883547\n",
      "Loss for  13090 th iteration => 0.0264096245461\n",
      "Loss for  13091 th iteration => 0.026407587906\n",
      "Loss for  13092 th iteration => 0.0264043665982\n",
      "Loss for  13093 th iteration => 0.0264022443915\n",
      "Loss for  13094 th iteration => 0.0263999152744\n",
      "Loss for  13095 th iteration => 0.0263978304919\n",
      "Loss for  13096 th iteration => 0.0263957888414\n",
      "Loss for  13097 th iteration => 0.0263934367418\n",
      "Loss for  13098 th iteration => 0.0263905199582\n",
      "Loss for  13099 th iteration => 0.0263890375443\n",
      "Loss for  13100 th iteration => 0.0263856536032\n",
      "Loss for  13101 th iteration => 0.0263828301748\n",
      "Loss for  13102 th iteration => 0.0263825783897\n",
      "Loss for  13103 th iteration => 0.0263790358411\n",
      "Loss for  13104 th iteration => 0.0263767315383\n",
      "Loss for  13105 th iteration => 0.0263752150312\n",
      "Loss for  13106 th iteration => 0.0263710571048\n",
      "Loss for  13107 th iteration => 0.0263693193384\n",
      "Loss for  13108 th iteration => 0.0263672833091\n",
      "Loss for  13109 th iteration => 0.0263651271973\n",
      "Loss for  13110 th iteration => 0.026363042975\n",
      "Loss for  13111 th iteration => 0.0263605648951\n",
      "Loss for  13112 th iteration => 0.0263579670994\n",
      "Loss for  13113 th iteration => 0.0263561390883\n",
      "Loss for  13114 th iteration => 0.0263523775001\n",
      "Loss for  13115 th iteration => 0.026351710967\n",
      "Loss for  13116 th iteration => 0.0263489465074\n",
      "Loss for  13117 th iteration => 0.026346443908\n",
      "Loss for  13118 th iteration => 0.0263445650735\n",
      "Loss for  13119 th iteration => 0.0263412151214\n",
      "Loss for  13120 th iteration => 0.026339185969\n",
      "Loss for  13121 th iteration => 0.0263369603025\n",
      "Loss for  13122 th iteration => 0.0263349022871\n",
      "Loss for  13123 th iteration => 0.0263326890627\n",
      "Loss for  13124 th iteration => 0.0263303968744\n",
      "Loss for  13125 th iteration => 0.0263275745724\n",
      "Loss for  13126 th iteration => 0.0263259032179\n",
      "Loss for  13127 th iteration => 0.0263219685805\n",
      "Loss for  13128 th iteration => 0.0263199911401\n",
      "Loss for  13129 th iteration => 0.0263194350977\n",
      "Loss for  13130 th iteration => 0.0263162077596\n",
      "Loss for  13131 th iteration => 0.0263136266519\n",
      "Loss for  13132 th iteration => 0.0263123981715\n",
      "Loss for  13133 th iteration => 0.0263080922615\n",
      "Loss for  13134 th iteration => 0.0263057297081\n",
      "Loss for  13135 th iteration => 0.0263052801243\n",
      "Loss for  13136 th iteration => 0.0263022423173\n",
      "Loss for  13137 th iteration => 0.0263000328955\n",
      "Loss for  13138 th iteration => 0.0262978759647\n",
      "Loss for  13139 th iteration => 0.0262948147513\n",
      "Loss for  13140 th iteration => 0.0262936298115\n",
      "Loss for  13141 th iteration => 0.026289493474\n",
      "Loss for  13142 th iteration => 0.0262888727551\n",
      "Loss for  13143 th iteration => 0.0262860363056\n",
      "Loss for  13144 th iteration => 0.0262837629992\n",
      "Loss for  13145 th iteration => 0.026281546637\n",
      "Loss for  13146 th iteration => 0.0262787009166\n",
      "Loss for  13147 th iteration => 0.0262764436671\n",
      "Loss for  13148 th iteration => 0.0262740300378\n",
      "Loss for  13149 th iteration => 0.0262720484164\n",
      "Loss for  13150 th iteration => 0.0262700229675\n",
      "Loss for  13151 th iteration => 0.026267564086\n",
      "Loss for  13152 th iteration => 0.0262649345827\n",
      "Loss for  13153 th iteration => 0.0262631584907\n",
      "Loss for  13154 th iteration => 0.0262592744338\n",
      "Loss for  13155 th iteration => 0.0262573616658\n",
      "Loss for  13156 th iteration => 0.0262566811196\n",
      "Loss for  13157 th iteration => 0.0262535915412\n",
      "Loss for  13158 th iteration => 0.026250906359\n",
      "Loss for  13159 th iteration => 0.0262497954401\n",
      "Loss for  13160 th iteration => 0.0262454651442\n",
      "Loss for  13161 th iteration => 0.0262431516345\n",
      "Loss for  13162 th iteration => 0.0262424287983\n",
      "Loss for  13163 th iteration => 0.0262397612735\n",
      "Loss for  13164 th iteration => 0.0262373854216\n",
      "Loss for  13165 th iteration => 0.0262352400431\n",
      "Loss for  13166 th iteration => 0.0262324018938\n",
      "Loss for  13167 th iteration => 0.0262299076658\n",
      "Loss for  13168 th iteration => 0.0262271380365\n",
      "Loss for  13169 th iteration => 0.0262268044549\n",
      "Loss for  13170 th iteration => 0.0262233796669\n",
      "Loss for  13171 th iteration => 0.0262210358117\n",
      "Loss for  13172 th iteration => 0.0262195953468\n",
      "Loss for  13173 th iteration => 0.0262154545357\n",
      "Loss for  13174 th iteration => 0.026213761312\n",
      "Loss for  13175 th iteration => 0.0262117172244\n",
      "Loss for  13176 th iteration => 0.0262096125602\n",
      "Loss for  13177 th iteration => 0.0262075210596\n",
      "Loss for  13178 th iteration => 0.0262051019276\n",
      "Loss for  13179 th iteration => 0.0262025068111\n",
      "Loss for  13180 th iteration => 0.0262007186903\n",
      "Loss for  13181 th iteration => 0.0261969808472\n",
      "Loss for  13182 th iteration => 0.0261963484082\n",
      "Loss for  13183 th iteration => 0.0261935863491\n",
      "Loss for  13184 th iteration => 0.0261911459887\n",
      "Loss for  13185 th iteration => 0.0261892533182\n",
      "Loss for  13186 th iteration => 0.0261859722\n",
      "Loss for  13187 th iteration => 0.0261849712398\n",
      "Loss for  13188 th iteration => 0.0261807508203\n",
      "Loss for  13189 th iteration => 0.0261796707288\n",
      "Loss for  13190 th iteration => 0.0261776178331\n",
      "Loss for  13191 th iteration => 0.0261751696183\n",
      "Loss for  13192 th iteration => 0.0261726087067\n",
      "Loss for  13193 th iteration => 0.0261706697358\n",
      "Loss for  13194 th iteration => 0.0261679108286\n",
      "Loss for  13195 th iteration => 0.0261647287684\n",
      "Loss for  13196 th iteration => 0.0261647211254\n",
      "Loss for  13197 th iteration => 0.0261609895514\n",
      "Loss for  13198 th iteration => 0.0261589746845\n",
      "Loss for  13199 th iteration => 0.0261572245191\n",
      "Loss for  13200 th iteration => 0.0261533337864\n",
      "Loss for  13201 th iteration => 0.0261517168025\n",
      "Loss for  13202 th iteration => 0.026148821103\n",
      "Loss for  13203 th iteration => 0.0261476441223\n",
      "Loss for  13204 th iteration => 0.0261454178569\n",
      "Loss for  13205 th iteration => 0.0261426432199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  13206 th iteration => 0.0261409268866\n",
      "Loss for  13207 th iteration => 0.0261378650745\n",
      "Loss for  13208 th iteration => 0.0261348583297\n",
      "Loss for  13209 th iteration => 0.0261340310811\n",
      "Loss for  13210 th iteration => 0.0261315881747\n",
      "Loss for  13211 th iteration => 0.0261290281486\n",
      "Loss for  13212 th iteration => 0.0261271019733\n",
      "Loss for  13213 th iteration => 0.0261240558715\n",
      "Loss for  13214 th iteration => 0.0261217464347\n",
      "Loss for  13215 th iteration => 0.026118835401\n",
      "Loss for  13216 th iteration => 0.0261179781612\n",
      "Loss for  13217 th iteration => 0.0261154683334\n",
      "Loss for  13218 th iteration => 0.0261129850309\n",
      "Loss for  13219 th iteration => 0.0261109879333\n",
      "Loss for  13220 th iteration => 0.0261081031328\n",
      "Loss for  13221 th iteration => 0.0261056971895\n",
      "Loss for  13222 th iteration => 0.0261035579978\n",
      "Loss for  13223 th iteration => 0.0261016679268\n",
      "Loss for  13224 th iteration => 0.0260994070434\n",
      "Loss for  13225 th iteration => 0.0260971922617\n",
      "Loss for  13226 th iteration => 0.0260944329102\n",
      "Loss for  13227 th iteration => 0.0260927563461\n",
      "Loss for  13228 th iteration => 0.026088937835\n",
      "Loss for  13229 th iteration => 0.026087699615\n",
      "Loss for  13230 th iteration => 0.0260860030964\n",
      "Loss for  13231 th iteration => 0.0260832762289\n",
      "Loss for  13232 th iteration => 0.0260809802425\n",
      "Loss for  13233 th iteration => 0.026078852313\n",
      "Loss for  13234 th iteration => 0.0260761277303\n",
      "Loss for  13235 th iteration => 0.0260729384007\n",
      "Loss for  13236 th iteration => 0.0260718031613\n",
      "Loss for  13237 th iteration => 0.026069870593\n",
      "Loss for  13238 th iteration => 0.0260673380242\n",
      "Loss for  13239 th iteration => 0.026064902987\n",
      "Loss for  13240 th iteration => 0.0260628740519\n",
      "Loss for  13241 th iteration => 0.0260601976098\n",
      "Loss for  13242 th iteration => 0.0260569891553\n",
      "Loss for  13243 th iteration => 0.0260570699941\n",
      "Loss for  13244 th iteration => 0.0260532765997\n",
      "Loss for  13245 th iteration => 0.0260513736402\n",
      "Loss for  13246 th iteration => 0.0260495385584\n",
      "Loss for  13247 th iteration => 0.0260457466417\n",
      "Loss for  13248 th iteration => 0.0260448428644\n",
      "Loss for  13249 th iteration => 0.0260411065541\n",
      "Loss for  13250 th iteration => 0.0260396764892\n",
      "Loss for  13251 th iteration => 0.0260380240928\n",
      "Loss for  13252 th iteration => 0.0260352687599\n",
      "Loss for  13253 th iteration => 0.0260330258109\n",
      "Loss for  13254 th iteration => 0.0260309589187\n",
      "Loss for  13255 th iteration => 0.0260273743956\n",
      "Loss for  13256 th iteration => 0.0260259367294\n",
      "Loss for  13257 th iteration => 0.0260245393066\n",
      "Loss for  13258 th iteration => 0.0260215342255\n",
      "Loss for  13259 th iteration => 0.0260195381599\n",
      "Loss for  13260 th iteration => 0.0260171311959\n",
      "Loss for  13261 th iteration => 0.0260146211448\n",
      "Loss for  13262 th iteration => 0.0260113529207\n",
      "Loss for  13263 th iteration => 0.0260101959272\n",
      "Loss for  13264 th iteration => 0.0260083736085\n",
      "Loss for  13265 th iteration => 0.0260057972607\n",
      "Loss for  13266 th iteration => 0.0260033834652\n",
      "Loss for  13267 th iteration => 0.0260013980104\n",
      "Loss for  13268 th iteration => 0.0259986144428\n",
      "Loss for  13269 th iteration => 0.0259954676122\n",
      "Loss for  13270 th iteration => 0.0259949298863\n",
      "Loss for  13271 th iteration => 0.0259920873218\n",
      "Loss for  13272 th iteration => 0.0259899364393\n",
      "Loss for  13273 th iteration => 0.0259876934227\n",
      "Loss for  13274 th iteration => 0.0259849656045\n",
      "Loss for  13275 th iteration => 0.0259834677005\n",
      "Loss for  13276 th iteration => 0.025979588347\n",
      "Loss for  13277 th iteration => 0.0259783829346\n",
      "Loss for  13278 th iteration => 0.0259765238172\n",
      "Loss for  13279 th iteration => 0.02597399442\n",
      "Loss for  13280 th iteration => 0.02597154916\n",
      "Loss for  13281 th iteration => 0.0259696440726\n",
      "Loss for  13282 th iteration => 0.0259659910791\n",
      "Loss for  13283 th iteration => 0.0259638894467\n",
      "Loss for  13284 th iteration => 0.0259635952981\n",
      "Loss for  13285 th iteration => 0.0259601939309\n",
      "Loss for  13286 th iteration => 0.0259579531604\n",
      "Loss for  13287 th iteration => 0.025956473307\n",
      "Loss for  13288 th iteration => 0.0259524816399\n",
      "Loss for  13289 th iteration => 0.0259500802114\n",
      "Loss for  13290 th iteration => 0.0259488917846\n",
      "Loss for  13291 th iteration => 0.0259471382288\n",
      "Loss for  13292 th iteration => 0.0259445131052\n",
      "Loss for  13293 th iteration => 0.0259421723709\n",
      "Loss for  13294 th iteration => 0.0259401338153\n",
      "Loss for  13295 th iteration => 0.0259373980046\n",
      "Loss for  13296 th iteration => 0.0259342363672\n",
      "Loss for  13297 th iteration => 0.02593375914\n",
      "Loss for  13298 th iteration => 0.0259308636379\n",
      "Loss for  13299 th iteration => 0.025928789787\n",
      "Loss for  13300 th iteration => 0.0259264896968\n",
      "Loss for  13301 th iteration => 0.0259238428127\n",
      "Loss for  13302 th iteration => 0.0259222874621\n",
      "Loss for  13303 th iteration => 0.0259184547536\n",
      "Loss for  13304 th iteration => 0.0259172199277\n",
      "Loss for  13305 th iteration => 0.0259154408662\n",
      "Loss for  13306 th iteration => 0.0259128513642\n",
      "Loss for  13307 th iteration => 0.0259104899142\n",
      "Loss for  13308 th iteration => 0.0259085220578\n",
      "Loss for  13309 th iteration => 0.0259049012639\n",
      "Loss for  13310 th iteration => 0.0259028068777\n",
      "Loss for  13311 th iteration => 0.025902558297\n",
      "Loss for  13312 th iteration => 0.0258991263365\n",
      "Loss for  13313 th iteration => 0.0258969444389\n",
      "Loss for  13314 th iteration => 0.0258954208732\n",
      "Loss for  13315 th iteration => 0.0258914742018\n",
      "Loss for  13316 th iteration => 0.0258907203639\n",
      "Loss for  13317 th iteration => 0.02588766742\n",
      "Loss for  13318 th iteration => 0.0258851811783\n",
      "Loss for  13319 th iteration => 0.0258839602743\n",
      "Loss for  13320 th iteration => 0.0258814919003\n",
      "Loss for  13321 th iteration => 0.0258783787111\n",
      "Loss for  13322 th iteration => 0.0258777779703\n",
      "Loss for  13323 th iteration => 0.0258733539774\n",
      "Loss for  13324 th iteration => 0.0258712588515\n",
      "Loss for  13325 th iteration => 0.0258709804538\n",
      "Loss for  13326 th iteration => 0.0258675853638\n",
      "Loss for  13327 th iteration => 0.0258653824354\n",
      "Loss for  13328 th iteration => 0.025863887061\n",
      "Loss for  13329 th iteration => 0.0258599412952\n",
      "Loss for  13330 th iteration => 0.0258584691489\n",
      "Loss for  13331 th iteration => 0.025856237951\n",
      "Loss for  13332 th iteration => 0.0258540946771\n",
      "Loss for  13333 th iteration => 0.0258522302413\n",
      "Loss for  13334 th iteration => 0.0258498682554\n",
      "Loss for  13335 th iteration => 0.0258471903433\n",
      "Loss for  13336 th iteration => 0.0258457002265\n",
      "Loss for  13337 th iteration => 0.0258418479424\n",
      "Loss for  13338 th iteration => 0.0258404697513\n",
      "Loss for  13339 th iteration => 0.0258390136545\n",
      "Loss for  13340 th iteration => 0.025836250508\n",
      "Loss for  13341 th iteration => 0.0258339678295\n",
      "Loss for  13342 th iteration => 0.0258320260337\n",
      "Loss for  13343 th iteration => 0.0258291176662\n",
      "Loss for  13344 th iteration => 0.0258261275982\n",
      "Loss for  13345 th iteration => 0.02582490285\n",
      "Loss for  13346 th iteration => 0.0258231783729\n",
      "Loss for  13347 th iteration => 0.0258205641483\n",
      "Loss for  13348 th iteration => 0.0258182622859\n",
      "Loss for  13349 th iteration => 0.02581623762\n",
      "Loss for  13350 th iteration => 0.0258126985483\n",
      "Loss for  13351 th iteration => 0.0258105260376\n",
      "Loss for  13352 th iteration => 0.0258104471768\n",
      "Loss for  13353 th iteration => 0.0258068694392\n",
      "Loss for  13354 th iteration => 0.0258048733713\n",
      "Loss for  13355 th iteration => 0.0258031881766\n",
      "Loss for  13356 th iteration => 0.0257994136288\n",
      "Loss for  13357 th iteration => 0.0257985182382\n",
      "Loss for  13358 th iteration => 0.025795612814\n",
      "Loss for  13359 th iteration => 0.0257930149831\n",
      "Loss for  13360 th iteration => 0.0257919827977\n",
      "Loss for  13361 th iteration => 0.0257893498883\n",
      "Loss for  13362 th iteration => 0.0257864406626\n",
      "Loss for  13363 th iteration => 0.025785660394\n",
      "Loss for  13364 th iteration => 0.025781368939\n",
      "Loss for  13365 th iteration => 0.0257791838829\n",
      "Loss for  13366 th iteration => 0.0257790955979\n",
      "Loss for  13367 th iteration => 0.0257755345405\n",
      "Loss for  13368 th iteration => 0.025773536974\n",
      "Loss for  13369 th iteration => 0.0257718606352\n",
      "Loss for  13370 th iteration => 0.025768101006\n",
      "Loss for  13371 th iteration => 0.0257671999201\n",
      "Loss for  13372 th iteration => 0.0257643161697\n",
      "Loss for  13373 th iteration => 0.0257617074439\n",
      "Loss for  13374 th iteration => 0.0257606824158\n",
      "Loss for  13375 th iteration => 0.025758049813\n",
      "Loss for  13376 th iteration => 0.0257551549991\n",
      "Loss for  13377 th iteration => 0.0257543678808\n",
      "Loss for  13378 th iteration => 0.0257501148698\n",
      "Loss for  13379 th iteration => 0.0257479117775\n",
      "Loss for  13380 th iteration => 0.0257471044854\n",
      "Loss for  13381 th iteration => 0.025744589846\n",
      "Loss for  13382 th iteration => 0.0257422570373\n",
      "Loss for  13383 th iteration => 0.0257402315097\n",
      "Loss for  13384 th iteration => 0.025737515063\n",
      "Loss for  13385 th iteration => 0.0257351083269\n",
      "Loss for  13386 th iteration => 0.0257324140642\n",
      "Loss for  13387 th iteration => 0.0257315307518\n",
      "Loss for  13388 th iteration => 0.0257290184584\n",
      "Loss for  13389 th iteration => 0.0257265281678\n",
      "Loss for  13390 th iteration => 0.0257248263882\n",
      "Loss for  13391 th iteration => 0.0257217064396\n",
      "Loss for  13392 th iteration => 0.0257188826699\n",
      "Loss for  13393 th iteration => 0.0257175488113\n",
      "Loss for  13394 th iteration => 0.0257160367412\n",
      "Loss for  13395 th iteration => 0.0257132451441\n",
      "Loss for  13396 th iteration => 0.0257111605415\n",
      "Loss for  13397 th iteration => 0.0257089450898\n",
      "Loss for  13398 th iteration => 0.0257055418758\n",
      "Loss for  13399 th iteration => 0.0257033596482\n",
      "Loss for  13400 th iteration => 0.0257027273582\n",
      "Loss for  13401 th iteration => 0.0257000319087\n",
      "Loss for  13402 th iteration => 0.0256977315221\n",
      "Loss for  13403 th iteration => 0.0256958500257\n",
      "Loss for  13404 th iteration => 0.02569282014\n",
      "Loss for  13405 th iteration => 0.0256907918683\n",
      "Loss for  13406 th iteration => 0.0256885627724\n",
      "Loss for  13407 th iteration => 0.0256865573775\n",
      "Loss for  13408 th iteration => 0.0256846711263\n",
      "Loss for  13409 th iteration => 0.0256823827209\n",
      "Loss for  13410 th iteration => 0.0256796954447\n",
      "Loss for  13411 th iteration => 0.0256782028458\n",
      "Loss for  13412 th iteration => 0.0256750647227\n",
      "Loss for  13413 th iteration => 0.0256722727362\n",
      "Loss for  13414 th iteration => 0.0256716410762\n",
      "Loss for  13415 th iteration => 0.0256689208388\n",
      "Loss for  13416 th iteration => 0.025666658625\n",
      "Loss for  13417 th iteration => 0.0256647483554\n",
      "Loss for  13418 th iteration => 0.0256617709831\n",
      "Loss for  13419 th iteration => 0.0256597299708\n",
      "Loss for  13420 th iteration => 0.0256575254817\n",
      "Loss for  13421 th iteration => 0.0256554752504\n",
      "Loss for  13422 th iteration => 0.0256536327969\n",
      "Loss for  13423 th iteration => 0.0256513100385\n",
      "Loss for  13424 th iteration => 0.025648670138\n",
      "Loss for  13425 th iteration => 0.0256471406051\n",
      "Loss for  13426 th iteration => 0.0256433961055\n",
      "Loss for  13427 th iteration => 0.0256413244139\n",
      "Loss for  13428 th iteration => 0.0256410905665\n",
      "Loss for  13429 th iteration => 0.0256377054386\n",
      "Loss for  13430 th iteration => 0.0256356011283\n",
      "Loss for  13431 th iteration => 0.0256340623675\n",
      "Loss for  13432 th iteration => 0.0256302607981\n",
      "Loss for  13433 th iteration => 0.0256286917023\n",
      "Loss for  13434 th iteration => 0.0256266484301\n",
      "Loss for  13435 th iteration => 0.0256243598602\n",
      "Loss for  13436 th iteration => 0.025622760442\n",
      "Loss for  13437 th iteration => 0.0256202042792\n",
      "Loss for  13438 th iteration => 0.0256178118177\n",
      "Loss for  13439 th iteration => 0.0256161247501\n",
      "Loss for  13440 th iteration => 0.0256124621957\n",
      "Loss for  13441 th iteration => 0.0256109586047\n",
      "Loss for  13442 th iteration => 0.025609782068\n",
      "Loss for  13443 th iteration => 0.0256068102614\n",
      "Loss for  13444 th iteration => 0.0256048269379\n",
      "Loss for  13445 th iteration => 0.0256026567763\n",
      "Loss for  13446 th iteration => 0.0255999796174\n",
      "Loss for  13447 th iteration => 0.0255977672225\n",
      "Loss for  13448 th iteration => 0.0255950520738\n",
      "Loss for  13449 th iteration => 0.0255937990599\n",
      "Loss for  13450 th iteration => 0.0255918051017\n",
      "Loss for  13451 th iteration => 0.025589014594\n",
      "Loss for  13452 th iteration => 0.0255874950756\n",
      "Loss for  13453 th iteration => 0.0255844541303\n",
      "Loss for  13454 th iteration => 0.0255816064099\n",
      "Loss for  13455 th iteration => 0.0255807734983\n",
      "Loss for  13456 th iteration => 0.0255778513934\n",
      "Loss for  13457 th iteration => 0.0255763330951\n",
      "Loss for  13458 th iteration => 0.0255742301729\n",
      "Loss for  13459 th iteration => 0.0255709048391\n",
      "Loss for  13460 th iteration => 0.0255705853493\n",
      "Loss for  13461 th iteration => 0.0255661278846\n",
      "Loss for  13462 th iteration => 0.0255641829442\n",
      "Loss for  13463 th iteration => 0.0255637234227\n",
      "Loss for  13464 th iteration => 0.0255605790178\n",
      "Loss for  13465 th iteration => 0.0255582759706\n",
      "Loss for  13466 th iteration => 0.025556951288\n",
      "Loss for  13467 th iteration => 0.0255530387148\n",
      "Loss for  13468 th iteration => 0.0255515794278\n",
      "Loss for  13469 th iteration => 0.0255500513933\n",
      "Loss for  13470 th iteration => 0.0255469548692\n",
      "Loss for  13471 th iteration => 0.0255456454645\n",
      "Loss for  13472 th iteration => 0.025543343679\n",
      "Loss for  13473 th iteration => 0.0255402266908\n",
      "Loss for  13474 th iteration => 0.0255397089339\n",
      "Loss for  13475 th iteration => 0.0255353921321\n",
      "Loss for  13476 th iteration => 0.0255333245607\n",
      "Loss for  13477 th iteration => 0.0255330566212\n",
      "Loss for  13478 th iteration => 0.0255297303892\n",
      "Loss for  13479 th iteration => 0.0255276190615\n",
      "Loss for  13480 th iteration => 0.0255261124682\n",
      "Loss for  13481 th iteration => 0.0255223541261\n",
      "Loss for  13482 th iteration => 0.025520787906\n",
      "Loss for  13483 th iteration => 0.0255180751296\n",
      "Loss for  13484 th iteration => 0.0255168429649\n",
      "Loss for  13485 th iteration => 0.0255148416266\n",
      "Loss for  13486 th iteration => 0.0255120879019\n",
      "Loss for  13487 th iteration => 0.025510556568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  13488 th iteration => 0.0255075488397\n",
      "Loss for  13489 th iteration => 0.025504701251\n",
      "Loss for  13490 th iteration => 0.0255039043412\n",
      "Loss for  13491 th iteration => 0.0255009585519\n",
      "Loss for  13492 th iteration => 0.0254994927562\n",
      "Loss for  13493 th iteration => 0.0254973559794\n",
      "Loss for  13494 th iteration => 0.0254940994201\n",
      "Loss for  13495 th iteration => 0.025493730034\n",
      "Loss for  13496 th iteration => 0.0254893365369\n",
      "Loss for  13497 th iteration => 0.0254873600967\n",
      "Loss for  13498 th iteration => 0.0254869662409\n",
      "Loss for  13499 th iteration => 0.0254837749616\n",
      "Loss for  13500 th iteration => 0.0254815534087\n",
      "Loss for  13501 th iteration => 0.0254801662487\n",
      "Loss for  13502 th iteration => 0.0254763301585\n",
      "Loss for  13503 th iteration => 0.0254748272651\n",
      "Loss for  13504 th iteration => 0.0254727956523\n",
      "Loss for  13505 th iteration => 0.0254705141075\n",
      "Loss for  13506 th iteration => 0.025468932937\n",
      "Loss for  13507 th iteration => 0.0254664048569\n",
      "Loss for  13508 th iteration => 0.0254640473365\n",
      "Loss for  13509 th iteration => 0.0254623895541\n",
      "Loss for  13510 th iteration => 0.0254587813118\n",
      "Loss for  13511 th iteration => 0.0254572576767\n",
      "Loss for  13512 th iteration => 0.0254561191524\n",
      "Loss for  13513 th iteration => 0.0254531557893\n",
      "Loss for  13514 th iteration => 0.0254512264443\n",
      "Loss for  13515 th iteration => 0.0254490488096\n",
      "Loss for  13516 th iteration => 0.0254464342906\n",
      "Loss for  13517 th iteration => 0.0254442453784\n",
      "Loss for  13518 th iteration => 0.0254415437337\n",
      "Loss for  13519 th iteration => 0.0254403213253\n",
      "Loss for  13520 th iteration => 0.0254383299755\n",
      "Loss for  13521 th iteration => 0.0254355953556\n",
      "Loss for  13522 th iteration => 0.025434069611\n",
      "Loss for  13523 th iteration => 0.0254310799386\n",
      "Loss for  13524 th iteration => 0.0254282412661\n",
      "Loss for  13525 th iteration => 0.0254274771181\n",
      "Loss for  13526 th iteration => 0.0254245025412\n",
      "Loss for  13527 th iteration => 0.025423099864\n",
      "Loss for  13528 th iteration => 0.0254209186226\n",
      "Loss for  13529 th iteration => 0.0254177408607\n",
      "Loss for  13530 th iteration => 0.025417311556\n",
      "Loss for  13531 th iteration => 0.0254129888783\n",
      "Loss for  13532 th iteration => 0.02541097413\n",
      "Loss for  13533 th iteration => 0.0254106549984\n",
      "Loss for  13534 th iteration => 0.0254074077693\n",
      "Loss for  13535 th iteration => 0.0254052762376\n",
      "Loss for  13536 th iteration => 0.025403818052\n",
      "Loss for  13537 th iteration => 0.0254000700267\n",
      "Loss for  13538 th iteration => 0.0253985317448\n",
      "Loss for  13539 th iteration => 0.0253958383075\n",
      "Loss for  13540 th iteration => 0.0253946238969\n",
      "Loss for  13541 th iteration => 0.0253926315776\n",
      "Loss for  13542 th iteration => 0.0253899156149\n",
      "Loss for  13543 th iteration => 0.0253883859611\n",
      "Loss for  13544 th iteration => 0.0253854097186\n",
      "Loss for  13545 th iteration => 0.0253825774108\n",
      "Loss for  13546 th iteration => 0.0253818164238\n",
      "Loss for  13547 th iteration => 0.0253788610641\n",
      "Loss for  13548 th iteration => 0.0253774567001\n",
      "Loss for  13549 th iteration => 0.0253752877188\n",
      "Loss for  13550 th iteration => 0.0253721191029\n",
      "Loss for  13551 th iteration => 0.0253716913686\n",
      "Loss for  13552 th iteration => 0.0253673709458\n",
      "Loss for  13553 th iteration => 0.0253653723604\n",
      "Loss for  13554 th iteration => 0.0253650629894\n",
      "Loss for  13555 th iteration => 0.025361816712\n",
      "Loss for  13556 th iteration => 0.0253597053743\n",
      "Loss for  13557 th iteration => 0.0253582378464\n",
      "Loss for  13558 th iteration => 0.0253545023398\n",
      "Loss for  13559 th iteration => 0.0253529703373\n",
      "Loss for  13560 th iteration => 0.0253502779722\n",
      "Loss for  13561 th iteration => 0.0253490939537\n",
      "Loss for  13562 th iteration => 0.0253470873742\n",
      "Loss for  13563 th iteration => 0.0253444027757\n",
      "Loss for  13564 th iteration => 0.0253428564502\n",
      "Loss for  13565 th iteration => 0.0253398920632\n",
      "Loss for  13566 th iteration => 0.0253370598509\n",
      "Loss for  13567 th iteration => 0.0253363313665\n",
      "Loss for  13568 th iteration => 0.0253333566112\n",
      "Loss for  13569 th iteration => 0.0253319959783\n",
      "Loss for  13570 th iteration => 0.0253297944029\n",
      "Loss for  13571 th iteration => 0.0253266785876\n",
      "Loss for  13572 th iteration => 0.0253262093213\n",
      "Loss for  13573 th iteration => 0.0253219145864\n",
      "Loss for  13574 th iteration => 0.0253199098773\n",
      "Loss for  13575 th iteration => 0.025319650108\n",
      "Loss for  13576 th iteration => 0.0253163654228\n",
      "Loss for  13577 th iteration => 0.0253143125698\n",
      "Loss for  13578 th iteration => 0.0253127978807\n",
      "Loss for  13579 th iteration => 0.0253091016621\n",
      "Loss for  13580 th iteration => 0.0253075621114\n",
      "Loss for  13581 th iteration => 0.0253048662804\n",
      "Loss for  13582 th iteration => 0.0253037268659\n",
      "Loss for  13583 th iteration => 0.0253016977383\n",
      "Loss for  13584 th iteration => 0.0252990523827\n",
      "Loss for  13585 th iteration => 0.0252974814446\n",
      "Loss for  13586 th iteration => 0.0252945277385\n",
      "Loss for  13587 th iteration => 0.025291691648\n",
      "Loss for  13588 th iteration => 0.0252910146371\n",
      "Loss for  13589 th iteration => 0.025287995218\n",
      "Loss for  13590 th iteration => 0.0252867079925\n",
      "Loss for  13591 th iteration => 0.0252844444975\n",
      "Loss for  13592 th iteration => 0.0252814099892\n",
      "Loss for  13593 th iteration => 0.0252808710273\n",
      "Loss for  13594 th iteration => 0.0252766175568\n",
      "Loss for  13595 th iteration => 0.0252745918632\n",
      "Loss for  13596 th iteration => 0.0252744077151\n",
      "Loss for  13597 th iteration => 0.0252710588995\n",
      "Loss for  13598 th iteration => 0.0252690895203\n",
      "Loss for  13599 th iteration => 0.0252675029709\n",
      "Loss for  13600 th iteration => 0.0252638637722\n",
      "Loss for  13601 th iteration => 0.0252623074352\n",
      "Loss for  13602 th iteration => 0.0252596053011\n",
      "Loss for  13603 th iteration => 0.0252585193387\n",
      "Loss for  13604 th iteration => 0.0252564626486\n",
      "Loss for  13605 th iteration => 0.0252538612556\n",
      "Loss for  13606 th iteration => 0.0252522609187\n",
      "Loss for  13607 th iteration => 0.0252493169879\n",
      "Loss for  13608 th iteration => 0.0252464745478\n",
      "Loss for  13609 th iteration => 0.0252458611752\n",
      "Loss for  13610 th iteration => 0.0252427805894\n",
      "Loss for  13611 th iteration => 0.0252415861063\n",
      "Loss for  13612 th iteration => 0.0252392415672\n",
      "Loss for  13613 th iteration => 0.0252363069211\n",
      "Loss for  13614 th iteration => 0.0252356799133\n",
      "Loss for  13615 th iteration => 0.0252314781179\n",
      "Loss for  13616 th iteration => 0.0252294214623\n",
      "Loss for  13617 th iteration => 0.0252293298743\n",
      "Loss for  13618 th iteration => 0.0252259001646\n",
      "Loss for  13619 th iteration => 0.0252240305093\n",
      "Loss for  13620 th iteration => 0.0252223560202\n",
      "Loss for  13621 th iteration => 0.0252187856297\n",
      "Loss for  13622 th iteration => 0.0252172062897\n",
      "Loss for  13623 th iteration => 0.0252144961276\n",
      "Loss for  13624 th iteration => 0.0252134689523\n",
      "Loss for  13625 th iteration => 0.0252113818241\n",
      "Loss for  13626 th iteration => 0.0252088270494\n",
      "Loss for  13627 th iteration => 0.0252071945882\n",
      "Loss for  13628 th iteration => 0.0252042597102\n",
      "Loss for  13629 th iteration => 0.0252014219105\n",
      "Loss for  13630 th iteration => 0.0251995501142\n",
      "Loss for  13631 th iteration => 0.0251984466693\n",
      "Loss for  13632 th iteration => 0.0251964044352\n",
      "Loss for  13633 th iteration => 0.0251938126592\n",
      "Loss for  13634 th iteration => 0.0251922219321\n",
      "Loss for  13635 th iteration => 0.0251892905765\n",
      "Loss for  13636 th iteration => 0.0251864708305\n",
      "Loss for  13637 th iteration => 0.0251858214222\n",
      "Loss for  13638 th iteration => 0.0251835006308\n",
      "Loss for  13639 th iteration => 0.0251811829132\n",
      "Loss for  13640 th iteration => 0.0251793227139\n",
      "Loss for  13641 th iteration => 0.025176562605\n",
      "Loss for  13642 th iteration => 0.0251751453634\n",
      "Loss for  13643 th iteration => 0.0251722805014\n",
      "Loss for  13644 th iteration => 0.0251695371948\n",
      "Loss for  13645 th iteration => 0.0251685922997\n",
      "Loss for  13646 th iteration => 0.025166428111\n",
      "Loss for  13647 th iteration => 0.0251639667281\n",
      "Loss for  13648 th iteration => 0.0251622554563\n",
      "Loss for  13649 th iteration => 0.0251593767848\n",
      "Loss for  13650 th iteration => 0.0251565372204\n",
      "Loss for  13651 th iteration => 0.0251546403474\n",
      "Loss for  13652 th iteration => 0.0251536225528\n",
      "Loss for  13653 th iteration => 0.0251515020506\n",
      "Loss for  13654 th iteration => 0.0251490047448\n",
      "Loss for  13655 th iteration => 0.0251473341029\n",
      "Loss for  13656 th iteration => 0.0251444575789\n",
      "Loss for  13657 th iteration => 0.0251416041827\n",
      "Loss for  13658 th iteration => 0.0251410410488\n",
      "Loss for  13659 th iteration => 0.0251386424656\n",
      "Loss for  13660 th iteration => 0.0251364186999\n",
      "Loss for  13661 th iteration => 0.0251344790854\n",
      "Loss for  13662 th iteration => 0.0251318144334\n",
      "Loss for  13663 th iteration => 0.0251303162582\n",
      "Loss for  13664 th iteration => 0.0251275041841\n",
      "Loss for  13665 th iteration => 0.0251247268088\n",
      "Loss for  13666 th iteration => 0.0251238714406\n",
      "Loss for  13667 th iteration => 0.0251216287164\n",
      "Loss for  13668 th iteration => 0.0251192618834\n",
      "Loss for  13669 th iteration => 0.0251174705653\n",
      "Loss for  13670 th iteration => 0.0251146700668\n",
      "Loss for  13671 th iteration => 0.0251133840919\n",
      "Loss for  13672 th iteration => 0.0251096505366\n",
      "Loss for  13673 th iteration => 0.0251092141924\n",
      "Loss for  13674 th iteration => 0.025106539712\n",
      "Loss for  13675 th iteration => 0.0251046061963\n",
      "Loss for  13676 th iteration => 0.0251023875461\n",
      "Loss for  13677 th iteration => 0.0251000160271\n",
      "Loss for  13678 th iteration => 0.0250983273953\n",
      "Loss for  13679 th iteration => 0.0250948170714\n",
      "Loss for  13680 th iteration => 0.0250937203028\n",
      "Loss for  13681 th iteration => 0.0250920803537\n",
      "Loss for  13682 th iteration => 0.0250895726605\n",
      "Loss for  13683 th iteration => 0.0250874852057\n",
      "Loss for  13684 th iteration => 0.0250854256465\n",
      "Loss for  13685 th iteration => 0.0250829612599\n",
      "Loss for  13686 th iteration => 0.0250799571381\n",
      "Loss for  13687 th iteration => 0.0250781389617\n",
      "Loss for  13688 th iteration => 0.02507765765\n",
      "Loss for  13689 th iteration => 0.0250746474421\n",
      "Loss for  13690 th iteration => 0.025072437687\n",
      "Loss for  13691 th iteration => 0.0250711336281\n",
      "Loss for  13692 th iteration => 0.025067348579\n",
      "Loss for  13693 th iteration => 0.0250650738264\n",
      "Loss for  13694 th iteration => 0.0250633703446\n",
      "Loss for  13695 th iteration => 0.0250627495523\n",
      "Loss for  13696 th iteration => 0.0250598803456\n",
      "Loss for  13697 th iteration => 0.0250575397593\n",
      "Loss for  13698 th iteration => 0.0250563681303\n",
      "Loss for  13699 th iteration => 0.0250524816365\n",
      "Loss for  13700 th iteration => 0.0250502274308\n",
      "Loss for  13701 th iteration => 0.025049861296\n",
      "Loss for  13702 th iteration => 0.0250471508044\n",
      "Loss for  13703 th iteration => 0.0250452750528\n",
      "Loss for  13704 th iteration => 0.0250430176274\n",
      "Loss for  13705 th iteration => 0.0250407064707\n",
      "Loss for  13706 th iteration => 0.0250389537381\n",
      "Loss for  13707 th iteration => 0.0250354734198\n",
      "Loss for  13708 th iteration => 0.0250343890066\n",
      "Loss for  13709 th iteration => 0.0250328075333\n",
      "Loss for  13710 th iteration => 0.0250302603358\n",
      "Loss for  13711 th iteration => 0.0250282339125\n",
      "Loss for  13712 th iteration => 0.0250261322723\n",
      "Loss for  13713 th iteration => 0.0250236776189\n",
      "Loss for  13714 th iteration => 0.0250221968305\n",
      "Loss for  13715 th iteration => 0.0250186473099\n",
      "Loss for  13716 th iteration => 0.0250175083944\n",
      "Loss for  13717 th iteration => 0.025015797648\n",
      "Loss for  13718 th iteration => 0.0250133849344\n",
      "Loss for  13719 th iteration => 0.025011235688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  13720 th iteration => 0.0250092704658\n",
      "Loss for  13721 th iteration => 0.0250059596352\n",
      "Loss for  13722 th iteration => 0.0250039232996\n",
      "Loss for  13723 th iteration => 0.0250039019958\n",
      "Loss for  13724 th iteration => 0.0250004549231\n",
      "Loss for  13725 th iteration => 0.0249987053205\n",
      "Loss for  13726 th iteration => 0.0249969643714\n",
      "Loss for  13727 th iteration => 0.0249935639137\n",
      "Loss for  13728 th iteration => 0.0249924947563\n",
      "Loss for  13729 th iteration => 0.0249892182899\n",
      "Loss for  13730 th iteration => 0.0249877987504\n",
      "Loss for  13731 th iteration => 0.024986499249\n",
      "Loss for  13732 th iteration => 0.024983685829\n",
      "Loss for  13733 th iteration => 0.0249819452492\n",
      "Loss for  13734 th iteration => 0.0249796033862\n",
      "Loss for  13735 th iteration => 0.0249764890805\n",
      "Loss for  13736 th iteration => 0.0249752353413\n",
      "Loss for  13737 th iteration => 0.0249726651036\n",
      "Loss for  13738 th iteration => 0.0249718769378\n",
      "Loss for  13739 th iteration => 0.0249691937579\n",
      "Loss for  13740 th iteration => 0.0249667128174\n",
      "Loss for  13741 th iteration => 0.0249657005114\n",
      "Loss for  13742 th iteration => 0.024961772393\n",
      "Loss for  13743 th iteration => 0.0249596382875\n",
      "Loss for  13744 th iteration => 0.0249585128304\n",
      "Loss for  13745 th iteration => 0.024956895265\n",
      "Loss for  13746 th iteration => 0.0249544082341\n",
      "Loss for  13747 th iteration => 0.0249523535024\n",
      "Loss for  13748 th iteration => 0.0249503041524\n",
      "Loss for  13749 th iteration => 0.0249478999574\n",
      "Loss for  13750 th iteration => 0.024944830055\n",
      "Loss for  13751 th iteration => 0.02494321842\n",
      "Loss for  13752 th iteration => 0.0249425115505\n",
      "Loss for  13753 th iteration => 0.0249397548883\n",
      "Loss for  13754 th iteration => 0.0249373590452\n",
      "Loss for  13755 th iteration => 0.0249362695259\n",
      "Loss for  13756 th iteration => 0.0249323582008\n",
      "Loss for  13757 th iteration => 0.0249301442625\n",
      "Loss for  13758 th iteration => 0.0249305532016\n",
      "Loss for  13759 th iteration => 0.0249266984492\n",
      "Loss for  13760 th iteration => 0.0249253802737\n",
      "Loss for  13761 th iteration => 0.024923230586\n",
      "Loss for  13762 th iteration => 0.0249202497335\n",
      "Loss for  13763 th iteration => 0.0249197410904\n",
      "Loss for  13764 th iteration => 0.0249155554334\n",
      "Loss for  13765 th iteration => 0.024913605918\n",
      "Loss for  13766 th iteration => 0.0249134805832\n",
      "Loss for  13767 th iteration => 0.0249101569014\n",
      "Loss for  13768 th iteration => 0.0249083278638\n",
      "Loss for  13769 th iteration => 0.0249066860063\n",
      "Loss for  13770 th iteration => 0.0249032168763\n",
      "Loss for  13771 th iteration => 0.0249031936568\n",
      "Loss for  13772 th iteration => 0.0248987837975\n",
      "Loss for  13773 th iteration => 0.0248970512398\n",
      "Loss for  13774 th iteration => 0.0248964800649\n",
      "Loss for  13775 th iteration => 0.0248936001524\n",
      "Loss for  13776 th iteration => 0.0248913454237\n",
      "Loss for  13777 th iteration => 0.024890127341\n",
      "Loss for  13778 th iteration => 0.0248863559871\n",
      "Loss for  13779 th iteration => 0.0248841147339\n",
      "Loss for  13780 th iteration => 0.0248824531122\n",
      "Loss for  13781 th iteration => 0.0248818213101\n",
      "Loss for  13782 th iteration => 0.0248790045361\n",
      "Loss for  13783 th iteration => 0.0248766948122\n",
      "Loss for  13784 th iteration => 0.0248755342947\n",
      "Loss for  13785 th iteration => 0.0248717021203\n",
      "Loss for  13786 th iteration => 0.0248694636647\n",
      "Loss for  13787 th iteration => 0.0248678659437\n",
      "Loss for  13788 th iteration => 0.0248671862249\n",
      "Loss for  13789 th iteration => 0.0248644200435\n",
      "Loss for  13790 th iteration => 0.024862067558\n",
      "Loss for  13791 th iteration => 0.024860952534\n",
      "Loss for  13792 th iteration => 0.0248570660874\n",
      "Loss for  13793 th iteration => 0.0248548580708\n",
      "Loss for  13794 th iteration => 0.0248553084347\n",
      "Loss for  13795 th iteration => 0.0248514300497\n",
      "Loss for  13796 th iteration => 0.0248501690345\n",
      "Loss for  13797 th iteration => 0.0248479801951\n",
      "Loss for  13798 th iteration => 0.0248450715186\n",
      "Loss for  13799 th iteration => 0.0248445089221\n",
      "Loss for  13800 th iteration => 0.0248403633839\n",
      "Loss for  13801 th iteration => 0.0248384053274\n",
      "Loss for  13802 th iteration => 0.0248383476496\n",
      "Loss for  13803 th iteration => 0.024834974425\n",
      "Loss for  13804 th iteration => 0.0248332277361\n",
      "Loss for  13805 th iteration => 0.0248315218545\n",
      "Loss for  13806 th iteration => 0.0248281490821\n",
      "Loss for  13807 th iteration => 0.0248280480369\n",
      "Loss for  13808 th iteration => 0.0248236911347\n",
      "Loss for  13809 th iteration => 0.0248219378451\n",
      "Loss for  13810 th iteration => 0.0248214563349\n",
      "Loss for  13811 th iteration => 0.0248185051406\n",
      "Loss for  13812 th iteration => 0.0248163538858\n",
      "Loss for  13813 th iteration => 0.0248150509167\n",
      "Loss for  13814 th iteration => 0.0248113365591\n",
      "Loss for  13815 th iteration => 0.0248091086873\n",
      "Loss for  13816 th iteration => 0.0248074176475\n",
      "Loss for  13817 th iteration => 0.0248068904992\n",
      "Loss for  13818 th iteration => 0.0248039876293\n",
      "Loss for  13819 th iteration => 0.0248017957778\n",
      "Loss for  13820 th iteration => 0.0248005361466\n",
      "Loss for  13821 th iteration => 0.024796770176\n",
      "Loss for  13822 th iteration => 0.024795409228\n",
      "Loss for  13823 th iteration => 0.0247927773638\n",
      "Loss for  13824 th iteration => 0.0247924995198\n",
      "Loss for  13825 th iteration => 0.0247893539923\n",
      "Loss for  13826 th iteration => 0.024787405599\n",
      "Loss for  13827 th iteration => 0.0247859091408\n",
      "Loss for  13828 th iteration => 0.0247823687392\n",
      "Loss for  13829 th iteration => 0.0247814893655\n",
      "Loss for  13830 th iteration => 0.0247781830997\n",
      "Loss for  13831 th iteration => 0.0247773622566\n",
      "Loss for  13832 th iteration => 0.0247751859487\n",
      "Loss for  13833 th iteration => 0.0247728433626\n",
      "Loss for  13834 th iteration => 0.0247711754678\n",
      "Loss for  13835 th iteration => 0.0247683558009\n",
      "Loss for  13836 th iteration => 0.0247662109156\n",
      "Loss for  13837 th iteration => 0.0247644592759\n",
      "Loss for  13838 th iteration => 0.0247618413546\n",
      "Loss for  13839 th iteration => 0.0247612134146\n",
      "Loss for  13840 th iteration => 0.0247584204387\n",
      "Loss for  13841 th iteration => 0.0247561417661\n",
      "Loss for  13842 th iteration => 0.0247549782188\n",
      "Loss for  13843 th iteration => 0.0247512114491\n",
      "Loss for  13844 th iteration => 0.0247505483642\n",
      "Loss for  13845 th iteration => 0.0247478061654\n",
      "Loss for  13846 th iteration => 0.0247454605665\n",
      "Loss for  13847 th iteration => 0.0247444372122\n",
      "Loss for  13848 th iteration => 0.0247420381622\n",
      "Loss for  13849 th iteration => 0.0247393822354\n",
      "Loss for  13850 th iteration => 0.0247385945991\n",
      "Loss for  13851 th iteration => 0.0247346468776\n",
      "Loss for  13852 th iteration => 0.0247325812326\n",
      "Loss for  13853 th iteration => 0.0247314411299\n",
      "Loss for  13854 th iteration => 0.024729837869\n",
      "Loss for  13855 th iteration => 0.0247274080955\n",
      "Loss for  13856 th iteration => 0.024725381841\n",
      "Loss for  13857 th iteration => 0.024723375447\n",
      "Loss for  13858 th iteration => 0.0247210415399\n",
      "Loss for  13859 th iteration => 0.0247180124068\n",
      "Loss for  13860 th iteration => 0.0247164338651\n",
      "Loss for  13861 th iteration => 0.024715700174\n",
      "Loss for  13862 th iteration => 0.0247130219145\n",
      "Loss for  13863 th iteration => 0.0247106516025\n",
      "Loss for  13864 th iteration => 0.0247095888213\n",
      "Loss for  13865 th iteration => 0.0247057643622\n",
      "Loss for  13866 th iteration => 0.0247035576633\n",
      "Loss for  13867 th iteration => 0.0247027093191\n",
      "Loss for  13868 th iteration => 0.0247008693287\n",
      "Loss for  13869 th iteration => 0.024698684506\n",
      "Loss for  13870 th iteration => 0.0246964227892\n",
      "Loss for  13871 th iteration => 0.0246946600953\n",
      "Loss for  13872 th iteration => 0.0246920461048\n",
      "Loss for  13873 th iteration => 0.0246891885477\n",
      "Loss for  13874 th iteration => 0.0246887483882\n",
      "Loss for  13875 th iteration => 0.0246862543354\n",
      "Loss for  13876 th iteration => 0.0246842961055\n",
      "Loss for  13877 th iteration => 0.0246822344783\n",
      "Loss for  13878 th iteration => 0.0246798604258\n",
      "Loss for  13879 th iteration => 0.0246782149478\n",
      "Loss for  13880 th iteration => 0.0246756540863\n",
      "Loss for  13881 th iteration => 0.0246728461856\n",
      "Loss for  13882 th iteration => 0.0246722029182\n",
      "Loss for  13883 th iteration => 0.0246698147803\n",
      "Loss for  13884 th iteration => 0.0246677609947\n",
      "Loss for  13885 th iteration => 0.0246657998907\n",
      "Loss for  13886 th iteration => 0.0246633354348\n",
      "Loss for  13887 th iteration => 0.0246618701958\n",
      "Loss for  13888 th iteration => 0.0246583677068\n",
      "Loss for  13889 th iteration => 0.0246566657562\n",
      "Loss for  13890 th iteration => 0.0246561354719\n",
      "Loss for  13891 th iteration => 0.0246532701549\n",
      "Loss for  13892 th iteration => 0.024651109161\n",
      "Loss for  13893 th iteration => 0.0246498535393\n",
      "Loss for  13894 th iteration => 0.0246461770333\n",
      "Loss for  13895 th iteration => 0.0246454868\n",
      "Loss for  13896 th iteration => 0.0246420807569\n",
      "Loss for  13897 th iteration => 0.0246408628344\n",
      "Loss for  13898 th iteration => 0.0246393195126\n",
      "Loss for  13899 th iteration => 0.0246368583463\n",
      "Loss for  13900 th iteration => 0.0246348980707\n",
      "Loss for  13901 th iteration => 0.0246328982987\n",
      "Loss for  13902 th iteration => 0.0246297036389\n",
      "Loss for  13903 th iteration => 0.0246277247552\n",
      "Loss for  13904 th iteration => 0.0246271825546\n",
      "Loss for  13905 th iteration => 0.0246246222056\n",
      "Loss for  13906 th iteration => 0.0246226170284\n",
      "Loss for  13907 th iteration => 0.0246207613443\n",
      "Loss for  13908 th iteration => 0.0246181024661\n",
      "Loss for  13909 th iteration => 0.0246160136007\n",
      "Loss for  13910 th iteration => 0.0246134422818\n",
      "Loss for  13911 th iteration => 0.0246126222249\n",
      "Loss for  13912 th iteration => 0.0246105003507\n",
      "Loss for  13913 th iteration => 0.0246081649015\n",
      "Loss for  13914 th iteration => 0.024606540544\n",
      "Loss for  13915 th iteration => 0.0246037429158\n",
      "Loss for  13916 th iteration => 0.0246016324099\n",
      "Loss for  13917 th iteration => 0.0246006556613\n",
      "Loss for  13918 th iteration => 0.0245972009492\n",
      "Loss for  13919 th iteration => 0.024596163761\n",
      "Loss for  13920 th iteration => 0.0245941696022\n",
      "Loss for  13921 th iteration => 0.0245917171198\n",
      "Loss for  13922 th iteration => 0.0245902142159\n",
      "Loss for  13923 th iteration => 0.0245873887406\n",
      "Loss for  13924 th iteration => 0.0245853729492\n",
      "Loss for  13925 th iteration => 0.0245834656484\n",
      "Loss for  13926 th iteration => 0.0245810917516\n",
      "Loss for  13927 th iteration => 0.0245801854377\n",
      "Loss for  13928 th iteration => 0.024577708377\n",
      "Loss for  13929 th iteration => 0.0245752019556\n",
      "Loss for  13930 th iteration => 0.024574304304\n",
      "Loss for  13931 th iteration => 0.0245704795666\n",
      "Loss for  13932 th iteration => 0.0245684282636\n",
      "Loss for  13933 th iteration => 0.0245672518498\n",
      "Loss for  13934 th iteration => 0.0245657736996\n",
      "Loss for  13935 th iteration => 0.0245632704217\n",
      "Loss for  13936 th iteration => 0.0245613767326\n",
      "Loss for  13937 th iteration => 0.0245592893231\n",
      "Loss for  13938 th iteration => 0.0245570324613\n",
      "Loss for  13939 th iteration => 0.0245540621447\n",
      "Loss for  13940 th iteration => 0.024552396565\n",
      "Loss for  13941 th iteration => 0.0245518603112\n",
      "Loss for  13942 th iteration => 0.0245490245994\n",
      "Loss for  13943 th iteration => 0.0245468811544\n",
      "Loss for  13944 th iteration => 0.0245456319295\n",
      "Loss for  13945 th iteration => 0.0245419876041\n",
      "Loss for  13946 th iteration => 0.0245412935318\n",
      "Loss for  13947 th iteration => 0.0245379132681\n",
      "Loss for  13948 th iteration => 0.024536712648\n",
      "Loss for  13949 th iteration => 0.0245351934784\n",
      "Loss for  13950 th iteration => 0.0245327406656\n",
      "Loss for  13951 th iteration => 0.0245308100828\n",
      "Loss for  13952 th iteration => 0.0245287993425\n",
      "Loss for  13953 th iteration => 0.0245256379893\n",
      "Loss for  13954 th iteration => 0.0245236693951\n",
      "Loss for  13955 th iteration => 0.0245224839065\n",
      "Loss for  13956 th iteration => 0.0245209846146\n",
      "Loss for  13957 th iteration => 0.0245185163908\n",
      "Loss for  13958 th iteration => 0.0245166061585\n",
      "Loss for  13959 th iteration => 0.0245145644487\n",
      "Loss for  13960 th iteration => 0.0245113985187\n",
      "Loss for  13961 th iteration => 0.0245094437059\n",
      "Loss for  13962 th iteration => 0.0245089564834\n",
      "Loss for  13963 th iteration => 0.0245063982836\n",
      "Loss for  13964 th iteration => 0.0245044358947\n",
      "Loss for  13965 th iteration => 0.0245025722112\n",
      "Loss for  13966 th iteration => 0.0244999365197\n",
      "Loss for  13967 th iteration => 0.0244987651508\n",
      "Loss for  13968 th iteration => 0.0244951493288\n",
      "Loss for  13969 th iteration => 0.0244941088241\n",
      "Loss for  13970 th iteration => 0.0244926189665\n",
      "Loss for  13971 th iteration => 0.0244901500063\n",
      "Loss for  13972 th iteration => 0.024488246243\n",
      "Loss for  13973 th iteration => 0.0244861915265\n",
      "Loss for  13974 th iteration => 0.0244838946001\n",
      "Loss for  13975 th iteration => 0.0244816826201\n",
      "Loss for  13976 th iteration => 0.0244797283813\n",
      "Loss for  13977 th iteration => 0.0244779139697\n",
      "Loss for  13978 th iteration => 0.0244763346031\n",
      "Loss for  13979 th iteration => 0.0244739600244\n",
      "Loss for  13980 th iteration => 0.0244719714167\n",
      "Loss for  13981 th iteration => 0.0244700063593\n",
      "Loss for  13982 th iteration => 0.0244676849941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  13983 th iteration => 0.0244655806969\n",
      "Loss for  13984 th iteration => 0.0244635374162\n",
      "Loss for  13985 th iteration => 0.0244617354893\n",
      "Loss for  13986 th iteration => 0.0244600836195\n",
      "Loss for  13987 th iteration => 0.0244577864275\n",
      "Loss for  13988 th iteration => 0.0244557294079\n",
      "Loss for  13989 th iteration => 0.0244538861658\n",
      "Loss for  13990 th iteration => 0.0244506796346\n",
      "Loss for  13991 th iteration => 0.0244487913649\n",
      "Loss for  13992 th iteration => 0.0244479773222\n",
      "Loss for  13993 th iteration => 0.0244458097322\n",
      "Loss for  13994 th iteration => 0.0244435836796\n",
      "Loss for  13995 th iteration => 0.0244419003933\n",
      "Loss for  13996 th iteration => 0.0244392290396\n",
      "Loss for  13997 th iteration => 0.0244371496793\n",
      "Loss for  13998 th iteration => 0.0244345850903\n",
      "Loss for  13999 th iteration => 0.0244333753203\n",
      "Loss for  14000 th iteration => 0.0244319558068\n",
      "Loss for  14001 th iteration => 0.0244294356493\n",
      "Loss for  14002 th iteration => 0.0244276085753\n",
      "Loss for  14003 th iteration => 0.0244254996543\n",
      "Loss for  14004 th iteration => 0.0244224294245\n",
      "Loss for  14005 th iteration => 0.0244204544234\n",
      "Loss for  14006 th iteration => 0.0244192611177\n",
      "Loss for  14007 th iteration => 0.0244178636435\n",
      "Loss for  14008 th iteration => 0.0244153258641\n",
      "Loss for  14009 th iteration => 0.0244135211637\n",
      "Loss for  14010 th iteration => 0.0244113908684\n",
      "Loss for  14011 th iteration => 0.0244091939885\n",
      "Loss for  14012 th iteration => 0.0244077225669\n",
      "Loss for  14013 th iteration => 0.0244050998155\n",
      "Loss for  14014 th iteration => 0.0244027496802\n",
      "Loss for  14015 th iteration => 0.0244018487478\n",
      "Loss for  14016 th iteration => 0.024399406826\n",
      "Loss for  14017 th iteration => 0.0243969450308\n",
      "Loss for  14018 th iteration => 0.0243960437973\n",
      "Loss for  14019 th iteration => 0.0243922643838\n",
      "Loss for  14020 th iteration => 0.0243901561378\n",
      "Loss for  14021 th iteration => 0.0243892098066\n",
      "Loss for  14022 th iteration => 0.0243875343361\n",
      "Loss for  14023 th iteration => 0.0243852830221\n",
      "Loss for  14024 th iteration => 0.0243832030354\n",
      "Loss for  14025 th iteration => 0.0243813564932\n",
      "Loss for  14026 th iteration => 0.0243789161261\n",
      "Loss for  14027 th iteration => 0.0243760825559\n",
      "Loss for  14028 th iteration => 0.0243744001561\n",
      "Loss for  14029 th iteration => 0.024373982927\n",
      "Loss for  14030 th iteration => 0.0243710701054\n",
      "Loss for  14031 th iteration => 0.0243690807033\n",
      "Loss for  14032 th iteration => 0.0243677198432\n",
      "Loss for  14033 th iteration => 0.0243642161666\n",
      "Loss for  14034 th iteration => 0.0243643497803\n",
      "Loss for  14035 th iteration => 0.0243600338656\n",
      "Loss for  14036 th iteration => 0.0243590747535\n",
      "Loss for  14037 th iteration => 0.0243574185953\n",
      "Loss for  14038 th iteration => 0.0243551940932\n",
      "Loss for  14039 th iteration => 0.0243530612677\n",
      "Loss for  14040 th iteration => 0.0243513123582\n",
      "Loss for  14041 th iteration => 0.0243487774793\n",
      "Loss for  14042 th iteration => 0.024346027865\n",
      "Loss for  14043 th iteration => 0.0243442513182\n",
      "Loss for  14044 th iteration => 0.0243439637711\n",
      "Loss for  14045 th iteration => 0.0243409298236\n",
      "Loss for  14046 th iteration => 0.0243390719485\n",
      "Loss for  14047 th iteration => 0.024337588173\n",
      "Loss for  14048 th iteration => 0.0243342177363\n",
      "Loss for  14049 th iteration => 0.0243342267752\n",
      "Loss for  14050 th iteration => 0.0243300261143\n",
      "Loss for  14051 th iteration => 0.0243288266101\n",
      "Loss for  14052 th iteration => 0.0243275245774\n",
      "Loss for  14053 th iteration => 0.0243250568166\n",
      "Loss for  14054 th iteration => 0.0243230783026\n",
      "Loss for  14055 th iteration => 0.024321282341\n",
      "Loss for  14056 th iteration => 0.0243187020917\n",
      "Loss for  14057 th iteration => 0.0243167688238\n",
      "Loss for  14058 th iteration => 0.0243141502767\n",
      "Loss for  14059 th iteration => 0.0243133856702\n",
      "Loss for  14060 th iteration => 0.0243111988397\n",
      "Loss for  14061 th iteration => 0.0243090408395\n",
      "Loss for  14062 th iteration => 0.0243073306433\n",
      "Loss for  14063 th iteration => 0.0243047291496\n",
      "Loss for  14064 th iteration => 0.0243026694377\n",
      "Loss for  14065 th iteration => 0.0243001221002\n",
      "Loss for  14066 th iteration => 0.0242988995735\n",
      "Loss for  14067 th iteration => 0.0242975345791\n",
      "Loss for  14068 th iteration => 0.0242950015759\n",
      "Loss for  14069 th iteration => 0.0242932350704\n",
      "Loss for  14070 th iteration => 0.024291103798\n",
      "Loss for  14071 th iteration => 0.0242889592201\n",
      "Loss for  14072 th iteration => 0.0242866865098\n",
      "Loss for  14073 th iteration => 0.024284271988\n",
      "Loss for  14074 th iteration => 0.0242834951103\n",
      "Loss for  14075 th iteration => 0.0242812390585\n",
      "Loss for  14076 th iteration => 0.0242790649873\n",
      "Loss for  14077 th iteration => 0.0242774770352\n",
      "Loss for  14078 th iteration => 0.0242746976195\n",
      "Loss for  14079 th iteration => 0.0242728953329\n",
      "Loss for  14080 th iteration => 0.0242702250418\n",
      "Loss for  14081 th iteration => 0.0242690020288\n",
      "Loss for  14082 th iteration => 0.0242676035525\n",
      "Loss for  14083 th iteration => 0.0242651131625\n",
      "Loss for  14084 th iteration => 0.0242633161587\n",
      "Loss for  14085 th iteration => 0.0242612380381\n",
      "Loss for  14086 th iteration => 0.0242582456699\n",
      "Loss for  14087 th iteration => 0.0242562705972\n",
      "Loss for  14088 th iteration => 0.02425506594\n",
      "Loss for  14089 th iteration => 0.0242537038575\n",
      "Loss for  14090 th iteration => 0.0242511814417\n",
      "Loss for  14091 th iteration => 0.024249420619\n",
      "Loss for  14092 th iteration => 0.0242472971401\n",
      "Loss for  14093 th iteration => 0.02424515224\n",
      "Loss for  14094 th iteration => 0.0242436646521\n",
      "Loss for  14095 th iteration => 0.0242404024895\n",
      "Loss for  14096 th iteration => 0.0242391607872\n",
      "Loss for  14097 th iteration => 0.0242377612083\n",
      "Loss for  14098 th iteration => 0.0242352811084\n",
      "Loss for  14099 th iteration => 0.0242334855423\n",
      "Loss for  14100 th iteration => 0.0242314374646\n",
      "Loss for  14101 th iteration => 0.0242284220948\n",
      "Loss for  14102 th iteration => 0.0242264786202\n",
      "Loss for  14103 th iteration => 0.024225256063\n",
      "Loss for  14104 th iteration => 0.024223898861\n",
      "Loss for  14105 th iteration => 0.0242213807527\n",
      "Loss for  14106 th iteration => 0.0242196271319\n",
      "Loss for  14107 th iteration => 0.0242175109621\n",
      "Loss for  14108 th iteration => 0.0242145217294\n",
      "Loss for  14109 th iteration => 0.0242125710983\n",
      "Loss for  14110 th iteration => 0.0242113676082\n",
      "Loss for  14111 th iteration => 0.0242100493863\n",
      "Loss for  14112 th iteration => 0.0242074966504\n",
      "Loss for  14113 th iteration => 0.0242057816812\n",
      "Loss for  14114 th iteration => 0.0242036258742\n",
      "Loss for  14115 th iteration => 0.0242015287214\n",
      "Loss for  14116 th iteration => 0.0241999951073\n",
      "Loss for  14117 th iteration => 0.0241967563702\n",
      "Loss for  14118 th iteration => 0.024195516981\n",
      "Loss for  14119 th iteration => 0.024194163903\n",
      "Loss for  14120 th iteration => 0.0241916508213\n",
      "Loss for  14121 th iteration => 0.024189903617\n",
      "Loss for  14122 th iteration => 0.0241878079615\n",
      "Loss for  14123 th iteration => 0.0241848040654\n",
      "Loss for  14124 th iteration => 0.0241828795549\n",
      "Loss for  14125 th iteration => 0.0241816599602\n",
      "Loss for  14126 th iteration => 0.0241803509447\n",
      "Loss for  14127 th iteration => 0.0241777981515\n",
      "Loss for  14128 th iteration => 0.0241760944928\n",
      "Loss for  14129 th iteration => 0.024173936505\n",
      "Loss for  14130 th iteration => 0.0241718526783\n",
      "Loss for  14131 th iteration => 0.0241703322895\n",
      "Loss for  14132 th iteration => 0.0241678086108\n",
      "Loss for  14133 th iteration => 0.0241654394903\n",
      "Loss for  14134 th iteration => 0.0241646641174\n",
      "Loss for  14135 th iteration => 0.0241621518581\n",
      "Loss for  14136 th iteration => 0.0241598626952\n",
      "Loss for  14137 th iteration => 0.0241588447082\n",
      "Loss for  14138 th iteration => 0.0241552173301\n",
      "Loss for  14139 th iteration => 0.02415312862\n",
      "Loss for  14140 th iteration => 0.0241521578116\n",
      "Loss for  14141 th iteration => 0.0241506141341\n",
      "Loss for  14142 th iteration => 0.0241483043288\n",
      "Loss for  14143 th iteration => 0.0241463671982\n",
      "Loss for  14144 th iteration => 0.024144451024\n",
      "Loss for  14145 th iteration => 0.0241421349059\n",
      "Loss for  14146 th iteration => 0.0241407707333\n",
      "Loss for  14147 th iteration => 0.0241373898356\n",
      "Loss for  14148 th iteration => 0.0241363785446\n",
      "Loss for  14149 th iteration => 0.0241348062754\n",
      "Loss for  14150 th iteration => 0.0241325298445\n",
      "Loss for  14151 th iteration => 0.0241305666826\n",
      "Loss for  14152 th iteration => 0.024128681294\n",
      "Loss for  14153 th iteration => 0.0241263667326\n",
      "Loss for  14154 th iteration => 0.0241235906284\n",
      "Loss for  14155 th iteration => 0.0241218709687\n",
      "Loss for  14156 th iteration => 0.0241215294544\n",
      "Loss for  14157 th iteration => 0.0241185988754\n",
      "Loss for  14158 th iteration => 0.0241167372942\n",
      "Loss for  14159 th iteration => 0.0241153072805\n",
      "Loss for  14160 th iteration => 0.0241119812189\n",
      "Loss for  14161 th iteration => 0.0241119965856\n",
      "Loss for  14162 th iteration => 0.0241078645252\n",
      "Loss for  14163 th iteration => 0.0241061658354\n",
      "Loss for  14164 th iteration => 0.0241057377888\n",
      "Loss for  14165 th iteration => 0.0241028961523\n",
      "Loss for  14166 th iteration => 0.0241009544844\n",
      "Loss for  14167 th iteration => 0.0240996070337\n",
      "Loss for  14168 th iteration => 0.0240962267287\n",
      "Loss for  14169 th iteration => 0.0240955232254\n",
      "Loss for  14170 th iteration => 0.0240921665133\n",
      "Loss for  14171 th iteration => 0.0240911188614\n",
      "Loss for  14172 th iteration => 0.0240895680417\n",
      "Loss for  14173 th iteration => 0.0240872840732\n",
      "Loss for  14174 th iteration => 0.0240853450783\n",
      "Loss for  14175 th iteration => 0.0240834494117\n",
      "Loss for  14176 th iteration => 0.0240811816443\n",
      "Loss for  14177 th iteration => 0.0240784109614\n",
      "Loss for  14178 th iteration => 0.0240766865663\n",
      "Loss for  14179 th iteration => 0.0240763196204\n",
      "Loss for  14180 th iteration => 0.0240734243052\n",
      "Loss for  14181 th iteration => 0.0240715478759\n",
      "Loss for  14182 th iteration => 0.0240701426775\n",
      "Loss for  14183 th iteration => 0.024066826678\n",
      "Loss for  14184 th iteration => 0.0240660723581\n",
      "Loss for  14185 th iteration => 0.0240627458843\n",
      "Loss for  14186 th iteration => 0.0240616831557\n",
      "Loss for  14187 th iteration => 0.0240601798008\n",
      "Loss for  14188 th iteration => 0.0240578574829\n",
      "Loss for  14189 th iteration => 0.0240559667927\n",
      "Loss for  14190 th iteration => 0.0240540319327\n",
      "Loss for  14191 th iteration => 0.0240517864493\n",
      "Loss for  14192 th iteration => 0.0240490272244\n",
      "Loss for  14193 th iteration => 0.0240472739518\n",
      "Loss for  14194 th iteration => 0.0240469733692\n",
      "Loss for  14195 th iteration => 0.0240440191608\n",
      "Loss for  14196 th iteration => 0.02404221302\n",
      "Loss for  14197 th iteration => 0.0240407450707\n",
      "Loss for  14198 th iteration => 0.0240374972378\n",
      "Loss for  14199 th iteration => 0.0240366896529\n",
      "Loss for  14200 th iteration => 0.0240333935445\n",
      "Loss for  14201 th iteration => 0.0240323165933\n",
      "Loss for  14202 th iteration => 0.0240308606822\n",
      "Loss for  14203 th iteration => 0.0240285000044\n",
      "Loss for  14204 th iteration => 0.0240266575584\n",
      "Loss for  14205 th iteration => 0.0240246835343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  14206 th iteration => 0.0240224687057\n",
      "Loss for  14207 th iteration => 0.0240210940848\n",
      "Loss for  14208 th iteration => 0.024017797757\n",
      "Loss for  14209 th iteration => 0.0240166844142\n",
      "Loss for  14210 th iteration => 0.0240152138248\n",
      "Loss for  14211 th iteration => 0.0240128725657\n",
      "Loss for  14212 th iteration => 0.0240110174133\n",
      "Loss for  14213 th iteration => 0.0240090671006\n",
      "Loss for  14214 th iteration => 0.0240061427722\n",
      "Loss for  14215 th iteration => 0.0240041131568\n",
      "Loss for  14216 th iteration => 0.0240030189064\n",
      "Loss for  14217 th iteration => 0.0240016068541\n",
      "Loss for  14218 th iteration => 0.0239992113625\n",
      "Loss for  14219 th iteration => 0.0239974137683\n",
      "Loss for  14220 th iteration => 0.0239954039313\n",
      "Loss for  14221 th iteration => 0.02399323489\n",
      "Loss for  14222 th iteration => 0.0239918207444\n",
      "Loss for  14223 th iteration => 0.0239885530688\n",
      "Loss for  14224 th iteration => 0.0239874233902\n",
      "Loss for  14225 th iteration => 0.0239859971096\n",
      "Loss for  14226 th iteration => 0.0239836205704\n",
      "Loss for  14227 th iteration => 0.0239818106684\n",
      "Loss for  14228 th iteration => 0.0239798210666\n",
      "Loss for  14229 th iteration => 0.0239769275624\n",
      "Loss for  14230 th iteration => 0.023974899889\n",
      "Loss for  14231 th iteration => 0.023973789888\n",
      "Loss for  14232 th iteration => 0.0239724223586\n",
      "Loss for  14233 th iteration => 0.0239699913591\n",
      "Loss for  14234 th iteration => 0.023968239198\n",
      "Loss for  14235 th iteration => 0.0239661929379\n",
      "Loss for  14236 th iteration => 0.0239640701853\n",
      "Loss for  14237 th iteration => 0.0239626141516\n",
      "Loss for  14238 th iteration => 0.0239593754977\n",
      "Loss for  14239 th iteration => 0.0239582309913\n",
      "Loss for  14240 th iteration => 0.0239568493498\n",
      "Loss for  14241 th iteration => 0.0239544371697\n",
      "Loss for  14242 th iteration => 0.0239526727772\n",
      "Loss for  14243 th iteration => 0.0239506434363\n",
      "Loss for  14244 th iteration => 0.023948510265\n",
      "Loss for  14245 th iteration => 0.0239471030809\n",
      "Loss for  14246 th iteration => 0.0239438682352\n",
      "Loss for  14247 th iteration => 0.0239426899655\n",
      "Loss for  14248 th iteration => 0.0239413002143\n",
      "Loss for  14249 th iteration => 0.0239389008558\n",
      "Loss for  14250 th iteration => 0.0239371300233\n",
      "Loss for  14251 th iteration => 0.023935141476\n",
      "Loss for  14252 th iteration => 0.0239322751171\n",
      "Loss for  14253 th iteration => 0.0239302615139\n",
      "Loss for  14254 th iteration => 0.0239291041399\n",
      "Loss for  14255 th iteration => 0.0239277777479\n",
      "Loss for  14256 th iteration => 0.0239253193132\n",
      "Loss for  14257 th iteration => 0.0239236106413\n",
      "Loss for  14258 th iteration => 0.023921534575\n",
      "Loss for  14259 th iteration => 0.0239194575534\n",
      "Loss for  14260 th iteration => 0.0239179907093\n",
      "Loss for  14261 th iteration => 0.0239147903126\n",
      "Loss for  14262 th iteration => 0.0239136000381\n",
      "Loss for  14263 th iteration => 0.0239122638357\n",
      "Loss for  14264 th iteration => 0.0239098199024\n",
      "Loss for  14265 th iteration => 0.0239081031086\n",
      "Loss for  14266 th iteration => 0.0239060575976\n",
      "Loss for  14267 th iteration => 0.023903237486\n",
      "Loss for  14268 th iteration => 0.0239019359246\n",
      "Loss for  14269 th iteration => 0.0239000182386\n",
      "Loss for  14270 th iteration => 0.0238982026918\n",
      "Loss for  14271 th iteration => 0.0238966015685\n",
      "Loss for  14272 th iteration => 0.0238945539873\n",
      "Loss for  14273 th iteration => 0.0238923260704\n",
      "Loss for  14274 th iteration => 0.0238909007618\n",
      "Loss for  14275 th iteration => 0.0238883046232\n",
      "Loss for  14276 th iteration => 0.0238857699052\n",
      "Loss for  14277 th iteration => 0.0238845920408\n",
      "Loss for  14278 th iteration => 0.023883283537\n",
      "Loss for  14279 th iteration => 0.0238808207696\n",
      "Loss for  14280 th iteration => 0.0238791321746\n",
      "Loss for  14281 th iteration => 0.0238770495722\n",
      "Loss for  14282 th iteration => 0.0238749947162\n",
      "Loss for  14283 th iteration => 0.0238735265564\n",
      "Loss for  14284 th iteration => 0.0238703522479\n",
      "Loss for  14285 th iteration => 0.0238691424492\n",
      "Loss for  14286 th iteration => 0.0238678277476\n",
      "Loss for  14287 th iteration => 0.0238653758523\n",
      "Loss for  14288 th iteration => 0.0238636825978\n",
      "Loss for  14289 th iteration => 0.0238616331495\n",
      "Loss for  14290 th iteration => 0.0238588586928\n",
      "Loss for  14291 th iteration => 0.023857517528\n",
      "Loss for  14292 th iteration => 0.0238556338701\n",
      "Loss for  14293 th iteration => 0.0238537892628\n",
      "Loss for  14294 th iteration => 0.0238522339262\n",
      "Loss for  14295 th iteration => 0.0238501534174\n",
      "Loss for  14296 th iteration => 0.0238479744568\n",
      "Loss for  14297 th iteration => 0.0238465130715\n",
      "Loss for  14298 th iteration => 0.0238439669031\n",
      "Loss for  14299 th iteration => 0.0238414329043\n",
      "Loss for  14300 th iteration => 0.0238402361974\n",
      "Loss for  14301 th iteration => 0.0238389554009\n",
      "Loss for  14302 th iteration => 0.0238364784358\n",
      "Loss for  14303 th iteration => 0.0238348193589\n",
      "Loss for  14304 th iteration => 0.0238327207379\n",
      "Loss for  14305 th iteration => 0.0238306971217\n",
      "Loss for  14306 th iteration => 0.0238292121939\n",
      "Loss for  14307 th iteration => 0.023826068964\n",
      "Loss for  14308 th iteration => 0.023824841254\n",
      "Loss for  14309 th iteration => 0.0238235563226\n",
      "Loss for  14310 th iteration => 0.0238210881467\n",
      "Loss for  14311 th iteration => 0.0238194263725\n",
      "Loss for  14312 th iteration => 0.0238173592786\n",
      "Loss for  14313 th iteration => 0.023814646172\n",
      "Loss for  14314 th iteration => 0.0238132484193\n",
      "Loss for  14315 th iteration => 0.0238114113579\n",
      "Loss for  14316 th iteration => 0.0238095295619\n",
      "Loss for  14317 th iteration => 0.0238080323389\n",
      "Loss for  14318 th iteration => 0.0238059066215\n",
      "Loss for  14319 th iteration => 0.0238037884604\n",
      "Loss for  14320 th iteration => 0.023802279202\n",
      "Loss for  14321 th iteration => 0.0237997854542\n",
      "Loss for  14322 th iteration => 0.0237972509771\n",
      "Loss for  14323 th iteration => 0.0237960367253\n",
      "Loss for  14324 th iteration => 0.0237947898079\n",
      "Loss for  14325 th iteration => 0.0237922924189\n",
      "Loss for  14326 th iteration => 0.0237906687789\n",
      "Loss for  14327 th iteration => 0.023788548169\n",
      "Loss for  14328 th iteration => 0.0237865614657\n",
      "Loss for  14329 th iteration => 0.0237850497852\n",
      "Loss for  14330 th iteration => 0.0237819407813\n",
      "Loss for  14331 th iteration => 0.0237806964258\n",
      "Loss for  14332 th iteration => 0.0237794464081\n",
      "Loss for  14333 th iteration => 0.0237769567515\n",
      "Loss for  14334 th iteration => 0.0237753313809\n",
      "Loss for  14335 th iteration => 0.0237732378163\n",
      "Loss for  14336 th iteration => 0.0237705962784\n",
      "Loss for  14337 th iteration => 0.0237691558099\n",
      "Loss for  14338 th iteration => 0.0237667499788\n",
      "Loss for  14339 th iteration => 0.0237657530725\n",
      "Loss for  14340 th iteration => 0.0237639261719\n",
      "Loss for  14341 th iteration => 0.0237616096536\n",
      "Loss for  14342 th iteration => 0.0237602214831\n",
      "Loss for  14343 th iteration => 0.0237576013964\n",
      "Loss for  14344 th iteration => 0.023755702314\n",
      "Loss for  14345 th iteration => 0.0237533373045\n",
      "Loss for  14346 th iteration => 0.0237518701431\n",
      "Loss for  14347 th iteration => 0.0237503602656\n",
      "Loss for  14348 th iteration => 0.0237484233776\n",
      "Loss for  14349 th iteration => 0.0237467516846\n",
      "Loss for  14350 th iteration => 0.0237442633385\n",
      "Loss for  14351 th iteration => 0.0237423677328\n",
      "Loss for  14352 th iteration => 0.0237404367092\n",
      "Loss for  14353 th iteration => 0.0237385927766\n",
      "Loss for  14354 th iteration => 0.0237363982361\n",
      "Loss for  14355 th iteration => 0.0237355328712\n",
      "Loss for  14356 th iteration => 0.0237332054406\n",
      "Loss for  14357 th iteration => 0.0237309203877\n",
      "Loss for  14358 th iteration => 0.0237299943659\n",
      "Loss for  14359 th iteration => 0.0237265923294\n",
      "Loss for  14360 th iteration => 0.0237251311085\n",
      "Loss for  14361 th iteration => 0.0237233831698\n",
      "Loss for  14362 th iteration => 0.0237216604438\n",
      "Loss for  14363 th iteration => 0.0237199589154\n",
      "Loss for  14364 th iteration => 0.0237179704394\n",
      "Loss for  14365 th iteration => 0.0237158396192\n",
      "Loss for  14366 th iteration => 0.0237135749152\n",
      "Loss for  14367 th iteration => 0.023711964598\n",
      "Loss for  14368 th iteration => 0.0237100790288\n",
      "Loss for  14369 th iteration => 0.0237076870893\n",
      "Loss for  14370 th iteration => 0.0237066583633\n",
      "Loss for  14371 th iteration => 0.0237047093941\n",
      "Loss for  14372 th iteration => 0.0237024510543\n",
      "Loss for  14373 th iteration => 0.0237011106732\n",
      "Loss for  14374 th iteration => 0.0236984413816\n",
      "Loss for  14375 th iteration => 0.0236967036972\n",
      "Loss for  14376 th iteration => 0.0236948248591\n",
      "Loss for  14377 th iteration => 0.0236925432204\n",
      "Loss for  14378 th iteration => 0.0236917719396\n",
      "Loss for  14379 th iteration => 0.0236893614171\n",
      "Loss for  14380 th iteration => 0.0236871762295\n",
      "Loss for  14381 th iteration => 0.0236861614322\n",
      "Loss for  14382 th iteration => 0.023682827152\n",
      "Loss for  14383 th iteration => 0.0236814546984\n",
      "Loss for  14384 th iteration => 0.0236796575078\n",
      "Loss for  14385 th iteration => 0.0236777535821\n",
      "Loss for  14386 th iteration => 0.0236763171954\n",
      "Loss for  14387 th iteration => 0.0236741679605\n",
      "Loss for  14388 th iteration => 0.0236721225136\n",
      "Loss for  14389 th iteration => 0.0236705779172\n",
      "Loss for  14390 th iteration => 0.023668149426\n",
      "Loss for  14391 th iteration => 0.0236656259856\n",
      "Loss for  14392 th iteration => 0.0236638899591\n",
      "Loss for  14393 th iteration => 0.0236634885669\n",
      "Loss for  14394 th iteration => 0.0236607194082\n",
      "Loss for  14395 th iteration => 0.0236588968857\n",
      "Loss for  14396 th iteration => 0.0236575306598\n",
      "Loss for  14397 th iteration => 0.0236544584355\n",
      "Loss for  14398 th iteration => 0.0236534934408\n",
      "Loss for  14399 th iteration => 0.0236505668308\n",
      "Loss for  14400 th iteration => 0.0236491397269\n",
      "Loss for  14401 th iteration => 0.0236475955438\n",
      "Loss for  14402 th iteration => 0.023645724257\n",
      "Loss for  14403 th iteration => 0.0236440160261\n",
      "Loss for  14404 th iteration => 0.0236415952695\n",
      "Loss for  14405 th iteration => 0.0236396798394\n",
      "Loss for  14406 th iteration => 0.0236372619444\n",
      "Loss for  14407 th iteration => 0.0236359242531\n",
      "Loss for  14408 th iteration => 0.0236342889888\n",
      "Loss for  14409 th iteration => 0.0236325744811\n",
      "Loss for  14410 th iteration => 0.0236307144684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  14411 th iteration => 0.0236283983879\n",
      "Loss for  14412 th iteration => 0.0236271355418\n",
      "Loss for  14413 th iteration => 0.0236244893997\n",
      "Loss for  14414 th iteration => 0.0236220916261\n",
      "Loss for  14415 th iteration => 0.023620838813\n",
      "Loss for  14416 th iteration => 0.0236196732503\n",
      "Loss for  14417 th iteration => 0.0236171474493\n",
      "Loss for  14418 th iteration => 0.0236156147474\n",
      "Loss for  14419 th iteration => 0.0236134563203\n",
      "Loss for  14420 th iteration => 0.0236108806273\n",
      "Loss for  14421 th iteration => 0.0236099892598\n",
      "Loss for  14422 th iteration => 0.0236076853171\n",
      "Loss for  14423 th iteration => 0.0236052823117\n",
      "Loss for  14424 th iteration => 0.0236047409471\n",
      "Loss for  14425 th iteration => 0.0236021228932\n",
      "Loss for  14426 th iteration => 0.0236001775651\n",
      "Loss for  14427 th iteration => 0.0235989454756\n",
      "Loss for  14428 th iteration => 0.0235958057145\n",
      "Loss for  14429 th iteration => 0.0235948541051\n",
      "Loss for  14430 th iteration => 0.0235920113673\n",
      "Loss for  14431 th iteration => 0.0235904887475\n",
      "Loss for  14432 th iteration => 0.0235891611043\n",
      "Loss for  14433 th iteration => 0.0235870126224\n",
      "Loss for  14434 th iteration => 0.0235855086209\n",
      "Loss for  14435 th iteration => 0.0235830278343\n",
      "Loss for  14436 th iteration => 0.0235810374973\n",
      "Loss for  14437 th iteration => 0.0235793975649\n",
      "Loss for  14438 th iteration => 0.02357738555\n",
      "Loss for  14439 th iteration => 0.0235752597208\n",
      "Loss for  14440 th iteration => 0.0235743800845\n",
      "Loss for  14441 th iteration => 0.0235721024669\n",
      "Loss for  14442 th iteration => 0.0235698380156\n",
      "Loss for  14443 th iteration => 0.0235681696326\n",
      "Loss for  14444 th iteration => 0.0235656035272\n",
      "Loss for  14445 th iteration => 0.023564897434\n",
      "Loss for  14446 th iteration => 0.0235619831372\n",
      "Loss for  14447 th iteration => 0.023560724378\n",
      "Loss for  14448 th iteration => 0.0235592052475\n",
      "Loss for  14449 th iteration => 0.0235566576666\n",
      "Loss for  14450 th iteration => 0.023555560102\n",
      "Loss for  14451 th iteration => 0.0235527813576\n",
      "Loss for  14452 th iteration => 0.0235510267769\n",
      "Loss for  14453 th iteration => 0.0235498640291\n",
      "Loss for  14454 th iteration => 0.0235469310254\n",
      "Loss for  14455 th iteration => 0.0235456934936\n",
      "Loss for  14456 th iteration => 0.0235441444799\n",
      "Loss for  14457 th iteration => 0.0235416318262\n",
      "Loss for  14458 th iteration => 0.0235405038865\n",
      "Loss for  14459 th iteration => 0.0235377638526\n",
      "Loss for  14460 th iteration => 0.0235359797722\n",
      "Loss for  14461 th iteration => 0.0235342590084\n",
      "Loss for  14462 th iteration => 0.0235318589772\n",
      "Loss for  14463 th iteration => 0.0235307801956\n",
      "Loss for  14464 th iteration => 0.0235289871748\n",
      "Loss for  14465 th iteration => 0.0235267248036\n",
      "Loss for  14466 th iteration => 0.0235253515708\n",
      "Loss for  14467 th iteration => 0.0235228670964\n",
      "Loss for  14468 th iteration => 0.0235209164483\n",
      "Loss for  14469 th iteration => 0.0235192272187\n",
      "Loss for  14470 th iteration => 0.0235169076191\n",
      "Loss for  14471 th iteration => 0.0235162460296\n",
      "Loss for  14472 th iteration => 0.0235137661804\n",
      "Loss for  14473 th iteration => 0.0235117230819\n",
      "Loss for  14474 th iteration => 0.023510607017\n",
      "Loss for  14475 th iteration => 0.0235074152074\n",
      "Loss for  14476 th iteration => 0.0235058692575\n",
      "Loss for  14477 th iteration => 0.0235036883986\n",
      "Loss for  14478 th iteration => 0.0235028008403\n",
      "Loss for  14479 th iteration => 0.023500462326\n",
      "Loss for  14480 th iteration => 0.0234988693608\n",
      "Loss for  14481 th iteration => 0.0234973121182\n",
      "Loss for  14482 th iteration => 0.0234944677921\n",
      "Loss for  14483 th iteration => 0.0234933297769\n",
      "Loss for  14484 th iteration => 0.0234905414803\n",
      "Loss for  14485 th iteration => 0.0234891262013\n",
      "Loss for  14486 th iteration => 0.0234876896286\n",
      "Loss for  14487 th iteration => 0.0234857312687\n",
      "Loss for  14488 th iteration => 0.0234840677404\n",
      "Loss for  14489 th iteration => 0.0234817154286\n",
      "Loss for  14490 th iteration => 0.0234797069798\n",
      "Loss for  14491 th iteration => 0.023477989323\n",
      "Loss for  14492 th iteration => 0.0234761726989\n",
      "Loss for  14493 th iteration => 0.0234738250799\n",
      "Loss for  14494 th iteration => 0.0234732787836\n",
      "Loss for  14495 th iteration => 0.0234706946521\n",
      "Loss for  14496 th iteration => 0.023468771654\n",
      "Loss for  14497 th iteration => 0.0234675465878\n",
      "Loss for  14498 th iteration => 0.0234644699467\n",
      "Loss for  14499 th iteration => 0.023463492447\n",
      "Loss for  14500 th iteration => 0.0234612763404\n",
      "Loss for  14501 th iteration => 0.023458862393\n",
      "Loss for  14502 th iteration => 0.0234578299579\n",
      "Loss for  14503 th iteration => 0.0234560052836\n",
      "Loss for  14504 th iteration => 0.0234538000715\n",
      "Loss for  14505 th iteration => 0.0234523912616\n",
      "Loss for  14506 th iteration => 0.0234499552238\n",
      "Loss for  14507 th iteration => 0.0234479808849\n",
      "Loss for  14508 th iteration => 0.023446340411\n",
      "Loss for  14509 th iteration => 0.0234439945987\n",
      "Loss for  14510 th iteration => 0.0234429800169\n",
      "Loss for  14511 th iteration => 0.0234410546897\n",
      "Loss for  14512 th iteration => 0.023438870577\n",
      "Loss for  14513 th iteration => 0.0234375301491\n",
      "Loss for  14514 th iteration => 0.0234349491913\n",
      "Loss for  14515 th iteration => 0.0234325889403\n",
      "Loss for  14516 th iteration => 0.0234308266238\n",
      "Loss for  14517 th iteration => 0.0234300803726\n",
      "Loss for  14518 th iteration => 0.0234279517189\n",
      "Loss for  14519 th iteration => 0.023425970764\n",
      "Loss for  14520 th iteration => 0.0234244325174\n",
      "Loss for  14521 th iteration => 0.0234219622633\n",
      "Loss for  14522 th iteration => 0.0234201700042\n",
      "Loss for  14523 th iteration => 0.0234177594964\n",
      "Loss for  14524 th iteration => 0.0234164323846\n",
      "Loss for  14525 th iteration => 0.0234153847138\n",
      "Loss for  14526 th iteration => 0.0234128023807\n",
      "Loss for  14527 th iteration => 0.0234113978255\n",
      "Loss for  14528 th iteration => 0.0234091828554\n",
      "Loss for  14529 th iteration => 0.0234067117421\n",
      "Loss for  14530 th iteration => 0.0234058002134\n",
      "Loss for  14531 th iteration => 0.0234035491003\n",
      "Loss for  14532 th iteration => 0.0234011715215\n",
      "Loss for  14533 th iteration => 0.0234001309953\n",
      "Loss for  14534 th iteration => 0.0233983310321\n",
      "Loss for  14535 th iteration => 0.0233961219444\n",
      "Loss for  14536 th iteration => 0.0233947338174\n",
      "Loss for  14537 th iteration => 0.0233922839977\n",
      "Loss for  14538 th iteration => 0.0233903363563\n",
      "Loss for  14539 th iteration => 0.0233886911881\n",
      "Loss for  14540 th iteration => 0.0233863752374\n",
      "Loss for  14541 th iteration => 0.0233857642186\n",
      "Loss for  14542 th iteration => 0.0233832632233\n",
      "Loss for  14543 th iteration => 0.0233812956589\n",
      "Loss for  14544 th iteration => 0.0233801338283\n",
      "Loss for  14545 th iteration => 0.0233770021274\n",
      "Loss for  14546 th iteration => 0.0233761227407\n",
      "Loss for  14547 th iteration => 0.0233732853363\n",
      "Loss for  14548 th iteration => 0.0233718513274\n",
      "Loss for  14549 th iteration => 0.0233704532951\n",
      "Loss for  14550 th iteration => 0.0233684466231\n",
      "Loss for  14551 th iteration => 0.023366865306\n",
      "Loss for  14552 th iteration => 0.0233645123576\n",
      "Loss for  14553 th iteration => 0.0233625037233\n",
      "Loss for  14554 th iteration => 0.0233609005456\n",
      "Loss for  14555 th iteration => 0.0233589772396\n",
      "Loss for  14556 th iteration => 0.0233568088224\n",
      "Loss for  14557 th iteration => 0.0233560380104\n",
      "Loss for  14558 th iteration => 0.0233537011997\n",
      "Loss for  14559 th iteration => 0.0233515854951\n",
      "Loss for  14560 th iteration => 0.0233505763188\n",
      "Loss for  14561 th iteration => 0.0233473679989\n",
      "Loss for  14562 th iteration => 0.0233460254205\n",
      "Loss for  14563 th iteration => 0.0233442068697\n",
      "Loss for  14564 th iteration => 0.0233424396062\n",
      "Loss for  14565 th iteration => 0.0233409919905\n",
      "Loss for  14566 th iteration => 0.0233389481162\n",
      "Loss for  14567 th iteration => 0.0233369199861\n",
      "Loss for  14568 th iteration => 0.0233354523556\n",
      "Loss for  14569 th iteration => 0.0233330533336\n",
      "Loss for  14570 th iteration => 0.0233312108138\n",
      "Loss for  14571 th iteration => 0.0233294643843\n",
      "Loss for  14572 th iteration => 0.0233272206535\n",
      "Loss for  14573 th iteration => 0.0233259792412\n",
      "Loss for  14574 th iteration => 0.0233243628854\n",
      "Loss for  14575 th iteration => 0.0233219987295\n",
      "Loss for  14576 th iteration => 0.0233207866068\n",
      "Loss for  14577 th iteration => 0.0233182661892\n",
      "Loss for  14578 th iteration => 0.0233158659945\n",
      "Loss for  14579 th iteration => 0.0233141772002\n",
      "Loss for  14580 th iteration => 0.023313187437\n",
      "Loss for  14581 th iteration => 0.0233114065018\n",
      "Loss for  14582 th iteration => 0.0233092070778\n",
      "Loss for  14583 th iteration => 0.0233078348683\n",
      "Loss for  14584 th iteration => 0.0233053233331\n",
      "Loss for  14585 th iteration => 0.02330351581\n",
      "Loss for  14586 th iteration => 0.0233011698971\n",
      "Loss for  14587 th iteration => 0.023299889226\n",
      "Loss for  14588 th iteration => 0.0232987914696\n",
      "Loss for  14589 th iteration => 0.0232962936384\n",
      "Loss for  14590 th iteration => 0.0232948457322\n",
      "Loss for  14591 th iteration => 0.0232927008258\n",
      "Loss for  14592 th iteration => 0.0232902185305\n",
      "Loss for  14593 th iteration => 0.0232893345446\n",
      "Loss for  14594 th iteration => 0.0232871155675\n",
      "Loss for  14595 th iteration => 0.0232847317869\n",
      "Loss for  14596 th iteration => 0.0232837452096\n",
      "Loss for  14597 th iteration => 0.023281904516\n",
      "Loss for  14598 th iteration => 0.0232797769613\n",
      "Loss for  14599 th iteration => 0.0232783414565\n",
      "Loss for  14600 th iteration => 0.0232759621262\n",
      "Loss for  14601 th iteration => 0.0232740025287\n",
      "Loss for  14602 th iteration => 0.0232718022034\n",
      "Loss for  14603 th iteration => 0.0232704134289\n",
      "Loss for  14604 th iteration => 0.0232693911573\n",
      "Loss for  14605 th iteration => 0.0232668267526\n",
      "Loss for  14606 th iteration => 0.0232654568821\n",
      "Loss for  14607 th iteration => 0.0232632937452\n",
      "Loss for  14608 th iteration => 0.0232607636109\n",
      "Loss for  14609 th iteration => 0.0232593450529\n",
      "Loss for  14610 th iteration => 0.0232576335904\n",
      "Loss for  14611 th iteration => 0.0232560452357\n",
      "Loss for  14612 th iteration => 0.0232539532286\n",
      "Loss for  14613 th iteration => 0.0232527027704\n",
      "Loss for  14614 th iteration => 0.0232503706308\n",
      "Loss for  14615 th iteration => 0.0232487814809\n",
      "Loss for  14616 th iteration => 0.0232469866698\n",
      "Loss for  14617 th iteration => 0.023244217481\n",
      "Loss for  14618 th iteration => 0.0232429373473\n",
      "Loss for  14619 th iteration => 0.0232414179828\n",
      "Loss for  14620 th iteration => 0.0232396404225\n",
      "Loss for  14621 th iteration => 0.0232378685791\n",
      "Loss for  14622 th iteration => 0.0232356901076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  14623 th iteration => 0.0232343179106\n",
      "Loss for  14624 th iteration => 0.0232319426233\n",
      "Loss for  14625 th iteration => 0.0232295238563\n",
      "Loss for  14626 th iteration => 0.023228290045\n",
      "Loss for  14627 th iteration => 0.0232262595742\n",
      "Loss for  14628 th iteration => 0.0232252786593\n",
      "Loss for  14629 th iteration => 0.0232231778045\n",
      "Loss for  14630 th iteration => 0.023220885787\n",
      "Loss for  14631 th iteration => 0.0232200791861\n",
      "Loss for  14632 th iteration => 0.0232167574964\n",
      "Loss for  14633 th iteration => 0.0232148120487\n",
      "Loss for  14634 th iteration => 0.0232132190042\n",
      "Loss for  14635 th iteration => 0.0232127841755\n",
      "Loss for  14636 th iteration => 0.0232101472956\n",
      "Loss for  14637 th iteration => 0.0232083848046\n",
      "Loss for  14638 th iteration => 0.0232070586278\n",
      "Loss for  14639 th iteration => 0.0232040755416\n",
      "Loss for  14640 th iteration => 0.023203179217\n",
      "Loss for  14641 th iteration => 0.0232002822287\n",
      "Loss for  14642 th iteration => 0.0231990277384\n",
      "Loss for  14643 th iteration => 0.0231979675622\n",
      "Loss for  14644 th iteration => 0.0231954620363\n",
      "Loss for  14645 th iteration => 0.0231940559938\n",
      "Loss for  14646 th iteration => 0.0231918961925\n",
      "Loss for  14647 th iteration => 0.0231901565926\n",
      "Loss for  14648 th iteration => 0.0231885351868\n",
      "Loss for  14649 th iteration => 0.0231856778335\n",
      "Loss for  14650 th iteration => 0.0231844200699\n",
      "Loss for  14651 th iteration => 0.0231828472386\n",
      "Loss for  14652 th iteration => 0.0231811606131\n",
      "Loss for  14653 th iteration => 0.0231793986846\n",
      "Loss for  14654 th iteration => 0.0231771501071\n",
      "Loss for  14655 th iteration => 0.0231759459291\n",
      "Loss for  14656 th iteration => 0.0231733837019\n",
      "Loss for  14657 th iteration => 0.0231710426172\n",
      "Loss for  14658 th iteration => 0.0231699309732\n",
      "Loss for  14659 th iteration => 0.0231682733715\n",
      "Loss for  14660 th iteration => 0.0231666078607\n",
      "Loss for  14661 th iteration => 0.0231647453422\n",
      "Loss for  14662 th iteration => 0.023162684139\n",
      "Loss for  14663 th iteration => 0.0231612160318\n",
      "Loss for  14664 th iteration => 0.023158986544\n",
      "Loss for  14665 th iteration => 0.0231564712422\n",
      "Loss for  14666 th iteration => 0.0231548138082\n",
      "Loss for  14667 th iteration => 0.0231544555517\n",
      "Loss for  14668 th iteration => 0.0231517559184\n",
      "Loss for  14669 th iteration => 0.0231500789611\n",
      "Loss for  14670 th iteration => 0.0231486812052\n",
      "Loss for  14671 th iteration => 0.0231457896891\n",
      "Loss for  14672 th iteration => 0.0231448483025\n",
      "Loss for  14673 th iteration => 0.023141983462\n",
      "Loss for  14674 th iteration => 0.0231407221161\n",
      "Loss for  14675 th iteration => 0.0231391612953\n",
      "Loss for  14676 th iteration => 0.0231374639038\n",
      "Loss for  14677 th iteration => 0.0231357245983\n",
      "Loss for  14678 th iteration => 0.0231334699248\n",
      "Loss for  14679 th iteration => 0.0231322837195\n",
      "Loss for  14680 th iteration => 0.0231297360129\n",
      "Loss for  14681 th iteration => 0.0231273986275\n",
      "Loss for  14682 th iteration => 0.0231262863327\n",
      "Loss for  14683 th iteration => 0.0231246401367\n",
      "Loss for  14684 th iteration => 0.0231229666137\n",
      "Loss for  14685 th iteration => 0.0231211246624\n",
      "Loss for  14686 th iteration => 0.0231190585094\n",
      "Loss for  14687 th iteration => 0.0231176079012\n",
      "Loss for  14688 th iteration => 0.0231153870712\n",
      "Loss for  14689 th iteration => 0.0231128765006\n",
      "Loss for  14690 th iteration => 0.0231112354375\n",
      "Loss for  14691 th iteration => 0.0231108549655\n",
      "Loss for  14692 th iteration => 0.0231081867806\n",
      "Loss for  14693 th iteration => 0.0231064970851\n",
      "Loss for  14694 th iteration => 0.0231051214222\n",
      "Loss for  14695 th iteration => 0.0231022265703\n",
      "Loss for  14696 th iteration => 0.0231013023816\n",
      "Loss for  14697 th iteration => 0.0230984403323\n",
      "Loss for  14698 th iteration => 0.0230971901663\n",
      "Loss for  14699 th iteration => 0.0230956230477\n",
      "Loss for  14700 th iteration => 0.0230939432125\n",
      "Loss for  14701 th iteration => 0.023092198401\n",
      "Loss for  14702 th iteration => 0.0230899649301\n",
      "Loss for  14703 th iteration => 0.0230887695927\n",
      "Loss for  14704 th iteration => 0.0230862427152\n",
      "Loss for  14705 th iteration => 0.0230839065495\n",
      "Loss for  14706 th iteration => 0.0230828044222\n",
      "Loss for  14707 th iteration => 0.0230811591423\n",
      "Loss for  14708 th iteration => 0.0230794953224\n",
      "Loss for  14709 th iteration => 0.0230776562554\n",
      "Loss for  14710 th iteration => 0.0230756022355\n",
      "Loss for  14711 th iteration => 0.0230741520798\n",
      "Loss for  14712 th iteration => 0.023071940416\n",
      "Loss for  14713 th iteration => 0.0230694445191\n",
      "Loss for  14714 th iteration => 0.0230677919743\n",
      "Loss for  14715 th iteration => 0.0230674399889\n",
      "Loss for  14716 th iteration => 0.0230647531773\n",
      "Loss for  14717 th iteration => 0.0230630996107\n",
      "Loss for  14718 th iteration => 0.0230616977867\n",
      "Loss for  14719 th iteration => 0.0230588389616\n",
      "Loss for  14720 th iteration => 0.0230579024696\n",
      "Loss for  14721 th iteration => 0.0230550501279\n",
      "Loss for  14722 th iteration => 0.0230538204802\n",
      "Loss for  14723 th iteration => 0.0230522353085\n",
      "Loss for  14724 th iteration => 0.0230505912263\n",
      "Loss for  14725 th iteration => 0.0230488228165\n",
      "Loss for  14726 th iteration => 0.0230466280728\n",
      "Loss for  14727 th iteration => 0.0230454061835\n",
      "Loss for  14728 th iteration => 0.0230429041892\n",
      "Loss for  14729 th iteration => 0.0230405745945\n",
      "Loss for  14730 th iteration => 0.0230389299327\n",
      "Loss for  14731 th iteration => 0.0230381483334\n",
      "Loss for  14732 th iteration => 0.0230361114739\n",
      "Loss for  14733 th iteration => 0.0230341831841\n",
      "Loss for  14734 th iteration => 0.0230327006385\n",
      "Loss for  14735 th iteration => 0.0230303017353\n",
      "Loss for  14736 th iteration => 0.0230285937828\n",
      "Loss for  14737 th iteration => 0.0230262173812\n",
      "Loss for  14738 th iteration => 0.0230250032357\n",
      "Loss for  14739 th iteration => 0.0230239001913\n",
      "Loss for  14740 th iteration => 0.0230214884906\n",
      "Loss for  14741 th iteration => 0.0230200492357\n",
      "Loss for  14742 th iteration => 0.0230179735502\n",
      "Loss for  14743 th iteration => 0.0230162100037\n",
      "Loss for  14744 th iteration => 0.0230146560221\n",
      "Loss for  14745 th iteration => 0.0230118156458\n",
      "Loss for  14746 th iteration => 0.0230106158779\n",
      "Loss for  14747 th iteration => 0.0230089879383\n",
      "Loss for  14748 th iteration => 0.0230074124925\n",
      "Loss for  14749 th iteration => 0.0230055876614\n",
      "Loss for  14750 th iteration => 0.0230034641584\n",
      "Loss for  14751 th iteration => 0.0230021832641\n",
      "Loss for  14752 th iteration => 0.0229997289151\n",
      "Loss for  14753 th iteration => 0.0229974073019\n",
      "Loss for  14754 th iteration => 0.0229957323584\n",
      "Loss for  14755 th iteration => 0.0229950160371\n",
      "Loss for  14756 th iteration => 0.0229929212777\n",
      "Loss for  14757 th iteration => 0.0229910656322\n",
      "Loss for  14758 th iteration => 0.02298952268\n",
      "Loss for  14759 th iteration => 0.0229871868567\n",
      "Loss for  14760 th iteration => 0.0229854492063\n",
      "Loss for  14761 th iteration => 0.0229830853112\n",
      "Loss for  14762 th iteration => 0.0229818818487\n",
      "Loss for  14763 th iteration => 0.0229807913852\n",
      "Loss for  14764 th iteration => 0.0229783797513\n",
      "Loss for  14765 th iteration => 0.0229769546976\n",
      "Loss for  14766 th iteration => 0.0229748774529\n",
      "Loss for  14767 th iteration => 0.0229731296527\n",
      "Loss for  14768 th iteration => 0.0229715585463\n",
      "Loss for  14769 th iteration => 0.0229687345435\n",
      "Loss for  14770 th iteration => 0.0229675690262\n",
      "Loss for  14771 th iteration => 0.02296589322\n",
      "Loss for  14772 th iteration => 0.0229643947778\n",
      "Loss for  14773 th iteration => 0.0229625051865\n",
      "Loss for  14774 th iteration => 0.0229604609117\n",
      "Loss for  14775 th iteration => 0.0229591130531\n",
      "Loss for  14776 th iteration => 0.0229567072779\n",
      "Loss for  14777 th iteration => 0.0229543978245\n",
      "Loss for  14778 th iteration => 0.0229526843565\n",
      "Loss for  14779 th iteration => 0.0229520436839\n",
      "Loss for  14780 th iteration => 0.0229498840333\n",
      "Loss for  14781 th iteration => 0.0229481077041\n",
      "Loss for  14782 th iteration => 0.0229464976887\n",
      "Loss for  14783 th iteration => 0.0229442282494\n",
      "Loss for  14784 th iteration => 0.0229424584681\n",
      "Loss for  14785 th iteration => 0.0229401067413\n",
      "Loss for  14786 th iteration => 0.0229389380277\n",
      "Loss for  14787 th iteration => 0.0229372797899\n",
      "Loss for  14788 th iteration => 0.0229357635164\n",
      "Loss for  14789 th iteration => 0.0229338991479\n",
      "Loss for  14790 th iteration => 0.0229318413584\n",
      "Loss for  14791 th iteration => 0.0229305144187\n",
      "Loss for  14792 th iteration => 0.0229281135456\n",
      "Loss for  14793 th iteration => 0.0229257728973\n",
      "Loss for  14794 th iteration => 0.0229241327166\n",
      "Loss for  14795 th iteration => 0.0229238349624\n",
      "Loss for  14796 th iteration => 0.022921125887\n",
      "Loss for  14797 th iteration => 0.0229195530785\n",
      "Loss for  14798 th iteration => 0.0229181028284\n",
      "Loss for  14799 th iteration => 0.0229153415499\n",
      "Loss for  14800 th iteration => 0.022914324774\n",
      "Loss for  14801 th iteration => 0.0229115766341\n",
      "Loss for  14802 th iteration => 0.022910334067\n",
      "Loss for  14803 th iteration => 0.0229083667592\n",
      "Loss for  14804 th iteration => 0.0229074270722\n",
      "Loss for  14805 th iteration => 0.022905354876\n",
      "Loss for  14806 th iteration => 0.0229031656236\n",
      "Loss for  14807 th iteration => 0.0229023269784\n",
      "Loss for  14808 th iteration => 0.0228991186291\n",
      "Loss for  14809 th iteration => 0.022897227102\n",
      "Loss for  14810 th iteration => 0.0228961841327\n",
      "Loss for  14811 th iteration => 0.0228949694755\n",
      "Loss for  14812 th iteration => 0.0228927066713\n",
      "Loss for  14813 th iteration => 0.0228911617412\n",
      "Loss for  14814 th iteration => 0.0228892289959\n",
      "Loss for  14815 th iteration => 0.0228873654793\n",
      "Loss for  14816 th iteration => 0.0228858771103\n",
      "Loss for  14817 th iteration => 0.0228830358957\n",
      "Loss for  14818 th iteration => 0.0228812925617\n",
      "Loss for  14819 th iteration => 0.0228807527062\n",
      "Loss for  14820 th iteration => 0.0228785234804\n",
      "Loss for  14821 th iteration => 0.022876840872\n",
      "Loss for  14822 th iteration => 0.0228751572577\n",
      "Loss for  14823 th iteration => 0.0228729572482\n",
      "Loss for  14824 th iteration => 0.0228711426987\n",
      "Loss for  14825 th iteration => 0.0228688143552\n",
      "Loss for  14826 th iteration => 0.0228676910915\n",
      "Loss for  14827 th iteration => 0.0228659933886\n",
      "Loss for  14828 th iteration => 0.0228645722131\n",
      "Loss for  14829 th iteration => 0.0228626328567\n",
      "Loss for  14830 th iteration => 0.0228606739506\n",
      "Loss for  14831 th iteration => 0.0228592682714\n",
      "Loss for  14832 th iteration => 0.0228569152097\n",
      "Loss for  14833 th iteration => 0.0228552199488\n",
      "Loss for  14834 th iteration => 0.0228529024116\n",
      "Loss for  14835 th iteration => 0.0228521263056\n",
      "Loss for  14836 th iteration => 0.0228502887569\n",
      "Loss for  14837 th iteration => 0.0228483041054\n",
      "Loss for  14838 th iteration => 0.0228468509436\n",
      "Loss for  14839 th iteration => 0.0228445064948\n",
      "Loss for  14840 th iteration => 0.0228427078343\n",
      "Loss for  14841 th iteration => 0.0228410305093\n",
      "Loss for  14842 th iteration => 0.0228393172234\n",
      "Loss for  14843 th iteration => 0.0228371222941\n",
      "Loss for  14844 th iteration => 0.0228365439822\n",
      "Loss for  14845 th iteration => 0.022834130652\n",
      "Loss for  14846 th iteration => 0.0228323040746\n",
      "Loss for  14847 th iteration => 0.0228311230836\n",
      "Loss for  14848 th iteration => 0.0228282172165\n",
      "Loss for  14849 th iteration => 0.0228267574914\n",
      "Loss for  14850 th iteration => 0.0228245916877\n",
      "Loss for  14851 th iteration => 0.0228237495488\n",
      "Loss for  14852 th iteration => 0.0228214587875\n",
      "Loss for  14853 th iteration => 0.0228202724823\n",
      "Loss for  14854 th iteration => 0.0228184624867\n",
      "Loss for  14855 th iteration => 0.0228160521334\n",
      "Loss for  14856 th iteration => 0.0228154504671\n",
      "Loss for  14857 th iteration => 0.0228121554767\n",
      "Loss for  14858 th iteration => 0.0228103634025\n",
      "Loss for  14859 th iteration => 0.0228091937795\n",
      "Loss for  14860 th iteration => 0.0228080788544\n",
      "Loss for  14861 th iteration => 0.0228057418284\n",
      "Loss for  14862 th iteration => 0.0228043018032\n",
      "Loss for  14863 th iteration => 0.0228022896257\n",
      "Loss for  14864 th iteration => 0.0228005437544\n",
      "Loss for  14865 th iteration => 0.0227984989715\n",
      "Loss for  14866 th iteration => 0.0227961937418\n",
      "Loss for  14867 th iteration => 0.0227955885322\n",
      "Loss for  14868 th iteration => 0.0227930737539\n",
      "Loss for  14869 th iteration => 0.0227921284448\n",
      "Loss for  14870 th iteration => 0.022790086803\n",
      "Loss for  14871 th iteration => 0.0227879145495\n",
      "Loss for  14872 th iteration => 0.0227870841407\n",
      "Loss for  14873 th iteration => 0.0227839342873\n",
      "Loss for  14874 th iteration => 0.0227826686495\n",
      "Loss for  14875 th iteration => 0.0227804573698\n",
      "Loss for  14876 th iteration => 0.0227795284962\n",
      "Loss for  14877 th iteration => 0.022777851685\n",
      "Loss for  14878 th iteration => 0.0227757327664\n",
      "Loss for  14879 th iteration => 0.0227744340382\n",
      "Loss for  14880 th iteration => 0.0227719620454\n",
      "Loss for  14881 th iteration => 0.0227703436998\n",
      "Loss for  14882 th iteration => 0.0227685316606\n",
      "Loss for  14883 th iteration => 0.0227669268672\n",
      "Loss for  14884 th iteration => 0.0227650894514\n",
      "Loss for  14885 th iteration => 0.0227633831588\n",
      "Loss for  14886 th iteration => 0.0227620356824\n",
      "Loss for  14887 th iteration => 0.0227599435861\n",
      "Loss for  14888 th iteration => 0.0227582813252\n",
      "Loss for  14889 th iteration => 0.0227566216032\n",
      "Loss for  14890 th iteration => 0.0227539331175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  14891 th iteration => 0.0227522017123\n",
      "Loss for  14892 th iteration => 0.0227510207135\n",
      "Loss for  14893 th iteration => 0.0227498482533\n",
      "Loss for  14894 th iteration => 0.022747585379\n",
      "Loss for  14895 th iteration => 0.0227460929787\n",
      "Loss for  14896 th iteration => 0.0227441551768\n",
      "Loss for  14897 th iteration => 0.022741716213\n",
      "Loss for  14898 th iteration => 0.0227403776621\n",
      "Loss for  14899 th iteration => 0.0227387754664\n",
      "Loss for  14900 th iteration => 0.0227369476594\n",
      "Loss for  14901 th iteration => 0.0227352433973\n",
      "Loss for  14902 th iteration => 0.0227339141293\n",
      "Loss for  14903 th iteration => 0.0227318120024\n",
      "Loss for  14904 th iteration => 0.0227301691515\n",
      "Loss for  14905 th iteration => 0.0227284893662\n",
      "Loss for  14906 th iteration => 0.0227258007948\n",
      "Loss for  14907 th iteration => 0.0227240819343\n",
      "Loss for  14908 th iteration => 0.0227229098432\n",
      "Loss for  14909 th iteration => 0.0227217568134\n",
      "Loss for  14910 th iteration => 0.0227194826825\n",
      "Loss for  14911 th iteration => 0.0227180108673\n",
      "Loss for  14912 th iteration => 0.0227160552307\n",
      "Loss for  14913 th iteration => 0.0227143039407\n",
      "Loss for  14914 th iteration => 0.0227122906568\n",
      "Loss for  14915 th iteration => 0.0227100392268\n",
      "Loss for  14916 th iteration => 0.0227093150341\n",
      "Loss for  14917 th iteration => 0.0227069749704\n",
      "Loss for  14918 th iteration => 0.0227058657234\n",
      "Loss for  14919 th iteration => 0.0227040044438\n",
      "Loss for  14920 th iteration => 0.022701690596\n",
      "Loss for  14921 th iteration => 0.022701018475\n",
      "Loss for  14922 th iteration => 0.022697788834\n",
      "Loss for  14923 th iteration => 0.0226959308205\n",
      "Loss for  14924 th iteration => 0.0226944044685\n",
      "Loss for  14925 th iteration => 0.0226940051026\n",
      "Loss for  14926 th iteration => 0.0226914460171\n",
      "Loss for  14927 th iteration => 0.022689819892\n",
      "Loss for  14928 th iteration => 0.0226884719615\n",
      "Loss for  14929 th iteration => 0.0226856879913\n",
      "Loss for  14930 th iteration => 0.0226847772918\n",
      "Loss for  14931 th iteration => 0.022681992034\n",
      "Loss for  14932 th iteration => 0.0226808733738\n",
      "Loss for  14933 th iteration => 0.0226792118153\n",
      "Loss for  14934 th iteration => 0.0226778073482\n",
      "Loss for  14935 th iteration => 0.0226759020923\n",
      "Loss for  14936 th iteration => 0.0226739756617\n",
      "Loss for  14937 th iteration => 0.0226725884033\n",
      "Loss for  14938 th iteration => 0.022670279491\n",
      "Loss for  14939 th iteration => 0.0226685981746\n",
      "Loss for  14940 th iteration => 0.0226663196235\n",
      "Loss for  14941 th iteration => 0.0226651407524\n",
      "Loss for  14942 th iteration => 0.0226639935386\n",
      "Loss for  14943 th iteration => 0.0226617302594\n",
      "Loss for  14944 th iteration => 0.0226602673039\n",
      "Loss for  14945 th iteration => 0.0226583230865\n",
      "Loss for  14946 th iteration => 0.0226559124723\n",
      "Loss for  14947 th iteration => 0.022654572801\n",
      "Loss for  14948 th iteration => 0.0226529888693\n",
      "Loss for  14949 th iteration => 0.0226511757834\n",
      "Loss for  14950 th iteration => 0.022649475553\n",
      "Loss for  14951 th iteration => 0.0226481815272\n",
      "Loss for  14952 th iteration => 0.0226460689782\n",
      "Loss for  14953 th iteration => 0.0226444651875\n",
      "Loss for  14954 th iteration => 0.0226427628267\n",
      "Loss for  14955 th iteration => 0.0226401018252\n",
      "Loss for  14956 th iteration => 0.0226383971948\n",
      "Loss for  14957 th iteration => 0.0226372295491\n",
      "Loss for  14958 th iteration => 0.0226361163837\n",
      "Loss for  14959 th iteration => 0.0226338272011\n",
      "Loss for  14960 th iteration => 0.0226323989007\n",
      "Loss for  14961 th iteration => 0.0226304245466\n",
      "Loss for  14962 th iteration => 0.022628711075\n",
      "Loss for  14963 th iteration => 0.0226266765752\n",
      "Loss for  14964 th iteration => 0.0226244503849\n",
      "Loss for  14965 th iteration => 0.0226237622627\n",
      "Loss for  14966 th iteration => 0.0226213893967\n",
      "Loss for  14967 th iteration => 0.0226203648003\n",
      "Loss for  14968 th iteration => 0.0226184384368\n",
      "Loss for  14969 th iteration => 0.0226162227301\n",
      "Loss for  14970 th iteration => 0.0226154722318\n",
      "Loss for  14971 th iteration => 0.0226123155603\n",
      "Loss for  14972 th iteration => 0.0226110675116\n",
      "Loss for  14973 th iteration => 0.0226089199053\n",
      "Loss for  14974 th iteration => 0.0226079715767\n",
      "Loss for  14975 th iteration => 0.0226063644737\n",
      "Loss for  14976 th iteration => 0.0226042349814\n",
      "Loss for  14977 th iteration => 0.0226029954236\n",
      "Loss for  14978 th iteration => 0.0226005179525\n",
      "Loss for  14979 th iteration => 0.0225989465093\n",
      "Loss for  14980 th iteration => 0.0225971417581\n",
      "Loss for  14981 th iteration => 0.0225955638125\n",
      "Loss for  14982 th iteration => 0.0225937644383\n",
      "Loss for  14983 th iteration => 0.0225920640868\n",
      "Loss for  14984 th iteration => 0.022590782658\n",
      "Loss for  14985 th iteration => 0.0225886740504\n",
      "Loss for  14986 th iteration => 0.0225870854613\n",
      "Loss for  14987 th iteration => 0.0225853836037\n",
      "Loss for  14988 th iteration => 0.0225827501928\n",
      "Loss for  14989 th iteration => 0.0225810451025\n",
      "Loss for  14990 th iteration => 0.0225798764042\n",
      "Loss for  14991 th iteration => 0.0225787792078\n",
      "Loss for  14992 th iteration => 0.0225764905911\n",
      "Loss for  14993 th iteration => 0.0225750807473\n",
      "Loss for  14994 th iteration => 0.0225731044616\n",
      "Loss for  14995 th iteration => 0.0225714067284\n",
      "Loss for  14996 th iteration => 0.0225693825625\n",
      "Loss for  14997 th iteration => 0.0225671519208\n",
      "Loss for  14998 th iteration => 0.022566502579\n",
      "Loss for  14999 th iteration => 0.0225640939858\n",
      "Loss for  15000 th iteration => 0.0225631262368\n",
      "Loss for  15001 th iteration => 0.0225611561171\n",
      "Loss for  15002 th iteration => 0.0225590061945\n",
      "Loss for  15003 th iteration => 0.0225582031342\n",
      "Loss for  15004 th iteration => 0.0225551066128\n",
      "Loss for  15005 th iteration => 0.0225538480361\n",
      "Loss for  15006 th iteration => 0.0225516930978\n",
      "Loss for  15007 th iteration => 0.0225507811068\n",
      "Loss for  15008 th iteration => 0.0225491548904\n",
      "Loss for  15009 th iteration => 0.0225470634165\n",
      "Loss for  15010 th iteration => 0.0225458021701\n",
      "Loss for  15011 th iteration => 0.0225433574205\n",
      "Loss for  15012 th iteration => 0.0225424481228\n",
      "Loss for  15013 th iteration => 0.022539997785\n",
      "Loss for  15014 th iteration => 0.0225377541198\n",
      "Loss for  15015 th iteration => 0.0225371003257\n",
      "Loss for  15016 th iteration => 0.0225347031142\n",
      "Loss for  15017 th iteration => 0.0225337131508\n",
      "Loss for  15018 th iteration => 0.022531771296\n",
      "Loss for  15019 th iteration => 0.0225296054523\n",
      "Loss for  15020 th iteration => 0.0225288244446\n",
      "Loss for  15021 th iteration => 0.0225257377229\n",
      "Loss for  15022 th iteration => 0.0225245005865\n",
      "Loss for  15023 th iteration => 0.0225223298573\n",
      "Loss for  15024 th iteration => 0.0225214219414\n",
      "Loss for  15025 th iteration => 0.0225197788844\n",
      "Loss for  15026 th iteration => 0.0225177142475\n",
      "Loss for  15027 th iteration => 0.0225164345172\n",
      "Loss for  15028 th iteration => 0.0225140272453\n",
      "Loss for  15029 th iteration => 0.0225124472341\n",
      "Loss for  15030 th iteration => 0.0225106418357\n",
      "Loss for  15031 th iteration => 0.0225085378784\n",
      "Loss for  15032 th iteration => 0.0225076554725\n",
      "Loss for  15033 th iteration => 0.0225054887161\n",
      "Loss for  15034 th iteration => 0.0225042644482\n",
      "Loss for  15035 th iteration => 0.022502560066\n",
      "Loss for  15036 th iteration => 0.022500188206\n",
      "Loss for  15037 th iteration => 0.0224989165746\n",
      "Loss for  15038 th iteration => 0.0224964183342\n",
      "Loss for  15039 th iteration => 0.0224952727962\n",
      "Loss for  15040 th iteration => 0.02249312696\n",
      "Loss for  15041 th iteration => 0.0224920997766\n",
      "Loss for  15042 th iteration => 0.0224904853306\n",
      "Loss for  15043 th iteration => 0.0224884059566\n",
      "Loss for  15044 th iteration => 0.0224871486397\n",
      "Loss for  15045 th iteration => 0.022484822452\n",
      "Loss for  15046 th iteration => 0.0224830646866\n",
      "Loss for  15047 th iteration => 0.0224809459525\n",
      "Loss for  15048 th iteration => 0.0224796921873\n",
      "Loss for  15049 th iteration => 0.0224787460304\n",
      "Loss for  15050 th iteration => 0.0224763355343\n",
      "Loss for  15051 th iteration => 0.0224750797089\n",
      "Loss for  15052 th iteration => 0.0224729785539\n",
      "Loss for  15053 th iteration => 0.0224714238457\n",
      "Loss for  15054 th iteration => 0.0224698042839\n",
      "Loss for  15055 th iteration => 0.0224671707476\n",
      "Loss for  15056 th iteration => 0.022466010694\n",
      "Loss for  15057 th iteration => 0.0224640185561\n",
      "Loss for  15058 th iteration => 0.022463259249\n",
      "Loss for  15059 th iteration => 0.0224611049422\n",
      "Loss for  15060 th iteration => 0.0224591752077\n",
      "Loss for  15061 th iteration => 0.0224581764108\n",
      "Loss for  15062 th iteration => 0.0224552271776\n",
      "Loss for  15063 th iteration => 0.0224539895462\n",
      "Loss for  15064 th iteration => 0.0224517417595\n",
      "Loss for  15065 th iteration => 0.0224505737096\n",
      "Loss for  15066 th iteration => 0.0224494788977\n",
      "Loss for  15067 th iteration => 0.0224472249064\n",
      "Loss for  15068 th iteration => 0.0224458239075\n",
      "Loss for  15069 th iteration => 0.0224438757582\n",
      "Loss for  15070 th iteration => 0.0224421904826\n",
      "Loss for  15071 th iteration => 0.0224402156952\n",
      "Loss for  15072 th iteration => 0.0224379772093\n",
      "Loss for  15073 th iteration => 0.0224373703537\n",
      "Loss for  15074 th iteration => 0.0224349463719\n",
      "Loss for  15075 th iteration => 0.0224340243341\n",
      "Loss for  15076 th iteration => 0.0224320368449\n",
      "Loss for  15077 th iteration => 0.0224299554478\n",
      "Loss for  15078 th iteration => 0.0224291125192\n",
      "Loss for  15079 th iteration => 0.0224260927147\n",
      "Loss for  15080 th iteration => 0.0224248473963\n",
      "Loss for  15081 th iteration => 0.0224226761788\n",
      "Loss for  15082 th iteration => 0.0224218279094\n",
      "Loss for  15083 th iteration => 0.0224201594931\n",
      "Loss for  15084 th iteration => 0.0224181532151\n",
      "Loss for  15085 th iteration => 0.0224168433795\n",
      "Loss for  15086 th iteration => 0.0224144899275\n",
      "Loss for  15087 th iteration => 0.0224135259426\n",
      "Loss for  15088 th iteration => 0.0224111449133\n",
      "Loss for  15089 th iteration => 0.0224088920097\n",
      "Loss for  15090 th iteration => 0.0224078919378\n",
      "Loss for  15091 th iteration => 0.022406088029\n",
      "Loss for  15092 th iteration => 0.022404874526\n",
      "Loss for  15093 th iteration => 0.0224028521868\n",
      "Loss for  15094 th iteration => 0.0224011391557\n",
      "Loss for  15095 th iteration => 0.0223996333902\n",
      "Loss for  15096 th iteration => 0.022397025919\n",
      "Loss for  15097 th iteration => 0.0223958161509\n",
      "Loss for  15098 th iteration => 0.0223940901642\n",
      "Loss for  15099 th iteration => 0.0223924880218\n",
      "Loss for  15100 th iteration => 0.0223912392812\n",
      "Loss for  15101 th iteration => 0.0223891553102\n",
      "Loss for  15102 th iteration => 0.0223876050203\n",
      "Loss for  15103 th iteration => 0.0223858222336\n",
      "Loss for  15104 th iteration => 0.0223840550207\n",
      "Loss for  15105 th iteration => 0.0223821084289\n",
      "Loss for  15106 th iteration => 0.022379961876\n",
      "Loss for  15107 th iteration => 0.0223787361262\n",
      "Loss for  15108 th iteration => 0.0223771699707\n",
      "Loss for  15109 th iteration => 0.0223758234849\n",
      "Loss for  15110 th iteration => 0.0223738411896\n",
      "Loss for  15111 th iteration => 0.022372197515\n",
      "Loss for  15112 th iteration => 0.0223705825688\n",
      "Loss for  15113 th iteration => 0.0223680331618\n",
      "Loss for  15114 th iteration => 0.0223668306087\n",
      "Loss for  15115 th iteration => 0.0223656964139\n",
      "Loss for  15116 th iteration => 0.0223631780943\n",
      "Loss for  15117 th iteration => 0.0223619456405\n",
      "Loss for  15118 th iteration => 0.0223605242849\n",
      "Loss for  15119 th iteration => 0.0223582980029\n",
      "Loss for  15120 th iteration => 0.0223572234117\n",
      "Loss for  15121 th iteration => 0.0223548630283\n",
      "Loss for  15122 th iteration => 0.0223526864555\n",
      "Loss for  15123 th iteration => 0.0223510684454\n",
      "Loss for  15124 th iteration => 0.0223502148685\n",
      "Loss for  15125 th iteration => 0.0223485462477\n",
      "Loss for  15126 th iteration => 0.0223465647208\n",
      "Loss for  15127 th iteration => 0.0223452502131\n",
      "Loss for  15128 th iteration => 0.0223429257912\n",
      "Loss for  15129 th iteration => 0.0223419528533\n",
      "Loss for  15130 th iteration => 0.0223396150035\n",
      "Loss for  15131 th iteration => 0.0223373650606\n",
      "Loss for  15132 th iteration => 0.0223363639727\n",
      "Loss for  15133 th iteration => 0.0223345795466\n",
      "Loss for  15134 th iteration => 0.0223333526381\n",
      "Loss for  15135 th iteration => 0.0223313627816\n",
      "Loss for  15136 th iteration => 0.022329642369\n",
      "Loss for  15137 th iteration => 0.0223281638421\n",
      "Loss for  15138 th iteration => 0.0223255748148\n",
      "Loss for  15139 th iteration => 0.0223238008213\n",
      "Loss for  15140 th iteration => 0.022322608969\n",
      "Loss for  15141 th iteration => 0.0223211583236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  15142 th iteration => 0.0223196968967\n",
      "Loss for  15143 th iteration => 0.0223178451992\n",
      "Loss for  15144 th iteration => 0.0223160886008\n",
      "Loss for  15145 th iteration => 0.0223145331306\n",
      "Loss for  15146 th iteration => 0.0223119944409\n",
      "Loss for  15147 th iteration => 0.022310199754\n",
      "Loss for  15148 th iteration => 0.0223086458336\n",
      "Loss for  15149 th iteration => 0.0223083806797\n",
      "Loss for  15150 th iteration => 0.0223057719385\n",
      "Loss for  15151 th iteration => 0.0223043486041\n",
      "Loss for  15152 th iteration => 0.022302883381\n",
      "Loss for  15153 th iteration => 0.0223003420666\n",
      "Loss for  15154 th iteration => 0.0222999804919\n",
      "Loss for  15155 th iteration => 0.0222967292337\n",
      "Loss for  15156 th iteration => 0.022294952401\n",
      "Loss for  15157 th iteration => 0.0222940310116\n",
      "Loss for  15158 th iteration => 0.0222926909381\n",
      "Loss for  15159 th iteration => 0.0222907260707\n",
      "Loss for  15160 th iteration => 0.0222890883955\n",
      "Loss for  15161 th iteration => 0.0222874207594\n",
      "Loss for  15162 th iteration => 0.0222855103512\n",
      "Loss for  15163 th iteration => 0.022283729787\n",
      "Loss for  15164 th iteration => 0.0222814694661\n",
      "Loss for  15165 th iteration => 0.0222802989152\n",
      "Loss for  15166 th iteration => 0.0222788399518\n",
      "Loss for  15167 th iteration => 0.022277409066\n",
      "Loss for  15168 th iteration => 0.02227553891\n",
      "Loss for  15169 th iteration => 0.0222738145997\n",
      "Loss for  15170 th iteration => 0.0222722374676\n",
      "Loss for  15171 th iteration => 0.022270293058\n",
      "Loss for  15172 th iteration => 0.0222685633636\n",
      "Loss for  15173 th iteration => 0.0222662936115\n",
      "Loss for  15174 th iteration => 0.0222651358959\n",
      "Loss for  15175 th iteration => 0.0222640882617\n",
      "Loss for  15176 th iteration => 0.0222619392546\n",
      "Loss for  15177 th iteration => 0.0222603978544\n",
      "Loss for  15178 th iteration => 0.0222587388423\n",
      "Loss for  15179 th iteration => 0.0222567727426\n",
      "Loss for  15180 th iteration => 0.0222549894065\n",
      "Loss for  15181 th iteration => 0.0222527711937\n",
      "Loss for  15182 th iteration => 0.0222517826755\n",
      "Loss for  15183 th iteration => 0.0222500123161\n",
      "Loss for  15184 th iteration => 0.0222488175096\n",
      "Loss for  15185 th iteration => 0.0222468182367\n",
      "Loss for  15186 th iteration => 0.022245136369\n",
      "Loss for  15187 th iteration => 0.0222436324375\n",
      "Loss for  15188 th iteration => 0.0222410695669\n",
      "Loss for  15189 th iteration => 0.0222398597534\n",
      "Loss for  15190 th iteration => 0.0222381717386\n",
      "Loss for  15191 th iteration => 0.0222365870439\n",
      "Loss for  15192 th iteration => 0.0222349061723\n",
      "Loss for  15193 th iteration => 0.0222335725186\n",
      "Loss for  15194 th iteration => 0.0222317147289\n",
      "Loss for  15195 th iteration => 0.0222299003494\n",
      "Loss for  15196 th iteration => 0.0222285795716\n",
      "Loss for  15197 th iteration => 0.0222258797384\n",
      "Loss for  15198 th iteration => 0.0222241822812\n",
      "Loss for  15199 th iteration => 0.0222230021485\n",
      "Loss for  15200 th iteration => 0.0222220273217\n",
      "Loss for  15201 th iteration => 0.0222198170909\n",
      "Loss for  15202 th iteration => 0.0222183507093\n",
      "Loss for  15203 th iteration => 0.0222166282819\n",
      "Loss for  15204 th iteration => 0.0222147184132\n",
      "Loss for  15205 th iteration => 0.0222128993343\n",
      "Loss for  15206 th iteration => 0.0222106947849\n",
      "Loss for  15207 th iteration => 0.022210163696\n",
      "Loss for  15208 th iteration => 0.0222076930262\n",
      "Loss for  15209 th iteration => 0.0222069152274\n",
      "Loss for  15210 th iteration => 0.022204833918\n",
      "Loss for  15211 th iteration => 0.0222029341997\n",
      "Loss for  15212 th iteration => 0.0222019605399\n",
      "Loss for  15213 th iteration => 0.0221991073981\n",
      "Loss for  15214 th iteration => 0.022197822661\n",
      "Loss for  15215 th iteration => 0.0221956722832\n",
      "Loss for  15216 th iteration => 0.0221949367866\n",
      "Loss for  15217 th iteration => 0.0221927260483\n",
      "Loss for  15218 th iteration => 0.0221916497748\n",
      "Loss for  15219 th iteration => 0.0221898661825\n",
      "Loss for  15220 th iteration => 0.0221876809483\n",
      "Loss for  15221 th iteration => 0.022186992168\n",
      "Loss for  15222 th iteration => 0.022183951086\n",
      "Loss for  15223 th iteration => 0.0221827607447\n",
      "Loss for  15224 th iteration => 0.0221806558248\n",
      "Loss for  15225 th iteration => 0.0221797717001\n",
      "Loss for  15226 th iteration => 0.0221781811242\n",
      "Loss for  15227 th iteration => 0.0221761790993\n",
      "Loss for  15228 th iteration => 0.0221749324884\n",
      "Loss for  15229 th iteration => 0.0221725973066\n",
      "Loss for  15230 th iteration => 0.0221716825289\n",
      "Loss for  15231 th iteration => 0.0221693317837\n",
      "Loss for  15232 th iteration => 0.0221671241535\n",
      "Loss for  15233 th iteration => 0.0221661653738\n",
      "Loss for  15234 th iteration => 0.0221643603279\n",
      "Loss for  15235 th iteration => 0.0221632246017\n",
      "Loss for  15236 th iteration => 0.0221611893117\n",
      "Loss for  15237 th iteration => 0.0221595724947\n",
      "Loss for  15238 th iteration => 0.0221580330715\n",
      "Loss for  15239 th iteration => 0.0221555347941\n",
      "Loss for  15240 th iteration => 0.0221543053486\n",
      "Loss for  15241 th iteration => 0.0221526244311\n",
      "Loss for  15242 th iteration => 0.0221510851366\n",
      "Loss for  15243 th iteration => 0.0221493617234\n",
      "Loss for  15244 th iteration => 0.0221480995825\n",
      "Loss for  15245 th iteration => 0.0221461934217\n",
      "Loss for  15246 th iteration => 0.0221444560422\n",
      "Loss for  15247 th iteration => 0.0221430838222\n",
      "Loss for  15248 th iteration => 0.0221404561906\n",
      "Loss for  15249 th iteration => 0.0221387614415\n",
      "Loss for  15250 th iteration => 0.0221375438073\n",
      "Loss for  15251 th iteration => 0.0221361552145\n",
      "Loss for  15252 th iteration => 0.0221346692911\n",
      "Loss for  15253 th iteration => 0.0221328942276\n",
      "Loss for  15254 th iteration => 0.0221311233268\n",
      "Loss for  15255 th iteration => 0.0221296522493\n",
      "Loss for  15256 th iteration => 0.0221271003363\n",
      "Loss for  15257 th iteration => 0.022125336007\n",
      "Loss for  15258 th iteration => 0.02212387033\n",
      "Loss for  15259 th iteration => 0.0221235152247\n",
      "Loss for  15260 th iteration => 0.0221210345682\n",
      "Loss for  15261 th iteration => 0.0221195586263\n",
      "Loss for  15262 th iteration => 0.0221181846263\n",
      "Loss for  15263 th iteration => 0.0221156280705\n",
      "Loss for  15264 th iteration => 0.0221146819744\n",
      "Loss for  15265 th iteration => 0.0221120412063\n",
      "Loss for  15266 th iteration => 0.0221104762856\n",
      "Loss for  15267 th iteration => 0.0221097047964\n",
      "Loss for  15268 th iteration => 0.0221080137424\n",
      "Loss for  15269 th iteration => 0.0221061351669\n",
      "Loss for  15270 th iteration => 0.0221047849061\n",
      "Loss for  15271 th iteration => 0.0221025761889\n",
      "Loss for  15272 th iteration => 0.0221015547481\n",
      "Loss for  15273 th iteration => 0.022099281344\n",
      "Loss for  15274 th iteration => 0.0220970911262\n",
      "Loss for  15275 th iteration => 0.0220960201492\n",
      "Loss for  15276 th iteration => 0.0220944382137\n",
      "Loss for  15277 th iteration => 0.0220931012366\n",
      "Loss for  15278 th iteration => 0.0220912852799\n",
      "Loss for  15279 th iteration => 0.0220894736373\n",
      "Loss for  15280 th iteration => 0.0220881286593\n",
      "Loss for  15281 th iteration => 0.0220859575294\n",
      "Loss for  15282 th iteration => 0.022084348109\n",
      "Loss for  15283 th iteration => 0.0220821635238\n",
      "Loss for  15284 th iteration => 0.0220810519337\n",
      "Loss for  15285 th iteration => 0.0220799886502\n",
      "Loss for  15286 th iteration => 0.0220778078799\n",
      "Loss for  15287 th iteration => 0.0220764553179\n",
      "Loss for  15288 th iteration => 0.0220745634192\n",
      "Loss for  15289 th iteration => 0.0220729336891\n",
      "Loss for  15290 th iteration => 0.0220709950849\n",
      "Loss for  15291 th iteration => 0.022068824618\n",
      "Loss for  15292 th iteration => 0.0220673551555\n",
      "Loss for  15293 th iteration => 0.0220664107262\n",
      "Loss for  15294 th iteration => 0.0220648816604\n",
      "Loss for  15295 th iteration => 0.0220628573261\n",
      "Loss for  15296 th iteration => 0.0220616641521\n",
      "Loss for  15297 th iteration => 0.0220593165137\n",
      "Loss for  15298 th iteration => 0.0220578104326\n",
      "Loss for  15299 th iteration => 0.0220560728221\n",
      "Loss for  15300 th iteration => 0.0220545982496\n",
      "Loss for  15301 th iteration => 0.022052856585\n",
      "Loss for  15302 th iteration => 0.0220512459018\n",
      "Loss for  15303 th iteration => 0.0220500250018\n",
      "Loss for  15304 th iteration => 0.0220480095155\n",
      "Loss for  15305 th iteration => 0.0220465059526\n",
      "Loss for  15306 th iteration => 0.0220448271074\n",
      "Loss for  15307 th iteration => 0.0220423886159\n",
      "Loss for  15308 th iteration => 0.0220412521171\n",
      "Loss for  15309 th iteration => 0.0220396662217\n",
      "Loss for  15310 th iteration => 0.0220379559125\n",
      "Loss for  15311 th iteration => 0.022036825708\n",
      "Loss for  15312 th iteration => 0.022034818982\n",
      "Loss for  15313 th iteration => 0.0220332151691\n",
      "Loss for  15314 th iteration => 0.0220316783898\n",
      "Loss for  15315 th iteration => 0.0220297132831\n",
      "Loss for  15316 th iteration => 0.0220279136619\n",
      "Loss for  15317 th iteration => 0.0220258702183\n",
      "Loss for  15318 th iteration => 0.0220247603082\n",
      "Loss for  15319 th iteration => 0.0220231103194\n",
      "Loss for  15320 th iteration => 0.0220218830479\n",
      "Loss for  15321 th iteration => 0.0220199762839\n",
      "Loss for  15322 th iteration => 0.0220182803607\n",
      "Loss for  15323 th iteration => 0.0220168454882\n",
      "Loss for  15324 th iteration => 0.0220143519695\n",
      "Loss for  15325 th iteration => 0.022013042615\n",
      "Loss for  15326 th iteration => 0.0220115542977\n",
      "Loss for  15327 th iteration => 0.0220098731191\n",
      "Loss for  15328 th iteration => 0.0220082840796\n",
      "Loss for  15329 th iteration => 0.022006961115\n",
      "Loss for  15330 th iteration => 0.0220051529583\n",
      "Loss for  15331 th iteration => 0.0220033661575\n",
      "Loss for  15332 th iteration => 0.0220020611932\n",
      "Loss for  15333 th iteration => 0.0219994480155\n",
      "Loss for  15334 th iteration => 0.0219982822828\n",
      "Loss for  15335 th iteration => 0.0219971858706\n",
      "Loss for  15336 th iteration => 0.02199474246\n",
      "Loss for  15337 th iteration => 0.0219935503442\n",
      "Loss for  15338 th iteration => 0.021992191174\n",
      "Loss for  15339 th iteration => 0.0219900251219\n",
      "Loss for  15340 th iteration => 0.0219889927858\n",
      "Loss for  15341 th iteration => 0.0219866657357\n",
      "Loss for  15342 th iteration => 0.0219845854951\n",
      "Loss for  15343 th iteration => 0.0219829844635\n",
      "Loss for  15344 th iteration => 0.0219822221555\n",
      "Loss for  15345 th iteration => 0.0219800982903\n",
      "Loss for  15346 th iteration => 0.0219789941241\n",
      "Loss for  15347 th iteration => 0.0219772831439\n",
      "Loss for  15348 th iteration => 0.0219751097835\n",
      "Loss for  15349 th iteration => 0.0219744543796\n",
      "Loss for  15350 th iteration => 0.02197146148\n",
      "Loss for  15351 th iteration => 0.0219697218864\n",
      "Loss for  15352 th iteration => 0.0219681896795\n",
      "Loss for  15353 th iteration => 0.021967456074\n",
      "Loss for  15354 th iteration => 0.0219656891588\n",
      "Loss for  15355 th iteration => 0.0219639348717\n",
      "Loss for  15356 th iteration => 0.0219624997257\n",
      "Loss for  15357 th iteration => 0.0219604239355\n",
      "Loss for  15358 th iteration => 0.0219593089675\n",
      "Loss for  15359 th iteration => 0.021957202549\n",
      "Loss for  15360 th iteration => 0.0219549162378\n",
      "Loss for  15361 th iteration => 0.0219534795809\n",
      "Loss for  15362 th iteration => 0.0219530845355\n",
      "Loss for  15363 th iteration => 0.0219506793833\n",
      "Loss for  15364 th iteration => 0.0219491959329\n",
      "Loss for  15365 th iteration => 0.0219478654325\n",
      "Loss for  15366 th iteration => 0.0219453309641\n",
      "Loss for  15367 th iteration => 0.0219450380449\n",
      "Loss for  15368 th iteration => 0.0219418682793\n",
      "Loss for  15369 th iteration => 0.0219401482869\n",
      "Loss for  15370 th iteration => 0.0219391346776\n",
      "Loss for  15371 th iteration => 0.0219380121649\n",
      "Loss for  15372 th iteration => 0.0219360243648\n",
      "Loss for  15373 th iteration => 0.0219344337076\n",
      "Loss for  15374 th iteration => 0.0219329104357\n",
      "Loss for  15375 th iteration => 0.0219308954665\n",
      "Loss for  15376 th iteration => 0.0219292381464\n",
      "Loss for  15377 th iteration => 0.0219270601818\n",
      "Loss for  15378 th iteration => 0.0219260660395\n",
      "Loss for  15379 th iteration => 0.0219244144429\n",
      "Loss for  15380 th iteration => 0.0219232020141\n",
      "Loss for  15381 th iteration => 0.0219213070432\n",
      "Loss for  15382 th iteration => 0.0219196311599\n",
      "Loss for  15383 th iteration => 0.021918196037\n",
      "Loss for  15384 th iteration => 0.021916139731\n",
      "Loss for  15385 th iteration => 0.0219145205976\n",
      "Loss for  15386 th iteration => 0.0219123478317\n",
      "Loss for  15387 th iteration => 0.0219113142171\n",
      "Loss for  15388 th iteration => 0.0219097064819\n",
      "Loss for  15389 th iteration => 0.021908422319\n",
      "Loss for  15390 th iteration => 0.0219066021456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  15391 th iteration => 0.0219048586318\n",
      "Loss for  15392 th iteration => 0.0219034942127\n",
      "Loss for  15393 th iteration => 0.0219014071091\n",
      "Loss for  15394 th iteration => 0.0218998185063\n",
      "Loss for  15395 th iteration => 0.021897650626\n",
      "Loss for  15396 th iteration => 0.0218965893749\n",
      "Loss for  15397 th iteration => 0.0218954722121\n",
      "Loss for  15398 th iteration => 0.0218933967762\n",
      "Loss for  15399 th iteration => 0.0218919994986\n",
      "Loss for  15400 th iteration => 0.0218902037397\n",
      "Loss for  15401 th iteration => 0.021888559983\n",
      "Loss for  15402 th iteration => 0.0218866720701\n",
      "Loss for  15403 th iteration => 0.0218845515989\n",
      "Loss for  15404 th iteration => 0.0218830773357\n",
      "Loss for  15405 th iteration => 0.0218821543623\n",
      "Loss for  15406 th iteration => 0.0218806420787\n",
      "Loss for  15407 th iteration => 0.0218786614991\n",
      "Loss for  15408 th iteration => 0.0218774754717\n",
      "Loss for  15409 th iteration => 0.0218751817725\n",
      "Loss for  15410 th iteration => 0.0218738258525\n",
      "Loss for  15411 th iteration => 0.0218719881383\n",
      "Loss for  15412 th iteration => 0.0218703761554\n",
      "Loss for  15413 th iteration => 0.0218684467223\n",
      "Loss for  15414 th iteration => 0.021867443609\n",
      "Loss for  15415 th iteration => 0.0218659747052\n",
      "Loss for  15416 th iteration => 0.0218639571851\n",
      "Loss for  15417 th iteration => 0.021862811781\n",
      "Loss for  15418 th iteration => 0.0218605201902\n",
      "Loss for  15419 th iteration => 0.0218585335908\n",
      "Loss for  15420 th iteration => 0.0218573380183\n",
      "Loss for  15421 th iteration => 0.0218557935103\n",
      "Loss for  15422 th iteration => 0.0218541718224\n",
      "Loss for  15423 th iteration => 0.0218525994152\n",
      "Loss for  15424 th iteration => 0.0218512806226\n",
      "Loss for  15425 th iteration => 0.0218495089663\n",
      "Loss for  15426 th iteration => 0.0218477395527\n",
      "Loss for  15427 th iteration => 0.0218464711252\n",
      "Loss for  15428 th iteration => 0.0218438530847\n",
      "Loss for  15429 th iteration => 0.0218422148446\n",
      "Loss for  15430 th iteration => 0.0218415840421\n",
      "Loss for  15431 th iteration => 0.0218393363607\n",
      "Loss for  15432 th iteration => 0.0218384280759\n",
      "Loss for  15433 th iteration => 0.0218365534845\n",
      "Loss for  15434 th iteration => 0.0218345947078\n",
      "Loss for  15435 th iteration => 0.0218337572754\n",
      "Loss for  15436 th iteration => 0.021830923326\n",
      "Loss for  15437 th iteration => 0.0218297140076\n",
      "Loss for  15438 th iteration => 0.0218276248231\n",
      "Loss for  15439 th iteration => 0.0218269064402\n",
      "Loss for  15440 th iteration => 0.0218247701975\n",
      "Loss for  15441 th iteration => 0.0218237362854\n",
      "Loss for  15442 th iteration => 0.0218219887325\n",
      "Loss for  15443 th iteration => 0.0218199111716\n",
      "Loss for  15444 th iteration => 0.0218191940055\n",
      "Loss for  15445 th iteration => 0.0218162856437\n",
      "Loss for  15446 th iteration => 0.0218151155221\n",
      "Loss for  15447 th iteration => 0.0218130469675\n",
      "Loss for  15448 th iteration => 0.0218122586658\n",
      "Loss for  15449 th iteration => 0.0218106640527\n",
      "Loss for  15450 th iteration => 0.0218087867039\n",
      "Loss for  15451 th iteration => 0.0218075171724\n",
      "Loss for  15452 th iteration => 0.0218053247585\n",
      "Loss for  15453 th iteration => 0.0218043689851\n",
      "Loss for  15454 th iteration => 0.0218020993177\n",
      "Loss for  15455 th iteration => 0.0217999962873\n",
      "Loss for  15456 th iteration => 0.0217984961877\n",
      "Loss for  15457 th iteration => 0.0217976476351\n",
      "Loss for  15458 th iteration => 0.0217960810355\n",
      "Loss for  15459 th iteration => 0.0217941817621\n",
      "Loss for  15460 th iteration => 0.0217929378869\n",
      "Loss for  15461 th iteration => 0.0217907258219\n",
      "Loss for  15462 th iteration => 0.0217897934263\n",
      "Loss for  15463 th iteration => 0.0217875477117\n",
      "Loss for  15464 th iteration => 0.0217854164071\n",
      "Loss for  15465 th iteration => 0.0217843419995\n",
      "Loss for  15466 th iteration => 0.0217828223911\n",
      "Loss for  15467 th iteration => 0.0217816022307\n",
      "Loss for  15468 th iteration => 0.021779660987\n",
      "Loss for  15469 th iteration => 0.0217781686691\n",
      "Loss for  15470 th iteration => 0.0217764991116\n",
      "Loss for  15471 th iteration => 0.0217747968693\n",
      "Loss for  15472 th iteration => 0.0217730079291\n",
      "Loss for  15473 th iteration => 0.0217708744184\n",
      "Loss for  15474 th iteration => 0.0217697897804\n",
      "Loss for  15475 th iteration => 0.021768278741\n",
      "Loss for  15476 th iteration => 0.0217670299614\n",
      "Loss for  15477 th iteration => 0.0217651212111\n",
      "Loss for  15478 th iteration => 0.0217636021648\n",
      "Loss for  15479 th iteration => 0.0217619786073\n",
      "Loss for  15480 th iteration => 0.0217596555151\n",
      "Loss for  15481 th iteration => 0.0217584468605\n",
      "Loss for  15482 th iteration => 0.0217564638363\n",
      "Loss for  15483 th iteration => 0.0217556136232\n",
      "Loss for  15484 th iteration => 0.0217536000956\n",
      "Loss for  15485 th iteration => 0.0217524585479\n",
      "Loss for  15486 th iteration => 0.0217508321844\n",
      "Loss for  15487 th iteration => 0.0217486632549\n",
      "Loss for  15488 th iteration => 0.0217480512124\n",
      "Loss for  15489 th iteration => 0.021745121579\n",
      "Loss for  15490 th iteration => 0.0217439366968\n",
      "Loss for  15491 th iteration => 0.0217424902758\n",
      "Loss for  15492 th iteration => 0.0217407391774\n",
      "Loss for  15493 th iteration => 0.0217392462053\n",
      "Loss for  15494 th iteration => 0.0217379512173\n",
      "Loss for  15495 th iteration => 0.021736096465\n",
      "Loss for  15496 th iteration => 0.0217345343834\n",
      "Loss for  15497 th iteration => 0.0217330018157\n",
      "Loss for  15498 th iteration => 0.0217306055423\n",
      "Loss for  15499 th iteration => 0.0217289316524\n",
      "Loss for  15500 th iteration => 0.021727935371\n",
      "Loss for  15501 th iteration => 0.0217263347357\n",
      "Loss for  15502 th iteration => 0.0217250938653\n",
      "Loss for  15503 th iteration => 0.0217232787282\n",
      "Loss for  15504 th iteration => 0.021721591577\n",
      "Loss for  15505 th iteration => 0.0217202192123\n",
      "Loss for  15506 th iteration => 0.0217181785246\n",
      "Loss for  15507 th iteration => 0.0217166145998\n",
      "Loss for  15508 th iteration => 0.0217144638513\n",
      "Loss for  15509 th iteration => 0.0217134502312\n",
      "Loss for  15510 th iteration => 0.0217118659069\n",
      "Loss for  15511 th iteration => 0.0217105959525\n",
      "Loss for  15512 th iteration => 0.0217088132652\n",
      "Loss for  15513 th iteration => 0.0217070996451\n",
      "Loss for  15514 th iteration => 0.0217057631285\n",
      "Loss for  15515 th iteration => 0.0217032725053\n",
      "Loss for  15516 th iteration => 0.0217021305613\n",
      "Loss for  15517 th iteration => 0.0217000798793\n",
      "Loss for  15518 th iteration => 0.021699287446\n",
      "Loss for  15519 th iteration => 0.0216977109232\n",
      "Loss for  15520 th iteration => 0.0216958526358\n",
      "Loss for  15521 th iteration => 0.0216945948784\n",
      "Loss for  15522 th iteration => 0.0216924275884\n",
      "Loss for  15523 th iteration => 0.0216914775311\n",
      "Loss for  15524 th iteration => 0.0216892392292\n",
      "Loss for  15525 th iteration => 0.0216871614896\n",
      "Loss for  15526 th iteration => 0.0216856617007\n",
      "Loss for  15527 th iteration => 0.0216848313568\n",
      "Loss for  15528 th iteration => 0.0216832685136\n",
      "Loss for  15529 th iteration => 0.0216814021541\n",
      "Loss for  15530 th iteration => 0.0216801562348\n",
      "Loss for  15531 th iteration => 0.0216779826469\n",
      "Loss for  15532 th iteration => 0.0216770426503\n",
      "Loss for  15533 th iteration => 0.0216748282391\n",
      "Loss for  15534 th iteration => 0.0216727290943\n",
      "Loss for  15535 th iteration => 0.0216712563692\n",
      "Loss for  15536 th iteration => 0.0216703983398\n",
      "Loss for  15537 th iteration => 0.0216688425059\n",
      "Loss for  15538 th iteration => 0.0216669745554\n",
      "Loss for  15539 th iteration => 0.0216657340281\n",
      "Loss for  15540 th iteration => 0.021663560406\n",
      "Loss for  15541 th iteration => 0.0216626242423\n",
      "Loss for  15542 th iteration => 0.0216604338207\n",
      "Loss for  15543 th iteration => 0.021658317763\n",
      "Loss for  15544 th iteration => 0.0216572395604\n",
      "Loss for  15545 th iteration => 0.0216557555574\n",
      "Loss for  15546 th iteration => 0.0216545176567\n",
      "Loss for  15547 th iteration => 0.0216526292282\n",
      "Loss for  15548 th iteration => 0.0216511251547\n",
      "Loss for  15549 th iteration => 0.021649502411\n",
      "Loss for  15550 th iteration => 0.0216477960167\n",
      "Loss for  15551 th iteration => 0.0216460517781\n",
      "Loss for  15552 th iteration => 0.0216439355912\n",
      "Loss for  15553 th iteration => 0.0216428518892\n",
      "Loss for  15554 th iteration => 0.021641370671\n",
      "Loss for  15555 th iteration => 0.0216401181141\n",
      "Loss for  15556 th iteration => 0.0216382482133\n",
      "Loss for  15557 th iteration => 0.0216367309002\n",
      "Loss for  15558 th iteration => 0.0216351293807\n",
      "Loss for  15559 th iteration => 0.0216328518787\n",
      "Loss for  15560 th iteration => 0.0216316494556\n",
      "Loss for  15561 th iteration => 0.0216296556342\n",
      "Loss for  15562 th iteration => 0.0216288785747\n",
      "Loss for  15563 th iteration => 0.0216268083371\n",
      "Loss for  15564 th iteration => 0.0216257635561\n",
      "Loss for  15565 th iteration => 0.0216240682889\n",
      "Loss for  15566 th iteration => 0.0216220152104\n",
      "Loss for  15567 th iteration => 0.0216213154529\n",
      "Loss for  15568 th iteration => 0.0216184905476\n",
      "Loss for  15569 th iteration => 0.0216172969142\n",
      "Loss for  15570 th iteration => 0.0216153095778\n",
      "Loss for  15571 th iteration => 0.0216145170856\n",
      "Loss for  15572 th iteration => 0.0216124669872\n",
      "Loss for  15573 th iteration => 0.0216114028397\n",
      "Loss for  15574 th iteration => 0.0216097296868\n",
      "Loss for  15575 th iteration => 0.021607660399\n",
      "Loss for  15576 th iteration => 0.021606979638\n",
      "Loss for  15577 th iteration => 0.021604149865\n",
      "Loss for  15578 th iteration => 0.0216029618254\n",
      "Loss for  15579 th iteration => 0.0216009795074\n",
      "Loss for  15580 th iteration => 0.021600177407\n",
      "Loss for  15581 th iteration => 0.021598140081\n",
      "Loss for  15582 th iteration => 0.0215970651563\n",
      "Loss for  15583 th iteration => 0.0215954056373\n",
      "Loss for  15584 th iteration => 0.0215933284351\n",
      "Loss for  15585 th iteration => 0.0215926584821\n",
      "Loss for  15586 th iteration => 0.021589829456\n",
      "Loss for  15587 th iteration => 0.0215886442867\n",
      "Loss for  15588 th iteration => 0.0215866657389\n",
      "Loss for  15589 th iteration => 0.0215858589837\n",
      "Loss for  15590 th iteration => 0.0215838281583\n",
      "Loss for  15591 th iteration => 0.0215827497657\n",
      "Loss for  15592 th iteration => 0.0215810966635\n",
      "Loss for  15593 th iteration => 0.0215790186046\n",
      "Loss for  15594 th iteration => 0.0215783524918\n",
      "Loss for  15595 th iteration => 0.0215755289966\n",
      "Loss for  15596 th iteration => 0.0215743443765\n",
      "Loss for  15597 th iteration => 0.0215723685368\n",
      "Loss for  15598 th iteration => 0.0215715613382\n",
      "Loss for  15599 th iteration => 0.0215695316745\n",
      "Loss for  15600 th iteration => 0.0215684560328\n",
      "Loss for  15601 th iteration => 0.0215668032061\n",
      "Loss for  15602 th iteration => 0.0215647302947\n",
      "Loss for  15603 th iteration => 0.0215640620936\n",
      "Loss for  15604 th iteration => 0.0215612482061\n",
      "Loss for  15605 th iteration => 0.0215600621569\n",
      "Loss for  15606 th iteration => 0.0215580881217\n",
      "Loss for  15607 th iteration => 0.021557284059\n",
      "Loss for  15608 th iteration => 0.0215552510127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  15609 th iteration => 0.0215541834118\n",
      "Loss for  15610 th iteration => 0.0215525256359\n",
      "Loss for  15611 th iteration => 0.0215504629789\n",
      "Loss for  15612 th iteration => 0.0215497876462\n",
      "Loss for  15613 th iteration => 0.0215469868408\n",
      "Loss for  15614 th iteration => 0.0215457976759\n",
      "Loss for  15615 th iteration => 0.0215438246765\n",
      "Loss for  15616 th iteration => 0.0215430267907\n",
      "Loss for  15617 th iteration => 0.0215409864946\n",
      "Loss for  15618 th iteration => 0.0215399314332\n",
      "Loss for  15619 th iteration => 0.021538264264\n",
      "Loss for  15620 th iteration => 0.0215362162038\n",
      "Loss for  15621 th iteration => 0.0215355294502\n",
      "Loss for  15622 th iteration => 0.021532744688\n",
      "Loss for  15623 th iteration => 0.0215315509697\n",
      "Loss for  15624 th iteration => 0.0215295783523\n",
      "Loss for  15625 th iteration => 0.0215287892259\n",
      "Loss for  15626 th iteration => 0.0215267383891\n",
      "Loss for  15627 th iteration => 0.0215256996922\n",
      "Loss for  15628 th iteration => 0.0215240193503\n",
      "Loss for  15629 th iteration => 0.0215219895784\n",
      "Loss for  15630 th iteration => 0.0215212877569\n",
      "Loss for  15631 th iteration => 0.0215185215616\n",
      "Loss for  15632 th iteration => 0.0215173220641\n",
      "Loss for  15633 th iteration => 0.0215153492729\n",
      "Loss for  15634 th iteration => 0.0215145710979\n",
      "Loss for  15635 th iteration => 0.0215125069207\n",
      "Loss for  15636 th iteration => 0.0215114878391\n",
      "Loss for  15637 th iteration => 0.0215097911117\n",
      "Loss for  15638 th iteration => 0.0215077827646\n",
      "Loss for  15639 th iteration => 0.0215070627756\n",
      "Loss for  15640 th iteration => 0.0215043172985\n",
      "Loss for  15641 th iteration => 0.0215031109762\n",
      "Loss for  15642 th iteration => 0.0215011375388\n",
      "Loss for  15643 th iteration => 0.0215003721745\n",
      "Loss for  15644 th iteration => 0.0214982922756\n",
      "Loss for  15645 th iteration => 0.021497295571\n",
      "Loss for  15646 th iteration => 0.0214955797278\n",
      "Loss for  15647 th iteration => 0.0214935954698\n",
      "Loss for  15648 th iteration => 0.0214928546795\n",
      "Loss for  15649 th iteration => 0.0214901317545\n",
      "Loss for  15650 th iteration => 0.0214889177158\n",
      "Loss for  15651 th iteration => 0.0214869432307\n",
      "Loss for  15652 th iteration => 0.0214861922532\n",
      "Loss for  15653 th iteration => 0.0214840946078\n",
      "Loss for  15654 th iteration => 0.0214831226253\n",
      "Loss for  15655 th iteration => 0.021481385347\n",
      "Loss for  15656 th iteration => 0.0214794274396\n",
      "Loss for  15657 th iteration => 0.0214786636116\n",
      "Loss for  15658 th iteration => 0.0214759648019\n",
      "Loss for  15659 th iteration => 0.0214747422864\n",
      "Loss for  15660 th iteration => 0.0214727664127\n",
      "Loss for  15661 th iteration => 0.0214720311563\n",
      "Loss for  15662 th iteration => 0.0214699140434\n",
      "Loss for  15663 th iteration => 0.0214689687732\n",
      "Loss for  15664 th iteration => 0.0214672080907\n",
      "Loss for  15665 th iteration => 0.0214652784526\n",
      "Loss for  15666 th iteration => 0.0214644896887\n",
      "Loss for  15667 th iteration => 0.021461816327\n",
      "Loss for  15668 th iteration => 0.0214605846859\n",
      "Loss for  15669 th iteration => 0.0214586071342\n",
      "Loss for  15670 th iteration => 0.0214578887279\n",
      "Loss for  15671 th iteration => 0.0214557506851\n",
      "Loss for  15672 th iteration => 0.0214548338148\n",
      "Loss for  15673 th iteration => 0.0214530480576\n",
      "Loss for  15674 th iteration => 0.0214511483152\n",
      "Loss for  15675 th iteration => 0.0214503330056\n",
      "Loss for  15676 th iteration => 0.0214476862281\n",
      "Loss for  15677 th iteration => 0.0214464449077\n",
      "Loss for  15678 th iteration => 0.0214444692165\n",
      "Loss for  15679 th iteration => 0.0214434186337\n",
      "Loss for  15680 th iteration => 0.021441793656\n",
      "Loss for  15681 th iteration => 0.0214406487169\n",
      "Loss for  15682 th iteration => 0.021438811323\n",
      "Loss for  15683 th iteration => 0.021437242542\n",
      "Loss for  15684 th iteration => 0.0214358638383\n",
      "Loss for  15685 th iteration => 0.0214335387608\n",
      "Loss for  15686 th iteration => 0.0214322956488\n",
      "Loss for  15687 th iteration => 0.0214304247392\n",
      "Loss for  15688 th iteration => 0.021429569225\n",
      "Loss for  15689 th iteration => 0.0214275604334\n",
      "Loss for  15690 th iteration => 0.0214265306286\n",
      "Loss for  15691 th iteration => 0.0214248621737\n",
      "Loss for  15692 th iteration => 0.0214228581532\n",
      "Loss for  15693 th iteration => 0.0214221515878\n",
      "Loss for  15694 th iteration => 0.0214194516206\n",
      "Loss for  15695 th iteration => 0.0214181890666\n",
      "Loss for  15696 th iteration => 0.0214163111252\n",
      "Loss for  15697 th iteration => 0.021415492388\n",
      "Loss for  15698 th iteration => 0.0214134377721\n",
      "Loss for  15699 th iteration => 0.0214124651816\n",
      "Loss for  15700 th iteration => 0.0214107432055\n",
      "Loss for  15701 th iteration => 0.0214087969163\n",
      "Loss for  15702 th iteration => 0.0214080363287\n",
      "Loss for  15703 th iteration => 0.0214053816379\n",
      "Loss for  15704 th iteration => 0.0214041006545\n",
      "Loss for  15705 th iteration => 0.0214022161651\n",
      "Loss for  15706 th iteration => 0.0214014322588\n",
      "Loss for  15707 th iteration => 0.0213993342196\n",
      "Loss for  15708 th iteration => 0.0213984160161\n",
      "Loss for  15709 th iteration => 0.0213966433027\n",
      "Loss for  15710 th iteration => 0.0213947520082\n",
      "Loss for  15711 th iteration => 0.0213939400924\n",
      "Loss for  15712 th iteration => 0.0213913288859\n",
      "Loss for  15713 th iteration => 0.0213900303359\n",
      "Loss for  15714 th iteration => 0.0213881397127\n",
      "Loss for  15715 th iteration => 0.0213873889689\n",
      "Loss for  15716 th iteration => 0.0213852495586\n",
      "Loss for  15717 th iteration => 0.0213843833226\n",
      "Loss for  15718 th iteration => 0.0213825622537\n",
      "Loss for  15719 th iteration => 0.0213807236109\n",
      "Loss for  15720 th iteration => 0.0213798626728\n",
      "Loss for  15721 th iteration => 0.0213772934223\n",
      "Loss for  15722 th iteration => 0.0213759780409\n",
      "Loss for  15723 th iteration => 0.0213740816386\n",
      "Loss for  15724 th iteration => 0.0213733626254\n",
      "Loss for  15725 th iteration => 0.0213711919747\n",
      "Loss for  15726 th iteration => 0.0213699630503\n",
      "Loss for  15727 th iteration => 0.0213687099444\n",
      "Loss for  15728 th iteration => 0.0213666420247\n",
      "Loss for  15729 th iteration => 0.0213656810668\n",
      "Loss for  15730 th iteration => 0.0213635340871\n",
      "Loss for  15731 th iteration => 0.0213615731469\n",
      "Loss for  15732 th iteration => 0.0213600095055\n",
      "Loss for  15733 th iteration => 0.021359311539\n",
      "Loss for  15734 th iteration => 0.0213572385204\n",
      "Loss for  15735 th iteration => 0.0213562600656\n",
      "Loss for  15736 th iteration => 0.0213545555403\n",
      "Loss for  15737 th iteration => 0.0213526132508\n",
      "Loss for  15738 th iteration => 0.0213518603817\n",
      "Loss for  15739 th iteration => 0.0213491746247\n",
      "Loss for  15740 th iteration => 0.0213475330579\n",
      "Loss for  15741 th iteration => 0.0213459519775\n",
      "Loss for  15742 th iteration => 0.0213453630069\n",
      "Loss for  15743 th iteration => 0.021343590144\n",
      "Loss for  15744 th iteration => 0.0213420421525\n",
      "Loss for  15745 th iteration => 0.0213405705349\n",
      "Loss for  15746 th iteration => 0.0213387303121\n",
      "Loss for  15747 th iteration => 0.021337549643\n",
      "Loss for  15748 th iteration => 0.0213356149306\n",
      "Loss for  15749 th iteration => 0.0213335441365\n",
      "Loss for  15750 th iteration => 0.0213319493154\n",
      "Loss for  15751 th iteration => 0.0213309363831\n",
      "Loss for  15752 th iteration => 0.0213298510372\n",
      "Loss for  15753 th iteration => 0.0213279837524\n",
      "Loss for  15754 th iteration => 0.0213264783682\n",
      "Loss for  15755 th iteration => 0.0213250278007\n",
      "Loss for  15756 th iteration => 0.0213231489301\n",
      "Loss for  15757 th iteration => 0.0213215950808\n",
      "Loss for  15758 th iteration => 0.0213195365064\n",
      "Loss for  15759 th iteration => 0.0213180631104\n",
      "Loss for  15760 th iteration => 0.0213173090277\n",
      "Loss for  15761 th iteration => 0.0213157094705\n",
      "Loss for  15762 th iteration => 0.0213139986615\n",
      "Loss for  15763 th iteration => 0.0213126966979\n",
      "Loss for  15764 th iteration => 0.0213106972072\n",
      "Loss for  15765 th iteration => 0.0213096826411\n",
      "Loss for  15766 th iteration => 0.0213076355599\n",
      "Loss for  15767 th iteration => 0.0213055854944\n",
      "Loss for  15768 th iteration => 0.0213040867531\n",
      "Loss for  15769 th iteration => 0.0213033718378\n",
      "Loss for  15770 th iteration => 0.0213017374962\n",
      "Loss for  15771 th iteration => 0.0213000654545\n",
      "Loss for  15772 th iteration => 0.0212987286334\n",
      "Loss for  15773 th iteration => 0.0212967679707\n",
      "Loss for  15774 th iteration => 0.0212957184887\n",
      "Loss for  15775 th iteration => 0.0212936936022\n",
      "Loss for  15776 th iteration => 0.0212916519918\n",
      "Loss for  15777 th iteration => 0.0212901286226\n",
      "Loss for  15778 th iteration => 0.0212894514861\n",
      "Loss for  15779 th iteration => 0.0212877836612\n",
      "Loss for  15780 th iteration => 0.0212861490926\n",
      "Loss for  15781 th iteration => 0.0212847786956\n",
      "Loss for  15782 th iteration => 0.0212828555859\n",
      "Loss for  15783 th iteration => 0.0212817724503\n",
      "Loss for  15784 th iteration => 0.0212797691608\n",
      "Loss for  15785 th iteration => 0.0212777359825\n",
      "Loss for  15786 th iteration => 0.0212761886251\n",
      "Loss for  15787 th iteration => 0.0212755479959\n",
      "Loss for  15788 th iteration => 0.0212738478991\n",
      "Loss for  15789 th iteration => 0.0212722495977\n",
      "Loss for  15790 th iteration => 0.0212708468185\n",
      "Loss for  15791 th iteration => 0.0212689600729\n",
      "Loss for  15792 th iteration => 0.0212678444606\n",
      "Loss for  15793 th iteration => 0.0212658621913\n",
      "Loss for  15794 th iteration => 0.0212638374484\n",
      "Loss for  15795 th iteration => 0.0212622729167\n",
      "Loss for  15796 th iteration => 0.0212613115721\n",
      "Loss for  15797 th iteration => 0.0212601387567\n",
      "Loss for  15798 th iteration => 0.0212582933312\n",
      "Loss for  15799 th iteration => 0.0212568702606\n",
      "Loss for  15800 th iteration => 0.0212552745695\n",
      "Loss for  15801 th iteration => 0.0212536318443\n",
      "Loss for  15802 th iteration => 0.0212519385527\n",
      "Loss for  15803 th iteration => 0.0212499280873\n",
      "Loss for  15804 th iteration => 0.0212484410388\n",
      "Loss for  15805 th iteration => 0.0212477043051\n",
      "Loss for  15806 th iteration => 0.0212461138702\n",
      "Loss for  15807 th iteration => 0.0212444154911\n",
      "Loss for  15808 th iteration => 0.0212431198473\n",
      "Loss for  15809 th iteration => 0.0212411354763\n",
      "Loss for  15810 th iteration => 0.0212401245479\n",
      "Loss for  15811 th iteration => 0.0212380653731\n",
      "Loss for  15812 th iteration => 0.0212360661332\n",
      "Loss for  15813 th iteration => 0.0212345487695\n",
      "Loss for  15814 th iteration => 0.0212338574114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  15815 th iteration => 0.0212322281577\n",
      "Loss for  15816 th iteration => 0.0212305724309\n",
      "Loss for  15817 th iteration => 0.0212292380386\n",
      "Loss for  15818 th iteration => 0.0212272962416\n",
      "Loss for  15819 th iteration => 0.0212262466456\n",
      "Loss for  15820 th iteration => 0.0212242097695\n",
      "Loss for  15821 th iteration => 0.0212222213724\n",
      "Loss for  15822 th iteration => 0.0212206750726\n",
      "Loss for  15823 th iteration => 0.0212200267761\n",
      "Loss for  15824 th iteration => 0.0212183606406\n",
      "Loss for  15825 th iteration => 0.0212167456512\n",
      "Loss for  15826 th iteration => 0.0212153744085\n",
      "Loss for  15827 th iteration => 0.0212134733085\n",
      "Loss for  15828 th iteration => 0.0212123869051\n",
      "Loss for  15829 th iteration => 0.0212103716726\n",
      "Loss for  15830 th iteration => 0.0212083938092\n",
      "Loss for  15831 th iteration => 0.0212068202571\n",
      "Loss for  15832 th iteration => 0.0212057541044\n",
      "Loss for  15833 th iteration => 0.0212047899379\n",
      "Loss for  15834 th iteration => 0.0212028342131\n",
      "Loss for  15835 th iteration => 0.0212014570327\n",
      "Loss for  15836 th iteration => 0.0211999110584\n",
      "Loss for  15837 th iteration => 0.021198147556\n",
      "Loss for  15838 th iteration => 0.0211965073058\n",
      "Loss for  15839 th iteration => 0.0211945472037\n",
      "Loss for  15840 th iteration => 0.0211930822011\n",
      "Loss for  15841 th iteration => 0.021192318062\n",
      "Loss for  15842 th iteration => 0.0211903704914\n",
      "Loss for  15843 th iteration => 0.0211893040665\n",
      "Loss for  15844 th iteration => 0.0211877208548\n",
      "Loss for  15845 th iteration => 0.0211857219589\n",
      "Loss for  15846 th iteration => 0.0211850594474\n",
      "Loss for  15847 th iteration => 0.0211823409112\n",
      "Loss for  15848 th iteration => 0.0211807253339\n",
      "Loss for  15849 th iteration => 0.0211792147906\n",
      "Loss for  15850 th iteration => 0.0211786179589\n",
      "Loss for  15851 th iteration => 0.0211768800379\n",
      "Loss for  15852 th iteration => 0.0211753506582\n",
      "Loss for  15853 th iteration => 0.0211739049101\n",
      "Loss for  15854 th iteration => 0.0211720920222\n",
      "Loss for  15855 th iteration => 0.0211709285108\n",
      "Loss for  15856 th iteration => 0.021168992596\n",
      "Loss for  15857 th iteration => 0.0211669525645\n",
      "Loss for  15858 th iteration => 0.0211654056713\n",
      "Loss for  15859 th iteration => 0.0211645045546\n",
      "Loss for  15860 th iteration => 0.0211632982461\n",
      "Loss for  15861 th iteration => 0.0211615123771\n",
      "Loss for  15862 th iteration => 0.0211600603331\n",
      "Loss for  15863 th iteration => 0.0211585196695\n",
      "Loss for  15864 th iteration => 0.0211568353526\n",
      "Loss for  15865 th iteration => 0.0211551870456\n",
      "Loss for  15866 th iteration => 0.0211531686354\n",
      "Loss for  15867 th iteration => 0.0211516900768\n",
      "Loss for  15868 th iteration => 0.0211510330639\n",
      "Loss for  15869 th iteration => 0.0211493820347\n",
      "Loss for  15870 th iteration => 0.0211477745971\n",
      "Loss for  15871 th iteration => 0.0211464140321\n",
      "Loss for  15872 th iteration => 0.0211445247455\n",
      "Loss for  15873 th iteration => 0.021143444761\n",
      "Loss for  15874 th iteration => 0.0211414316938\n",
      "Loss for  15875 th iteration => 0.0211394310993\n",
      "Loss for  15876 th iteration => 0.0211379114662\n",
      "Loss for  15877 th iteration => 0.0211373128322\n",
      "Loss for  15878 th iteration => 0.0211356153921\n",
      "Loss for  15879 th iteration => 0.0211340579021\n",
      "Loss for  15880 th iteration => 0.0211326513068\n",
      "Loss for  15881 th iteration => 0.0211308115886\n",
      "Loss for  15882 th iteration => 0.0211296859562\n",
      "Loss for  15883 th iteration => 0.0211276941012\n",
      "Loss for  15884 th iteration => 0.0211257101916\n",
      "Loss for  15885 th iteration => 0.0211241523412\n",
      "Loss for  15886 th iteration => 0.0211236077161\n",
      "Loss for  15887 th iteration => 0.0211218671989\n",
      "Loss for  15888 th iteration => 0.021120356378\n",
      "Loss for  15889 th iteration => 0.0211189070047\n",
      "Loss for  15890 th iteration => 0.0211171136557\n",
      "Loss for  15891 th iteration => 0.0211159455485\n",
      "Loss for  15892 th iteration => 0.0211139741464\n",
      "Loss for  15893 th iteration => 0.0211120059613\n",
      "Loss for  15894 th iteration => 0.0211104330157\n",
      "Loss for  15895 th iteration => 0.021109404037\n",
      "Loss for  15896 th iteration => 0.0211084739824\n",
      "Loss for  15897 th iteration => 0.0211065090979\n",
      "Loss for  15898 th iteration => 0.0211051716191\n",
      "Loss for  15899 th iteration => 0.0211036109393\n",
      "Loss for  15900 th iteration => 0.0211018797438\n",
      "Loss for  15901 th iteration => 0.0211007609765\n",
      "Loss for  15902 th iteration => 0.0210983218867\n",
      "Loss for  15903 th iteration => 0.0210967355469\n",
      "Loss for  15904 th iteration => 0.0210956960871\n",
      "Loss for  15905 th iteration => 0.0210943671857\n",
      "Loss for  15906 th iteration => 0.0210930701433\n",
      "Loss for  15907 th iteration => 0.0210913934718\n",
      "Loss for  15908 th iteration => 0.0210898561079\n",
      "Loss for  15909 th iteration => 0.0210884192106\n",
      "Loss for  15910 th iteration => 0.021086684608\n",
      "Loss for  15911 th iteration => 0.0210851108254\n",
      "Loss for  15912 th iteration => 0.0210830559818\n",
      "Loss for  15913 th iteration => 0.0210816452973\n",
      "Loss for  15914 th iteration => 0.0210809234072\n",
      "Loss for  15915 th iteration => 0.0210793052744\n",
      "Loss for  15916 th iteration => 0.0210776886351\n",
      "Loss for  15917 th iteration => 0.0210763555637\n",
      "Loss for  15918 th iteration => 0.0210744622815\n",
      "Loss for  15919 th iteration => 0.0210734045857\n",
      "Loss for  15920 th iteration => 0.0210714376066\n",
      "Loss for  15921 th iteration => 0.0210694106337\n",
      "Loss for  15922 th iteration => 0.021067940315\n",
      "Loss for  15923 th iteration => 0.021067301529\n",
      "Loss for  15924 th iteration => 0.0210656209135\n",
      "Loss for  15925 th iteration => 0.0210640698394\n",
      "Loss for  15926 th iteration => 0.0210626752009\n",
      "Loss for  15927 th iteration => 0.0210608465852\n",
      "Loss for  15928 th iteration => 0.0210597282254\n",
      "Loss for  15929 th iteration => 0.0210577826668\n",
      "Loss for  15930 th iteration => 0.0210557811977\n",
      "Loss for  15931 th iteration => 0.0210542566028\n",
      "Loss for  15932 th iteration => 0.021053693034\n",
      "Loss for  15933 th iteration => 0.021051955692\n",
      "Loss for  15934 th iteration => 0.0210504645378\n",
      "Loss for  15935 th iteration => 0.0210490139348\n",
      "Loss for  15936 th iteration => 0.0210472444896\n",
      "Loss for  15937 th iteration => 0.0210460709187\n",
      "Loss for  15938 th iteration => 0.021044145793\n",
      "Loss for  15939 th iteration => 0.0210421677932\n",
      "Loss for  15940 th iteration => 0.0210406112723\n",
      "Loss for  15941 th iteration => 0.0210395968638\n",
      "Loss for  15942 th iteration => 0.0210386363146\n",
      "Loss for  15943 th iteration => 0.0210367193785\n",
      "Loss for  15944 th iteration => 0.0210353573939\n",
      "Loss for  15945 th iteration => 0.0210338387091\n",
      "Loss for  15946 th iteration => 0.0210320887726\n",
      "Loss for  15947 th iteration => 0.0210310112479\n",
      "Loss for  15948 th iteration => 0.0210285739511\n",
      "Loss for  15949 th iteration => 0.0210269987893\n",
      "Loss for  15950 th iteration => 0.0210259570873\n",
      "Loss for  15951 th iteration => 0.0210246574726\n",
      "Loss for  15952 th iteration => 0.021023332463\n",
      "Loss for  15953 th iteration => 0.0210217023773\n",
      "Loss for  15954 th iteration => 0.0210201403899\n",
      "Loss for  15955 th iteration => 0.0210187467286\n",
      "Loss for  15956 th iteration => 0.021017001134\n",
      "Loss for  15957 th iteration => 0.0210149615811\n",
      "Loss for  15958 th iteration => 0.0210133727534\n",
      "Loss for  15959 th iteration => 0.0210120525581\n",
      "Loss for  15960 th iteration => 0.0210116842205\n",
      "Loss for  15961 th iteration => 0.0210094487267\n",
      "Loss for  15962 th iteration => 0.02100815486\n",
      "Loss for  15963 th iteration => 0.0210068333237\n",
      "Loss for  15964 th iteration => 0.0210046447943\n",
      "Loss for  15965 th iteration => 0.0210042066246\n",
      "Loss for  15966 th iteration => 0.0210014072052\n",
      "Loss for  15967 th iteration => 0.0209998199052\n",
      "Loss for  15968 th iteration => 0.020998379567\n",
      "Loss for  15969 th iteration => 0.0209982259334\n",
      "Loss for  15970 th iteration => 0.0209957813383\n",
      "Loss for  15971 th iteration => 0.0209946973907\n",
      "Loss for  15972 th iteration => 0.0209931715072\n",
      "Loss for  15973 th iteration => 0.0209911882399\n",
      "Loss for  15974 th iteration => 0.0209905503483\n",
      "Loss for  15975 th iteration => 0.0209878674987\n",
      "Loss for  15976 th iteration => 0.0209862817931\n",
      "Loss for  15977 th iteration => 0.0209847494614\n",
      "Loss for  15978 th iteration => 0.0209842738152\n",
      "Loss for  15979 th iteration => 0.0209824436764\n",
      "Loss for  15980 th iteration => 0.0209810884984\n",
      "Loss for  15981 th iteration => 0.0209794996667\n",
      "Loss for  15982 th iteration => 0.020977910749\n",
      "Loss for  15983 th iteration => 0.0209765550935\n",
      "Loss for  15984 th iteration => 0.0209748262453\n",
      "Loss for  15985 th iteration => 0.0209727580246\n",
      "Loss for  15986 th iteration => 0.0209711914777\n",
      "Loss for  15987 th iteration => 0.020970277996\n",
      "Loss for  15988 th iteration => 0.0209692143982\n",
      "Loss for  15989 th iteration => 0.0209674176564\n",
      "Loss for  15990 th iteration => 0.0209659587266\n",
      "Loss for  15991 th iteration => 0.0209645541678\n",
      "Loss for  15992 th iteration => 0.0209627131738\n",
      "Loss for  15993 th iteration => 0.0209617270472\n",
      "Loss for  15994 th iteration => 0.0209592533554\n",
      "Loss for  15995 th iteration => 0.0209576729616\n",
      "Loss for  15996 th iteration => 0.0209563390195\n",
      "Loss for  15997 th iteration => 0.0209560089794\n",
      "Loss for  15998 th iteration => 0.0209537471105\n",
      "Loss for  15999 th iteration => 0.0209524995534\n",
      "Loss for  16000 th iteration => 0.0209511437427\n",
      "Loss for  16001 th iteration => 0.0209490092098\n",
      "Loss for  16002 th iteration => 0.0209485291891\n",
      "Loss for  16003 th iteration => 0.0209457695911\n",
      "Loss for  16004 th iteration => 0.0209441906752\n",
      "Loss for  16005 th iteration => 0.0209427288718\n",
      "Loss for  16006 th iteration => 0.0209426274564\n",
      "Loss for  16007 th iteration => 0.0209401427257\n",
      "Loss for  16008 th iteration => 0.0209391185443\n",
      "Loss for  16009 th iteration => 0.0209375450858\n",
      "Loss for  16010 th iteration => 0.0209356288219\n",
      "Loss for  16011 th iteration => 0.0209349362242\n",
      "Loss for  16012 th iteration => 0.0209323002069\n",
      "Loss for  16013 th iteration => 0.0209307228405\n",
      "Loss for  16014 th iteration => 0.0209291833321\n",
      "Loss for  16015 th iteration => 0.0209282703018\n",
      "Loss for  16016 th iteration => 0.0209271924742\n",
      "Loss for  16017 th iteration => 0.0209254202621\n",
      "Loss for  16018 th iteration => 0.020923951225\n",
      "Loss for  16019 th iteration => 0.0209225670962\n",
      "Loss for  16020 th iteration => 0.0209207199671\n",
      "Loss for  16021 th iteration => 0.0209197629748\n",
      "Loss for  16022 th iteration => 0.0209172726512\n",
      "Loss for  16023 th iteration => 0.0209157061159\n",
      "Loss for  16024 th iteration => 0.0209147541969\n",
      "Loss for  16025 th iteration => 0.0209137863962\n",
      "Loss for  16026 th iteration => 0.0209119084832\n",
      "Loss for  16027 th iteration => 0.0209105473928\n",
      "Loss for  16028 th iteration => 0.0209090596438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  16029 th iteration => 0.0209073184223\n",
      "Loss for  16030 th iteration => 0.0209062403771\n",
      "Loss for  16031 th iteration => 0.0209038373521\n",
      "Loss for  16032 th iteration => 0.020902265103\n",
      "Loss for  16033 th iteration => 0.020900870029\n",
      "Loss for  16034 th iteration => 0.0209006645205\n",
      "Loss for  16035 th iteration => 0.0208982910928\n",
      "Loss for  16036 th iteration => 0.0208971731718\n",
      "Loss for  16037 th iteration => 0.0208957007847\n",
      "Loss for  16038 th iteration => 0.0208937007581\n",
      "Loss for  16039 th iteration => 0.0208930993752\n",
      "Loss for  16040 th iteration => 0.0208904225973\n",
      "Loss for  16041 th iteration => 0.02088885181\n",
      "Loss for  16042 th iteration => 0.0208873436157\n",
      "Loss for  16043 th iteration => 0.0208868548006\n",
      "Loss for  16044 th iteration => 0.0208850607162\n",
      "Loss for  16045 th iteration => 0.0208836998086\n",
      "Loss for  16046 th iteration => 0.0208821426754\n",
      "Loss for  16047 th iteration => 0.0208805522095\n",
      "Loss for  16048 th iteration => 0.0208792240652\n",
      "Loss for  16049 th iteration => 0.0208774946787\n",
      "Loss for  16050 th iteration => 0.0208754521815\n",
      "Loss for  16051 th iteration => 0.020873910356\n",
      "Loss for  16052 th iteration => 0.0208729636383\n",
      "Loss for  16053 th iteration => 0.0208719744864\n",
      "Loss for  16054 th iteration => 0.0208701281289\n",
      "Loss for  16055 th iteration => 0.0208687498169\n",
      "Loss for  16056 th iteration => 0.0208672895165\n",
      "Loss for  16057 th iteration => 0.0208655350538\n",
      "Loss for  16058 th iteration => 0.0208644916952\n",
      "Loss for  16059 th iteration => 0.0208620709524\n",
      "Loss for  16060 th iteration => 0.0208605052145\n",
      "Loss for  16061 th iteration => 0.0208591522562\n",
      "Loss for  16062 th iteration => 0.0208588872184\n",
      "Loss for  16063 th iteration => 0.0208565810685\n",
      "Loss for  16064 th iteration => 0.0208554124549\n",
      "Loss for  16065 th iteration => 0.0208539986177\n",
      "Loss for  16066 th iteration => 0.020851956406\n",
      "Loss for  16067 th iteration => 0.0208514051726\n",
      "Loss for  16068 th iteration => 0.0208487099505\n",
      "Loss for  16069 th iteration => 0.020847145605\n",
      "Loss for  16070 th iteration => 0.0208456576134\n",
      "Loss for  16071 th iteration => 0.020845158001\n",
      "Loss for  16072 th iteration => 0.0208433773819\n",
      "Loss for  16073 th iteration => 0.0208420161345\n",
      "Loss for  16074 th iteration => 0.0208404704022\n",
      "Loss for  16075 th iteration => 0.020838881579\n",
      "Loss for  16076 th iteration => 0.0208375628496\n",
      "Loss for  16077 th iteration => 0.0208358402348\n",
      "Loss for  16078 th iteration => 0.0208337992573\n",
      "Loss for  16079 th iteration => 0.020832270603\n",
      "Loss for  16080 th iteration => 0.0208313215121\n",
      "Loss for  16081 th iteration => 0.0208303427535\n",
      "Loss for  16082 th iteration => 0.0208284964554\n",
      "Loss for  16083 th iteration => 0.0208271316098\n",
      "Loss for  16084 th iteration => 0.0208256683161\n",
      "Loss for  16085 th iteration => 0.0208239302726\n",
      "Loss for  16086 th iteration => 0.0208228833288\n",
      "Loss for  16087 th iteration => 0.0208204709924\n",
      "Loss for  16088 th iteration => 0.0208189115645\n",
      "Loss for  16089 th iteration => 0.0208175662036\n",
      "Loss for  16090 th iteration => 0.0208173047607\n",
      "Loss for  16091 th iteration => 0.0208150035473\n",
      "Loss for  16092 th iteration => 0.0208138452485\n",
      "Loss for  16093 th iteration => 0.0208124297181\n",
      "Loss for  16094 th iteration => 0.0208104042777\n",
      "Loss for  16095 th iteration => 0.0208098449826\n",
      "Loss for  16096 th iteration => 0.0208071626592\n",
      "Loss for  16097 th iteration => 0.0208056045788\n",
      "Loss for  16098 th iteration => 0.0208041181235\n",
      "Loss for  16099 th iteration => 0.0208036310129\n",
      "Loss for  16100 th iteration => 0.0208018497956\n",
      "Loss for  16101 th iteration => 0.0208005018169\n",
      "Loss for  16102 th iteration => 0.0207989538923\n",
      "Loss for  16103 th iteration => 0.0207973798646\n",
      "Loss for  16104 th iteration => 0.0207960574147\n",
      "Loss for  16105 th iteration => 0.0207943403095\n",
      "Loss for  16106 th iteration => 0.0207923106478\n",
      "Loss for  16107 th iteration => 0.0207907877304\n",
      "Loss for  16108 th iteration => 0.0207898315557\n",
      "Loss for  16109 th iteration => 0.0207888827905\n",
      "Loss for  16110 th iteration => 0.020787017086\n",
      "Loss for  16111 th iteration => 0.0207856846399\n",
      "Loss for  16112 th iteration => 0.0207841995529\n",
      "Loss for  16113 th iteration => 0.0207824962126\n",
      "Loss for  16114 th iteration => 0.0207814219708\n",
      "Loss for  16115 th iteration => 0.0207790345491\n",
      "Loss for  16116 th iteration => 0.0207774812928\n",
      "Loss for  16117 th iteration => 0.0207761215494\n",
      "Loss for  16118 th iteration => 0.0207759033537\n",
      "Loss for  16119 th iteration => 0.0207735678994\n",
      "Loss for  16120 th iteration => 0.0207724582253\n",
      "Loss for  16121 th iteration => 0.0207710031544\n",
      "Loss for  16122 th iteration => 0.020769031495\n",
      "Loss for  16123 th iteration => 0.0207684275796\n",
      "Loss for  16124 th iteration => 0.0207657780996\n",
      "Loss for  16125 th iteration => 0.0207642261613\n",
      "Loss for  16126 th iteration => 0.0207627413655\n",
      "Loss for  16127 th iteration => 0.020761797789\n",
      "Loss for  16128 th iteration => 0.0207607963457\n",
      "Loss for  16129 th iteration => 0.0207589896891\n",
      "Loss for  16130 th iteration => 0.0207576087985\n",
      "Loss for  16131 th iteration => 0.0207561785443\n",
      "Loss for  16132 th iteration => 0.020754430854\n",
      "Loss for  16133 th iteration => 0.0207534284302\n",
      "Loss for  16134 th iteration => 0.0207509872823\n",
      "Loss for  16135 th iteration => 0.0207494652362\n",
      "Loss for  16136 th iteration => 0.0207484777533\n",
      "Loss for  16137 th iteration => 0.020747610972\n",
      "Loss for  16138 th iteration => 0.0207456741617\n",
      "Loss for  16139 th iteration => 0.020744424993\n",
      "Loss for  16140 th iteration => 0.0207428675234\n",
      "Loss for  16141 th iteration => 0.0207412486791\n",
      "Loss for  16142 th iteration => 0.0207400946355\n",
      "Loss for  16143 th iteration => 0.0207377623082\n",
      "Loss for  16144 th iteration => 0.0207362150371\n",
      "Loss for  16145 th iteration => 0.0207348087361\n",
      "Loss for  16146 th iteration => 0.0207346925498\n",
      "Loss for  16147 th iteration => 0.0207322648159\n",
      "Loss for  16148 th iteration => 0.0207312605708\n",
      "Loss for  16149 th iteration => 0.0207297098614\n",
      "Loss for  16150 th iteration => 0.0207278468897\n",
      "Loss for  16151 th iteration => 0.0207271441357\n",
      "Loss for  16152 th iteration => 0.0207245567359\n",
      "Loss for  16153 th iteration => 0.0207230107754\n",
      "Loss for  16154 th iteration => 0.0207215258924\n",
      "Loss for  16155 th iteration => 0.0207205515945\n",
      "Loss for  16156 th iteration => 0.0207196315363\n",
      "Loss for  16157 th iteration => 0.0207177543399\n",
      "Loss for  16158 th iteration => 0.0207164560883\n",
      "Loss for  16159 th iteration => 0.0207149540572\n",
      "Loss for  16160 th iteration => 0.0207132901864\n",
      "Loss for  16161 th iteration => 0.02071220811\n",
      "Loss for  16162 th iteration => 0.0207098227928\n",
      "Loss for  16163 th iteration => 0.0207083001095\n",
      "Loss for  16164 th iteration => 0.0207072829087\n",
      "Loss for  16165 th iteration => 0.0207064961157\n",
      "Loss for  16166 th iteration => 0.0207044901398\n",
      "Loss for  16167 th iteration => 0.0207033222173\n",
      "Loss for  16168 th iteration => 0.0207016943409\n",
      "Loss for  16169 th iteration => 0.0207001579274\n",
      "Loss for  16170 th iteration => 0.0206989252424\n",
      "Loss for  16171 th iteration => 0.020696648622\n",
      "Loss for  16172 th iteration => 0.020695107296\n",
      "Loss for  16173 th iteration => 0.0206936632789\n",
      "Loss for  16174 th iteration => 0.020692690357\n",
      "Loss for  16175 th iteration => 0.0206917321957\n",
      "Loss for  16176 th iteration => 0.0206898994526\n",
      "Loss for  16177 th iteration => 0.0206885671797\n",
      "Loss for  16178 th iteration => 0.0206871055387\n",
      "Loss for  16179 th iteration => 0.0206854115924\n",
      "Loss for  16180 th iteration => 0.0206843900993\n",
      "Loss for  16181 th iteration => 0.0206819561193\n",
      "Loss for  16182 th iteration => 0.0206804696642\n",
      "Loss for  16183 th iteration => 0.0206794503969\n",
      "Loss for  16184 th iteration => 0.0206786393519\n",
      "Loss for  16185 th iteration => 0.0206766641023\n",
      "Loss for  16186 th iteration => 0.0206754755724\n",
      "Loss for  16187 th iteration => 0.020673874795\n",
      "Loss for  16188 th iteration => 0.0206723212945\n",
      "Loss for  16189 th iteration => 0.0206711326714\n",
      "Loss for  16190 th iteration => 0.0206688183955\n",
      "Loss for  16191 th iteration => 0.0206672941337\n",
      "Loss for  16192 th iteration => 0.0206662337665\n",
      "Loss for  16193 th iteration => 0.0206655521937\n",
      "Loss for  16194 th iteration => 0.0206634519142\n",
      "Loss for  16195 th iteration => 0.0206623899896\n",
      "Loss for  16196 th iteration => 0.0206606670475\n",
      "Loss for  16197 th iteration => 0.0206592373478\n",
      "Loss for  16198 th iteration => 0.0206579018031\n",
      "Loss for  16199 th iteration => 0.0206556944347\n",
      "Loss for  16200 th iteration => 0.0206541589594\n",
      "Loss for  16201 th iteration => 0.0206527129348\n",
      "Loss for  16202 th iteration => 0.0206516982293\n",
      "Loss for  16203 th iteration => 0.0206508427619\n",
      "Loss for  16204 th iteration => 0.0206489182146\n",
      "Loss for  16205 th iteration => 0.0206476894259\n",
      "Loss for  16206 th iteration => 0.0206461352058\n",
      "Loss for  16207 th iteration => 0.0206445454727\n",
      "Loss for  16208 th iteration => 0.0206434233922\n",
      "Loss for  16209 th iteration => 0.0206410579007\n",
      "Loss for  16210 th iteration => 0.0206395693636\n",
      "Loss for  16211 th iteration => 0.0206385099692\n",
      "Loss for  16212 th iteration => 0.0206377982181\n",
      "Loss for  16213 th iteration => 0.0206357345238\n",
      "Loss for  16214 th iteration => 0.0206346461382\n",
      "Loss for  16215 th iteration => 0.0206329560814\n",
      "Loss for  16216 th iteration => 0.0206315035127\n",
      "Loss for  16217 th iteration => 0.0206302175627\n",
      "Loss for  16218 th iteration => 0.02062797024\n",
      "Loss for  16219 th iteration => 0.0206264437651\n",
      "Loss for  16220 th iteration => 0.0206253743745\n",
      "Loss for  16221 th iteration => 0.0206237852924\n",
      "Loss for  16222 th iteration => 0.0206232511143\n",
      "Loss for  16223 th iteration => 0.0206212584769\n",
      "Loss for  16224 th iteration => 0.0206198678302\n",
      "Loss for  16225 th iteration => 0.0206187209872\n",
      "Loss for  16226 th iteration => 0.0206165038633\n",
      "Loss for  16227 th iteration => 0.0206157670746\n",
      "Loss for  16228 th iteration => 0.0206133746958\n",
      "Loss for  16229 th iteration => 0.0206119101993\n",
      "Loss for  16230 th iteration => 0.0206108505251\n",
      "Loss for  16231 th iteration => 0.0206101295867\n",
      "Loss for  16232 th iteration => 0.0206080817091\n",
      "Loss for  16233 th iteration => 0.0206069869909\n",
      "Loss for  16234 th iteration => 0.0206053099116\n",
      "Loss for  16235 th iteration => 0.0206038537633\n",
      "Loss for  16236 th iteration => 0.0206025889562\n",
      "Loss for  16237 th iteration => 0.0206003225229\n",
      "Loss for  16238 th iteration => 0.0205988171495\n",
      "Loss for  16239 th iteration => 0.0205977237134\n",
      "Loss for  16240 th iteration => 0.0205961866528\n",
      "Loss for  16241 th iteration => 0.0205956002081\n",
      "Loss for  16242 th iteration => 0.0205936647432\n",
      "Loss for  16243 th iteration => 0.0205922279701\n",
      "Loss for  16244 th iteration => 0.0205911322338\n",
      "Loss for  16245 th iteration => 0.020588883395\n",
      "Loss for  16246 th iteration => 0.0205881696573\n",
      "Loss for  16247 th iteration => 0.0205857664809\n",
      "Loss for  16248 th iteration => 0.0205843198686\n",
      "Loss for  16249 th iteration => 0.0205832559451\n",
      "Loss for  16250 th iteration => 0.0205825426742\n",
      "Loss for  16251 th iteration => 0.0205804938967\n",
      "Loss for  16252 th iteration => 0.0205794091449\n",
      "Loss for  16253 th iteration => 0.020577728881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  16254 th iteration => 0.0205762849113\n",
      "Loss for  16255 th iteration => 0.0205750208796\n",
      "Loss for  16256 th iteration => 0.0205727492483\n",
      "Loss for  16257 th iteration => 0.0205712595407\n",
      "Loss for  16258 th iteration => 0.0205701539279\n",
      "Loss for  16259 th iteration => 0.0205690982435\n",
      "Loss for  16260 th iteration => 0.0205677149643\n",
      "Loss for  16261 th iteration => 0.0205662835562\n",
      "Loss for  16262 th iteration => 0.0205646424787\n",
      "Loss for  16263 th iteration => 0.020563467661\n",
      "Loss for  16264 th iteration => 0.0205615986273\n",
      "Loss for  16265 th iteration => 0.0205602215777\n",
      "Loss for  16266 th iteration => 0.0205587119453\n",
      "Loss for  16267 th iteration => 0.0205568304565\n",
      "Loss for  16268 th iteration => 0.0205556978781\n",
      "Loss for  16269 th iteration => 0.0205546003737\n",
      "Loss for  16270 th iteration => 0.0205532529985\n",
      "Loss for  16271 th iteration => 0.0205517698962\n",
      "Loss for  16272 th iteration => 0.0205502049331\n",
      "Loss for  16273 th iteration => 0.0205489388432\n",
      "Loss for  16274 th iteration => 0.0205472241195\n",
      "Loss for  16275 th iteration => 0.0205456640357\n",
      "Loss for  16276 th iteration => 0.0205437978962\n",
      "Loss for  16277 th iteration => 0.0205426967152\n",
      "Loss for  16278 th iteration => 0.0205415568449\n",
      "Loss for  16279 th iteration => 0.0205403028783\n",
      "Loss for  16280 th iteration => 0.0205387302931\n",
      "Loss for  16281 th iteration => 0.0205372566057\n",
      "Loss for  16282 th iteration => 0.0205359031756\n",
      "Loss for  16283 th iteration => 0.0205342403988\n",
      "Loss for  16284 th iteration => 0.0205326410314\n",
      "Loss for  16285 th iteration => 0.0205313161612\n",
      "Loss for  16286 th iteration => 0.0205294090472\n",
      "Loss for  16287 th iteration => 0.0205281985178\n",
      "Loss for  16288 th iteration => 0.0205267345489\n",
      "Loss for  16289 th iteration => 0.0205260920972\n",
      "Loss for  16290 th iteration => 0.0205242259787\n",
      "Loss for  16291 th iteration => 0.0205227460842\n",
      "Loss for  16292 th iteration => 0.0205217069731\n",
      "Loss for  16293 th iteration => 0.0205194730759\n",
      "Loss for  16294 th iteration => 0.0205186890476\n",
      "Loss for  16295 th iteration => 0.0205164081991\n",
      "Loss for  16296 th iteration => 0.0205153044508\n",
      "Loss for  16297 th iteration => 0.0205135851878\n",
      "Loss for  16298 th iteration => 0.0205133184662\n",
      "Loss for  16299 th iteration => 0.0205110841606\n",
      "Loss for  16300 th iteration => 0.0205099699216\n",
      "Loss for  16301 th iteration => 0.0205085726168\n",
      "Loss for  16302 th iteration => 0.0205066525307\n",
      "Loss for  16303 th iteration => 0.0205056160814\n",
      "Loss for  16304 th iteration => 0.0205034403395\n",
      "Loss for  16305 th iteration => 0.0205020234218\n",
      "Loss for  16306 th iteration => 0.0205008229785\n",
      "Loss for  16307 th iteration => 0.0204993385926\n",
      "Loss for  16308 th iteration => 0.0204987476727\n",
      "Loss for  16309 th iteration => 0.0204968362213\n",
      "Loss for  16310 th iteration => 0.02049541056\n",
      "Loss for  16311 th iteration => 0.0204943234579\n",
      "Loss for  16312 th iteration => 0.0204921405661\n",
      "Loss for  16313 th iteration => 0.0204913148969\n",
      "Loss for  16314 th iteration => 0.0204890558036\n",
      "Loss for  16315 th iteration => 0.0204879627234\n",
      "Loss for  16316 th iteration => 0.0204862217796\n",
      "Loss for  16317 th iteration => 0.0204860082184\n",
      "Loss for  16318 th iteration => 0.0204837269591\n",
      "Loss for  16319 th iteration => 0.0204826685326\n",
      "Loss for  16320 th iteration => 0.0204812216646\n",
      "Loss for  16321 th iteration => 0.0204793539\n",
      "Loss for  16322 th iteration => 0.0204782748307\n",
      "Loss for  16323 th iteration => 0.0204761273266\n",
      "Loss for  16324 th iteration => 0.0204751918846\n",
      "Loss for  16325 th iteration => 0.0204735574302\n",
      "Loss for  16326 th iteration => 0.0204719526996\n",
      "Loss for  16327 th iteration => 0.0204710851737\n",
      "Loss for  16328 th iteration => 0.0204697453051\n",
      "Loss for  16329 th iteration => 0.0204680417933\n",
      "Loss for  16330 th iteration => 0.0204669543366\n",
      "Loss for  16331 th iteration => 0.0204650494147\n",
      "Loss for  16332 th iteration => 0.0204636873831\n",
      "Loss for  16333 th iteration => 0.0204621883954\n",
      "Loss for  16334 th iteration => 0.0204607279164\n",
      "Loss for  16335 th iteration => 0.0204589458037\n",
      "Loss for  16336 th iteration => 0.0204582521315\n",
      "Loss for  16337 th iteration => 0.020456792487\n",
      "Loss for  16338 th iteration => 0.0204552100092\n",
      "Loss for  16339 th iteration => 0.0204540056887\n",
      "Loss for  16340 th iteration => 0.0204521760485\n",
      "Loss for  16341 th iteration => 0.0204507975828\n",
      "Loss for  16342 th iteration => 0.0204492305205\n",
      "Loss for  16343 th iteration => 0.020447487972\n",
      "Loss for  16344 th iteration => 0.0204463191433\n",
      "Loss for  16345 th iteration => 0.0204447087768\n",
      "Loss for  16346 th iteration => 0.0204438694332\n",
      "Loss for  16347 th iteration => 0.0204425032562\n",
      "Loss for  16348 th iteration => 0.0204408342493\n",
      "Loss for  16349 th iteration => 0.0204397193994\n",
      "Loss for  16350 th iteration => 0.0204378436299\n",
      "Loss for  16351 th iteration => 0.0204364636308\n",
      "Loss for  16352 th iteration => 0.0204349789563\n",
      "Loss for  16353 th iteration => 0.0204335244727\n",
      "Loss for  16354 th iteration => 0.0204317329122\n",
      "Loss for  16355 th iteration => 0.020431070757\n",
      "Loss for  16356 th iteration => 0.0204295829167\n",
      "Loss for  16357 th iteration => 0.0204280367607\n",
      "Loss for  16358 th iteration => 0.0204268032357\n",
      "Loss for  16359 th iteration => 0.0204250100271\n",
      "Loss for  16360 th iteration => 0.0204240223563\n",
      "Loss for  16361 th iteration => 0.0204220945438\n",
      "Loss for  16362 th iteration => 0.0204207506473\n",
      "Loss for  16363 th iteration => 0.0204188759871\n",
      "Loss for  16364 th iteration => 0.0204176901981\n",
      "Loss for  16365 th iteration => 0.0204162005941\n",
      "Loss for  16366 th iteration => 0.0204156094062\n",
      "Loss for  16367 th iteration => 0.020413715005\n",
      "Loss for  16368 th iteration => 0.0204123022089\n",
      "Loss for  16369 th iteration => 0.0204112191991\n",
      "Loss for  16370 th iteration => 0.0204090630832\n",
      "Loss for  16371 th iteration => 0.0204082445971\n",
      "Loss for  16372 th iteration => 0.0204060072931\n",
      "Loss for  16373 th iteration => 0.0204049370619\n",
      "Loss for  16374 th iteration => 0.0204031773422\n",
      "Loss for  16375 th iteration => 0.0204025252262\n",
      "Loss for  16376 th iteration => 0.0204009803959\n",
      "Loss for  16377 th iteration => 0.0203995199001\n",
      "Loss for  16378 th iteration => 0.0203981894113\n",
      "Loss for  16379 th iteration => 0.0203965288569\n",
      "Loss for  16380 th iteration => 0.0203949920486\n",
      "Loss for  16381 th iteration => 0.0203936319811\n",
      "Loss for  16382 th iteration => 0.0203917670788\n",
      "Loss for  16383 th iteration => 0.0203906147742\n",
      "Loss for  16384 th iteration => 0.0203890588707\n",
      "Loss for  16385 th iteration => 0.0203885824735\n",
      "Loss for  16386 th iteration => 0.0203865802311\n",
      "Loss for  16387 th iteration => 0.0203852828087\n",
      "Loss for  16388 th iteration => 0.0203840913985\n",
      "Loss for  16389 th iteration => 0.0203820355867\n",
      "Loss for  16390 th iteration => 0.0203811386366\n",
      "Loss for  16391 th iteration => 0.0203789317998\n",
      "Loss for  16392 th iteration => 0.0203778930082\n",
      "Loss for  16393 th iteration => 0.0203760907053\n",
      "Loss for  16394 th iteration => 0.0203750557627\n",
      "Loss for  16395 th iteration => 0.0203742113906\n",
      "Loss for  16396 th iteration => 0.0203723437835\n",
      "Loss for  16397 th iteration => 0.0203711471054\n",
      "Loss for  16398 th iteration => 0.0203696289503\n",
      "Loss for  16399 th iteration => 0.0203680915476\n",
      "Loss for  16400 th iteration => 0.0203670173879\n",
      "Loss for  16401 th iteration => 0.0203647005168\n",
      "Loss for  16402 th iteration => 0.0203632755074\n",
      "Loss for  16403 th iteration => 0.0203621849255\n",
      "Loss for  16404 th iteration => 0.0203615328846\n",
      "Loss for  16405 th iteration => 0.0203594778954\n",
      "Loss for  16406 th iteration => 0.0203584686433\n",
      "Loss for  16407 th iteration => 0.0203567680026\n",
      "Loss for  16408 th iteration => 0.0203554132372\n",
      "Loss for  16409 th iteration => 0.0203541159021\n",
      "Loss for  16410 th iteration => 0.0203519579816\n",
      "Loss for  16411 th iteration => 0.0203504787171\n",
      "Loss for  16412 th iteration => 0.0203494175263\n",
      "Loss for  16413 th iteration => 0.0203478356854\n",
      "Loss for  16414 th iteration => 0.0203473752492\n",
      "Loss for  16415 th iteration => 0.0203453655411\n",
      "Loss for  16416 th iteration => 0.020344090042\n",
      "Loss for  16417 th iteration => 0.0203428852853\n",
      "Loss for  16418 th iteration => 0.0203408291913\n",
      "Loss for  16419 th iteration => 0.0203400042956\n",
      "Loss for  16420 th iteration => 0.0203377464532\n",
      "Loss for  16421 th iteration => 0.0203363114279\n",
      "Loss for  16422 th iteration => 0.0203352017039\n",
      "Loss for  16423 th iteration => 0.0203341671545\n",
      "Loss for  16424 th iteration => 0.0203328061332\n",
      "Loss for  16425 th iteration => 0.0203314128609\n",
      "Loss for  16426 th iteration => 0.0203298059864\n",
      "Loss for  16427 th iteration => 0.0203286573826\n",
      "Loss for  16428 th iteration => 0.0203268516787\n",
      "Loss for  16429 th iteration => 0.0203254725182\n",
      "Loss for  16430 th iteration => 0.0203240304954\n",
      "Loss for  16431 th iteration => 0.0203221914107\n",
      "Loss for  16432 th iteration => 0.0203210472333\n",
      "Loss for  16433 th iteration => 0.0203200101124\n",
      "Loss for  16434 th iteration => 0.0203186474816\n",
      "Loss for  16435 th iteration => 0.0203172408108\n",
      "Loss for  16436 th iteration => 0.0203156701543\n",
      "Loss for  16437 th iteration => 0.020314470927\n",
      "Loss for  16438 th iteration => 0.020312763563\n",
      "Loss for  16439 th iteration => 0.0203112588285\n",
      "Loss for  16440 th iteration => 0.0203094380304\n",
      "Loss for  16441 th iteration => 0.0203083606117\n",
      "Loss for  16442 th iteration => 0.0203067348197\n",
      "Loss for  16443 th iteration => 0.0203063707446\n",
      "Loss for  16444 th iteration => 0.0203042741809\n",
      "Loss for  16445 th iteration => 0.0203030983714\n",
      "Loss for  16446 th iteration => 0.0203018034883\n",
      "Loss for  16447 th iteration => 0.0202998537005\n",
      "Loss for  16448 th iteration => 0.0202989169606\n",
      "Loss for  16449 th iteration => 0.020296738558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  16450 th iteration => 0.0202953256415\n",
      "Loss for  16451 th iteration => 0.0202942021173\n",
      "Loss for  16452 th iteration => 0.0202931475109\n",
      "Loss for  16453 th iteration => 0.0202918390913\n",
      "Loss for  16454 th iteration => 0.0202903854611\n",
      "Loss for  16455 th iteration => 0.0202888686779\n",
      "Loss for  16456 th iteration => 0.0202876228331\n",
      "Loss for  16457 th iteration => 0.0202859469417\n",
      "Loss for  16458 th iteration => 0.0202844168513\n",
      "Loss for  16459 th iteration => 0.0202831132476\n",
      "Loss for  16460 th iteration => 0.0202816131381\n",
      "Loss for  16461 th iteration => 0.0202798204113\n",
      "Loss for  16462 th iteration => 0.0202787602559\n",
      "Loss for  16463 th iteration => 0.0202779787346\n",
      "Loss for  16464 th iteration => 0.0202760723483\n",
      "Loss for  16465 th iteration => 0.0202749438162\n",
      "Loss for  16466 th iteration => 0.0202733816304\n",
      "Loss for  16467 th iteration => 0.0202719174483\n",
      "Loss for  16468 th iteration => 0.0202707904701\n",
      "Loss for  16469 th iteration => 0.0202685433354\n",
      "Loss for  16470 th iteration => 0.0202671196036\n",
      "Loss for  16471 th iteration => 0.0202660028379\n",
      "Loss for  16472 th iteration => 0.0202649789399\n",
      "Loss for  16473 th iteration => 0.0202636100168\n",
      "Loss for  16474 th iteration => 0.0202622422682\n",
      "Loss for  16475 th iteration => 0.0202606310652\n",
      "Loss for  16476 th iteration => 0.0202595044194\n",
      "Loss for  16477 th iteration => 0.0202577059546\n",
      "Loss for  16478 th iteration => 0.0202563491088\n",
      "Loss for  16479 th iteration => 0.020254444785\n",
      "Loss for  16480 th iteration => 0.0202530610075\n",
      "Loss for  16481 th iteration => 0.0202519809674\n",
      "Loss for  16482 th iteration => 0.0202512993673\n",
      "Loss for  16483 th iteration => 0.0202493005314\n",
      "Loss for  16484 th iteration => 0.0202482707993\n",
      "Loss for  16485 th iteration => 0.0202466172901\n",
      "Loss for  16486 th iteration => 0.0202452507936\n",
      "Loss for  16487 th iteration => 0.0202440072551\n",
      "Loss for  16488 th iteration => 0.0202418396054\n",
      "Loss for  16489 th iteration => 0.0202403934712\n",
      "Loss for  16490 th iteration => 0.0202393135067\n",
      "Loss for  16491 th iteration => 0.0202377991427\n",
      "Loss for  16492 th iteration => 0.020237278613\n",
      "Loss for  16493 th iteration => 0.0202353502933\n",
      "Loss for  16494 th iteration => 0.0202340337833\n",
      "Loss for  16495 th iteration => 0.0202328915785\n",
      "Loss for  16496 th iteration => 0.0202308218995\n",
      "Loss for  16497 th iteration => 0.0202300254114\n",
      "Loss for  16498 th iteration => 0.0202277809775\n",
      "Loss for  16499 th iteration => 0.0202263701209\n",
      "Loss for  16500 th iteration => 0.0202252602721\n",
      "Loss for  16501 th iteration => 0.0202242447996\n",
      "Loss for  16502 th iteration => 0.0202228920806\n",
      "Loss for  16503 th iteration => 0.0202215185767\n",
      "Loss for  16504 th iteration => 0.0202199252683\n",
      "Loss for  16505 th iteration => 0.0202187911817\n",
      "Loss for  16506 th iteration => 0.0202170065357\n",
      "Loss for  16507 th iteration => 0.0202156353363\n",
      "Loss for  16508 th iteration => 0.0202142088978\n",
      "Loss for  16509 th iteration => 0.0202123939175\n",
      "Loss for  16510 th iteration => 0.0202112506016\n",
      "Loss for  16511 th iteration => 0.0202102374071\n",
      "Loss for  16512 th iteration => 0.0202088821461\n",
      "Loss for  16513 th iteration => 0.0202074965633\n",
      "Loss for  16514 th iteration => 0.0202059374214\n",
      "Loss for  16515 th iteration => 0.0202047551359\n",
      "Loss for  16516 th iteration => 0.020203060001\n",
      "Loss for  16517 th iteration => 0.020201566\n",
      "Loss for  16518 th iteration => 0.0201997704776\n",
      "Loss for  16519 th iteration => 0.0201987131177\n",
      "Loss for  16520 th iteration => 0.0201970683912\n",
      "Loss for  16521 th iteration => 0.0201963460216\n",
      "Loss for  16522 th iteration => 0.0201948953355\n",
      "Loss for  16523 th iteration => 0.0201933853657\n",
      "Loss for  16524 th iteration => 0.0201921753182\n",
      "Loss for  16525 th iteration => 0.0201904377965\n",
      "Loss for  16526 th iteration => 0.0201890646076\n",
      "Loss for  16527 th iteration => 0.0201876057719\n",
      "Loss for  16528 th iteration => 0.0201858002109\n",
      "Loss for  16529 th iteration => 0.0201846900983\n",
      "Loss for  16530 th iteration => 0.0201831828025\n",
      "Loss for  16531 th iteration => 0.0201826990399\n",
      "Loss for  16532 th iteration => 0.0201807453506\n",
      "Loss for  16533 th iteration => 0.0201794728868\n",
      "Loss for  16534 th iteration => 0.0201782981352\n",
      "Loss for  16535 th iteration => 0.0201762851785\n",
      "Loss for  16536 th iteration => 0.0201754185242\n",
      "Loss for  16537 th iteration => 0.0201732256621\n",
      "Loss for  16538 th iteration => 0.0201718486379\n",
      "Loss for  16539 th iteration => 0.0201707238232\n",
      "Loss for  16540 th iteration => 0.0201697130846\n",
      "Loss for  16541 th iteration => 0.0201683824762\n",
      "Loss for  16542 th iteration => 0.0201669828165\n",
      "Loss for  16543 th iteration => 0.0201654493275\n",
      "Loss for  16544 th iteration => 0.020164251966\n",
      "Loss for  16545 th iteration => 0.0201625676682\n",
      "Loss for  16546 th iteration => 0.0201610707431\n",
      "Loss for  16547 th iteration => 0.0201597791224\n",
      "Loss for  16548 th iteration => 0.0201582815701\n",
      "Loss for  16549 th iteration => 0.0201565208626\n",
      "Loss for  16550 th iteration => 0.0201554714259\n",
      "Loss for  16551 th iteration => 0.020154712369\n",
      "Loss for  16552 th iteration => 0.0201528137301\n",
      "Loss for  16553 th iteration => 0.0201517158001\n",
      "Loss for  16554 th iteration => 0.0201501532835\n",
      "Loss for  16555 th iteration => 0.0201487275279\n",
      "Loss for  16556 th iteration => 0.020147587354\n",
      "Loss for  16557 th iteration => 0.0201453699532\n",
      "Loss for  16558 th iteration => 0.0201439652542\n",
      "Loss for  16559 th iteration => 0.020142864983\n",
      "Loss for  16560 th iteration => 0.0201418483887\n",
      "Loss for  16561 th iteration => 0.0201405183901\n",
      "Loss for  16562 th iteration => 0.0201391432163\n",
      "Loss for  16563 th iteration => 0.0201375761435\n",
      "Loss for  16564 th iteration => 0.0201364368818\n",
      "Loss for  16565 th iteration => 0.0201346779266\n",
      "Loss for  16566 th iteration => 0.0201333139875\n",
      "Loss for  16567 th iteration => 0.0201318970859\n",
      "Loss for  16568 th iteration => 0.0201300967541\n",
      "Loss for  16569 th iteration => 0.0201289636826\n",
      "Loss for  16570 th iteration => 0.0201279519059\n",
      "Loss for  16571 th iteration => 0.0201266196744\n",
      "Loss for  16572 th iteration => 0.0201252323774\n",
      "Loss for  16573 th iteration => 0.0201236990236\n",
      "Loss for  16574 th iteration => 0.0201225122656\n",
      "Loss for  16575 th iteration => 0.0201208380634\n",
      "Loss for  16576 th iteration => 0.0201193507694\n",
      "Loss for  16577 th iteration => 0.0201175706221\n",
      "Loss for  16578 th iteration => 0.0201165433685\n",
      "Loss for  16579 th iteration => 0.0201148706104\n",
      "Loss for  16580 th iteration => 0.0201141903074\n",
      "Loss for  16581 th iteration => 0.0201127231878\n",
      "Loss for  16582 th iteration => 0.0201112537678\n",
      "Loss for  16583 th iteration => 0.02011002427\n",
      "Loss for  16584 th iteration => 0.0201083238878\n",
      "Loss for  16585 th iteration => 0.02010732419\n",
      "Loss for  16586 th iteration => 0.0201055214253\n",
      "Loss for  16587 th iteration => 0.0201036959514\n",
      "Loss for  16588 th iteration => 0.0201022881808\n",
      "Loss for  16589 th iteration => 0.020101296151\n",
      "Loss for  16590 th iteration => 0.0201005576387\n",
      "Loss for  16591 th iteration => 0.0200986525935\n",
      "Loss for  16592 th iteration => 0.0200975755538\n",
      "Loss for  16593 th iteration => 0.0200960063008\n",
      "Loss for  16594 th iteration => 0.0200946017406\n",
      "Loss for  16595 th iteration => 0.02009340393\n",
      "Loss for  16596 th iteration => 0.0200912750574\n",
      "Loss for  16597 th iteration => 0.020089823435\n",
      "Loss for  16598 th iteration => 0.0200884559956\n",
      "Loss for  16599 th iteration => 0.020087935044\n",
      "Loss for  16600 th iteration => 0.0200863652195\n",
      "Loss for  16601 th iteration => 0.0200850040401\n",
      "Loss for  16602 th iteration => 0.0200836737423\n",
      "Loss for  16603 th iteration => 0.0200820797339\n",
      "Loss for  16604 th iteration => 0.0200809811082\n",
      "Loss for  16605 th iteration => 0.0200792235254\n",
      "Loss for  16606 th iteration => 0.0200778829673\n",
      "Loss for  16607 th iteration => 0.0200760032412\n",
      "Loss for  16608 th iteration => 0.0200749825996\n",
      "Loss for  16609 th iteration => 0.0200733813044\n",
      "Loss for  16610 th iteration => 0.0200730193144\n",
      "Loss for  16611 th iteration => 0.0200709673582\n",
      "Loss for  16612 th iteration => 0.0200698295981\n",
      "Loss for  16613 th iteration => 0.0200685438368\n",
      "Loss for  16614 th iteration => 0.0200666553906\n",
      "Loss for  16615 th iteration => 0.0200661109747\n",
      "Loss for  16616 th iteration => 0.0200636442998\n",
      "Loss for  16617 th iteration => 0.0200621966001\n",
      "Loss for  16618 th iteration => 0.0200607800174\n",
      "Loss for  16619 th iteration => 0.0200598721156\n",
      "Loss for  16620 th iteration => 0.0200590063796\n",
      "Loss for  16621 th iteration => 0.0200572379806\n",
      "Loss for  16622 th iteration => 0.0200560383578\n",
      "Loss for  16623 th iteration => 0.020054601135\n",
      "Loss for  16624 th iteration => 0.0200530784878\n",
      "Loss for  16625 th iteration => 0.0200520071326\n",
      "Loss for  16626 th iteration => 0.0200498273813\n",
      "Loss for  16627 th iteration => 0.0200483819938\n",
      "Loss for  16628 th iteration => 0.0200470089556\n",
      "Loss for  16629 th iteration => 0.0200465071276\n",
      "Loss for  16630 th iteration => 0.0200449196276\n",
      "Loss for  16631 th iteration => 0.0200436065425\n",
      "Loss for  16632 th iteration => 0.0200422212104\n",
      "Loss for  16633 th iteration => 0.0200407119815\n",
      "Loss for  16634 th iteration => 0.0200395222021\n",
      "Loss for  16635 th iteration => 0.0200378805134\n",
      "Loss for  16636 th iteration => 0.0200364672801\n",
      "Loss for  16637 th iteration => 0.0200346164634\n",
      "Loss for  16638 th iteration => 0.0200332583818\n",
      "Loss for  16639 th iteration => 0.0200327212904\n",
      "Loss for  16640 th iteration => 0.0200311678603\n",
      "Loss for  16641 th iteration => 0.0200298070526\n",
      "Loss for  16642 th iteration => 0.0200284902991\n",
      "Loss for  16643 th iteration => 0.0200268994129\n",
      "Loss for  16644 th iteration => 0.0200258115877\n",
      "Loss for  16645 th iteration => 0.020024066378\n",
      "Loss for  16646 th iteration => 0.0200227371402\n",
      "Loss for  16647 th iteration => 0.0200208712858\n",
      "Loss for  16648 th iteration => 0.0200195303092\n",
      "Loss for  16649 th iteration => 0.0200184575418\n",
      "Loss for  16650 th iteration => 0.02001777822\n",
      "Loss for  16651 th iteration => 0.0200158338445\n",
      "Loss for  16652 th iteration => 0.0200148226209\n",
      "Loss for  16653 th iteration => 0.0200132074564\n",
      "Loss for  16654 th iteration => 0.0200118750876\n",
      "Loss for  16655 th iteration => 0.0200106610879\n",
      "Loss for  16656 th iteration => 0.0200085525531\n",
      "Loss for  16657 th iteration => 0.0200071361499\n",
      "Loss for  16658 th iteration => 0.020006080333\n",
      "Loss for  16659 th iteration => 0.0200045700173\n",
      "Loss for  16660 th iteration => 0.0200040982113\n",
      "Loss for  16661 th iteration => 0.0200021683442\n",
      "Loss for  16662 th iteration => 0.0200009347325\n",
      "Loss for  16663 th iteration => 0.0199997572673\n",
      "Loss for  16664 th iteration => 0.0199978005893\n",
      "Loss for  16665 th iteration => 0.0199969610773\n",
      "Loss for  16666 th iteration => 0.0199948127947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  16667 th iteration => 0.0199934171976\n",
      "Loss for  16668 th iteration => 0.0199923370649\n",
      "Loss for  16669 th iteration => 0.0199913034677\n",
      "Loss for  16670 th iteration => 0.0199900462217\n",
      "Loss for  16671 th iteration => 0.0199886367003\n",
      "Loss for  16672 th iteration => 0.0199871482056\n",
      "Loss for  16673 th iteration => 0.0199859687897\n",
      "Loss for  16674 th iteration => 0.0199842843727\n",
      "Loss for  16675 th iteration => 0.019982907151\n",
      "Loss for  16676 th iteration => 0.0199815180282\n",
      "Loss for  16677 th iteration => 0.019979744931\n",
      "Loss for  16678 th iteration => 0.0199786349233\n",
      "Loss for  16679 th iteration => 0.0199771663657\n",
      "Loss for  16680 th iteration => 0.0199766672843\n",
      "Loss for  16681 th iteration => 0.0199747697818\n",
      "Loss for  16682 th iteration => 0.0199735139142\n",
      "Loss for  16683 th iteration => 0.0199723638571\n",
      "Loss for  16684 th iteration => 0.0199704075268\n",
      "Loss for  16685 th iteration => 0.0199695374448\n",
      "Loss for  16686 th iteration => 0.0199674171759\n",
      "Loss for  16687 th iteration => 0.0199660610828\n",
      "Loss for  16688 th iteration => 0.0199649305793\n",
      "Loss for  16689 th iteration => 0.0199639646003\n",
      "Loss for  16690 th iteration => 0.0199626317106\n",
      "Loss for  16691 th iteration => 0.0199612871448\n",
      "Loss for  16692 th iteration => 0.0199597591141\n",
      "Loss for  16693 th iteration => 0.0199586091065\n",
      "Loss for  16694 th iteration => 0.0199569363367\n",
      "Loss for  16695 th iteration => 0.0199555048028\n",
      "Loss for  16696 th iteration => 0.0199541842632\n",
      "Loss for  16697 th iteration => 0.0199524259649\n",
      "Loss for  16698 th iteration => 0.019951269885\n",
      "Loss for  16699 th iteration => 0.0199498178732\n",
      "Loss for  16700 th iteration => 0.0199493280729\n",
      "Loss for  16701 th iteration => 0.0199474268368\n",
      "Loss for  16702 th iteration => 0.0199461840857\n",
      "Loss for  16703 th iteration => 0.0199450265119\n",
      "Loss for  16704 th iteration => 0.0199431015586\n",
      "Loss for  16705 th iteration => 0.0199421741953\n",
      "Loss for  16706 th iteration => 0.01994010673\n",
      "Loss for  16707 th iteration => 0.0199391057579\n",
      "Loss for  16708 th iteration => 0.0199373826499\n",
      "Loss for  16709 th iteration => 0.0199363157006\n",
      "Loss for  16710 th iteration => 0.0199356336563\n",
      "Loss for  16711 th iteration => 0.0199337116492\n",
      "Loss for  16712 th iteration => 0.019932703901\n",
      "Loss for  16713 th iteration => 0.0199311049493\n",
      "Loss for  16714 th iteration => 0.0199297820267\n",
      "Loss for  16715 th iteration => 0.0199285861189\n",
      "Loss for  16716 th iteration => 0.0199264732531\n",
      "Loss for  16717 th iteration => 0.0199250835414\n",
      "Loss for  16718 th iteration => 0.0199240146792\n",
      "Loss for  16719 th iteration => 0.0199225512852\n",
      "Loss for  16720 th iteration => 0.0199220530848\n",
      "Loss for  16721 th iteration => 0.0199201655423\n",
      "Loss for  16722 th iteration => 0.019918918772\n",
      "Loss for  16723 th iteration => 0.0199177705679\n",
      "Loss for  16724 th iteration => 0.019915817221\n",
      "Loss for  16725 th iteration => 0.0199149826772\n",
      "Loss for  16726 th iteration => 0.0199128453205\n",
      "Loss for  16727 th iteration => 0.0199114704666\n",
      "Loss for  16728 th iteration => 0.0199103887228\n",
      "Loss for  16729 th iteration => 0.0199093790117\n",
      "Loss for  16730 th iteration => 0.0199081243357\n",
      "Loss for  16731 th iteration => 0.019906732859\n",
      "Loss for  16732 th iteration => 0.0199052506567\n",
      "Loss for  16733 th iteration => 0.0199040855735\n",
      "Loss for  16734 th iteration => 0.0199024103515\n",
      "Loss for  16735 th iteration => 0.0199010418776\n",
      "Loss for  16736 th iteration => 0.0198996551402\n",
      "Loss for  16737 th iteration => 0.0198979039517\n",
      "Loss for  16738 th iteration => 0.0198968006682\n",
      "Loss for  16739 th iteration => 0.0198953334826\n",
      "Loss for  16740 th iteration => 0.0198948743128\n",
      "Loss for  16741 th iteration => 0.019892953635\n",
      "Loss for  16742 th iteration => 0.0198917487486\n",
      "Loss for  16743 th iteration => 0.0198905645988\n",
      "Loss for  16744 th iteration => 0.0198886655948\n",
      "Loss for  16745 th iteration => 0.0198877570717\n",
      "Loss for  16746 th iteration => 0.0198856711574\n",
      "Loss for  16747 th iteration => 0.019884326295\n",
      "Loss for  16748 th iteration => 0.0198832090132\n",
      "Loss for  16749 th iteration => 0.0198817824687\n",
      "Loss for  16750 th iteration => 0.0198812827512\n",
      "Loss for  16751 th iteration => 0.0198794047707\n",
      "Loss for  16752 th iteration => 0.0198781626685\n",
      "Loss for  16753 th iteration => 0.0198770179232\n",
      "Loss for  16754 th iteration => 0.0198750874864\n",
      "Loss for  16755 th iteration => 0.0198742024261\n",
      "Loss for  16756 th iteration => 0.0198720984718\n",
      "Loss for  16757 th iteration => 0.0198707657401\n",
      "Loss for  16758 th iteration => 0.0198696376742\n",
      "Loss for  16759 th iteration => 0.0198682442244\n",
      "Loss for  16760 th iteration => 0.0198677150487\n",
      "Loss for  16761 th iteration => 0.0198658688158\n",
      "Loss for  16762 th iteration => 0.0198646002298\n",
      "Loss for  16763 th iteration => 0.0198634842934\n",
      "Loss for  16764 th iteration => 0.0198615314873\n",
      "Loss for  16765 th iteration => 0.0198606625329\n",
      "Loss for  16766 th iteration => 0.0198585444942\n",
      "Loss for  16767 th iteration => 0.0198572223744\n",
      "Loss for  16768 th iteration => 0.0198560940431\n",
      "Loss for  16769 th iteration => 0.0198551726628\n",
      "Loss for  16770 th iteration => 0.0198538372786\n",
      "Loss for  16771 th iteration => 0.0198525228904\n",
      "Loss for  16772 th iteration => 0.0198509965602\n",
      "Loss for  16773 th iteration => 0.0198498725352\n",
      "Loss for  16774 th iteration => 0.01984819932\n",
      "Loss for  16775 th iteration => 0.0198467857564\n",
      "Loss for  16776 th iteration => 0.0198454664248\n",
      "Loss for  16777 th iteration => 0.0198440686631\n",
      "Loss for  16778 th iteration => 0.0198423444365\n",
      "Loss for  16779 th iteration => 0.0198412723312\n",
      "Loss for  16780 th iteration => 0.0198406693007\n",
      "Loss for  16781 th iteration => 0.0198386917412\n",
      "Loss for  16782 th iteration => 0.0198377675069\n",
      "Loss for  16783 th iteration => 0.0198361085432\n",
      "Loss for  16784 th iteration => 0.0198348734431\n",
      "Loss for  16785 th iteration => 0.0198335958518\n",
      "Loss for  16786 th iteration => 0.0198315522381\n",
      "Loss for  16787 th iteration => 0.0198306286831\n",
      "Loss for  16788 th iteration => 0.0198291591796\n",
      "Loss for  16789 th iteration => 0.0198275774129\n",
      "Loss for  16790 th iteration => 0.0198267932605\n",
      "Loss for  16791 th iteration => 0.0198255388766\n",
      "Loss for  16792 th iteration => 0.0198239592953\n",
      "Loss for  16793 th iteration => 0.0198228954569\n",
      "Loss for  16794 th iteration => 0.019821130977\n",
      "Loss for  16795 th iteration => 0.0198202514445\n",
      "Loss for  16796 th iteration => 0.0198184202298\n",
      "Loss for  16797 th iteration => 0.019816682999\n",
      "Loss for  16798 th iteration => 0.0198153132652\n",
      "Loss for  16799 th iteration => 0.0198143381283\n",
      "Loss for  16800 th iteration => 0.019813173661\n",
      "Loss for  16801 th iteration => 0.0198120920554\n",
      "Loss for  16802 th iteration => 0.0198105518732\n",
      "Loss for  16803 th iteration => 0.0198092470681\n",
      "Loss for  16804 th iteration => 0.0198079289635\n",
      "Loss for  16805 th iteration => 0.0198064355001\n",
      "Loss for  16806 th iteration => 0.0198049477076\n",
      "Loss for  16807 th iteration => 0.0198032003029\n",
      "Loss for  16808 th iteration => 0.0198018421988\n",
      "Loss for  16809 th iteration => 0.0198008718334\n",
      "Loss for  16810 th iteration => 0.0197997115675\n",
      "Loss for  16811 th iteration => 0.0197986273089\n",
      "Loss for  16812 th iteration => 0.0197970930621\n",
      "Loss for  16813 th iteration => 0.0197957865771\n",
      "Loss for  16814 th iteration => 0.0197944734365\n",
      "Loss for  16815 th iteration => 0.019792980165\n",
      "Loss for  16816 th iteration => 0.0197914915244\n",
      "Loss for  16817 th iteration => 0.0197897362293\n",
      "Loss for  16818 th iteration => 0.019788393317\n",
      "Loss for  16819 th iteration => 0.0197878633419\n",
      "Loss for  16820 th iteration => 0.0197859648193\n",
      "Loss for  16821 th iteration => 0.0197853729105\n",
      "Loss for  16822 th iteration => 0.0197836048704\n",
      "Loss for  16823 th iteration => 0.019782287766\n",
      "Loss for  16824 th iteration => 0.0197812359889\n",
      "Loss for  16825 th iteration => 0.0197792417902\n",
      "Loss for  16826 th iteration => 0.0197784632926\n",
      "Loss for  16827 th iteration => 0.0197763221972\n",
      "Loss for  16828 th iteration => 0.0197749794658\n",
      "Loss for  16829 th iteration => 0.0197739080082\n",
      "Loss for  16830 th iteration => 0.0197729216444\n",
      "Loss for  16831 th iteration => 0.0197716756529\n",
      "Loss for  16832 th iteration => 0.0197703095991\n",
      "Loss for  16833 th iteration => 0.0197688425241\n",
      "Loss for  16834 th iteration => 0.0197676964387\n",
      "Loss for  16835 th iteration => 0.0197660414503\n",
      "Loss for  16836 th iteration => 0.0197646852808\n",
      "Loss for  16837 th iteration => 0.0197633160687\n",
      "Loss for  16838 th iteration => 0.0197615877812\n",
      "Loss for  16839 th iteration => 0.0197604990135\n",
      "Loss for  16840 th iteration => 0.0197590492169\n",
      "Loss for  16841 th iteration => 0.019758618678\n",
      "Loss for  16842 th iteration => 0.0197566966596\n",
      "Loss for  16843 th iteration => 0.0197555398214\n",
      "Loss for  16844 th iteration => 0.0197543351751\n",
      "Loss for  16845 th iteration => 0.0197524950723\n",
      "Loss for  16846 th iteration => 0.0197515621533\n",
      "Loss for  16847 th iteration => 0.0197495206702\n",
      "Loss for  16848 th iteration => 0.0197486157936\n",
      "Loss for  16849 th iteration => 0.0197471318906\n",
      "Loss for  16850 th iteration => 0.0197456391873\n",
      "Loss for  16851 th iteration => 0.0197448054311\n",
      "Loss for  16852 th iteration => 0.0197435905775\n",
      "Loss for  16853 th iteration => 0.0197419794689\n",
      "Loss for  16854 th iteration => 0.0197409839927\n",
      "Loss for  16855 th iteration => 0.01973917588\n",
      "Loss for  16856 th iteration => 0.0197379766864\n",
      "Loss for  16857 th iteration => 0.0197364436499\n",
      "Loss for  16858 th iteration => 0.0197348263388\n",
      "Loss for  16859 th iteration => 0.0197337449296\n",
      "Loss for  16860 th iteration => 0.0197322625125\n",
      "Loss for  16861 th iteration => 0.0197314361271\n",
      "Loss for  16862 th iteration => 0.0197302142892\n",
      "Loss for  16863 th iteration => 0.0197286141809\n",
      "Loss for  16864 th iteration => 0.0197276110332\n",
      "Loss for  16865 th iteration => 0.0197258118848\n",
      "Loss for  16866 th iteration => 0.0197246034325\n",
      "Loss for  16867 th iteration => 0.0197230791881\n",
      "Loss for  16868 th iteration => 0.0197214577817\n",
      "Loss for  16869 th iteration => 0.0197203770103\n",
      "Loss for  16870 th iteration => 0.0197189014701\n",
      "Loss for  16871 th iteration => 0.0197180855846\n",
      "Loss for  16872 th iteration => 0.019716854817\n",
      "Loss for  16873 th iteration => 0.0197152675962\n",
      "Loss for  16874 th iteration => 0.0197142548998\n",
      "Loss for  16875 th iteration => 0.0197124657128\n",
      "Loss for  16876 th iteration => 0.0197112480132\n",
      "Loss for  16877 th iteration => 0.0197097314842\n",
      "Loss for  16878 th iteration => 0.0197081063024\n",
      "Loss for  16879 th iteration => 0.0197070278873\n",
      "Loss for  16880 th iteration => 0.0197055562965\n",
      "Loss for  16881 th iteration => 0.0197047535619\n",
      "Loss for  16882 th iteration => 0.0197035122397\n",
      "Loss for  16883 th iteration => 0.0197019394803\n",
      "Loss for  16884 th iteration => 0.019700915669\n",
      "Loss for  16885 th iteration => 0.01969913726\n",
      "Loss for  16886 th iteration => 0.0196979103647\n",
      "Loss for  16887 th iteration => 0.0196964039249\n",
      "Loss for  16888 th iteration => 0.0196953041584\n",
      "Loss for  16889 th iteration => 0.0196937341908\n",
      "Loss for  16890 th iteration => 0.0196921396406\n",
      "Loss for  16891 th iteration => 0.0196910579578\n",
      "Loss for  16892 th iteration => 0.0196904390149\n",
      "Loss for  16893 th iteration => 0.0196885126338\n",
      "Loss for  16894 th iteration => 0.0196875846659\n",
      "Loss for  16895 th iteration => 0.0196859656894\n",
      "Loss for  16896 th iteration => 0.0196843356101\n",
      "Loss for  16897 th iteration => 0.0196834939775\n",
      "Loss for  16898 th iteration => 0.0196814851828\n",
      "Loss for  16899 th iteration => 0.0196804211013\n",
      "Loss for  16900 th iteration => 0.0196788288238\n",
      "Loss for  16901 th iteration => 0.0196777470158\n",
      "Loss for  16902 th iteration => 0.0196771471872\n",
      "Loss for  16903 th iteration => 0.0196752049757\n",
      "Loss for  16904 th iteration => 0.0196742966598\n",
      "Loss for  16905 th iteration => 0.0196726604143\n",
      "Loss for  16906 th iteration => 0.0196714534901\n",
      "Loss for  16907 th iteration => 0.0196702086876\n",
      "Loss for  16908 th iteration => 0.0196681307217\n",
      "Loss for  16909 th iteration => 0.0196668275258\n",
      "Loss for  16910 th iteration => 0.0196657329287\n",
      "Loss for  16911 th iteration => 0.0196643566419\n",
      "Loss for  16912 th iteration => 0.0196638844773\n",
      "Loss for  16913 th iteration => 0.0196620221619\n",
      "Loss for  16914 th iteration => 0.0196608389991\n",
      "Loss for  16915 th iteration => 0.0196596789523\n",
      "Loss for  16916 th iteration => 0.019657820074\n",
      "Loss for  16917 th iteration => 0.0196569130267\n",
      "Loss for  16918 th iteration => 0.0196548612144\n",
      "Loss for  16919 th iteration => 0.019654527794\n",
      "Loss for  16920 th iteration => 0.0196525356134\n",
      "Loss for  16921 th iteration => 0.019650951072\n",
      "Loss for  16922 th iteration => 0.0196498432216\n",
      "Loss for  16923 th iteration => 0.0196492695031\n",
      "Loss for  16924 th iteration => 0.0196473079797\n",
      "Loss for  16925 th iteration => 0.0196464271656\n",
      "Loss for  16926 th iteration => 0.0196447743671\n",
      "Loss for  16927 th iteration => 0.0196431935044\n",
      "Loss for  16928 th iteration => 0.0196423121709\n",
      "Loss for  16929 th iteration => 0.0196403386427\n",
      "Loss for  16930 th iteration => 0.0196392787938\n",
      "Loss for  16931 th iteration => 0.0196376929921\n",
      "Loss for  16932 th iteration => 0.0196365845429\n",
      "Loss for  16933 th iteration => 0.0196360335309\n",
      "Loss for  16934 th iteration => 0.0196340526095\n",
      "Loss for  16935 th iteration => 0.019633194898\n",
      "Loss for  16936 th iteration => 0.0196315181719\n",
      "Loss for  16937 th iteration => 0.0196303635612\n",
      "Loss for  16938 th iteration => 0.019629077695\n",
      "Loss for  16939 th iteration => 0.0196270430981\n",
      "Loss for  16940 th iteration => 0.0196266966709\n",
      "Loss for  16941 th iteration => 0.0196247236681\n",
      "Loss for  16942 th iteration => 0.0196231510224\n",
      "Loss for  16943 th iteration => 0.0196220160813\n",
      "Loss for  16944 th iteration => 0.0196214664985\n",
      "Loss for  16945 th iteration => 0.0196194875734\n",
      "Loss for  16946 th iteration => 0.0196186324003\n",
      "Loss for  16947 th iteration => 0.0196169692674\n",
      "Loss for  16948 th iteration => 0.0196154109325\n",
      "Loss for  16949 th iteration => 0.0196145133699\n",
      "Loss for  16950 th iteration => 0.0196125666832\n",
      "Loss for  16951 th iteration => 0.0196115053278\n",
      "Loss for  16952 th iteration => 0.0196099282595\n",
      "Loss for  16953 th iteration => 0.0196087921351\n",
      "Loss for  16954 th iteration => 0.0196082689305\n",
      "Loss for  16955 th iteration => 0.0196062669643\n",
      "Loss for  16956 th iteration => 0.0196054384331\n",
      "Loss for  16957 th iteration => 0.0196037457684\n",
      "Loss for  16958 th iteration => 0.0196022213246\n",
      "Loss for  16959 th iteration => 0.0196012931088\n",
      "Loss for  16960 th iteration => 0.0195993576952\n",
      "Loss for  16961 th iteration => 0.0195983041449\n",
      "Loss for  16962 th iteration => 0.0195967226894\n",
      "Loss for  16963 th iteration => 0.0195955857058\n",
      "Loss for  16964 th iteration => 0.0195950880507\n",
      "Loss for  16965 th iteration => 0.0195930638612\n",
      "Loss for  16966 th iteration => 0.0195922611559\n",
      "Loss for  16967 th iteration => 0.0195905398119\n",
      "Loss for  16968 th iteration => 0.019589048434\n",
      "Loss for  16969 th iteration => 0.0195880903778\n",
      "Loss for  16970 th iteration => 0.0195861841082\n",
      "Loss for  16971 th iteration => 0.0195860343261\n",
      "Loss for  16972 th iteration => 0.0195835915634\n",
      "Loss for  16973 th iteration => 0.0195823525938\n",
      "Loss for  16974 th iteration => 0.0195810651068\n",
      "Loss for  16975 th iteration => 0.0195805050457\n",
      "Loss for  16976 th iteration => 0.0195787458785\n",
      "Loss for  16977 th iteration => 0.0195774896692\n",
      "Loss for  16978 th iteration => 0.0195764181073\n",
      "Loss for  16979 th iteration => 0.0195745672269\n",
      "Loss for  16980 th iteration => 0.0195735931148\n",
      "Loss for  16981 th iteration => 0.0195717006473\n",
      "Loss for  16982 th iteration => 0.0195715286081\n",
      "Loss for  16983 th iteration => 0.0195691261692\n",
      "Loss for  16984 th iteration => 0.0195678848184\n",
      "Loss for  16985 th iteration => 0.0195665700879\n",
      "Loss for  16986 th iteration => 0.0195660263482\n",
      "Loss for  16987 th iteration => 0.0195642539354\n",
      "Loss for  16988 th iteration => 0.019563015598\n",
      "Loss for  16989 th iteration => 0.0195619292629\n",
      "Loss for  16990 th iteration => 0.0195601083891\n",
      "Loss for  16991 th iteration => 0.0195591152914\n",
      "Loss for  16992 th iteration => 0.0195572480207\n",
      "Loss for  16993 th iteration => 0.0195561978067\n",
      "Loss for  16994 th iteration => 0.019554623961\n",
      "Loss for  16995 th iteration => 0.0195534148895\n",
      "Loss for  16996 th iteration => 0.0195526241177\n",
      "Loss for  16997 th iteration => 0.0195511491649\n",
      "Loss for  16998 th iteration => 0.0195500498672\n",
      "Loss for  16999 th iteration => 0.0195484096041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  17000 th iteration => 0.0195470979017\n",
      "Loss for  17001 th iteration => 0.0195457324523\n",
      "Loss for  17002 th iteration => 0.0195444601797\n",
      "Loss for  17003 th iteration => 0.0195435257448\n",
      "Loss for  17004 th iteration => 0.0195415134117\n",
      "Loss for  17005 th iteration => 0.0195402900383\n",
      "Loss for  17006 th iteration => 0.0195390337643\n",
      "Loss for  17007 th iteration => 0.0195383913914\n",
      "Loss for  17008 th iteration => 0.0195367217325\n",
      "Loss for  17009 th iteration => 0.0195353918379\n",
      "Loss for  17010 th iteration => 0.0195344012628\n",
      "Loss for  17011 th iteration => 0.0195324923297\n",
      "Loss for  17012 th iteration => 0.0195316090757\n",
      "Loss for  17013 th iteration => 0.0195296657772\n",
      "Loss for  17014 th iteration => 0.0195286395114\n",
      "Loss for  17015 th iteration => 0.019527048896\n",
      "Loss for  17016 th iteration => 0.019525924617\n",
      "Loss for  17017 th iteration => 0.0195253834448\n",
      "Loss for  17018 th iteration => 0.0195234192207\n",
      "Loss for  17019 th iteration => 0.0195225776358\n",
      "Loss for  17020 th iteration => 0.0195209210147\n",
      "Loss for  17021 th iteration => 0.0195194228519\n",
      "Loss for  17022 th iteration => 0.019518487008\n",
      "Loss for  17023 th iteration => 0.0195165781923\n",
      "Loss for  17024 th iteration => 0.0195159719692\n",
      "Loss for  17025 th iteration => 0.0195139778532\n",
      "Loss for  17026 th iteration => 0.019512842124\n",
      "Loss for  17027 th iteration => 0.019511826159\n",
      "Loss for  17028 th iteration => 0.0195106822244\n",
      "Loss for  17029 th iteration => 0.0195092788702\n",
      "Loss for  17030 th iteration => 0.0195079260741\n",
      "Loss for  17031 th iteration => 0.0195067305017\n",
      "Loss for  17032 th iteration => 0.0195052652728\n",
      "Loss for  17033 th iteration => 0.0195037547063\n",
      "Loss for  17034 th iteration => 0.0195021649914\n",
      "Loss for  17035 th iteration => 0.019501557292\n",
      "Loss for  17036 th iteration => 0.019499641102\n",
      "Loss for  17037 th iteration => 0.0194988465292\n",
      "Loss for  17038 th iteration => 0.0194971818996\n",
      "Loss for  17039 th iteration => 0.0194964452357\n",
      "Loss for  17040 th iteration => 0.0194948768621\n",
      "Loss for  17041 th iteration => 0.0194934655845\n",
      "Loss for  17042 th iteration => 0.0194922162256\n",
      "Loss for  17043 th iteration => 0.0194905542173\n",
      "Loss for  17044 th iteration => 0.0194898086252\n",
      "Loss for  17045 th iteration => 0.0194881689157\n",
      "Loss for  17046 th iteration => 0.0194865405876\n",
      "Loss for  17047 th iteration => 0.0194854097083\n",
      "Loss for  17048 th iteration => 0.0194843911126\n",
      "Loss for  17049 th iteration => 0.0194828560409\n",
      "Loss for  17050 th iteration => 0.019482116835\n",
      "Loss for  17051 th iteration => 0.0194803587077\n",
      "Loss for  17052 th iteration => 0.0194793284409\n",
      "Loss for  17053 th iteration => 0.0194779452133\n",
      "Loss for  17054 th iteration => 0.019476131879\n",
      "Loss for  17055 th iteration => 0.0194751037336\n",
      "Loss for  17056 th iteration => 0.0194737742511\n",
      "Loss for  17057 th iteration => 0.019472186758\n",
      "Loss for  17058 th iteration => 0.0194714564468\n",
      "Loss for  17059 th iteration => 0.0194698156521\n",
      "Loss for  17060 th iteration => 0.0194690214869\n",
      "Loss for  17061 th iteration => 0.019467515218\n",
      "Loss for  17062 th iteration => 0.0194660482055\n",
      "Loss for  17063 th iteration => 0.0194652065232\n",
      "Loss for  17064 th iteration => 0.01946317598\n",
      "Loss for  17065 th iteration => 0.0194624727075\n",
      "Loss for  17066 th iteration => 0.0194604182751\n",
      "Loss for  17067 th iteration => 0.0194591452782\n",
      "Loss for  17068 th iteration => 0.0194580198277\n",
      "Loss for  17069 th iteration => 0.0194570722563\n",
      "Loss for  17070 th iteration => 0.0194558496247\n",
      "Loss for  17071 th iteration => 0.0194545383694\n",
      "Loss for  17072 th iteration => 0.0194531090978\n",
      "Loss for  17073 th iteration => 0.0194520034111\n",
      "Loss for  17074 th iteration => 0.0194504449025\n",
      "Loss for  17075 th iteration => 0.0194490703737\n",
      "Loss for  17076 th iteration => 0.019447799207\n",
      "Loss for  17077 th iteration => 0.0194461535093\n",
      "Loss for  17078 th iteration => 0.0194450546898\n",
      "Loss for  17079 th iteration => 0.0194435935563\n",
      "Loss for  17080 th iteration => 0.0194432368742\n",
      "Loss for  17081 th iteration => 0.0194413038755\n",
      "Loss for  17082 th iteration => 0.0194402644516\n",
      "Loss for  17083 th iteration => 0.019439005854\n",
      "Loss for  17084 th iteration => 0.0194373543137\n",
      "Loss for  17085 th iteration => 0.0194363154579\n",
      "Loss for  17086 th iteration => 0.01943446485\n",
      "Loss for  17087 th iteration => 0.01943395373\n",
      "Loss for  17088 th iteration => 0.0194321560677\n",
      "Loss for  17089 th iteration => 0.0194306135818\n",
      "Loss for  17090 th iteration => 0.0194293767073\n",
      "Loss for  17091 th iteration => 0.0194286081196\n",
      "Loss for  17092 th iteration => 0.0194271247783\n",
      "Loss for  17093 th iteration => 0.0194260648262\n",
      "Loss for  17094 th iteration => 0.0194244372533\n",
      "Loss for  17095 th iteration => 0.0194231613915\n",
      "Loss for  17096 th iteration => 0.0194217935925\n",
      "Loss for  17097 th iteration => 0.019420541415\n",
      "Loss for  17098 th iteration => 0.0194196096881\n",
      "Loss for  17099 th iteration => 0.0194176595204\n",
      "Loss for  17100 th iteration => 0.0194164546865\n",
      "Loss for  17101 th iteration => 0.0194151408513\n",
      "Loss for  17102 th iteration => 0.0194141917317\n",
      "Loss for  17103 th iteration => 0.0194130899941\n",
      "Loss for  17104 th iteration => 0.0194114852734\n",
      "Loss for  17105 th iteration => 0.0194102088269\n",
      "Loss for  17106 th iteration => 0.0194088447708\n",
      "Loss for  17107 th iteration => 0.0194075742546\n",
      "Loss for  17108 th iteration => 0.0194066586122\n",
      "Loss for  17109 th iteration => 0.0194046931523\n",
      "Loss for  17110 th iteration => 0.0194035276336\n",
      "Loss for  17111 th iteration => 0.0194021568405\n",
      "Loss for  17112 th iteration => 0.0194012725938\n",
      "Loss for  17113 th iteration => 0.0194001332882\n",
      "Loss for  17114 th iteration => 0.0193985631617\n",
      "Loss for  17115 th iteration => 0.0193975963768\n",
      "Loss for  17116 th iteration => 0.0193959371291\n",
      "Loss for  17117 th iteration => 0.0193946423997\n",
      "Loss for  17118 th iteration => 0.019393338316\n",
      "Loss for  17119 th iteration => 0.0193920412507\n",
      "Loss for  17120 th iteration => 0.0193904186096\n",
      "Loss for  17121 th iteration => 0.0193892900106\n",
      "Loss for  17122 th iteration => 0.0193883602679\n",
      "Loss for  17123 th iteration => 0.0193871084165\n",
      "Loss for  17124 th iteration => 0.0193858429855\n",
      "Loss for  17125 th iteration => 0.0193843884789\n",
      "Loss for  17126 th iteration => 0.0193833246421\n",
      "Loss for  17127 th iteration => 0.019381756135\n",
      "Loss for  17128 th iteration => 0.0193804157551\n",
      "Loss for  17129 th iteration => 0.0193787561785\n",
      "Loss for  17130 th iteration => 0.0193774863663\n",
      "Loss for  17131 th iteration => 0.0193763817415\n",
      "Loss for  17132 th iteration => 0.0193754286496\n",
      "Loss for  17133 th iteration => 0.0193742308787\n",
      "Loss for  17134 th iteration => 0.0193729148904\n",
      "Loss for  17135 th iteration => 0.0193715137032\n",
      "Loss for  17136 th iteration => 0.0193704000716\n",
      "Loss for  17137 th iteration => 0.0193688635177\n",
      "Loss for  17138 th iteration => 0.0193675009241\n",
      "Loss for  17139 th iteration => 0.0193662352953\n",
      "Loss for  17140 th iteration => 0.0193645976211\n",
      "Loss for  17141 th iteration => 0.0193635298172\n",
      "Loss for  17142 th iteration => 0.0193620476555\n",
      "Loss for  17143 th iteration => 0.019361372864\n",
      "Loss for  17144 th iteration => 0.0193600023713\n",
      "Loss for  17145 th iteration => 0.0193586583044\n",
      "Loss for  17146 th iteration => 0.0193574911152\n",
      "Loss for  17147 th iteration => 0.0193559877174\n",
      "Loss for  17148 th iteration => 0.0193546159923\n",
      "Loss for  17149 th iteration => 0.0193533480671\n",
      "Loss for  17150 th iteration => 0.019352110267\n",
      "Loss for  17151 th iteration => 0.0193507083183\n",
      "Loss for  17152 th iteration => 0.019349153921\n",
      "Loss for  17153 th iteration => 0.0193480260909\n",
      "Loss for  17154 th iteration => 0.0193471045243\n",
      "Loss for  17155 th iteration => 0.0193458230117\n",
      "Loss for  17156 th iteration => 0.0193445968706\n",
      "Loss for  17157 th iteration => 0.0193431290846\n",
      "Loss for  17158 th iteration => 0.0193417490089\n",
      "Loss for  17159 th iteration => 0.0193404924305\n",
      "Loss for  17160 th iteration => 0.0193391967012\n",
      "Loss for  17161 th iteration => 0.0193378724586\n",
      "Loss for  17162 th iteration => 0.0193362731001\n",
      "Loss for  17163 th iteration => 0.0193351703914\n",
      "Loss for  17164 th iteration => 0.0193342255545\n",
      "Loss for  17165 th iteration => 0.0193330002951\n",
      "Loss for  17166 th iteration => 0.0193317214471\n",
      "Loss for  17167 th iteration => 0.0193302963126\n",
      "Loss for  17168 th iteration => 0.0193292162869\n",
      "Loss for  17169 th iteration => 0.0193276773299\n",
      "Loss for  17170 th iteration => 0.0193263261808\n",
      "Loss for  17171 th iteration => 0.0193246627526\n",
      "Loss for  17172 th iteration => 0.0193234083024\n",
      "Loss for  17173 th iteration => 0.0193223321606\n",
      "Loss for  17174 th iteration => 0.0193213604463\n",
      "Loss for  17175 th iteration => 0.0193201975011\n",
      "Loss for  17176 th iteration => 0.0193188599095\n",
      "Loss for  17177 th iteration => 0.0193174960544\n",
      "Loss for  17178 th iteration => 0.0193163583212\n",
      "Loss for  17179 th iteration => 0.0193148555813\n",
      "Loss for  17180 th iteration => 0.0193134691249\n",
      "Loss for  17181 th iteration => 0.0193122347853\n",
      "Loss for  17182 th iteration => 0.0193105870288\n",
      "Loss for  17183 th iteration => 0.0193095278453\n",
      "Loss for  17184 th iteration => 0.0193080714707\n",
      "Loss for  17185 th iteration => 0.0193077506632\n",
      "Loss for  17186 th iteration => 0.0193058086209\n",
      "Loss for  17187 th iteration => 0.0193048232772\n",
      "Loss for  17188 th iteration => 0.0193035376752\n",
      "Loss for  17189 th iteration => 0.0193019366271\n",
      "Loss for  17190 th iteration => 0.0193008924844\n",
      "Loss for  17191 th iteration => 0.0192990897838\n",
      "Loss for  17192 th iteration => 0.0192985452557\n",
      "Loss for  17193 th iteration => 0.019296811827\n",
      "Loss for  17194 th iteration => 0.0192952520199\n",
      "Loss for  17195 th iteration => 0.0192940650255\n",
      "Loss for  17196 th iteration => 0.019292897388\n",
      "Loss for  17197 th iteration => 0.019292143318\n",
      "Loss for  17198 th iteration => 0.0192906319275\n",
      "Loss for  17199 th iteration => 0.0192892288914\n",
      "Loss for  17200 th iteration => 0.0192883585278\n",
      "Loss for  17201 th iteration => 0.0192864119661\n",
      "Loss for  17202 th iteration => 0.0192856524953\n",
      "Loss for  17203 th iteration => 0.0192836825022\n",
      "Loss for  17204 th iteration => 0.019282717036\n",
      "Loss for  17205 th iteration => 0.0192811289118\n",
      "Loss for  17206 th iteration => 0.0192800951537\n",
      "Loss for  17207 th iteration => 0.019279465175\n",
      "Loss for  17208 th iteration => 0.0192776474799\n",
      "Loss for  17209 th iteration => 0.0192767329579\n",
      "Loss for  17210 th iteration => 0.0192751974698\n",
      "Loss for  17211 th iteration => 0.0192740073971\n",
      "Loss for  17212 th iteration => 0.0192728339013\n",
      "Loss for  17213 th iteration => 0.0192709238411\n",
      "Loss for  17214 th iteration => 0.019269589463\n",
      "Loss for  17215 th iteration => 0.0192683371324\n",
      "Loss for  17216 th iteration => 0.0192672870042\n",
      "Loss for  17217 th iteration => 0.0192667447181\n",
      "Loss for  17218 th iteration => 0.0192648431376\n",
      "Loss for  17219 th iteration => 0.019264014646\n",
      "Loss for  17220 th iteration => 0.0192623969315\n",
      "Loss for  17221 th iteration => 0.0192612912598\n",
      "Loss for  17222 th iteration => 0.0192600139392\n",
      "Loss for  17223 th iteration => 0.0192581992597\n",
      "Loss for  17224 th iteration => 0.0192576339213\n",
      "Loss for  17225 th iteration => 0.019255615958\n",
      "Loss for  17226 th iteration => 0.0192545649518\n",
      "Loss for  17227 th iteration => 0.0192530711927\n",
      "Loss for  17228 th iteration => 0.0192524442742\n",
      "Loss for  17229 th iteration => 0.0192510477784\n",
      "Loss for  17230 th iteration => 0.0192497605489\n",
      "Loss for  17231 th iteration => 0.019248563349\n",
      "Loss for  17232 th iteration => 0.0192471164719\n",
      "Loss for  17233 th iteration => 0.0192457264661\n",
      "Loss for  17234 th iteration => 0.0192445060587\n",
      "Loss for  17235 th iteration => 0.0192432344789\n",
      "Loss for  17236 th iteration => 0.0192418895813\n",
      "Loss for  17237 th iteration => 0.0192403371047\n",
      "Loss for  17238 th iteration => 0.0192392262473\n",
      "Loss for  17239 th iteration => 0.0192382862133\n",
      "Loss for  17240 th iteration => 0.0192370701562\n",
      "Loss for  17241 th iteration => 0.0192358054147\n",
      "Loss for  17242 th iteration => 0.0192343987197\n",
      "Loss for  17243 th iteration => 0.0192330004388\n",
      "Loss for  17244 th iteration => 0.0192317911665\n",
      "Loss for  17245 th iteration => 0.0192304526533\n",
      "Loss for  17246 th iteration => 0.0192291991277\n",
      "Loss for  17247 th iteration => 0.0192275937336\n",
      "Loss for  17248 th iteration => 0.0192265102094\n",
      "Loss for  17249 th iteration => 0.0192255437789\n",
      "Loss for  17250 th iteration => 0.0192243912447\n",
      "Loss for  17251 th iteration => 0.0192230665434\n",
      "Loss for  17252 th iteration => 0.0192217174515\n",
      "Loss for  17253 th iteration => 0.0192205882709\n",
      "Loss for  17254 th iteration => 0.0192191158687\n",
      "Loss for  17255 th iteration => 0.0192177324099\n",
      "Loss for  17256 th iteration => 0.0192161255124\n",
      "Loss for  17257 th iteration => 0.0192148664958\n",
      "Loss for  17258 th iteration => 0.0192138111961\n",
      "Loss for  17259 th iteration => 0.0192128155112\n",
      "Loss for  17260 th iteration => 0.0192117310878\n",
      "Loss for  17261 th iteration => 0.0192103418517\n",
      "Loss for  17262 th iteration => 0.019209059623\n",
      "Loss for  17263 th iteration => 0.0192078671561\n",
      "Loss for  17264 th iteration => 0.019206433765\n",
      "Loss for  17265 th iteration => 0.0192050298702\n",
      "Loss for  17266 th iteration => 0.0192038345368\n",
      "Loss for  17267 th iteration => 0.0192025765045\n",
      "Loss for  17268 th iteration => 0.0192012068113\n",
      "Loss for  17269 th iteration => 0.0191996753916\n",
      "Loss for  17270 th iteration => 0.0191985792852\n",
      "Loss for  17271 th iteration => 0.0191976292649\n",
      "Loss for  17272 th iteration => 0.0191964261824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  17273 th iteration => 0.0191951582524\n",
      "Loss for  17274 th iteration => 0.0191937694148\n",
      "Loss for  17275 th iteration => 0.0191923568364\n",
      "Loss for  17276 th iteration => 0.019191172955\n",
      "Loss for  17277 th iteration => 0.0191898397984\n",
      "Loss for  17278 th iteration => 0.0191885733392\n",
      "Loss for  17279 th iteration => 0.0191869821275\n",
      "Loss for  17280 th iteration => 0.0191859155267\n",
      "Loss for  17281 th iteration => 0.0191849361682\n",
      "Loss for  17282 th iteration => 0.0191838034024\n",
      "Loss for  17283 th iteration => 0.0191824687609\n",
      "Loss for  17284 th iteration => 0.019181141408\n",
      "Loss for  17285 th iteration => 0.0191800003228\n",
      "Loss for  17286 th iteration => 0.0191785505094\n",
      "Loss for  17287 th iteration => 0.019177162278\n",
      "Loss for  17288 th iteration => 0.0191755563139\n",
      "Loss for  17289 th iteration => 0.019174305179\n",
      "Loss for  17290 th iteration => 0.0191732685987\n",
      "Loss for  17291 th iteration => 0.0191722575512\n",
      "Loss for  17292 th iteration => 0.019171198609\n",
      "Loss for  17293 th iteration => 0.0191697937527\n",
      "Loss for  17294 th iteration => 0.0191685387869\n",
      "Loss for  17295 th iteration => 0.019167328924\n",
      "Loss for  17296 th iteration => 0.0191659210727\n",
      "Loss for  17297 th iteration => 0.0191645024577\n",
      "Loss for  17298 th iteration => 0.0191633327502\n",
      "Loss for  17299 th iteration => 0.0191620652566\n",
      "Loss for  17300 th iteration => 0.0191607043505\n",
      "Loss for  17301 th iteration => 0.0191591745134\n",
      "Loss for  17302 th iteration => 0.0191580991206\n",
      "Loss for  17303 th iteration => 0.0191571310958\n",
      "Loss for  17304 th iteration => 0.019155959861\n",
      "Loss for  17305 th iteration => 0.019154669975\n",
      "Loss for  17306 th iteration => 0.0191533091024\n",
      "Loss for  17307 th iteration => 0.0191518818126\n",
      "Loss for  17308 th iteration => 0.0191507234858\n",
      "Loss for  17309 th iteration => 0.0191493760382\n",
      "Loss for  17310 th iteration => 0.0191481256083\n",
      "Loss for  17311 th iteration => 0.0191465317943\n",
      "Loss for  17312 th iteration => 0.0191454870568\n",
      "Loss for  17313 th iteration => 0.0191444881119\n",
      "Loss for  17314 th iteration => 0.0191433910619\n",
      "Loss for  17315 th iteration => 0.0191420306141\n",
      "Loss for  17316 th iteration => 0.0191407404024\n",
      "Loss for  17317 th iteration => 0.0191395720917\n",
      "Loss for  17318 th iteration => 0.0191381529684\n",
      "Loss for  17319 th iteration => 0.0191367512258\n",
      "Loss for  17320 th iteration => 0.0191351579335\n",
      "Loss for  17321 th iteration => 0.019134331478\n",
      "Loss for  17322 th iteration => 0.0191329217149\n",
      "Loss for  17323 th iteration => 0.019131873386\n",
      "Loss for  17324 th iteration => 0.0191303758849\n",
      "Loss for  17325 th iteration => 0.0191297545459\n",
      "Loss for  17326 th iteration => 0.0191279626968\n",
      "Loss for  17327 th iteration => 0.0191270675998\n",
      "Loss for  17328 th iteration => 0.0191255744515\n",
      "Loss for  17329 th iteration => 0.019124040669\n",
      "Loss for  17330 th iteration => 0.0191232263476\n",
      "Loss for  17331 th iteration => 0.0191213686079\n",
      "Loss for  17332 th iteration => 0.0191207532559\n",
      "Loss for  17333 th iteration => 0.0191188653046\n",
      "Loss for  17334 th iteration => 0.0191178059363\n",
      "Loss for  17335 th iteration => 0.0191167442283\n",
      "Loss for  17336 th iteration => 0.0191153322101\n",
      "Loss for  17337 th iteration => 0.0191145633375\n",
      "Loss for  17338 th iteration => 0.0191129209027\n",
      "Loss for  17339 th iteration => 0.0191118840807\n",
      "Loss for  17340 th iteration => 0.0191105888936\n",
      "Loss for  17341 th iteration => 0.0191088505403\n",
      "Loss for  17342 th iteration => 0.0191078014534\n",
      "Loss for  17343 th iteration => 0.0191065693034\n",
      "Loss for  17344 th iteration => 0.0191050391765\n",
      "Loss for  17345 th iteration => 0.0191043172557\n",
      "Loss for  17346 th iteration => 0.0191027048429\n",
      "Loss for  17347 th iteration => 0.0191020119784\n",
      "Loss for  17348 th iteration => 0.0191004769955\n",
      "Loss for  17349 th iteration => 0.0190991597682\n",
      "Loss for  17350 th iteration => 0.0190982415416\n",
      "Loss for  17351 th iteration => 0.0190963836067\n",
      "Loss for  17352 th iteration => 0.019095626305\n",
      "Loss for  17353 th iteration => 0.0190937044586\n",
      "Loss for  17354 th iteration => 0.019092829522\n",
      "Loss for  17355 th iteration => 0.0190914418876\n",
      "Loss for  17356 th iteration => 0.0190900648244\n",
      "Loss for  17357 th iteration => 0.0190891402664\n",
      "Loss for  17358 th iteration => 0.0190877881308\n",
      "Loss for  17359 th iteration => 0.0190867862334\n",
      "Loss for  17360 th iteration => 0.0190855591937\n",
      "Loss for  17361 th iteration => 0.0190839669927\n",
      "Loss for  17362 th iteration => 0.0190829930801\n",
      "Loss for  17363 th iteration => 0.0190812370986\n",
      "Loss for  17364 th iteration => 0.0190802128752\n",
      "Loss for  17365 th iteration => 0.0190789618641\n",
      "Loss for  17366 th iteration => 0.0190774582153\n",
      "Loss for  17367 th iteration => 0.0190767097733\n",
      "Loss for  17368 th iteration => 0.0190751344128\n",
      "Loss for  17369 th iteration => 0.0190744064393\n",
      "Loss for  17370 th iteration => 0.0190729114341\n",
      "Loss for  17371 th iteration => 0.0190715639746\n",
      "Loss for  17372 th iteration => 0.0190706809096\n",
      "Loss for  17373 th iteration => 0.0190688036117\n",
      "Loss for  17374 th iteration => 0.0190680660463\n",
      "Loss for  17375 th iteration => 0.019066138216\n",
      "Loss for  17376 th iteration => 0.0190652773979\n",
      "Loss for  17377 th iteration => 0.0190638832093\n",
      "Loss for  17378 th iteration => 0.0190628625591\n",
      "Loss for  17379 th iteration => 0.0190613408597\n",
      "Loss for  17380 th iteration => 0.0190603755739\n",
      "Loss for  17381 th iteration => 0.0190591962456\n",
      "Loss for  17382 th iteration => 0.0190579376115\n",
      "Loss for  17383 th iteration => 0.0190566039986\n",
      "Loss for  17384 th iteration => 0.0190551660096\n",
      "Loss for  17385 th iteration => 0.019054044586\n",
      "Loss for  17386 th iteration => 0.0190527254338\n",
      "Loss for  17387 th iteration => 0.0190514590555\n",
      "Loss for  17388 th iteration => 0.0190499149268\n",
      "Loss for  17389 th iteration => 0.0190488526163\n",
      "Loss for  17390 th iteration => 0.0190478495727\n",
      "Loss for  17391 th iteration => 0.0190467597202\n",
      "Loss for  17392 th iteration => 0.019045415325\n",
      "Loss for  17393 th iteration => 0.0190441368367\n",
      "Loss for  17394 th iteration => 0.0190429800683\n",
      "Loss for  17395 th iteration => 0.0190415990636\n",
      "Loss for  17396 th iteration => 0.0190402101384\n",
      "Loss for  17397 th iteration => 0.0190386475081\n",
      "Loss for  17398 th iteration => 0.0190374076927\n",
      "Loss for  17399 th iteration => 0.0190363806366\n",
      "Loss for  17400 th iteration => 0.019035344146\n",
      "Loss for  17401 th iteration => 0.0190339523288\n",
      "Loss for  17402 th iteration => 0.0190331821207\n",
      "Loss for  17403 th iteration => 0.0190315605262\n",
      "Loss for  17404 th iteration => 0.0190305252804\n",
      "Loss for  17405 th iteration => 0.0190292125527\n",
      "Loss for  17406 th iteration => 0.0190275351023\n",
      "Loss for  17407 th iteration => 0.0190268839838\n",
      "Loss for  17408 th iteration => 0.0190249444363\n",
      "Loss for  17409 th iteration => 0.019023970485\n",
      "Loss for  17410 th iteration => 0.0190224564913\n",
      "Loss for  17411 th iteration => 0.0190214352624\n",
      "Loss for  17412 th iteration => 0.0190208051958\n",
      "Loss for  17413 th iteration => 0.0190190476968\n",
      "Loss for  17414 th iteration => 0.0190181493554\n",
      "Loss for  17415 th iteration => 0.0190166663839\n",
      "Loss for  17416 th iteration => 0.019015197879\n",
      "Loss for  17417 th iteration => 0.0190143426965\n",
      "Loss for  17418 th iteration => 0.0190124968276\n",
      "Loss for  17419 th iteration => 0.0190116676914\n",
      "Loss for  17420 th iteration => 0.0190102425537\n",
      "Loss for  17421 th iteration => 0.019008831423\n",
      "Loss for  17422 th iteration => 0.0190080070853\n",
      "Loss for  17423 th iteration => 0.019006589237\n",
      "Loss for  17424 th iteration => 0.0190056812252\n",
      "Loss for  17425 th iteration => 0.0190043771432\n",
      "Loss for  17426 th iteration => 0.0190028905618\n",
      "Loss for  17427 th iteration => 0.0190018074086\n",
      "Loss for  17428 th iteration => 0.0190001370777\n",
      "Loss for  17429 th iteration => 0.0189995144456\n",
      "Loss for  17430 th iteration => 0.0189979004875\n",
      "Loss for  17431 th iteration => 0.0189963693244\n",
      "Loss for  17432 th iteration => 0.0189952404865\n",
      "Loss for  17433 th iteration => 0.0189940141965\n",
      "Loss for  17434 th iteration => 0.0189934316715\n",
      "Loss for  17435 th iteration => 0.018991808693\n",
      "Loss for  17436 th iteration => 0.01899061334\n",
      "Loss for  17437 th iteration => 0.0189895957472\n",
      "Loss for  17438 th iteration => 0.0189878734546\n",
      "Loss for  17439 th iteration => 0.0189869861712\n",
      "Loss for  17440 th iteration => 0.0189851739016\n",
      "Loss for  17441 th iteration => 0.0189843226765\n",
      "Loss for  17442 th iteration => 0.0189829213797\n",
      "Loss for  17443 th iteration => 0.0189814956069\n",
      "Loss for  17444 th iteration => 0.0189806989185\n",
      "Loss for  17445 th iteration => 0.0189792476661\n",
      "Loss for  17446 th iteration => 0.018978387737\n",
      "Loss for  17447 th iteration => 0.0189770414502\n",
      "Loss for  17448 th iteration => 0.0189756018481\n",
      "Loss for  17449 th iteration => 0.0189744844906\n",
      "Loss for  17450 th iteration => 0.0189728562915\n",
      "Loss for  17451 th iteration => 0.0189721916042\n",
      "Loss for  17452 th iteration => 0.0189706143446\n",
      "Loss for  17453 th iteration => 0.0189690778251\n",
      "Loss for  17454 th iteration => 0.0189679619448\n",
      "Loss for  17455 th iteration => 0.0189667059162\n",
      "Loss for  17456 th iteration => 0.0189661739098\n",
      "Loss for  17457 th iteration => 0.018964506309\n",
      "Loss for  17458 th iteration => 0.0189633636455\n",
      "Loss for  17459 th iteration => 0.0189622992936\n",
      "Loss for  17460 th iteration => 0.0189606278329\n",
      "Loss for  17461 th iteration => 0.0189597033043\n",
      "Loss for  17462 th iteration => 0.0189579286225\n",
      "Loss for  17463 th iteration => 0.0189570515014\n",
      "Loss for  17464 th iteration => 0.0189556776381\n",
      "Loss for  17465 th iteration => 0.0189542337529\n",
      "Loss for  17466 th iteration => 0.018953469072\n",
      "Loss for  17467 th iteration => 0.0189519787132\n",
      "Loss for  17468 th iteration => 0.0189511734408\n",
      "Loss for  17469 th iteration => 0.0189497784397\n",
      "Loss for  17470 th iteration => 0.0189483913469\n",
      "Loss for  17471 th iteration => 0.018947235432\n",
      "Loss for  17472 th iteration => 0.0189456536013\n",
      "Loss for  17473 th iteration => 0.0189449415711\n",
      "Loss for  17474 th iteration => 0.018943405283\n",
      "Loss for  17475 th iteration => 0.0189418615508\n",
      "Loss for  17476 th iteration => 0.0189407604105\n",
      "Loss for  17477 th iteration => 0.0189394705775\n",
      "Loss for  17478 th iteration => 0.0189389946676\n",
      "Loss for  17479 th iteration => 0.0189372769178\n",
      "Loss for  17480 th iteration => 0.0189361923345\n",
      "Loss for  17481 th iteration => 0.0189350758821\n",
      "Loss for  17482 th iteration => 0.0189334597843\n",
      "Loss for  17483 th iteration => 0.0189324944747\n",
      "Loss for  17484 th iteration => 0.0189307601449\n",
      "Loss for  17485 th iteration => 0.018929854443\n",
      "Loss for  17486 th iteration => 0.0189285105887\n",
      "Loss for  17487 th iteration => 0.0189270497977\n",
      "Loss for  17488 th iteration => 0.0189259701884\n",
      "Loss for  17489 th iteration => 0.0189250146596\n",
      "Loss for  17490 th iteration => 0.0189238957327\n",
      "Loss for  17491 th iteration => 0.0189226096008\n",
      "Loss for  17492 th iteration => 0.0189213235556\n",
      "Loss for  17493 th iteration => 0.0189198805212\n",
      "Loss for  17494 th iteration => 0.0189187997427\n",
      "Loss for  17495 th iteration => 0.0189174359773\n",
      "Loss for  17496 th iteration => 0.018916242944\n",
      "Loss for  17497 th iteration => 0.0189146946547\n",
      "Loss for  17498 th iteration => 0.0189136532515\n",
      "Loss for  17499 th iteration => 0.0189126563053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  17500 th iteration => 0.0189116347515\n",
      "Loss for  17501 th iteration => 0.0189102549736\n",
      "Loss for  17502 th iteration => 0.0189090487911\n",
      "Loss for  17503 th iteration => 0.0189078526529\n",
      "Loss for  17504 th iteration => 0.0189065255681\n",
      "Loss for  17505 th iteration => 0.0189050951592\n",
      "Loss for  17506 th iteration => 0.0189036279629\n",
      "Loss for  17507 th iteration => 0.0189027172944\n",
      "Loss for  17508 th iteration => 0.0189013863884\n",
      "Loss for  17509 th iteration => 0.0189000228011\n",
      "Loss for  17510 th iteration => 0.0188991471615\n",
      "Loss for  17511 th iteration => 0.0188977579616\n",
      "Loss for  17512 th iteration => 0.0188968793513\n",
      "Loss for  17513 th iteration => 0.0188955671361\n",
      "Loss for  17514 th iteration => 0.0188941074977\n",
      "Loss for  17515 th iteration => 0.0188930506078\n",
      "Loss for  17516 th iteration => 0.0188914135027\n",
      "Loss for  17517 th iteration => 0.0188903262786\n",
      "Loss for  17518 th iteration => 0.0188891571335\n",
      "Loss for  17519 th iteration => 0.0188876427417\n",
      "Loss for  17520 th iteration => 0.0188869746495\n",
      "Loss for  17521 th iteration => 0.0188853212325\n",
      "Loss for  17522 th iteration => 0.0188847646453\n",
      "Loss for  17523 th iteration => 0.0188831369099\n",
      "Loss for  17524 th iteration => 0.0188819814706\n",
      "Loss for  17525 th iteration => 0.0188809453321\n",
      "Loss for  17526 th iteration => 0.0188792585579\n",
      "Loss for  17527 th iteration => 0.0188783911858\n",
      "Loss for  17528 th iteration => 0.018876618084\n",
      "Loss for  17529 th iteration => 0.0188756610315\n",
      "Loss for  17530 th iteration => 0.0188743766785\n",
      "Loss for  17531 th iteration => 0.0188729766658\n",
      "Loss for  17532 th iteration => 0.0188721564425\n",
      "Loss for  17533 th iteration => 0.018870697617\n",
      "Loss for  17534 th iteration => 0.0188699093271\n",
      "Loss for  17535 th iteration => 0.0188685131361\n",
      "Loss for  17536 th iteration => 0.0188671359748\n",
      "Loss for  17537 th iteration => 0.0188660166874\n",
      "Loss for  17538 th iteration => 0.0188644366742\n",
      "Loss for  17539 th iteration => 0.018863317465\n",
      "Loss for  17540 th iteration => 0.0188618687572\n",
      "Loss for  17541 th iteration => 0.0188612393109\n",
      "Loss for  17542 th iteration => 0.0188599015756\n",
      "Loss for  17543 th iteration => 0.018858395944\n",
      "Loss for  17544 th iteration => 0.0188573746225\n",
      "Loss for  17545 th iteration => 0.0188559902728\n",
      "Loss for  17546 th iteration => 0.018855249338\n",
      "Loss for  17547 th iteration => 0.0188536603657\n",
      "Loss for  17548 th iteration => 0.018852311693\n",
      "Loss for  17549 th iteration => 0.0188513723865\n",
      "Loss for  17550 th iteration => 0.018849686612\n",
      "Loss for  17551 th iteration => 0.0188489980826\n",
      "Loss for  17552 th iteration => 0.0188471996307\n",
      "Loss for  17553 th iteration => 0.0188461591986\n",
      "Loss for  17554 th iteration => 0.0188451662009\n",
      "Loss for  17555 th iteration => 0.0188436907943\n",
      "Loss for  17556 th iteration => 0.0188431046998\n",
      "Loss for  17557 th iteration => 0.018841343558\n",
      "Loss for  17558 th iteration => 0.0188405019924\n",
      "Loss for  17559 th iteration => 0.0188390562548\n",
      "Loss for  17560 th iteration => 0.0188375349529\n",
      "Loss for  17561 th iteration => 0.018836381735\n",
      "Loss for  17562 th iteration => 0.0188349662415\n",
      "Loss for  17563 th iteration => 0.0188342963152\n",
      "Loss for  17564 th iteration => 0.0188330006831\n",
      "Loss for  17565 th iteration => 0.0188314824589\n",
      "Loss for  17566 th iteration => 0.0188304806707\n",
      "Loss for  17567 th iteration => 0.0188290618596\n",
      "Loss for  17568 th iteration => 0.0188283765139\n",
      "Loss for  17569 th iteration => 0.0188267343193\n",
      "Loss for  17570 th iteration => 0.0188254547354\n",
      "Loss for  17571 th iteration => 0.018824452741\n",
      "Loss for  17572 th iteration => 0.0188228172386\n",
      "Loss for  17573 th iteration => 0.018822106312\n",
      "Loss for  17574 th iteration => 0.0188203218989\n",
      "Loss for  17575 th iteration => 0.0188192797009\n",
      "Loss for  17576 th iteration => 0.0188183055451\n",
      "Loss for  17577 th iteration => 0.0188168015748\n",
      "Loss for  17578 th iteration => 0.0188158927085\n",
      "Loss for  17579 th iteration => 0.0188147195488\n",
      "Loss for  17580 th iteration => 0.0188135131218\n",
      "Loss for  17581 th iteration => 0.0188122178335\n",
      "Loss for  17582 th iteration => 0.0188107689129\n",
      "Loss for  17583 th iteration => 0.0188097234742\n",
      "Loss for  17584 th iteration => 0.0188084288395\n",
      "Loss for  17585 th iteration => 0.0188071439667\n",
      "Loss for  17586 th iteration => 0.0188056874999\n",
      "Loss for  17587 th iteration => 0.0188046231326\n",
      "Loss for  17588 th iteration => 0.0188036628861\n",
      "Loss for  17589 th iteration => 0.0188026086485\n",
      "Loss for  17590 th iteration => 0.0188012871714\n",
      "Loss for  17591 th iteration => 0.0188000581611\n",
      "Loss for  17592 th iteration => 0.018798599271\n",
      "Loss for  17593 th iteration => 0.0187975660445\n",
      "Loss for  17594 th iteration => 0.0187961578473\n",
      "Loss for  17595 th iteration => 0.0187947502244\n",
      "Loss for  17596 th iteration => 0.0187940747162\n",
      "Loss for  17597 th iteration => 0.0187923473474\n",
      "Loss for  17598 th iteration => 0.0187915718282\n",
      "Loss for  17599 th iteration => 0.0187900457493\n",
      "Loss for  17600 th iteration => 0.0187889704833\n",
      "Loss for  17601 th iteration => 0.0187881490275\n",
      "Loss for  17602 th iteration => 0.0187864503127\n",
      "Loss for  17603 th iteration => 0.0187854199203\n",
      "Loss for  17604 th iteration => 0.0187839757635\n",
      "Loss for  17605 th iteration => 0.0187829604791\n",
      "Loss for  17606 th iteration => 0.0187819458678\n",
      "Loss for  17607 th iteration => 0.0187801596524\n",
      "Loss for  17608 th iteration => 0.0187790722629\n",
      "Loss for  17609 th iteration => 0.0187777932648\n",
      "Loss for  17610 th iteration => 0.0187768837853\n",
      "Loss for  17611 th iteration => 0.0187759622274\n",
      "Loss for  17612 th iteration => 0.0187743489086\n",
      "Loss for  17613 th iteration => 0.0187735780733\n",
      "Loss for  17614 th iteration => 0.0187718674378\n",
      "Loss for  17615 th iteration => 0.0187708225165\n",
      "Loss for  17616 th iteration => 0.018769411069\n",
      "Loss for  17617 th iteration => 0.0187680210112\n",
      "Loss for  17618 th iteration => 0.0187673500589\n",
      "Loss for  17619 th iteration => 0.0187655876522\n",
      "Loss for  17620 th iteration => 0.018764531521\n",
      "Loss for  17621 th iteration => 0.0187635433831\n",
      "Loss for  17622 th iteration => 0.018762119267\n",
      "Loss for  17623 th iteration => 0.0187614727123\n",
      "Loss for  17624 th iteration => 0.0187597916844\n",
      "Loss for  17625 th iteration => 0.0187585792386\n",
      "Loss for  17626 th iteration => 0.0187575250969\n",
      "Loss for  17627 th iteration => 0.0187559322404\n",
      "Loss for  17628 th iteration => 0.018755221531\n",
      "Loss for  17629 th iteration => 0.018753434237\n",
      "Loss for  17630 th iteration => 0.0187524128829\n",
      "Loss for  17631 th iteration => 0.0187514526717\n",
      "Loss for  17632 th iteration => 0.0187499519239\n",
      "Loss for  17633 th iteration => 0.018749045344\n",
      "Loss for  17634 th iteration => 0.0187479052957\n",
      "Loss for  17635 th iteration => 0.0187466819054\n",
      "Loss for  17636 th iteration => 0.018745403588\n",
      "Loss for  17637 th iteration => 0.0187439665277\n",
      "Loss for  17638 th iteration => 0.0187429265589\n",
      "Loss for  17639 th iteration => 0.0187416091534\n",
      "Loss for  17640 th iteration => 0.0187403747619\n",
      "Loss for  17641 th iteration => 0.0187388992418\n",
      "Loss for  17642 th iteration => 0.0187378570927\n",
      "Loss for  17643 th iteration => 0.0187368979573\n",
      "Loss for  17644 th iteration => 0.0187358793152\n",
      "Loss for  17645 th iteration => 0.018734538385\n",
      "Loss for  17646 th iteration => 0.0187333428428\n",
      "Loss for  17647 th iteration => 0.0187321778531\n",
      "Loss for  17648 th iteration => 0.0187308762251\n",
      "Loss for  17649 th iteration => 0.0187294496098\n",
      "Loss for  17650 th iteration => 0.0187280196537\n",
      "Loss for  17651 th iteration => 0.0187271291196\n",
      "Loss for  17652 th iteration => 0.0187258056377\n",
      "Loss for  17653 th iteration => 0.0187248439725\n",
      "Loss for  17654 th iteration => 0.0187233512577\n",
      "Loss for  17655 th iteration => 0.0187224065928\n",
      "Loss for  17656 th iteration => 0.0187213375778\n",
      "Loss for  17657 th iteration => 0.018720049953\n",
      "Loss for  17658 th iteration => 0.0187188206075\n",
      "Loss for  17659 th iteration => 0.0187173704171\n",
      "Loss for  17660 th iteration => 0.0187163501166\n",
      "Loss for  17661 th iteration => 0.0187149730885\n",
      "Loss for  17662 th iteration => 0.0187138353778\n",
      "Loss for  17663 th iteration => 0.01871269441\n",
      "Loss for  17664 th iteration => 0.0187113173473\n",
      "Loss for  17665 th iteration => 0.0187103372234\n",
      "Loss for  17666 th iteration => 0.0187088670548\n",
      "Loss for  17667 th iteration => 0.0187079362237\n",
      "Loss for  17668 th iteration => 0.0187068197914\n",
      "Loss for  17669 th iteration => 0.0187055825431\n",
      "Loss for  17670 th iteration => 0.0187043349981\n",
      "Loss for  17671 th iteration => 0.0187028837399\n",
      "Loss for  17672 th iteration => 0.0187018687989\n",
      "Loss for  17673 th iteration => 0.0187005472399\n",
      "Loss for  17674 th iteration => 0.0186993261968\n",
      "Loss for  17675 th iteration => 0.0186978549582\n",
      "Loss for  17676 th iteration => 0.0186968235345\n",
      "Loss for  17677 th iteration => 0.0186958381251\n",
      "Loss for  17678 th iteration => 0.0186948480896\n",
      "Loss for  17679 th iteration => 0.018693488342\n",
      "Loss for  17680 th iteration => 0.0186923229951\n",
      "Loss for  17681 th iteration => 0.0186911376062\n",
      "Loss for  17682 th iteration => 0.0186898701703\n",
      "Loss for  17683 th iteration => 0.0186884431455\n",
      "Loss for  17684 th iteration => 0.0186870248702\n",
      "Loss for  17685 th iteration => 0.0186861228014\n",
      "Loss for  17686 th iteration => 0.0186848230595\n",
      "Loss for  17687 th iteration => 0.0186838448629\n",
      "Loss for  17688 th iteration => 0.0186823792367\n",
      "Loss for  17689 th iteration => 0.0186814061153\n",
      "Loss for  17690 th iteration => 0.0186803712717\n",
      "Loss for  17691 th iteration => 0.0186790592942\n",
      "Loss for  17692 th iteration => 0.0186778661546\n",
      "Loss for  17693 th iteration => 0.0186764019181\n",
      "Loss for  17694 th iteration => 0.0186754063525\n",
      "Loss for  17695 th iteration => 0.0186740169543\n",
      "Loss for  17696 th iteration => 0.0186726102414\n",
      "Loss for  17697 th iteration => 0.0186719515681\n",
      "Loss for  17698 th iteration => 0.0186702619935\n",
      "Loss for  17699 th iteration => 0.0186694599172\n",
      "Loss for  17700 th iteration => 0.0186679870377\n",
      "Loss for  17701 th iteration => 0.0186668906491\n",
      "Loss for  17702 th iteration => 0.0186661037726\n",
      "Loss for  17703 th iteration => 0.0186644070791\n",
      "Loss for  17704 th iteration => 0.0186634150968\n",
      "Loss for  17705 th iteration => 0.0186619643177\n",
      "Loss for  17706 th iteration => 0.0186609779472\n",
      "Loss for  17707 th iteration => 0.0186599747206\n",
      "Loss for  17708 th iteration => 0.0186582107969\n",
      "Loss for  17709 th iteration => 0.0186571487549\n",
      "Loss for  17710 th iteration => 0.018655874499\n",
      "Loss for  17711 th iteration => 0.0186549696684\n",
      "Loss for  17712 th iteration => 0.0186537019525\n",
      "Loss for  17713 th iteration => 0.0186527378779\n",
      "Loss for  17714 th iteration => 0.0186515566113\n",
      "Loss for  17715 th iteration => 0.0186500774839\n",
      "Loss for  17716 th iteration => 0.0186490526439\n",
      "Loss for  17717 th iteration => 0.0186474488159\n",
      "Loss for  17718 th iteration => 0.0186464766631\n",
      "Loss for  17719 th iteration => 0.0186452451499\n",
      "Loss for  17720 th iteration => 0.0186437897984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  17721 th iteration => 0.0186427678173\n",
      "Loss for  17722 th iteration => 0.0186417867748\n",
      "Loss for  17723 th iteration => 0.018640802549\n",
      "Loss for  17724 th iteration => 0.0186394497375\n",
      "Loss for  17725 th iteration => 0.0186382925476\n",
      "Loss for  17726 th iteration => 0.0186371117567\n",
      "Loss for  17727 th iteration => 0.0186358533834\n",
      "Loss for  17728 th iteration => 0.0186344305516\n",
      "Loss for  17729 th iteration => 0.0186330173134\n",
      "Loss for  17730 th iteration => 0.018632124967\n",
      "Loss for  17731 th iteration => 0.0186308346294\n",
      "Loss for  17732 th iteration => 0.0186298553048\n",
      "Loss for  17733 th iteration => 0.0186284047051\n",
      "Loss for  17734 th iteration => 0.0186274322505\n",
      "Loss for  17735 th iteration => 0.0186264115114\n",
      "Loss for  17736 th iteration => 0.0186250982176\n",
      "Loss for  17737 th iteration => 0.0186239174461\n",
      "Loss for  17738 th iteration => 0.0186224554371\n",
      "Loss for  17739 th iteration => 0.01862147173\n",
      "Loss for  17740 th iteration => 0.0186200819104\n",
      "Loss for  17741 th iteration => 0.0186186878775\n",
      "Loss for  17742 th iteration => 0.0186180326346\n",
      "Loss for  17743 th iteration => 0.0186163483846\n",
      "Loss for  17744 th iteration => 0.0186155599901\n",
      "Loss for  17745 th iteration => 0.0186140853798\n",
      "Loss for  17746 th iteration => 0.0186130034567\n",
      "Loss for  17747 th iteration => 0.01861221902\n",
      "Loss for  17748 th iteration => 0.0186105280558\n",
      "Loss for  17749 th iteration => 0.018609548281\n",
      "Loss for  17750 th iteration => 0.0186080991711\n",
      "Loss for  17751 th iteration => 0.018607121006\n",
      "Loss for  17752 th iteration => 0.0186061225364\n",
      "Loss for  17753 th iteration => 0.0186043617156\n",
      "Loss for  17754 th iteration => 0.0186033177291\n",
      "Loss for  17755 th iteration => 0.0186020358597\n",
      "Loss for  17756 th iteration => 0.018601157799\n",
      "Loss for  17757 th iteration => 0.0185998682645\n",
      "Loss for  17758 th iteration => 0.0185989495027\n",
      "Loss for  17759 th iteration => 0.0185977337202\n",
      "Loss for  17760 th iteration => 0.0185962967384\n",
      "Loss for  17761 th iteration => 0.0185952486375\n",
      "Loss for  17762 th iteration => 0.0185936761879\n",
      "Loss for  17763 th iteration => 0.0185926839443\n",
      "Loss for  17764 th iteration => 0.0185914833454\n",
      "Loss for  17765 th iteration => 0.0185900198999\n",
      "Loss for  17766 th iteration => 0.0185890151241\n",
      "Loss for  17767 th iteration => 0.0185880273475\n",
      "Loss for  17768 th iteration => 0.0185870731488\n",
      "Loss for  17769 th iteration => 0.0185857031612\n",
      "Loss for  17770 th iteration => 0.0185845775817\n",
      "Loss for  17771 th iteration => 0.0185833780397\n",
      "Loss for  17772 th iteration => 0.018582141813\n",
      "Loss for  17773 th iteration => 0.0185807100609\n",
      "Loss for  17774 th iteration => 0.0185793288607\n",
      "Loss for  17775 th iteration => 0.0185784080874\n",
      "Loss for  17776 th iteration => 0.0185771483613\n",
      "Loss for  17777 th iteration => 0.0185761726079\n",
      "Loss for  17778 th iteration => 0.0185747321003\n",
      "Loss for  17779 th iteration => 0.0185737512017\n",
      "Loss for  17780 th iteration => 0.0185727645998\n",
      "Loss for  17781 th iteration => 0.0185714300305\n",
      "Loss for  17782 th iteration => 0.0185702738915\n",
      "Loss for  17783 th iteration => 0.0185691079286\n",
      "Loss for  17784 th iteration => 0.0185678636082\n",
      "Loss for  17785 th iteration => 0.0185664570679\n",
      "Loss for  17786 th iteration => 0.0185650327025\n",
      "Loss for  17787 th iteration => 0.0185641723528\n",
      "Loss for  17788 th iteration => 0.0185628834502\n",
      "Loss for  17789 th iteration => 0.0185618937725\n",
      "Loss for  17790 th iteration => 0.0185604710504\n",
      "Loss for  17791 th iteration => 0.0185594928661\n",
      "Loss for  17792 th iteration => 0.0185584830141\n",
      "Loss for  17793 th iteration => 0.0185571747774\n",
      "Loss for  17794 th iteration => 0.018556013484\n",
      "Loss for  17795 th iteration => 0.0185545484737\n",
      "Loss for  17796 th iteration => 0.0185535855633\n",
      "Loss for  17797 th iteration => 0.0185522046449\n",
      "Loss for  17798 th iteration => 0.0185508135021\n",
      "Loss for  17799 th iteration => 0.0185501655388\n",
      "Loss for  17800 th iteration => 0.0185485022321\n",
      "Loss for  17801 th iteration => 0.0185477008623\n",
      "Loss for  17802 th iteration => 0.0185462539553\n",
      "Loss for  17803 th iteration => 0.018545161581\n",
      "Loss for  17804 th iteration => 0.018544396417\n",
      "Loss for  17805 th iteration => 0.0185427060403\n",
      "Loss for  17806 th iteration => 0.0185417474517\n",
      "Loss for  17807 th iteration => 0.0185402946708\n",
      "Loss for  17808 th iteration => 0.0185393330256\n",
      "Loss for  17809 th iteration => 0.0185383392631\n",
      "Loss for  17810 th iteration => 0.0185365913528\n",
      "Loss for  17811 th iteration => 0.0185355609703\n",
      "Loss for  17812 th iteration => 0.018534281841\n",
      "Loss for  17813 th iteration => 0.0185334118255\n",
      "Loss for  17814 th iteration => 0.0185321198309\n",
      "Loss for  17815 th iteration => 0.0185312225131\n",
      "Loss for  17816 th iteration => 0.0185299984022\n",
      "Loss for  17817 th iteration => 0.0185285883144\n",
      "Loss for  17818 th iteration => 0.0185275369251\n",
      "Loss for  17819 th iteration => 0.0185259913214\n",
      "Loss for  17820 th iteration => 0.0185249829254\n",
      "Loss for  17821 th iteration => 0.0185238145141\n",
      "Loss for  17822 th iteration => 0.018522348384\n",
      "Loss for  17823 th iteration => 0.0185213600938\n",
      "Loss for  17824 th iteration => 0.0185203603626\n",
      "Loss for  17825 th iteration => 0.0185190558521\n",
      "Loss for  17826 th iteration => 0.0185183290418\n",
      "Loss for  17827 th iteration => 0.0185167819216\n",
      "Loss for  17828 th iteration => 0.0185158202069\n",
      "Loss for  17829 th iteration => 0.0185145567927\n",
      "Loss for  17830 th iteration => 0.0185129859598\n",
      "Loss for  17831 th iteration => 0.0185119701433\n",
      "Loss for  17832 th iteration => 0.0185104886877\n",
      "Loss for  17833 th iteration => 0.0185095573553\n",
      "Loss for  17834 th iteration => 0.0185082143534\n",
      "Loss for  17835 th iteration => 0.0185074299053\n",
      "Loss for  17836 th iteration => 0.0185064526894\n",
      "Loss for  17837 th iteration => 0.0185049677971\n",
      "Loss for  17838 th iteration => 0.0185041335087\n",
      "Loss for  17839 th iteration => 0.0185025096387\n",
      "Loss for  17840 th iteration => 0.0185018137933\n",
      "Loss for  17841 th iteration => 0.0185001260529\n",
      "Loss for  17842 th iteration => 0.0184987640261\n",
      "Loss for  17843 th iteration => 0.018497797258\n",
      "Loss for  17844 th iteration => 0.0184963096319\n",
      "Loss for  17845 th iteration => 0.0184958118017\n",
      "Loss for  17846 th iteration => 0.0184940752459\n",
      "Loss for  17847 th iteration => 0.0184932508576\n",
      "Loss for  17848 th iteration => 0.0184922883685\n",
      "Loss for  17849 th iteration => 0.0184907931799\n",
      "Loss for  17850 th iteration => 0.0184899723932\n",
      "Loss for  17851 th iteration => 0.0184883448698\n",
      "Loss for  17852 th iteration => 0.0184873688494\n",
      "Loss for  17853 th iteration => 0.0184859744818\n",
      "Loss for  17854 th iteration => 0.0184845688929\n",
      "Loss for  17855 th iteration => 0.0184837042856\n",
      "Loss for  17856 th iteration => 0.0184824332175\n",
      "Loss for  17857 th iteration => 0.0184814606674\n",
      "Loss for  17858 th iteration => 0.0184800412936\n",
      "Loss for  17859 th iteration => 0.0184790799103\n",
      "Loss for  17860 th iteration => 0.0184780815364\n",
      "Loss for  17861 th iteration => 0.0184767808319\n",
      "Loss for  17862 th iteration => 0.0184756190839\n",
      "Loss for  17863 th iteration => 0.01847418276\n",
      "Loss for  17864 th iteration => 0.0184732118918\n",
      "Loss for  17865 th iteration => 0.0184718380256\n",
      "Loss for  17866 th iteration => 0.0184704672383\n",
      "Loss for  17867 th iteration => 0.0184698313104\n",
      "Loss for  17868 th iteration => 0.0184681484516\n",
      "Loss for  17869 th iteration => 0.0184674132664\n",
      "Loss for  17870 th iteration => 0.0184659182202\n",
      "Loss for  17871 th iteration => 0.0184648911201\n",
      "Loss for  17872 th iteration => 0.0184640910174\n",
      "Loss for  17873 th iteration => 0.0184624431074\n",
      "Loss for  17874 th iteration => 0.0184617809444\n",
      "Loss for  17875 th iteration => 0.0184600707933\n",
      "Loss for  17876 th iteration => 0.0184590896289\n",
      "Loss for  17877 th iteration => 0.0184577421052\n",
      "Loss for  17878 th iteration => 0.0184563495219\n",
      "Loss for  17879 th iteration => 0.018455727237\n",
      "Loss for  17880 th iteration => 0.0184540551052\n",
      "Loss for  17881 th iteration => 0.0184532924217\n",
      "Loss for  17882 th iteration => 0.0184518276224\n",
      "Loss for  17883 th iteration => 0.018450783051\n",
      "Loss for  17884 th iteration => 0.0184499857607\n",
      "Loss for  17885 th iteration => 0.0184483453617\n",
      "Loss for  17886 th iteration => 0.0184473697527\n",
      "Loss for  17887 th iteration => 0.0184459580607\n",
      "Loss for  17888 th iteration => 0.0184449708901\n",
      "Loss for  17889 th iteration => 0.0184440167449\n",
      "Loss for  17890 th iteration => 0.0184422686804\n",
      "Loss for  17891 th iteration => 0.0184412676873\n",
      "Loss for  17892 th iteration => 0.0184399651133\n",
      "Loss for  17893 th iteration => 0.0184391565698\n",
      "Loss for  17894 th iteration => 0.0184378191646\n",
      "Loss for  17895 th iteration => 0.0184369947475\n",
      "Loss for  17896 th iteration => 0.0184357164802\n",
      "Loss for  17897 th iteration => 0.018434377514\n",
      "Loss for  17898 th iteration => 0.0184332895114\n",
      "Loss for  17899 th iteration => 0.018431797694\n",
      "Loss for  17900 th iteration => 0.0184310950559\n",
      "Loss for  17901 th iteration => 0.0184296716013\n",
      "Loss for  17902 th iteration => 0.0184285660882\n",
      "Loss for  17903 th iteration => 0.018427203074\n",
      "Loss for  17904 th iteration => 0.0184259070818\n",
      "Loss for  17905 th iteration => 0.0184250878553\n",
      "Loss for  17906 th iteration => 0.0184237608181\n",
      "Loss for  17907 th iteration => 0.0184229230294\n",
      "Loss for  17908 th iteration => 0.0184216606239\n",
      "Loss for  17909 th iteration => 0.0184203166496\n",
      "Loss for  17910 th iteration => 0.0184192379806\n",
      "Loss for  17911 th iteration => 0.0184177412469\n",
      "Loss for  17912 th iteration => 0.0184170446719\n",
      "Loss for  17913 th iteration => 0.0184156249181\n",
      "Loss for  17914 th iteration => 0.0184141458109\n",
      "Loss for  17915 th iteration => 0.0184131303376\n",
      "Loss for  17916 th iteration => 0.0184118473161\n",
      "Loss for  17917 th iteration => 0.0184110860745\n",
      "Loss for  17918 th iteration => 0.0184100278519\n",
      "Loss for  17919 th iteration => 0.0184086507545\n",
      "Loss for  17920 th iteration => 0.0184077315742\n",
      "Loss for  17921 th iteration => 0.0184062533575\n",
      "Loss for  17922 th iteration => 0.0184051206729\n",
      "Loss for  17923 th iteration => 0.0184038884852\n",
      "Loss for  17924 th iteration => 0.0184024146056\n",
      "Loss for  17925 th iteration => 0.0184015373649\n",
      "Loss for  17926 th iteration => 0.0184000811746\n",
      "Loss for  17927 th iteration => 0.0183990941913\n",
      "Loss for  17928 th iteration => 0.0183982107578\n",
      "Loss for  17929 th iteration => 0.0183971352919\n",
      "Loss for  17930 th iteration => 0.0183959306158\n",
      "Loss for  17931 th iteration => 0.0183946925301\n",
      "Loss for  17932 th iteration => 0.0183936495732\n",
      "Loss for  17933 th iteration => 0.0183922993643\n",
      "Loss for  17934 th iteration => 0.0183910547815\n",
      "Loss for  17935 th iteration => 0.018389580913\n",
      "Loss for  17936 th iteration => 0.0183887614371\n",
      "Loss for  17937 th iteration => 0.0183875039408\n",
      "Loss for  17938 th iteration => 0.0183861139238\n",
      "Loss for  17939 th iteration => 0.0183854065407\n",
      "Loss for  17940 th iteration => 0.01838399252\n",
      "Loss for  17941 th iteration => 0.0183832614384\n",
      "Loss for  17942 th iteration => 0.0183819011631\n",
      "Loss for  17943 th iteration => 0.0183806423406\n",
      "Loss for  17944 th iteration => 0.0183798034688\n",
      "Loss for  17945 th iteration => 0.0183780814495\n",
      "Loss for  17946 th iteration => 0.0183773815963\n",
      "Loss for  17947 th iteration => 0.0183756000536\n",
      "Loss for  17948 th iteration => 0.0183747687378\n",
      "Loss for  17949 th iteration => 0.0183735252398\n",
      "Loss for  17950 th iteration => 0.0183721262072\n",
      "Loss for  17951 th iteration => 0.0183714290963\n",
      "Loss for  17952 th iteration => 0.0183699982474\n",
      "Loss for  17953 th iteration => 0.0183692860228\n",
      "Loss for  17954 th iteration => 0.0183679097744\n",
      "Loss for  17955 th iteration => 0.0183666710117\n",
      "Loss for  17956 th iteration => 0.0183658149817\n",
      "Loss for  17957 th iteration => 0.0183641164317\n",
      "Loss for  17958 th iteration => 0.0183634023943\n",
      "Loss for  17959 th iteration => 0.0183616405674\n",
      "Loss for  17960 th iteration => 0.0183607974302\n",
      "Loss for  17961 th iteration => 0.0183593123669\n",
      "Loss for  17962 th iteration => 0.0183583357835\n",
      "Loss for  17963 th iteration => 0.0183573678575\n",
      "Loss for  17964 th iteration => 0.0183560449206\n",
      "Loss for  17965 th iteration => 0.018355373938\n",
      "Loss for  17966 th iteration => 0.0183538082967\n",
      "Loss for  17967 th iteration => 0.0183529099222\n",
      "Loss for  17968 th iteration => 0.0183516110217\n",
      "Loss for  17969 th iteration => 0.0183501460424\n",
      "Loss for  17970 th iteration => 0.0183494284054\n",
      "Loss for  17971 th iteration => 0.0183476793963\n",
      "Loss for  17972 th iteration => 0.0183469071542\n",
      "Loss for  17973 th iteration => 0.0183455692039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  17974 th iteration => 0.0183442742234\n",
      "Loss for  17975 th iteration => 0.0183434762058\n",
      "Loss for  17976 th iteration => 0.0183421312007\n",
      "Loss for  17977 th iteration => 0.0183413397541\n",
      "Loss for  17978 th iteration => 0.0183400469688\n",
      "Loss for  17979 th iteration => 0.0183387569095\n",
      "Loss for  17980 th iteration => 0.0183376484494\n",
      "Loss for  17981 th iteration => 0.0183362054818\n",
      "Loss for  17982 th iteration => 0.0183354682754\n",
      "Loss for  17983 th iteration => 0.0183338131887\n",
      "Loss for  17984 th iteration => 0.0183331802693\n",
      "Loss for  17985 th iteration => 0.0183315155637\n",
      "Loss for  17986 th iteration => 0.0183304071251\n",
      "Loss for  17987 th iteration => 0.0183295720275\n",
      "Loss for  17988 th iteration => 0.018328103524\n",
      "Loss for  17989 th iteration => 0.0183275820659\n",
      "Loss for  17990 th iteration => 0.0183258847349\n",
      "Loss for  17991 th iteration => 0.018324825148\n",
      "Loss for  17992 th iteration => 0.0183237110575\n",
      "Loss for  17993 th iteration => 0.0183222790528\n",
      "Loss for  17994 th iteration => 0.0183216071376\n",
      "Loss for  17995 th iteration => 0.0183201382646\n",
      "Loss for  17996 th iteration => 0.0183191248352\n",
      "Loss for  17997 th iteration => 0.0183176971313\n",
      "Loss for  17998 th iteration => 0.0183165019522\n",
      "Loss for  17999 th iteration => 0.018315608061\n",
      "Loss for  18000 th iteration => 0.0183143423884\n",
      "Loss for  18001 th iteration => 0.0183134792828\n",
      "Loss for  18002 th iteration => 0.0183122624735\n",
      "Loss for  18003 th iteration => 0.0183109488194\n",
      "Loss for  18004 th iteration => 0.018309802675\n",
      "Loss for  18005 th iteration => 0.0183084063314\n",
      "Loss for  18006 th iteration => 0.0183076914276\n",
      "Loss for  18007 th iteration => 0.0183062680267\n",
      "Loss for  18008 th iteration => 0.0183052258833\n",
      "Loss for  18009 th iteration => 0.0183038271557\n",
      "Loss for  18010 th iteration => 0.0183026088542\n",
      "Loss for  18011 th iteration => 0.0183014585928\n",
      "Loss for  18012 th iteration => 0.0183006235682\n",
      "Loss for  18013 th iteration => 0.0182995118608\n",
      "Loss for  18014 th iteration => 0.0182983655591\n",
      "Loss for  18015 th iteration => 0.0182971616196\n",
      "Loss for  18016 th iteration => 0.018295747177\n",
      "Loss for  18017 th iteration => 0.0182948007688\n",
      "Loss for  18018 th iteration => 0.0182935164356\n",
      "Loss for  18019 th iteration => 0.0182923862334\n",
      "Loss for  18020 th iteration => 0.0182913289023\n",
      "Loss for  18021 th iteration => 0.0182899510984\n",
      "Loss for  18022 th iteration => 0.0182888136038\n",
      "Loss for  18023 th iteration => 0.0182878225437\n",
      "Loss for  18024 th iteration => 0.018286632669\n",
      "Loss for  18025 th iteration => 0.0182857054771\n",
      "Loss for  18026 th iteration => 0.0182845571453\n",
      "Loss for  18027 th iteration => 0.0182831810671\n",
      "Loss for  18028 th iteration => 0.018282124088\n",
      "Loss for  18029 th iteration => 0.0182806872041\n",
      "Loss for  18030 th iteration => 0.0182796319765\n",
      "Loss for  18031 th iteration => 0.0182785329289\n",
      "Loss for  18032 th iteration => 0.0182774469966\n",
      "Loss for  18033 th iteration => 0.0182761753447\n",
      "Loss for  18034 th iteration => 0.0182752083599\n",
      "Loss for  18035 th iteration => 0.0182738361572\n",
      "Loss for  18036 th iteration => 0.0182728371777\n",
      "Loss for  18037 th iteration => 0.018271591903\n",
      "Loss for  18038 th iteration => 0.0182707964868\n",
      "Loss for  18039 th iteration => 0.0182694353047\n",
      "Loss for  18040 th iteration => 0.018268043558\n",
      "Loss for  18041 th iteration => 0.0182669130234\n",
      "Loss for  18042 th iteration => 0.018265608433\n",
      "Loss for  18043 th iteration => 0.0182649475672\n",
      "Loss for  18044 th iteration => 0.0182633701795\n",
      "Loss for  18045 th iteration => 0.0182626057893\n",
      "Loss for  18046 th iteration => 0.0182611844463\n",
      "Loss for  18047 th iteration => 0.0182600904177\n",
      "Loss for  18048 th iteration => 0.0182590821844\n",
      "Loss for  18049 th iteration => 0.0182579386452\n",
      "Loss for  18050 th iteration => 0.0182570090571\n",
      "Loss for  18051 th iteration => 0.0182554252105\n",
      "Loss for  18052 th iteration => 0.0182545981848\n",
      "Loss for  18053 th iteration => 0.0182529835231\n",
      "Loss for  18054 th iteration => 0.0182520218504\n",
      "Loss for  18055 th iteration => 0.0182508708026\n",
      "Loss for  18056 th iteration => 0.0182495255781\n",
      "Loss for  18057 th iteration => 0.0182487995997\n",
      "Loss for  18058 th iteration => 0.0182473424879\n",
      "Loss for  18059 th iteration => 0.0182463591125\n",
      "Loss for  18060 th iteration => 0.0182455299274\n",
      "Loss for  18061 th iteration => 0.0182439698138\n",
      "Loss for  18062 th iteration => 0.0182432715828\n",
      "Loss for  18063 th iteration => 0.0182416426383\n",
      "Loss for  18064 th iteration => 0.0182406963333\n",
      "Loss for  18065 th iteration => 0.0182393888156\n",
      "Loss for  18066 th iteration => 0.0182379565491\n",
      "Loss for  18067 th iteration => 0.0182370714182\n",
      "Loss for  18068 th iteration => 0.0182357061113\n",
      "Loss for  18069 th iteration => 0.0182350054779\n",
      "Loss for  18070 th iteration => 0.0182335942932\n",
      "Loss for  18071 th iteration => 0.0182329028988\n",
      "Loss for  18072 th iteration => 0.0182315316954\n",
      "Loss for  18073 th iteration => 0.0182303303723\n",
      "Loss for  18074 th iteration => 0.0182294629974\n",
      "Loss for  18075 th iteration => 0.0182278215142\n",
      "Loss for  18076 th iteration => 0.0182270806838\n",
      "Loss for  18077 th iteration => 0.0182253808674\n",
      "Loss for  18078 th iteration => 0.0182245207192\n",
      "Loss for  18079 th iteration => 0.0182230734694\n",
      "Loss for  18080 th iteration => 0.0182220995323\n",
      "Loss for  18081 th iteration => 0.0182211630371\n",
      "Loss for  18082 th iteration => 0.018219818955\n",
      "Loss for  18083 th iteration => 0.0182192136158\n",
      "Loss for  18084 th iteration => 0.0182176132437\n",
      "Loss for  18085 th iteration => 0.0182167868471\n",
      "Loss for  18086 th iteration => 0.0182154443201\n",
      "Loss for  18087 th iteration => 0.0182140713424\n",
      "Loss for  18088 th iteration => 0.0182132909783\n",
      "Loss for  18089 th iteration => 0.0182116314398\n",
      "Loss for  18090 th iteration => 0.0182108135338\n",
      "Loss for  18091 th iteration => 0.018209537306\n",
      "Loss for  18092 th iteration => 0.0182082248345\n",
      "Loss for  18093 th iteration => 0.0182074897308\n",
      "Loss for  18094 th iteration => 0.0182060787942\n",
      "Loss for  18095 th iteration => 0.0182054088189\n",
      "Loss for  18096 th iteration => 0.0182040216272\n",
      "Loss for  18097 th iteration => 0.0182028591304\n",
      "Loss for  18098 th iteration => 0.0182016680648\n",
      "Loss for  18099 th iteration => 0.0182003468483\n",
      "Loss for  18100 th iteration => 0.0181995174381\n",
      "Loss for  18101 th iteration => 0.0181979579216\n",
      "Loss for  18102 th iteration => 0.0181972967616\n",
      "Loss for  18103 th iteration => 0.0181956669082\n",
      "Loss for  18104 th iteration => 0.0181945827569\n",
      "Loss for  18105 th iteration => 0.0181934644215\n",
      "Loss for  18106 th iteration => 0.018192485574\n",
      "Loss for  18107 th iteration => 0.0181917054798\n",
      "Loss for  18108 th iteration => 0.0181901098958\n",
      "Loss for  18109 th iteration => 0.01818945988\n",
      "Loss for  18110 th iteration => 0.0181878051125\n",
      "Loss for  18111 th iteration => 0.0181868659927\n",
      "Loss for  18112 th iteration => 0.0181855217172\n",
      "Loss for  18113 th iteration => 0.0181842271399\n",
      "Loss for  18114 th iteration => 0.0181835692604\n",
      "Loss for  18115 th iteration => 0.0181819385577\n",
      "Loss for  18116 th iteration => 0.0181809389166\n",
      "Loss for  18117 th iteration => 0.0181800056275\n",
      "Loss for  18118 th iteration => 0.0181786239352\n",
      "Loss for  18119 th iteration => 0.0181777488148\n",
      "Loss for  18120 th iteration => 0.0181766594652\n",
      "Loss for  18121 th iteration => 0.0181755189175\n",
      "Loss for  18122 th iteration => 0.0181743426742\n",
      "Loss for  18123 th iteration => 0.0181729665096\n",
      "Loss for  18124 th iteration => 0.0181716767981\n",
      "Loss for  18125 th iteration => 0.0181707444727\n",
      "Loss for  18126 th iteration => 0.0181695818594\n",
      "Loss for  18127 th iteration => 0.018168263187\n",
      "Loss for  18128 th iteration => 0.0181675330664\n",
      "Loss for  18129 th iteration => 0.0181660975024\n",
      "Loss for  18130 th iteration => 0.0181651211458\n",
      "Loss for  18131 th iteration => 0.0181643002467\n",
      "Loss for  18132 th iteration => 0.0181627535179\n",
      "Loss for  18133 th iteration => 0.0181620609286\n",
      "Loss for  18134 th iteration => 0.0181604475421\n",
      "Loss for  18135 th iteration => 0.0181595024936\n",
      "Loss for  18136 th iteration => 0.0181582063668\n",
      "Loss for  18137 th iteration => 0.0181567880929\n",
      "Loss for  18138 th iteration => 0.0181559086869\n",
      "Loss for  18139 th iteration => 0.018154552934\n",
      "Loss for  18140 th iteration => 0.0181538704661\n",
      "Loss for  18141 th iteration => 0.0181524568488\n",
      "Loss for  18142 th iteration => 0.0181517943399\n",
      "Loss for  18143 th iteration => 0.0181504097828\n",
      "Loss for  18144 th iteration => 0.018149246628\n",
      "Loss for  18145 th iteration => 0.0181483567406\n",
      "Loss for  18146 th iteration => 0.0181467578662\n",
      "Loss for  18147 th iteration => 0.0181459929656\n",
      "Loss for  18148 th iteration => 0.0181443282061\n",
      "Loss for  18149 th iteration => 0.0181434619359\n",
      "Loss for  18150 th iteration => 0.0181420304345\n",
      "Loss for  18151 th iteration => 0.0181410626973\n",
      "Loss for  18152 th iteration => 0.0181401495955\n",
      "Loss for  18153 th iteration => 0.0181387948598\n",
      "Loss for  18154 th iteration => 0.0181382286086\n",
      "Loss for  18155 th iteration => 0.0181366074798\n",
      "Loss for  18156 th iteration => 0.0181358237625\n",
      "Loss for  18157 th iteration => 0.0181344491061\n",
      "Loss for  18158 th iteration => 0.0181331367144\n",
      "Loss for  18159 th iteration => 0.0181323131838\n",
      "Loss for  18160 th iteration => 0.0181307033363\n",
      "Loss for  18161 th iteration => 0.0181298636138\n",
      "Loss for  18162 th iteration => 0.0181286228181\n",
      "Loss for  18163 th iteration => 0.0181272991336\n",
      "Loss for  18164 th iteration => 0.0181262927594\n",
      "Loss for  18165 th iteration => 0.0181253751366\n",
      "Loss for  18166 th iteration => 0.0181244168515\n",
      "Loss for  18167 th iteration => 0.0181231584383\n",
      "Loss for  18168 th iteration => 0.0181220533031\n",
      "Loss for  18169 th iteration => 0.0181206638229\n",
      "Loss for  18170 th iteration => 0.0181197365861\n",
      "Loss for  18171 th iteration => 0.0181183878467\n",
      "Loss for  18172 th iteration => 0.0181171164457\n",
      "Loss for  18173 th iteration => 0.0181164562867\n",
      "Loss for  18174 th iteration => 0.0181148316775\n",
      "Loss for  18175 th iteration => 0.0181138467382\n",
      "Loss for  18176 th iteration => 0.0181129230187\n",
      "Loss for  18177 th iteration => 0.0181115484964\n",
      "Loss for  18178 th iteration => 0.0181106827636\n",
      "Loss for  18179 th iteration => 0.0181096127231\n",
      "Loss for  18180 th iteration => 0.0181084684169\n",
      "Loss for  18181 th iteration => 0.0181073036971\n",
      "Loss for  18182 th iteration => 0.0181059195079\n",
      "Loss for  18183 th iteration => 0.0181049916282\n",
      "Loss for  18184 th iteration => 0.0181037325446\n",
      "Loss for  18185 th iteration => 0.0181025972177\n",
      "Loss for  18186 th iteration => 0.0181012161952\n",
      "Loss for  18187 th iteration => 0.0181002402568\n",
      "Loss for  18188 th iteration => 0.0180993182214\n",
      "Loss for  18189 th iteration => 0.0180980149636\n",
      "Loss for  18190 th iteration => 0.0180973865959\n",
      "Loss for  18191 th iteration => 0.01809583594\n",
      "Loss for  18192 th iteration => 0.0180949942801\n",
      "Loss for  18193 th iteration => 0.0180937029064\n",
      "Loss for  18194 th iteration => 0.0180922944072\n",
      "Loss for  18195 th iteration => 0.018091574385\n",
      "Loss for  18196 th iteration => 0.0180899029991\n",
      "Loss for  18197 th iteration => 0.0180893665682\n",
      "Loss for  18198 th iteration => 0.0180876476545\n",
      "Loss for  18199 th iteration => 0.0180866717628\n",
      "Loss for  18200 th iteration => 0.0180857652524\n",
      "Loss for  18201 th iteration => 0.0180844209917\n",
      "Loss for  18202 th iteration => 0.0180838507704\n",
      "Loss for  18203 th iteration => 0.0180822454959\n",
      "Loss for  18204 th iteration => 0.0180814612874\n",
      "Loss for  18205 th iteration => 0.0180801094588\n",
      "Loss for  18206 th iteration => 0.0180787812314\n",
      "Loss for  18207 th iteration => 0.0180779845521\n",
      "Loss for  18208 th iteration => 0.0180763727637\n",
      "Loss for  18209 th iteration => 0.018075548893\n",
      "Loss for  18210 th iteration => 0.0180742980941\n",
      "Loss for  18211 th iteration => 0.0180729990635\n",
      "Loss for  18212 th iteration => 0.0180719880733\n",
      "Loss for  18213 th iteration => 0.0180710766451\n",
      "Loss for  18214 th iteration => 0.0180701231982\n",
      "Loss for  18215 th iteration => 0.018068872393\n",
      "Loss for  18216 th iteration => 0.0180677813098\n",
      "Loss for  18217 th iteration => 0.0180663859939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  18218 th iteration => 0.0180654783848\n",
      "Loss for  18219 th iteration => 0.0180641280739\n",
      "Loss for  18220 th iteration => 0.018062875741\n",
      "Loss for  18221 th iteration => 0.0180622016544\n",
      "Loss for  18222 th iteration => 0.0180606016598\n",
      "Loss for  18223 th iteration => 0.0180596144419\n",
      "Loss for  18224 th iteration => 0.0180587008796\n",
      "Loss for  18225 th iteration => 0.0180573297355\n",
      "Loss for  18226 th iteration => 0.0180564661069\n",
      "Loss for  18227 th iteration => 0.018055409877\n",
      "Loss for  18228 th iteration => 0.0180542642557\n",
      "Loss for  18229 th iteration => 0.0180531185012\n",
      "Loss for  18230 th iteration => 0.0180517289039\n",
      "Loss for  18231 th iteration => 0.0180504884667\n",
      "Loss for  18232 th iteration => 0.0180495365425\n",
      "Loss for  18233 th iteration => 0.018048407067\n",
      "Loss for  18234 th iteration => 0.0180470949858\n",
      "Loss for  18235 th iteration => 0.0180464004576\n",
      "Loss for  18236 th iteration => 0.0180449556599\n",
      "Loss for  18237 th iteration => 0.018044006255\n",
      "Loss for  18238 th iteration => 0.0180432056837\n",
      "Loss for  18239 th iteration => 0.0180416703341\n",
      "Loss for  18240 th iteration => 0.0180409946819\n",
      "Loss for  18241 th iteration => 0.0180393838402\n",
      "Loss for  18242 th iteration => 0.0180384717559\n",
      "Loss for  18243 th iteration => 0.0180371533082\n",
      "Loss for  18244 th iteration => 0.0180357913704\n",
      "Loss for  18245 th iteration => 0.0180352535821\n",
      "Loss for  18246 th iteration => 0.0180335704376\n",
      "Loss for  18247 th iteration => 0.0180329574066\n",
      "Loss for  18248 th iteration => 0.0180314347345\n",
      "Loss for  18249 th iteration => 0.0180305398534\n",
      "Loss for  18250 th iteration => 0.0180296998996\n",
      "Loss for  18251 th iteration => 0.0180282067964\n",
      "Loss for  18252 th iteration => 0.0180274923068\n",
      "Loss for  18253 th iteration => 0.0180259120978\n",
      "Loss for  18254 th iteration => 0.0180249951119\n",
      "Loss for  18255 th iteration => 0.0180236701778\n",
      "Loss for  18256 th iteration => 0.0180223403813\n",
      "Loss for  18257 th iteration => 0.0180215106849\n",
      "Loss for  18258 th iteration => 0.0180202839453\n",
      "Loss for  18259 th iteration => 0.0180190620101\n",
      "Loss for  18260 th iteration => 0.0180182511313\n",
      "Loss for  18261 th iteration => 0.0180169346147\n",
      "Loss for  18262 th iteration => 0.0180162351837\n",
      "Loss for  18263 th iteration => 0.0180149123227\n",
      "Loss for  18264 th iteration => 0.0180137332672\n",
      "Loss for  18265 th iteration => 0.0180126132929\n",
      "Loss for  18266 th iteration => 0.0180112773331\n",
      "Loss for  18267 th iteration => 0.0180105019247\n",
      "Loss for  18268 th iteration => 0.0180089442833\n",
      "Loss for  18269 th iteration => 0.0180082737892\n",
      "Loss for  18270 th iteration => 0.018006673061\n",
      "Loss for  18271 th iteration => 0.0180056969174\n",
      "Loss for  18272 th iteration => 0.018004798262\n",
      "Loss for  18273 th iteration => 0.0180034254991\n",
      "Loss for  18274 th iteration => 0.0180025600402\n",
      "Loss for  18275 th iteration => 0.018001527848\n",
      "Loss for  18276 th iteration => 0.0180003707755\n",
      "Loss for  18277 th iteration => 0.0179992385513\n",
      "Loss for  18278 th iteration => 0.0179978649113\n",
      "Loss for  18279 th iteration => 0.0179969535685\n",
      "Loss for  18280 th iteration => 0.0179956723411\n",
      "Loss for  18281 th iteration => 0.0179946073417\n",
      "Loss for  18282 th iteration => 0.0179935484101\n",
      "Loss for  18283 th iteration => 0.0179922730158\n",
      "Loss for  18284 th iteration => 0.0179913774822\n",
      "Loss for  18285 th iteration => 0.0179900047173\n",
      "Loss for  18286 th iteration => 0.0179891079236\n",
      "Loss for  18287 th iteration => 0.0179881289048\n",
      "Loss for  18288 th iteration => 0.0179869221479\n",
      "Loss for  18289 th iteration => 0.0179858273472\n",
      "Loss for  18290 th iteration => 0.0179844464807\n",
      "Loss for  18291 th iteration => 0.0179835454127\n",
      "Loss for  18292 th iteration => 0.0179822312117\n",
      "Loss for  18293 th iteration => 0.01798096096\n",
      "Loss for  18294 th iteration => 0.017980305718\n",
      "Loss for  18295 th iteration => 0.0179787292557\n",
      "Loss for  18296 th iteration => 0.0179777401765\n",
      "Loss for  18297 th iteration => 0.0179768257196\n",
      "Loss for  18298 th iteration => 0.0179754759297\n",
      "Loss for  18299 th iteration => 0.0179746187746\n",
      "Loss for  18300 th iteration => 0.0179735550153\n",
      "Loss for  18301 th iteration => 0.0179724355114\n",
      "Loss for  18302 th iteration => 0.017971296131\n",
      "Loss for  18303 th iteration => 0.0179699346952\n",
      "Loss for  18304 th iteration => 0.0179686853131\n",
      "Loss for  18305 th iteration => 0.01796776118\n",
      "Loss for  18306 th iteration => 0.0179666290834\n",
      "Loss for  18307 th iteration => 0.0179653386983\n",
      "Loss for  18308 th iteration => 0.0179646293555\n",
      "Loss for  18309 th iteration => 0.0179632166107\n",
      "Loss for  18310 th iteration => 0.0179622579851\n",
      "Loss for  18311 th iteration => 0.0179614691215\n",
      "Loss for  18312 th iteration => 0.0179599437074\n",
      "Loss for  18313 th iteration => 0.0179592770413\n",
      "Loss for  18314 th iteration => 0.0179576836926\n",
      "Loss for  18315 th iteration => 0.0179567801601\n",
      "Loss for  18316 th iteration => 0.0179554810764\n",
      "Loss for  18317 th iteration => 0.0179541218513\n",
      "Loss for  18318 th iteration => 0.0179533515315\n",
      "Loss for  18319 th iteration => 0.0179521013304\n",
      "Loss for  18320 th iteration => 0.0179512052241\n",
      "Loss for  18321 th iteration => 0.0179498429002\n",
      "Loss for  18322 th iteration => 0.0179489395818\n",
      "Loss for  18323 th iteration => 0.0179479872829\n",
      "Loss for  18324 th iteration => 0.0179467631435\n",
      "Loss for  18325 th iteration => 0.0179456838491\n",
      "Loss for  18326 th iteration => 0.0179443057741\n",
      "Loss for  18327 th iteration => 0.0179434118814\n",
      "Loss for  18328 th iteration => 0.0179420891403\n",
      "Loss for  18329 th iteration => 0.0179408359314\n",
      "Loss for  18330 th iteration => 0.0179401843023\n",
      "Loss for  18331 th iteration => 0.0179386010013\n",
      "Loss for  18332 th iteration => 0.0179376313061\n",
      "Loss for  18333 th iteration => 0.0179367170875\n",
      "Loss for  18334 th iteration => 0.017935376887\n",
      "Loss for  18335 th iteration => 0.0179345119362\n",
      "Loss for  18336 th iteration => 0.017933477728\n",
      "Loss for  18337 th iteration => 0.0179323380167\n",
      "Loss for  18338 th iteration => 0.0179312156946\n",
      "Loss for  18339 th iteration => 0.0179298448438\n",
      "Loss for  18340 th iteration => 0.0179286230539\n",
      "Loss for  18341 th iteration => 0.0179276712514\n",
      "Loss for  18342 th iteration => 0.0179265752562\n",
      "Loss for  18343 th iteration => 0.0179252604426\n",
      "Loss for  18344 th iteration => 0.0179245968089\n",
      "Loss for  18345 th iteration => 0.0179231472898\n",
      "Loss for  18346 th iteration => 0.0179222269037\n",
      "Loss for  18347 th iteration => 0.0179214215809\n",
      "Loss for  18348 th iteration => 0.0179199227354\n",
      "Loss for  18349 th iteration => 0.0179192389058\n",
      "Loss for  18350 th iteration => 0.017917658788\n",
      "Loss for  18351 th iteration => 0.0179167627904\n",
      "Loss for  18352 th iteration => 0.0179154526482\n",
      "Loss for  18353 th iteration => 0.0179141241424\n",
      "Loss for  18354 th iteration => 0.0179133218221\n",
      "Loss for  18355 th iteration => 0.0179121030433\n",
      "Loss for  18356 th iteration => 0.0179112080983\n",
      "Loss for  18357 th iteration => 0.0179098543872\n",
      "Loss for  18358 th iteration => 0.0179089418445\n",
      "Loss for  18359 th iteration => 0.0179080221357\n",
      "Loss for  18360 th iteration => 0.0179067747416\n",
      "Loss for  18361 th iteration => 0.0179057143454\n",
      "Loss for  18362 th iteration => 0.0179043387806\n",
      "Loss for  18363 th iteration => 0.0179034522322\n",
      "Loss for  18364 th iteration => 0.01790211777\n",
      "Loss for  18365 th iteration => 0.01790088565\n",
      "Loss for  18366 th iteration => 0.0179002357148\n",
      "Loss for  18367 th iteration => 0.0178986445083\n",
      "Loss for  18368 th iteration => 0.0178976953491\n",
      "Loss for  18369 th iteration => 0.0178967829098\n",
      "Loss for  18370 th iteration => 0.0178954506698\n",
      "Loss for  18371 th iteration => 0.0178945757008\n",
      "Loss for  18372 th iteration => 0.0178935758535\n",
      "Loss for  18373 th iteration => 0.0178924111144\n",
      "Loss for  18374 th iteration => 0.0178913086427\n",
      "Loss for  18375 th iteration => 0.0178899366442\n",
      "Loss for  18376 th iteration => 0.0178890507775\n",
      "Loss for  18377 th iteration => 0.0178877677261\n",
      "Loss for  18378 th iteration => 0.0178867328655\n",
      "Loss for  18379 th iteration => 0.0178856666857\n",
      "Loss for  18380 th iteration => 0.0178844272893\n",
      "Loss for  18381 th iteration => 0.0178835212435\n",
      "Loss for  18382 th iteration => 0.0178821856112\n",
      "Loss for  18383 th iteration => 0.0178812737906\n",
      "Loss for  18384 th iteration => 0.0178803398979\n",
      "Loss for  18385 th iteration => 0.0178791127635\n",
      "Loss for  18386 th iteration => 0.0178780546432\n",
      "Loss for  18387 th iteration => 0.0178766749807\n",
      "Loss for  18388 th iteration => 0.0178757996705\n",
      "Loss for  18389 th iteration => 0.0178744817053\n",
      "Loss for  18390 th iteration => 0.0178732347456\n",
      "Loss for  18391 th iteration => 0.0178725915364\n",
      "Loss for  18392 th iteration => 0.0178710168539\n",
      "Loss for  18393 th iteration => 0.0178700631882\n",
      "Loss for  18394 th iteration => 0.0178691414863\n",
      "Loss for  18395 th iteration => 0.0178678254235\n",
      "Loss for  18396 th iteration => 0.0178669474169\n",
      "Loss for  18397 th iteration => 0.0178659419198\n",
      "Loss for  18398 th iteration => 0.0178647889665\n",
      "Loss for  18399 th iteration => 0.0178636934659\n",
      "Loss for  18400 th iteration => 0.0178623231205\n",
      "Loss for  18401 th iteration => 0.0178611207401\n",
      "Loss for  18402 th iteration => 0.0178601569411\n",
      "Loss for  18403 th iteration => 0.0178590896802\n",
      "Loss for  18404 th iteration => 0.0178577688492\n",
      "Loss for  18405 th iteration => 0.0178571255856\n",
      "Loss for  18406 th iteration => 0.017855670198\n",
      "Loss for  18407 th iteration => 0.0178547642033\n",
      "Loss for  18408 th iteration => 0.0178539618323\n",
      "Loss for  18409 th iteration => 0.0178524774873\n",
      "Loss for  18410 th iteration => 0.0178517947742\n",
      "Loss for  18411 th iteration => 0.0178502237445\n",
      "Loss for  18412 th iteration => 0.0178493471853\n",
      "Loss for  18413 th iteration => 0.0178480323415\n",
      "Loss for  18414 th iteration => 0.0178467336652\n",
      "Loss for  18415 th iteration => 0.0178459047295\n",
      "Loss for  18416 th iteration => 0.0178447212622\n",
      "Loss for  18417 th iteration => 0.0178435101115\n",
      "Loss for  18418 th iteration => 0.0178427263585\n",
      "Loss for  18419 th iteration => 0.0178414130684\n",
      "Loss for  18420 th iteration => 0.0178404195391\n",
      "Loss for  18421 th iteration => 0.0178396650194\n",
      "Loss for  18422 th iteration => 0.0178381377737\n",
      "Loss for  18423 th iteration => 0.0178375008117\n",
      "Loss for  18424 th iteration => 0.0178359146455\n",
      "Loss for  18425 th iteration => 0.017835025678\n",
      "Loss for  18426 th iteration => 0.0178337580059\n",
      "Loss for  18427 th iteration => 0.0178323869014\n",
      "Loss for  18428 th iteration => 0.0178318918354\n",
      "Loss for  18429 th iteration => 0.017830249717\n",
      "Loss for  18430 th iteration => 0.0178296140327\n",
      "Loss for  18431 th iteration => 0.0178281568591\n",
      "Loss for  18432 th iteration => 0.0178272624898\n",
      "Loss for  18433 th iteration => 0.0178264419166\n",
      "Loss for  18434 th iteration => 0.0178249829124\n",
      "Loss for  18435 th iteration => 0.0178242812381\n",
      "Loss for  18436 th iteration => 0.0178227382831\n",
      "Loss for  18437 th iteration => 0.0178218466435\n",
      "Loss for  18438 th iteration => 0.0178205549333\n",
      "Loss for  18439 th iteration => 0.0178192494183\n",
      "Loss for  18440 th iteration => 0.0178184270464\n",
      "Loss for  18441 th iteration => 0.0178172517332\n",
      "Loss for  18442 th iteration => 0.017816029904\n",
      "Loss for  18443 th iteration => 0.017815262556\n",
      "Loss for  18444 th iteration => 0.0178139387292\n",
      "Loss for  18445 th iteration => 0.0178129624931\n",
      "Loss for  18446 th iteration => 0.017812186675\n",
      "Loss for  18447 th iteration => 0.0178106877701\n",
      "Loss for  18448 th iteration => 0.0178100288594\n",
      "Loss for  18449 th iteration => 0.017808471999\n",
      "Loss for  18450 th iteration => 0.0178075686835\n",
      "Loss for  18451 th iteration => 0.0178063214029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  18452 th iteration => 0.017804948425\n",
      "Loss for  18453 th iteration => 0.0178038801368\n",
      "Loss for  18454 th iteration => 0.0178029589194\n",
      "Loss for  18455 th iteration => 0.0178020557757\n",
      "Loss for  18456 th iteration => 0.0178007827404\n",
      "Loss for  18457 th iteration => 0.0178002016991\n",
      "Loss for  18458 th iteration => 0.0177986698066\n",
      "Loss for  18459 th iteration => 0.0177978893626\n",
      "Loss for  18460 th iteration => 0.0177965749233\n",
      "Loss for  18461 th iteration => 0.0177953127144\n",
      "Loss for  18462 th iteration => 0.017794509183\n",
      "Loss for  18463 th iteration => 0.017792969701\n",
      "Loss for  18464 th iteration => 0.0177921298077\n",
      "Loss for  18465 th iteration => 0.0177909741671\n",
      "Loss for  18466 th iteration => 0.0177896636363\n",
      "Loss for  18467 th iteration => 0.0177887198326\n",
      "Loss for  18468 th iteration => 0.017787809188\n",
      "Loss for  18469 th iteration => 0.0177865859181\n",
      "Loss for  18470 th iteration => 0.0177859265697\n",
      "Loss for  18471 th iteration => 0.0177844751998\n",
      "Loss for  18472 th iteration => 0.0177836196582\n",
      "Loss for  18473 th iteration => 0.017782408911\n",
      "Loss for  18474 th iteration => 0.017781017729\n",
      "Loss for  18475 th iteration => 0.0177803446602\n",
      "Loss for  18476 th iteration => 0.0177787091072\n",
      "Loss for  18477 th iteration => 0.0177779629848\n",
      "Loss for  18478 th iteration => 0.0177767212201\n",
      "Loss for  18479 th iteration => 0.0177754806271\n",
      "Loss for  18480 th iteration => 0.0177747768437\n",
      "Loss for  18481 th iteration => 0.0177734264349\n",
      "Loss for  18482 th iteration => 0.0177728095706\n",
      "Loss for  18483 th iteration => 0.0177714502357\n",
      "Loss for  18484 th iteration => 0.0177703780615\n",
      "Loss for  18485 th iteration => 0.0177691993899\n",
      "Loss for  18486 th iteration => 0.0177679920609\n",
      "Loss for  18487 th iteration => 0.0177671391172\n",
      "Loss for  18488 th iteration => 0.017765684271\n",
      "Loss for  18489 th iteration => 0.0177650513154\n",
      "Loss for  18490 th iteration => 0.0177634730073\n",
      "Loss for  18491 th iteration => 0.0177624859617\n",
      "Loss for  18492 th iteration => 0.0177613421663\n",
      "Loss for  18493 th iteration => 0.017760516892\n",
      "Loss for  18494 th iteration => 0.0177593255065\n",
      "Loss for  18495 th iteration => 0.0177585255766\n",
      "Loss for  18496 th iteration => 0.0177573495911\n",
      "Loss for  18497 th iteration => 0.0177561260807\n",
      "Loss for  18498 th iteration => 0.0177550700221\n",
      "Loss for  18499 th iteration => 0.0177537462904\n",
      "Loss for  18500 th iteration => 0.0177530113565\n",
      "Loss for  18501 th iteration => 0.0177514969999\n",
      "Loss for  18502 th iteration => 0.0177508720653\n",
      "Loss for  18503 th iteration => 0.0177493151442\n",
      "Loss for  18504 th iteration => 0.0177482804624\n",
      "Loss for  18505 th iteration => 0.0177472152902\n",
      "Loss for  18506 th iteration => 0.0177462936314\n",
      "Loss for  18507 th iteration => 0.0177455440298\n",
      "Loss for  18508 th iteration => 0.0177440349279\n",
      "Loss for  18509 th iteration => 0.0177434018417\n",
      "Loss for  18510 th iteration => 0.0177418280113\n",
      "Loss for  18511 th iteration => 0.0177409379772\n",
      "Loss for  18512 th iteration => 0.017739629651\n",
      "Loss for  18513 th iteration => 0.0177384270041\n",
      "Loss for  18514 th iteration => 0.0177377890316\n",
      "Loss for  18515 th iteration => 0.0177362165521\n",
      "Loss for  18516 th iteration => 0.0177352827101\n",
      "Loss for  18517 th iteration => 0.0177344073191\n",
      "Loss for  18518 th iteration => 0.0177330778161\n",
      "Loss for  18519 th iteration => 0.017732218443\n",
      "Loss for  18520 th iteration => 0.0177312491859\n",
      "Loss for  18521 th iteration => 0.0177300909484\n",
      "Loss for  18522 th iteration => 0.0177290092557\n",
      "Loss for  18523 th iteration => 0.0177276796663\n",
      "Loss for  18524 th iteration => 0.0177267916239\n",
      "Loss for  18525 th iteration => 0.0177255171623\n",
      "Loss for  18526 th iteration => 0.0177242770782\n",
      "Loss for  18527 th iteration => 0.0177236538284\n",
      "Loss for  18528 th iteration => 0.0177221010088\n",
      "Loss for  18529 th iteration => 0.0177211512015\n",
      "Loss for  18530 th iteration => 0.0177202645082\n",
      "Loss for  18531 th iteration => 0.0177189500095\n",
      "Loss for  18532 th iteration => 0.0177181130475\n",
      "Loss for  18533 th iteration => 0.0177170965216\n",
      "Loss for  18534 th iteration => 0.0177159882653\n",
      "Loss for  18535 th iteration => 0.0177148830672\n",
      "Loss for  18536 th iteration => 0.017713554298\n",
      "Loss for  18537 th iteration => 0.0177126693208\n",
      "Loss for  18538 th iteration => 0.017711429749\n",
      "Loss for  18539 th iteration => 0.0177103960361\n",
      "Loss for  18540 th iteration => 0.0177093656396\n",
      "Loss for  18541 th iteration => 0.0177081364792\n",
      "Loss for  18542 th iteration => 0.0177072606835\n",
      "Loss for  18543 th iteration => 0.0177059380964\n",
      "Loss for  18544 th iteration => 0.0177050535716\n",
      "Loss for  18545 th iteration => 0.017704126398\n",
      "Loss for  18546 th iteration => 0.0177029324822\n",
      "Loss for  18547 th iteration => 0.0177018847995\n",
      "Loss for  18548 th iteration => 0.0177005464912\n",
      "Loss for  18549 th iteration => 0.0176996736895\n",
      "Loss for  18550 th iteration => 0.0176983845703\n",
      "Loss for  18551 th iteration => 0.0176971660101\n",
      "Loss for  18552 th iteration => 0.0176965349272\n",
      "Loss for  18553 th iteration => 0.0176949850097\n",
      "Loss for  18554 th iteration => 0.0176940466093\n",
      "Loss for  18555 th iteration => 0.0176931597697\n",
      "Loss for  18556 th iteration => 0.0176918519146\n",
      "Loss for  18557 th iteration => 0.017690990215\n",
      "Loss for  18558 th iteration => 0.0176900150976\n",
      "Loss for  18559 th iteration => 0.0176888718223\n",
      "Loss for  18560 th iteration => 0.0176878001042\n",
      "Loss for  18561 th iteration => 0.0176864630661\n",
      "Loss for  18562 th iteration => 0.0176855928661\n",
      "Loss for  18563 th iteration => 0.0176843310561\n",
      "Loss for  18564 th iteration => 0.0176830828537\n",
      "Loss for  18565 th iteration => 0.0176824634073\n",
      "Loss for  18566 th iteration => 0.0176809301843\n",
      "Loss for  18567 th iteration => 0.0176799791549\n",
      "Loss for  18568 th iteration => 0.0176790831934\n",
      "Loss for  18569 th iteration => 0.0176777880885\n",
      "Loss for  18570 th iteration => 0.0176769445024\n",
      "Loss for  18571 th iteration => 0.0176759318918\n",
      "Loss for  18572 th iteration => 0.0176748288901\n",
      "Loss for  18573 th iteration => 0.0176737390129\n",
      "Loss for  18574 th iteration => 0.0176724103387\n",
      "Loss for  18575 th iteration => 0.0176712228599\n",
      "Loss for  18576 th iteration => 0.0176702876402\n",
      "Loss for  18577 th iteration => 0.0176692379544\n",
      "Loss for  18578 th iteration => 0.0176679383475\n",
      "Loss for  18579 th iteration => 0.0176673202782\n",
      "Loss for  18580 th iteration => 0.0176658798411\n",
      "Loss for  18581 th iteration => 0.0176650055785\n",
      "Loss for  18582 th iteration => 0.0176642046239\n",
      "Loss for  18583 th iteration => 0.0176627678878\n",
      "Loss for  18584 th iteration => 0.017662081122\n",
      "Loss for  18585 th iteration => 0.0176605586221\n",
      "Loss for  18586 th iteration => 0.017659691385\n",
      "Loss for  18587 th iteration => 0.0176584014639\n",
      "Loss for  18588 th iteration => 0.0176571507389\n",
      "Loss for  18589 th iteration => 0.0176563040885\n",
      "Loss for  18590 th iteration => 0.0176551802545\n",
      "Loss for  18591 th iteration => 0.0176539503283\n",
      "Loss for  18592 th iteration => 0.0176532442041\n",
      "Loss for  18593 th iteration => 0.0176518938554\n",
      "Loss for  18594 th iteration => 0.0176509666419\n",
      "Loss for  18595 th iteration => 0.0176501931315\n",
      "Loss for  18596 th iteration => 0.0176487333235\n",
      "Loss for  18597 th iteration => 0.0176480725461\n",
      "Loss for  18598 th iteration => 0.0176465443547\n",
      "Loss for  18599 th iteration => 0.0176456670434\n",
      "Loss for  18600 th iteration => 0.0176444101126\n",
      "Loss for  18601 th iteration => 0.0176431077155\n",
      "Loss for  18602 th iteration => 0.0176423333577\n",
      "Loss for  18603 th iteration => 0.0176411503556\n",
      "Loss for  18604 th iteration => 0.0176399756674\n",
      "Loss for  18605 th iteration => 0.017639200625\n",
      "Loss for  18606 th iteration => 0.0176379239736\n",
      "Loss for  18607 th iteration => 0.0176372759908\n",
      "Loss for  18608 th iteration => 0.0176359722752\n",
      "Loss for  18609 th iteration => 0.0176348853292\n",
      "Loss for  18610 th iteration => 0.0176340154047\n",
      "Loss for  18611 th iteration => 0.0176325548337\n",
      "Loss for  18612 th iteration => 0.0176317512551\n",
      "Loss for  18613 th iteration => 0.0176302780683\n",
      "Loss for  18614 th iteration => 0.0176293319463\n",
      "Loss for  18615 th iteration => 0.0176283095665\n",
      "Loss for  18616 th iteration => 0.0176269848239\n",
      "Loss for  18617 th iteration => 0.017626405398\n",
      "Loss for  18618 th iteration => 0.0176249351729\n",
      "Loss for  18619 th iteration => 0.0176240937678\n",
      "Loss for  18620 th iteration => 0.017623264545\n",
      "Loss for  18621 th iteration => 0.017621866326\n",
      "Loss for  18622 th iteration => 0.0176211505118\n",
      "Loss for  18623 th iteration => 0.0176196626526\n",
      "Loss for  18624 th iteration => 0.0176187848134\n",
      "Loss for  18625 th iteration => 0.0176175096534\n",
      "Loss for  18626 th iteration => 0.0176162721074\n",
      "Loss for  18627 th iteration => 0.0176154075558\n",
      "Loss for  18628 th iteration => 0.0176143171865\n",
      "Loss for  18629 th iteration => 0.0176130565145\n",
      "Loss for  18630 th iteration => 0.0176123963563\n",
      "Loss for  18631 th iteration => 0.0176110089529\n",
      "Loss for  18632 th iteration => 0.0176101191328\n",
      "Loss for  18633 th iteration => 0.0176093150081\n",
      "Loss for  18634 th iteration => 0.0176078959742\n",
      "Loss for  18635 th iteration => 0.0176072038933\n",
      "Loss for  18636 th iteration => 0.0176057111757\n",
      "Loss for  18637 th iteration => 0.0176048240707\n",
      "Loss for  18638 th iteration => 0.0176035794534\n",
      "Loss for  18639 th iteration => 0.0176022945878\n",
      "Loss for  18640 th iteration => 0.0176014964255\n",
      "Loss for  18641 th iteration => 0.0176003520351\n",
      "Loss for  18642 th iteration => 0.0175991419705\n",
      "Loss for  18643 th iteration => 0.0175984189175\n",
      "Loss for  18644 th iteration => 0.0175970966629\n",
      "Loss for  18645 th iteration => 0.0175961705737\n",
      "Loss for  18646 th iteration => 0.0175953841798\n",
      "Loss for  18647 th iteration => 0.0175939515375\n",
      "Loss for  18648 th iteration => 0.0175932760334\n",
      "Loss for  18649 th iteration => 0.0175917820003\n",
      "Loss for  18650 th iteration => 0.0175908860541\n",
      "Loss for  18651 th iteration => 0.0175896674259\n",
      "Loss for  18652 th iteration => 0.0175883443331\n",
      "Loss for  18653 th iteration => 0.0175875997935\n",
      "Loss for  18654 th iteration => 0.0175864131073\n",
      "Loss for  18655 th iteration => 0.017585242378\n",
      "Loss for  18656 th iteration => 0.0175844713563\n",
      "Loss for  18657 th iteration => 0.0175832094797\n",
      "Loss for  18658 th iteration => 0.0175825444013\n",
      "Loss for  18659 th iteration => 0.0175812675918\n",
      "Loss for  18660 th iteration => 0.0175801704216\n",
      "Loss for  18661 th iteration => 0.0175793206196\n",
      "Loss for  18662 th iteration => 0.0175778576369\n",
      "Loss for  18663 th iteration => 0.0175770742527\n",
      "Loss for  18664 th iteration => 0.0175755981188\n",
      "Loss for  18665 th iteration => 0.0175746752826\n",
      "Loss for  18666 th iteration => 0.0175734066106\n",
      "Loss for  18667 th iteration => 0.0175725061338\n",
      "Loss for  18668 th iteration => 0.0175716386872\n",
      "Loss for  18669 th iteration => 0.0175703407379\n",
      "Loss for  18670 th iteration => 0.0175694686823\n",
      "Loss for  18671 th iteration => 0.0175685506599\n",
      "Loss for  18672 th iteration => 0.017567378097\n",
      "Loss for  18673 th iteration => 0.017566334326\n",
      "Loss for  18674 th iteration => 0.0175650392597\n",
      "Loss for  18675 th iteration => 0.0175641566709\n",
      "Loss for  18676 th iteration => 0.0175629001102\n",
      "Loss for  18677 th iteration => 0.0175616847826\n",
      "Loss for  18678 th iteration => 0.0175610892259\n",
      "Loss for  18679 th iteration => 0.0175595364067\n",
      "Loss for  18680 th iteration => 0.017558625358\n",
      "Loss for  18681 th iteration => 0.0175577504349\n",
      "Loss for  18682 th iteration => 0.0175564634876\n",
      "Loss for  18683 th iteration => 0.0175556062327\n",
      "Loss for  18684 th iteration => 0.0175546577003\n",
      "Loss for  18685 th iteration => 0.0175535184327\n",
      "Loss for  18686 th iteration => 0.017552460533\n",
      "Loss for  18687 th iteration => 0.0175511653124\n",
      "Loss for  18688 th iteration => 0.0175502865391\n",
      "Loss for  18689 th iteration => 0.0175490476314\n",
      "Loss for  18690 th iteration => 0.017547814066\n",
      "Loss for  18691 th iteration => 0.0175472253965\n",
      "Loss for  18692 th iteration => 0.0175456852807\n",
      "Loss for  18693 th iteration => 0.0175447662831\n",
      "Loss for  18694 th iteration => 0.0175438853039\n",
      "Loss for  18695 th iteration => 0.0175426078945\n",
      "Loss for  18696 th iteration => 0.0175417620428\n",
      "Loss for  18697 th iteration => 0.0175407905489\n",
      "Loss for  18698 th iteration => 0.0175396770881\n",
      "Loss for  18699 th iteration => 0.01753860929\n",
      "Loss for  18700 th iteration => 0.0175373132601\n",
      "Loss for  18701 th iteration => 0.0175364388938\n",
      "Loss for  18702 th iteration => 0.0175352133862\n",
      "Loss for  18703 th iteration => 0.0175339665337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  18704 th iteration => 0.0175333827752\n",
      "Loss for  18705 th iteration => 0.0175318535111\n",
      "Loss for  18706 th iteration => 0.0175309287646\n",
      "Loss for  18707 th iteration => 0.0175300429432\n",
      "Loss for  18708 th iteration => 0.0175287738205\n",
      "Loss for  18709 th iteration => 0.0175279365218\n",
      "Loss for  18710 th iteration => 0.0175269484061\n",
      "Loss for  18711 th iteration => 0.0175258544627\n",
      "Loss for  18712 th iteration => 0.0175247803103\n",
      "Loss for  18713 th iteration => 0.0175234829477\n",
      "Loss for  18714 th iteration => 0.0175226134573\n",
      "Loss for  18715 th iteration => 0.0175213977883\n",
      "Loss for  18716 th iteration => 0.0175201428129\n",
      "Loss for  18717 th iteration => 0.0175192542057\n",
      "Loss for  18718 th iteration => 0.0175180240224\n",
      "Loss for  18719 th iteration => 0.0175173720977\n",
      "Loss for  18720 th iteration => 0.0175160105431\n",
      "Loss for  18721 th iteration => 0.0175154715904\n",
      "Loss for  18722 th iteration => 0.0175140825113\n",
      "Loss for  18723 th iteration => 0.0175131155992\n",
      "Loss for  18724 th iteration => 0.0175121494585\n",
      "Loss for  18725 th iteration => 0.017510816067\n",
      "Loss for  18726 th iteration => 0.0175099101789\n",
      "Loss for  18727 th iteration => 0.0175085280172\n",
      "Loss for  18728 th iteration => 0.0175076057133\n",
      "Loss for  18729 th iteration => 0.0175065913434\n",
      "Loss for  18730 th iteration => 0.0175052615952\n",
      "Loss for  18731 th iteration => 0.017504361498\n",
      "Loss for  18732 th iteration => 0.0175034921594\n",
      "Loss for  18733 th iteration => 0.0175022382707\n",
      "Loss for  18734 th iteration => 0.0175017019038\n",
      "Loss for  18735 th iteration => 0.0175001904579\n",
      "Loss for  18736 th iteration => 0.0174994690452\n",
      "Loss for  18737 th iteration => 0.0174981564714\n",
      "Loss for  18738 th iteration => 0.0174969965479\n",
      "Loss for  18739 th iteration => 0.0174961523479\n",
      "Loss for  18740 th iteration => 0.0174947269198\n",
      "Loss for  18741 th iteration => 0.0174938489283\n",
      "Loss for  18742 th iteration => 0.017492794424\n",
      "Loss for  18743 th iteration => 0.0174914928536\n",
      "Loss for  18744 th iteration => 0.0174905832664\n",
      "Loss for  18745 th iteration => 0.0174897136086\n",
      "Loss for  18746 th iteration => 0.0174884769082\n",
      "Loss for  18747 th iteration => 0.0174879150733\n",
      "Loss for  18748 th iteration => 0.0174864317366\n",
      "Loss for  18749 th iteration => 0.017485686435\n",
      "Loss for  18750 th iteration => 0.017484410973\n",
      "Loss for  18751 th iteration => 0.01748320622\n",
      "Loss for  18752 th iteration => 0.017482409118\n",
      "Loss for  18753 th iteration => 0.0174809515718\n",
      "Loss for  18754 th iteration => 0.0174801077213\n",
      "Loss for  18755 th iteration => 0.0174790222255\n",
      "Loss for  18756 th iteration => 0.017477743609\n",
      "Loss for  18757 th iteration => 0.0174768261907\n",
      "Loss for  18758 th iteration => 0.0174759575769\n",
      "Loss for  18759 th iteration => 0.0174747330781\n",
      "Loss for  18760 th iteration => 0.0174741538286\n",
      "Loss for  18761 th iteration => 0.0174726906238\n",
      "Loss for  18762 th iteration => 0.0174719292586\n",
      "Loss for  18763 th iteration => 0.0174706809536\n",
      "Loss for  18764 th iteration => 0.017469443677\n",
      "Loss for  18765 th iteration => 0.0174686814911\n",
      "Loss for  18766 th iteration => 0.0174672011382\n",
      "Loss for  18767 th iteration => 0.0174663829109\n",
      "Loss for  18768 th iteration => 0.0174652741063\n",
      "Loss for  18769 th iteration => 0.0174640140368\n",
      "Loss for  18770 th iteration => 0.0174630902102\n",
      "Loss for  18771 th iteration => 0.0174622237357\n",
      "Loss for  18772 th iteration => 0.0174610073133\n",
      "Loss for  18773 th iteration => 0.0174604173254\n",
      "Loss for  18774 th iteration => 0.0174589676391\n",
      "Loss for  18775 th iteration => 0.0174581966958\n",
      "Loss for  18776 th iteration => 0.0174569672674\n",
      "Loss for  18777 th iteration => 0.0174557077366\n",
      "Loss for  18778 th iteration => 0.0174549703006\n",
      "Loss for  18779 th iteration => 0.0174534749149\n",
      "Loss for  18780 th iteration => 0.0174526751775\n",
      "Loss for  18781 th iteration => 0.017451549523\n",
      "Loss for  18782 th iteration => 0.0174503042783\n",
      "Loss for  18783 th iteration => 0.017449375268\n",
      "Loss for  18784 th iteration => 0.0174485118042\n",
      "Loss for  18785 th iteration => 0.0174473000536\n",
      "Loss for  18786 th iteration => 0.0174467048517\n",
      "Loss for  18787 th iteration => 0.0174452632109\n",
      "Loss for  18788 th iteration => 0.0174444880546\n",
      "Loss for  18789 th iteration => 0.0174432706235\n",
      "Loss for  18790 th iteration => 0.0174419974029\n",
      "Loss for  18791 th iteration => 0.0174412762378\n",
      "Loss for  18792 th iteration => 0.0174397723059\n",
      "Loss for  18793 th iteration => 0.0174389850845\n",
      "Loss for  18794 th iteration => 0.0174378480143\n",
      "Loss for  18795 th iteration => 0.017436614446\n",
      "Loss for  18796 th iteration => 0.0174356813094\n",
      "Loss for  18797 th iteration => 0.0174348215416\n",
      "Loss for  18798 th iteration => 0.0174336116599\n",
      "Loss for  18799 th iteration => 0.0174330158057\n",
      "Loss for  18800 th iteration => 0.0174315776907\n",
      "Loss for  18801 th iteration => 0.0174308027495\n",
      "Loss for  18802 th iteration => 0.0174295916087\n",
      "Loss for  18803 th iteration => 0.0174283118362\n",
      "Loss for  18804 th iteration => 0.0174275998746\n",
      "Loss for  18805 th iteration => 0.017426092806\n",
      "Loss for  18806 th iteration => 0.0174253130975\n",
      "Loss for  18807 th iteration => 0.0174241691871\n",
      "Loss for  18808 th iteration => 0.0174229446278\n",
      "Loss for  18809 th iteration => 0.0174220082816\n",
      "Loss for  18810 th iteration => 0.0174211527399\n",
      "Loss for  18811 th iteration => 0.0174199424275\n",
      "Loss for  18812 th iteration => 0.0174193496773\n",
      "Loss for  18813 th iteration => 0.0174179113658\n",
      "Loss for  18814 th iteration => 0.0174171402847\n",
      "Loss for  18815 th iteration => 0.0174159307074\n",
      "Loss for  18816 th iteration => 0.0174146503267\n",
      "Loss for  18817 th iteration => 0.017413941683\n",
      "Loss for  18818 th iteration => 0.0174124359856\n",
      "Loss for  18819 th iteration => 0.0174116595996\n",
      "Loss for  18820 th iteration => 0.0174105127048\n",
      "Loss for  18821 th iteration => 0.0174092948908\n",
      "Loss for  18822 th iteration => 0.0174083561327\n",
      "Loss for  18823 th iteration => 0.0174075052188\n",
      "Loss for  18824 th iteration => 0.0174062925958\n",
      "Loss for  18825 th iteration => 0.0174057060334\n",
      "Loss for  18826 th iteration => 0.0174042644689\n",
      "Loss for  18827 th iteration => 0.0174035002385\n",
      "Loss for  18828 th iteration => 0.0174022883179\n",
      "Loss for  18829 th iteration => 0.0174010122736\n",
      "Loss for  18830 th iteration => 0.0174003020509\n",
      "Loss for  18831 th iteration => 0.0173988014782\n",
      "Loss for  18832 th iteration => 0.0173980249049\n",
      "Loss for  18833 th iteration => 0.0173968782785\n",
      "Loss for  18834 th iteration => 0.0173956652845\n",
      "Loss for  18835 th iteration => 0.0173947248123\n",
      "Loss for  18836 th iteration => 0.0173938788205\n",
      "Loss for  18837 th iteration => 0.0173926623578\n",
      "Loss for  18838 th iteration => 0.0173920845048\n",
      "Loss for  18839 th iteration => 0.0173906371874\n",
      "Loss for  18840 th iteration => 0.0173898822514\n",
      "Loss for  18841 th iteration => 0.0173886647665\n",
      "Loss for  18842 th iteration => 0.0173873971672\n",
      "Loss for  18843 th iteration => 0.0173866812955\n",
      "Loss for  18844 th iteration => 0.0173851889696\n",
      "Loss for  18845 th iteration => 0.0173844092694\n",
      "Loss for  18846 th iteration => 0.0173832656589\n",
      "Loss for  18847 th iteration => 0.0173820558434\n",
      "Loss for  18848 th iteration => 0.01738111427\n",
      "Loss for  18849 th iteration => 0.0173802734065\n",
      "Loss for  18850 th iteration => 0.0173790518675\n",
      "Loss for  18851 th iteration => 0.0173784847758\n",
      "Loss for  18852 th iteration => 0.0173770296706\n",
      "Loss for  18853 th iteration => 0.0173762860159\n",
      "Loss for  18854 th iteration => 0.0173750603187\n",
      "Loss for  18855 th iteration => 0.0173738045736\n",
      "Loss for  18856 th iteration => 0.0173730796753\n",
      "Loss for  18857 th iteration => 0.01737159819\n",
      "Loss for  18858 th iteration => 0.0173708129005\n",
      "Loss for  18859 th iteration => 0.0173696746295\n",
      "Loss for  18860 th iteration => 0.0173684665896\n",
      "Loss for  18861 th iteration => 0.0173675244565\n",
      "Loss for  18862 th iteration => 0.0173666888539\n",
      "Loss for  18863 th iteration => 0.017365461246\n",
      "Loss for  18864 th iteration => 0.0173649065757\n",
      "Loss for  18865 th iteration => 0.0173634420355\n",
      "Loss for  18866 th iteration => 0.017362711268\n",
      "Loss for  18867 th iteration => 0.0173614751897\n",
      "Loss for  18868 th iteration => 0.0173602341227\n",
      "Loss for  18869 th iteration => 0.017359497399\n",
      "Loss for  18870 th iteration => 0.0173580289062\n",
      "Loss for  18871 th iteration => 0.017357235965\n",
      "Loss for  18872 th iteration => 0.0173561050017\n",
      "Loss for  18873 th iteration => 0.0173548975347\n",
      "Loss for  18874 th iteration => 0.0173539553225\n",
      "Loss for  18875 th iteration => 0.017353125053\n",
      "Loss for  18876 th iteration => 0.017351890587\n",
      "Loss for  18877 th iteration => 0.0173513496709\n",
      "Loss for  18878 th iteration => 0.0173498743727\n",
      "Loss for  18879 th iteration => 0.0173491577798\n",
      "Loss for  18880 th iteration => 0.0173479095519\n",
      "Loss for  18881 th iteration => 0.0173466854974\n",
      "Loss for  18882 th iteration => 0.0173459346341\n",
      "Loss for  18883 th iteration => 0.0173444809157\n",
      "Loss for  18884 th iteration => 0.0173436785953\n",
      "Loss for  18885 th iteration => 0.0173425566098\n",
      "Loss for  18886 th iteration => 0.0173413486814\n",
      "Loss for  18887 th iteration => 0.0173404068191\n",
      "Loss for  18888 th iteration => 0.0173395819053\n",
      "Loss for  18889 th iteration => 0.0173383399613\n",
      "Loss for  18890 th iteration => 0.0173378138594\n",
      "Loss for  18891 th iteration => 0.0173363267501\n",
      "Loss for  18892 th iteration => 0.017335625354\n",
      "Loss for  18893 th iteration => 0.0173343635424\n",
      "Loss for  18894 th iteration => 0.0173331584253\n",
      "Loss for  18895 th iteration => 0.0173323915133\n",
      "Loss for  18896 th iteration => 0.017330954042\n",
      "Loss for  18897 th iteration => 0.0173301408954\n",
      "Loss for  18898 th iteration => 0.0173290293076\n",
      "Loss for  18899 th iteration => 0.017327820025\n",
      "Loss for  18900 th iteration => 0.0173268788979\n",
      "Loss for  18901 th iteration => 0.0173260593211\n",
      "Loss for  18902 th iteration => 0.0173248094202\n",
      "Loss for  18903 th iteration => 0.0173242989651\n",
      "Loss for  18904 th iteration => 0.0173227992168\n",
      "Loss for  18905 th iteration => 0.0173221138185\n",
      "Loss for  18906 th iteration => 0.0173208372686\n",
      "Loss for  18907 th iteration => 0.0173196526712\n",
      "Loss for  18908 th iteration => 0.0173188681401\n",
      "Loss for  18909 th iteration => 0.0173174481298\n",
      "Loss for  18910 th iteration => 0.0173166229454\n",
      "Loss for  18911 th iteration => 0.017315522965\n",
      "Loss for  18912 th iteration => 0.0173143115546\n",
      "Loss for  18913 th iteration => 0.0173133715105\n",
      "Loss for  18914 th iteration => 0.0173125572189\n",
      "Loss for  18915 th iteration => 0.0173112989991\n",
      "Loss for  18916 th iteration => 0.017310804834\n",
      "Loss for  18917 th iteration => 0.0173092918062\n",
      "Loss for  18918 th iteration => 0.0173086230226\n",
      "Loss for  18919 th iteration => 0.0173073308126\n",
      "Loss for  18920 th iteration => 0.017306168031\n",
      "Loss for  18921 th iteration => 0.017305364594\n",
      "Loss for  18922 th iteration => 0.0173039630421\n",
      "Loss for  18923 th iteration => 0.0173031248051\n",
      "Loss for  18924 th iteration => 0.0173020374656\n",
      "Loss for  18925 th iteration => 0.017300823254\n",
      "Loss for  18926 th iteration => 0.0172998846088\n",
      "Loss for  18927 th iteration => 0.0172990755233\n",
      "Loss for  18928 th iteration => 0.0172978087201\n",
      "Loss for  18929 th iteration => 0.0172973313303\n",
      "Loss for  18930 th iteration => 0.0172958045388\n",
      "Loss for  18931 th iteration => 0.0172951528334\n",
      "Loss for  18932 th iteration => 0.0172938442361\n",
      "Loss for  18933 th iteration => 0.0172927043269\n",
      "Loss for  18934 th iteration => 0.0172918809338\n",
      "Loss for  18935 th iteration => 0.0172904986569\n",
      "Loss for  18936 th iteration => 0.0172896465176\n",
      "Loss for  18937 th iteration => 0.0172885727043\n",
      "Loss for  18938 th iteration => 0.0172873551028\n",
      "Loss for  18939 th iteration => 0.0172864181449\n",
      "Loss for  18940 th iteration => 0.0172856141645\n",
      "Loss for  18941 th iteration => 0.0172843385938\n",
      "Loss for  18942 th iteration => 0.0172838783337\n",
      "Loss for  18943 th iteration => 0.0172823374238\n",
      "Loss for  18944 th iteration => 0.0172817031327\n",
      "Loss for  18945 th iteration => 0.0172803775831\n",
      "Loss for  18946 th iteration => 0.0172792614028\n",
      "Loss for  18947 th iteration => 0.0172784172016\n",
      "Loss for  18948 th iteration => 0.0172770548647\n",
      "Loss for  18949 th iteration => 0.0172761881123\n",
      "Loss for  18950 th iteration => 0.0172751285856\n",
      "Loss for  18951 th iteration => 0.0172739070768\n",
      "Loss for  18952 th iteration => 0.017272972071\n",
      "Loss for  18953 th iteration => 0.017272173077\n",
      "Loss for  18954 th iteration => 0.0172708886215\n",
      "Loss for  18955 th iteration => 0.0172704457365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  18956 th iteration => 0.0172688904614\n",
      "Loss for  18957 th iteration => 0.0172682738149\n",
      "Loss for  18958 th iteration => 0.0172669308832\n",
      "Loss for  18959 th iteration => 0.0172658391211\n",
      "Loss for  18960 th iteration => 0.0172649734252\n",
      "Loss for  18961 th iteration => 0.017263631567\n",
      "Loss for  18962 th iteration => 0.0172627496063\n",
      "Loss for  18963 th iteration => 0.0172617096272\n",
      "Loss for  18964 th iteration => 0.0172607778089\n",
      "Loss for  18965 th iteration => 0.0172595689808\n",
      "Loss for  18966 th iteration => 0.0172587743918\n",
      "Loss for  18967 th iteration => 0.0172574788889\n",
      "Loss for  18968 th iteration => 0.0172566593028\n",
      "Loss for  18969 th iteration => 0.0172554593142\n",
      "Loss for  18970 th iteration => 0.0172548708929\n",
      "Loss for  18971 th iteration => 0.0172534960904\n",
      "Loss for  18972 th iteration => 0.0172524475134\n",
      "Loss for  18973 th iteration => 0.0172515417568\n",
      "Loss for  18974 th iteration => 0.017250235268\n",
      "Loss for  18975 th iteration => 0.0172493228916\n",
      "Loss for  18976 th iteration => 0.017248322599\n",
      "Loss for  18977 th iteration => 0.0172473542075\n",
      "Loss for  18978 th iteration => 0.01724616511\n",
      "Loss for  18979 th iteration => 0.017245374235\n",
      "Loss for  18980 th iteration => 0.0172440781385\n",
      "Loss for  18981 th iteration => 0.0172432622757\n",
      "Loss for  18982 th iteration => 0.0172420490784\n",
      "Loss for  18983 th iteration => 0.0172414871097\n",
      "Loss for  18984 th iteration => 0.0172400826155\n",
      "Loss for  18985 th iteration => 0.0172390746462\n",
      "Loss for  18986 th iteration => 0.0172381313739\n",
      "Loss for  18987 th iteration => 0.0172368583024\n",
      "Loss for  18988 th iteration => 0.0172359172644\n",
      "Loss for  18989 th iteration => 0.0172349545199\n",
      "Loss for  18990 th iteration => 0.0172339516658\n",
      "Loss for  18991 th iteration => 0.0172327813858\n",
      "Loss for  18992 th iteration => 0.0172319939279\n",
      "Loss for  18993 th iteration => 0.0172306975314\n",
      "Loss for  18994 th iteration => 0.0172298851001\n",
      "Loss for  18995 th iteration => 0.017228659599\n",
      "Loss for  18996 th iteration => 0.0172281225675\n",
      "Loss for  18997 th iteration => 0.0172266902364\n",
      "Loss for  18998 th iteration => 0.0172257206938\n",
      "Loss for  18999 th iteration => 0.0172247420581\n",
      "Loss for  19000 th iteration => 0.0172235007523\n",
      "Loss for  19001 th iteration => 0.0172225325333\n",
      "Loss for  19002 th iteration => 0.0172216055337\n",
      "Loss for  19003 th iteration => 0.017220569996\n",
      "Loss for  19004 th iteration => 0.0172194247927\n",
      "Loss for  19005 th iteration => 0.017218344921\n",
      "Loss for  19006 th iteration => 0.0172175638973\n",
      "Loss for  19007 th iteration => 0.0172163783785\n",
      "Loss for  19008 th iteration => 0.0172153786717\n",
      "Loss for  19009 th iteration => 0.0172147596828\n",
      "Loss for  19010 th iteration => 0.017213267839\n",
      "Loss for  19011 th iteration => 0.0172125014514\n",
      "Loss for  19012 th iteration => 0.0172111858031\n",
      "Loss for  19013 th iteration => 0.0172104102994\n",
      "Loss for  19014 th iteration => 0.0172091654363\n",
      "Loss for  19015 th iteration => 0.0172082805136\n",
      "Loss for  19016 th iteration => 0.017207206048\n",
      "Loss for  19017 th iteration => 0.0172060942038\n",
      "Loss for  19018 th iteration => 0.0172049822031\n",
      "Loss for  19019 th iteration => 0.0172042465484\n",
      "Loss for  19020 th iteration => 0.0172030188026\n",
      "Loss for  19021 th iteration => 0.0172020541974\n",
      "Loss for  19022 th iteration => 0.017201411042\n",
      "Loss for  19023 th iteration => 0.017199937028\n",
      "Loss for  19024 th iteration => 0.0171991673147\n",
      "Loss for  19025 th iteration => 0.0171978580629\n",
      "Loss for  19026 th iteration => 0.0171970792536\n",
      "Loss for  19027 th iteration => 0.0171958249455\n",
      "Loss for  19028 th iteration => 0.0171947263439\n",
      "Loss for  19029 th iteration => 0.0171940505938\n",
      "Loss for  19030 th iteration => 0.0171926470235\n",
      "Loss for  19031 th iteration => 0.0171917191575\n",
      "Loss for  19032 th iteration => 0.0171909228609\n",
      "Loss for  19033 th iteration => 0.0171896453013\n",
      "Loss for  19034 th iteration => 0.0171888354715\n",
      "Loss for  19035 th iteration => 0.0171879427341\n",
      "Loss for  19036 th iteration => 0.0171868290035\n",
      "Loss for  19037 th iteration => 0.0171858579802\n",
      "Loss for  19038 th iteration => 0.0171845456448\n",
      "Loss for  19039 th iteration => 0.0171837729777\n",
      "Loss for  19040 th iteration => 0.0171825063497\n",
      "Loss for  19041 th iteration => 0.0171814248614\n",
      "Loss for  19042 th iteration => 0.0171807428795\n",
      "Loss for  19043 th iteration => 0.0171793372586\n",
      "Loss for  19044 th iteration => 0.0171784176212\n",
      "Loss for  19045 th iteration => 0.0171776246389\n",
      "Loss for  19046 th iteration => 0.017176346839\n",
      "Loss for  19047 th iteration => 0.0171755294861\n",
      "Loss for  19048 th iteration => 0.0171743542286\n",
      "Loss for  19049 th iteration => 0.0171737593736\n",
      "Loss for  19050 th iteration => 0.0171724073679\n",
      "Loss for  19051 th iteration => 0.0171713526099\n",
      "Loss for  19052 th iteration => 0.0171704691107\n",
      "Loss for  19053 th iteration => 0.0171691625815\n",
      "Loss for  19054 th iteration => 0.0171682653258\n",
      "Loss for  19055 th iteration => 0.0171672710463\n",
      "Loss for  19056 th iteration => 0.0171663131514\n",
      "Loss for  19057 th iteration => 0.0171651359689\n",
      "Loss for  19058 th iteration => 0.017164349181\n",
      "Loss for  19059 th iteration => 0.0171630682171\n",
      "Loss for  19060 th iteration => 0.0171622570048\n",
      "Loss for  19061 th iteration => 0.0171610585872\n",
      "Loss for  19062 th iteration => 0.0171605062029\n",
      "Loss for  19063 th iteration => 0.0171591039116\n",
      "Loss for  19064 th iteration => 0.0171581156\n",
      "Loss for  19065 th iteration => 0.017157168987\n",
      "Loss for  19066 th iteration => 0.0171559155141\n",
      "Loss for  19067 th iteration => 0.0171549714571\n",
      "Loss for  19068 th iteration => 0.0171540369895\n",
      "Loss for  19069 th iteration => 0.0171530225574\n",
      "Loss for  19070 th iteration => 0.0171518741475\n",
      "Loss for  19071 th iteration => 0.0171510927973\n",
      "Loss for  19072 th iteration => 0.0171498094304\n",
      "Loss for  19073 th iteration => 0.0171490036172\n",
      "Loss for  19074 th iteration => 0.0171477846014\n",
      "Loss for  19075 th iteration => 0.0171472704657\n",
      "Loss for  19076 th iteration => 0.0171458231742\n",
      "Loss for  19077 th iteration => 0.0171448949109\n",
      "Loss for  19078 th iteration => 0.0171438915089\n",
      "Loss for  19079 th iteration => 0.0171426886733\n",
      "Loss for  19080 th iteration => 0.0171419688427\n",
      "Loss for  19081 th iteration => 0.0171408470483\n",
      "Loss for  19082 th iteration => 0.0171397686515\n",
      "Loss for  19083 th iteration => 0.0171386676474\n",
      "Loss for  19084 th iteration => 0.0171375064111\n",
      "Loss for  19085 th iteration => 0.0171365700618\n",
      "Loss for  19086 th iteration => 0.0171357721758\n",
      "Loss for  19087 th iteration => 0.0171345266072\n",
      "Loss for  19088 th iteration => 0.0171340595902\n",
      "Loss for  19089 th iteration => 0.0171325570024\n",
      "Loss for  19090 th iteration => 0.0171319238534\n",
      "Loss for  19091 th iteration => 0.0171306412809\n",
      "Loss for  19092 th iteration => 0.0171295171245\n",
      "Loss for  19093 th iteration => 0.0171287105384\n",
      "Loss for  19094 th iteration => 0.0171273660143\n",
      "Loss for  19095 th iteration => 0.017126512686\n",
      "Loss for  19096 th iteration => 0.0171254638285\n",
      "Loss for  19097 th iteration => 0.017124276651\n",
      "Loss for  19098 th iteration => 0.0171233496519\n",
      "Loss for  19099 th iteration => 0.0171225613325\n",
      "Loss for  19100 th iteration => 0.0171212920985\n",
      "Loss for  19101 th iteration => 0.0171205541912\n",
      "Loss for  19102 th iteration => 0.0171195671808\n",
      "Loss for  19103 th iteration => 0.0171185622642\n",
      "Loss for  19104 th iteration => 0.0171175008312\n",
      "Loss for  19105 th iteration => 0.0171163088631\n",
      "Loss for  19106 th iteration => 0.0171154325969\n",
      "Loss for  19107 th iteration => 0.0171142854101\n",
      "Loss for  19108 th iteration => 0.0171131054284\n",
      "Loss for  19109 th iteration => 0.0171125358015\n",
      "Loss for  19110 th iteration => 0.0171110655629\n",
      "Loss for  19111 th iteration => 0.0171101512715\n",
      "Loss for  19112 th iteration => 0.0171093680561\n",
      "Loss for  19113 th iteration => 0.0171080967055\n",
      "Loss for  19114 th iteration => 0.0171073418382\n",
      "Loss for  19115 th iteration => 0.0171063925823\n",
      "Loss for  19116 th iteration => 0.0171053531035\n",
      "Loss for  19117 th iteration => 0.0171043161644\n",
      "Loss for  19118 th iteration => 0.0171031185257\n",
      "Loss for  19119 th iteration => 0.0171022508684\n",
      "Loss for  19120 th iteration => 0.0171010838982\n",
      "Loss for  19121 th iteration => 0.0170999292763\n",
      "Loss for  19122 th iteration => 0.0170993493502\n",
      "Loss for  19123 th iteration => 0.0170978751545\n",
      "Loss for  19124 th iteration => 0.0170969767471\n",
      "Loss for  19125 th iteration => 0.0170959199063\n",
      "Loss for  19126 th iteration => 0.0170951359206\n",
      "Loss for  19127 th iteration => 0.0170940098809\n",
      "Loss for  19128 th iteration => 0.0170933258771\n",
      "Loss for  19129 th iteration => 0.0170921564269\n",
      "Loss for  19130 th iteration => 0.0170911082794\n",
      "Loss for  19131 th iteration => 0.0170900679418\n",
      "Loss for  19132 th iteration => 0.0170889153238\n",
      "Loss for  19133 th iteration => 0.0170881449161\n",
      "Loss for  19134 th iteration => 0.0170867805975\n",
      "Loss for  19135 th iteration => 0.0170859526189\n",
      "Loss for  19136 th iteration => 0.0170848849671\n",
      "Loss for  19137 th iteration => 0.0170840163002\n",
      "Loss for  19138 th iteration => 0.0170828159965\n",
      "Loss for  19139 th iteration => 0.0170820271433\n",
      "Loss for  19140 th iteration => 0.0170807678386\n",
      "Loss for  19141 th iteration => 0.0170799553217\n",
      "Loss for  19142 th iteration => 0.0170787871014\n",
      "Loss for  19143 th iteration => 0.0170782074086\n",
      "Loss for  19144 th iteration => 0.0170768685522\n",
      "Loss for  19145 th iteration => 0.0170758244938\n",
      "Loss for  19146 th iteration => 0.0170749492742\n",
      "Loss for  19147 th iteration => 0.0170736699155\n",
      "Loss for  19148 th iteration => 0.0170727657455\n",
      "Loss for  19149 th iteration => 0.0170717940759\n",
      "Loss for  19150 th iteration => 0.0170708330331\n",
      "Loss for  19151 th iteration => 0.0170696776091\n",
      "Loss for  19152 th iteration => 0.0170688976384\n",
      "Loss for  19153 th iteration => 0.017067632381\n",
      "Loss for  19154 th iteration => 0.0170668286227\n",
      "Loss for  19155 th iteration => 0.0170656262758\n",
      "Loss for  19156 th iteration => 0.0170651067846\n",
      "Loss for  19157 th iteration => 0.0170636946827\n",
      "Loss for  19158 th iteration => 0.0170627459832\n",
      "Loss for  19159 th iteration => 0.0170617790058\n",
      "Loss for  19160 th iteration => 0.0170605746084\n",
      "Loss for  19161 th iteration => 0.017059603359\n",
      "Loss for  19162 th iteration => 0.0170587165014\n",
      "Loss for  19163 th iteration => 0.0170576741323\n",
      "Loss for  19164 th iteration => 0.0170565643807\n",
      "Loss for  19165 th iteration => 0.0170555031625\n",
      "Loss for  19166 th iteration => 0.0170547408509\n",
      "Loss for  19167 th iteration => 0.0170535702065\n",
      "Loss for  19168 th iteration => 0.0170525794809\n",
      "Loss for  19169 th iteration => 0.0170519977586\n",
      "Loss for  19170 th iteration => 0.0170505036572\n",
      "Loss for  19171 th iteration => 0.0170497848337\n",
      "Loss for  19172 th iteration => 0.0170484602177\n",
      "Loss for  19173 th iteration => 0.0170477325149\n",
      "Loss for  19174 th iteration => 0.0170464641697\n",
      "Loss for  19175 th iteration => 0.0170454175676\n",
      "Loss for  19176 th iteration => 0.017044718397\n",
      "Loss for  19177 th iteration => 0.0170433563323\n",
      "Loss for  19178 th iteration => 0.0170424451521\n",
      "Loss for  19179 th iteration => 0.0170416617387\n",
      "Loss for  19180 th iteration => 0.0170404064718\n",
      "Loss for  19181 th iteration => 0.0170395996383\n",
      "Loss for  19182 th iteration => 0.0170384348008\n",
      "Loss for  19183 th iteration => 0.017037868489\n",
      "Loss for  19184 th iteration => 0.0170365192799\n",
      "Loss for  19185 th iteration => 0.0170354957426\n",
      "Loss for  19186 th iteration => 0.0170346081826\n",
      "Loss for  19187 th iteration => 0.0170333443079\n",
      "Loss for  19188 th iteration => 0.0170324343589\n",
      "Loss for  19189 th iteration => 0.0170314835632\n",
      "Loss for  19190 th iteration => 0.0170305099901\n",
      "Loss for  19191 th iteration => 0.017029367007\n",
      "Loss for  19192 th iteration => 0.0170285932327\n",
      "Loss for  19193 th iteration => 0.0170273312209\n",
      "Loss for  19194 th iteration => 0.0170265338813\n",
      "Loss for  19195 th iteration => 0.0170253315889\n",
      "Loss for  19196 th iteration => 0.0170248308105\n",
      "Loss for  19197 th iteration => 0.0170234014473\n",
      "Loss for  19198 th iteration => 0.0170224820422\n",
      "Loss for  19199 th iteration => 0.0170214940274\n",
      "Loss for  19200 th iteration => 0.0170203118093\n",
      "Loss for  19201 th iteration => 0.0170193285519\n",
      "Loss for  19202 th iteration => 0.0170184702125\n",
      "Loss for  19203 th iteration => 0.0170174077278\n",
      "Loss for  19204 th iteration => 0.0170163166386\n",
      "Loss for  19205 th iteration => 0.0170152477517\n",
      "Loss for  19206 th iteration => 0.0170145119839\n",
      "Loss for  19207 th iteration => 0.0170133232507\n",
      "Loss for  19208 th iteration => 0.0170123560604\n",
      "Loss for  19209 th iteration => 0.0170117701681\n",
      "Loss for  19210 th iteration => 0.0170102841643\n",
      "Loss for  19211 th iteration => 0.0170097923039\n",
      "Loss for  19212 th iteration => 0.0170082652732\n",
      "Loss for  19213 th iteration => 0.0170075420667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  19214 th iteration => 0.017006264858\n",
      "Loss for  19215 th iteration => 0.0170051880102\n",
      "Loss for  19216 th iteration => 0.0170043223317\n",
      "Loss for  19217 th iteration => 0.0170033191462\n",
      "Loss for  19218 th iteration => 0.0170024386959\n",
      "Loss for  19219 th iteration => 0.0170015573636\n",
      "Loss for  19220 th iteration => 0.0170002578664\n",
      "Loss for  19221 th iteration => 0.0169993972677\n",
      "Loss for  19222 th iteration => 0.0169983351309\n",
      "Loss for  19223 th iteration => 0.0169972835695\n",
      "Loss for  19224 th iteration => 0.0169967376507\n",
      "Loss for  19225 th iteration => 0.0169952437463\n",
      "Loss for  19226 th iteration => 0.0169945184278\n",
      "Loss for  19227 th iteration => 0.0169932187823\n",
      "Loss for  19228 th iteration => 0.0169921997863\n",
      "Loss for  19229 th iteration => 0.0169912530907\n",
      "Loss for  19230 th iteration => 0.0169903548722\n",
      "Loss for  19231 th iteration => 0.0169893420248\n",
      "Loss for  19232 th iteration => 0.0169885960384\n",
      "Loss for  19233 th iteration => 0.0169871951306\n",
      "Loss for  19234 th iteration => 0.0169864378602\n",
      "Loss for  19235 th iteration => 0.0169852760119\n",
      "Loss for  19236 th iteration => 0.0169842982623\n",
      "Loss for  19237 th iteration => 0.0169837048518\n",
      "Loss for  19238 th iteration => 0.016982240673\n",
      "Loss for  19239 th iteration => 0.0169815132226\n",
      "Loss for  19240 th iteration => 0.0169802133161\n",
      "Loss for  19241 th iteration => 0.0169794772047\n",
      "Loss for  19242 th iteration => 0.0169782334553\n",
      "Loss for  19243 th iteration => 0.0169771811027\n",
      "Loss for  19244 th iteration => 0.016976501122\n",
      "Loss for  19245 th iteration => 0.0169751426648\n",
      "Loss for  19246 th iteration => 0.0169742379645\n",
      "Loss for  19247 th iteration => 0.0169734621794\n",
      "Loss for  19248 th iteration => 0.0169722152752\n",
      "Loss for  19249 th iteration => 0.016971416499\n",
      "Loss for  19250 th iteration => 0.0169702557082\n",
      "Loss for  19251 th iteration => 0.0169696948598\n",
      "Loss for  19252 th iteration => 0.016968351667\n",
      "Loss for  19253 th iteration => 0.0169673507126\n",
      "Loss for  19254 th iteration => 0.0169664542834\n",
      "Loss for  19255 th iteration => 0.0169652136147\n",
      "Loss for  19256 th iteration => 0.0169642977516\n",
      "Loss for  19257 th iteration => 0.0169633721462\n",
      "Loss for  19258 th iteration => 0.0169623873834\n",
      "Loss for  19259 th iteration => 0.0169612612759\n",
      "Loss for  19260 th iteration => 0.0169604966119\n",
      "Loss for  19261 th iteration => 0.0169592414187\n",
      "Loss for  19262 th iteration => 0.0169584535789\n",
      "Loss for  19263 th iteration => 0.0169572490652\n",
      "Loss for  19264 th iteration => 0.0169567638811\n",
      "Loss for  19265 th iteration => 0.0169553274447\n",
      "Loss for  19266 th iteration => 0.0169544471134\n",
      "Loss for  19267 th iteration => 0.0169534338848\n",
      "Loss for  19268 th iteration => 0.0169522929342\n",
      "Loss for  19269 th iteration => 0.0169515403049\n",
      "Loss for  19270 th iteration => 0.0169504942236\n",
      "Loss for  19271 th iteration => 0.016949394033\n",
      "Loss for  19272 th iteration => 0.016948352368\n",
      "Loss for  19273 th iteration => 0.0169471993975\n",
      "Loss for  19274 th iteration => 0.0169462863409\n",
      "Loss for  19275 th iteration => 0.0169455107049\n",
      "Loss for  19276 th iteration => 0.0169442703917\n",
      "Loss for  19277 th iteration => 0.0169435385807\n",
      "Loss for  19278 th iteration => 0.0169425809549\n",
      "Loss for  19279 th iteration => 0.0169415852169\n",
      "Loss for  19280 th iteration => 0.0169405585577\n",
      "Loss for  19281 th iteration => 0.0169393795893\n",
      "Loss for  19282 th iteration => 0.0169385324531\n",
      "Loss for  19283 th iteration => 0.0169373961956\n",
      "Loss for  19284 th iteration => 0.0169362517333\n",
      "Loss for  19285 th iteration => 0.016935683397\n",
      "Loss for  19286 th iteration => 0.0169342449688\n",
      "Loss for  19287 th iteration => 0.0169333517418\n",
      "Loss for  19288 th iteration => 0.0169325857483\n",
      "Loss for  19289 th iteration => 0.0169313386086\n",
      "Loss for  19290 th iteration => 0.016930580212\n",
      "Loss for  19291 th iteration => 0.0169296815335\n",
      "Loss for  19292 th iteration => 0.0169286301626\n",
      "Loss for  19293 th iteration => 0.0169276397507\n",
      "Loss for  19294 th iteration => 0.0169264540007\n",
      "Loss for  19295 th iteration => 0.0169256163538\n",
      "Loss for  19296 th iteration => 0.0169244487802\n",
      "Loss for  19297 th iteration => 0.0169233466823\n",
      "Loss for  19298 th iteration => 0.0169225377063\n",
      "Loss for  19299 th iteration => 0.0169214883049\n",
      "Loss for  19300 th iteration => 0.01692034318\n",
      "Loss for  19301 th iteration => 0.016919439503\n",
      "Loss for  19302 th iteration => 0.0169186582979\n",
      "Loss for  19303 th iteration => 0.016917429823\n",
      "Loss for  19304 th iteration => 0.0169166974742\n",
      "Loss for  19305 th iteration => 0.0169157496723\n",
      "Loss for  19306 th iteration => 0.0169147499039\n",
      "Loss for  19307 th iteration => 0.0169137313029\n",
      "Loss for  19308 th iteration => 0.0169125464242\n",
      "Loss for  19309 th iteration => 0.0169117115545\n",
      "Loss for  19310 th iteration => 0.0169105690707\n",
      "Loss for  19311 th iteration => 0.0169094331985\n",
      "Loss for  19312 th iteration => 0.0169088644854\n",
      "Loss for  19313 th iteration => 0.0169074273439\n",
      "Loss for  19314 th iteration => 0.0169065446454\n",
      "Loss for  19315 th iteration => 0.016905773957\n",
      "Loss for  19316 th iteration => 0.016904537755\n",
      "Loss for  19317 th iteration => 0.016903777279\n",
      "Loss for  19318 th iteration => 0.016902891924\n",
      "Loss for  19319 th iteration => 0.0169018330459\n",
      "Loss for  19320 th iteration => 0.0169008525948\n",
      "Loss for  19321 th iteration => 0.01689966085\n",
      "Loss for  19322 th iteration => 0.0168988355175\n",
      "Loss for  19323 th iteration => 0.0168976598225\n",
      "Loss for  19324 th iteration => 0.0168965666198\n",
      "Loss for  19325 th iteration => 0.0168957563817\n",
      "Loss for  19326 th iteration => 0.0168947170794\n",
      "Loss for  19327 th iteration => 0.0168938585146\n",
      "Loss for  19328 th iteration => 0.0168926964873\n",
      "Loss for  19329 th iteration => 0.0168919110466\n",
      "Loss for  19330 th iteration => 0.0168906928321\n",
      "Loss for  19331 th iteration => 0.0168898850131\n",
      "Loss for  19332 th iteration => 0.0168887456826\n",
      "Loss for  19333 th iteration => 0.0168881868677\n",
      "Loss for  19334 th iteration => 0.0168868576213\n",
      "Loss for  19335 th iteration => 0.0168858655432\n",
      "Loss for  19336 th iteration => 0.0168849763471\n",
      "Loss for  19337 th iteration => 0.0168837559583\n",
      "Loss for  19338 th iteration => 0.016882829604\n",
      "Loss for  19339 th iteration => 0.0168819353387\n",
      "Loss for  19340 th iteration => 0.0168809357576\n",
      "Loss for  19341 th iteration => 0.0168798405066\n",
      "Loss for  19342 th iteration => 0.0168790684853\n",
      "Loss for  19343 th iteration => 0.0168778396069\n",
      "Loss for  19344 th iteration => 0.0168770449682\n",
      "Loss for  19345 th iteration => 0.0168758531648\n",
      "Loss for  19346 th iteration => 0.0168753839923\n",
      "Loss for  19347 th iteration => 0.0168739435305\n",
      "Loss for  19348 th iteration => 0.0168730946396\n",
      "Loss for  19349 th iteration => 0.0168720662851\n",
      "Loss for  19350 th iteration => 0.016870962283\n",
      "Loss for  19351 th iteration => 0.0168701861212\n",
      "Loss for  19352 th iteration => 0.0168689430422\n",
      "Loss for  19353 th iteration => 0.0168682438284\n",
      "Loss for  19354 th iteration => 0.0168669202294\n",
      "Loss for  19355 th iteration => 0.016866250621\n",
      "Loss for  19356 th iteration => 0.016864983454\n",
      "Loss for  19357 th iteration => 0.016864242876\n",
      "Loss for  19358 th iteration => 0.0168630896207\n",
      "Loss for  19359 th iteration => 0.0168621224197\n",
      "Loss for  19360 th iteration => 0.0168612252885\n",
      "Loss for  19361 th iteration => 0.0168603603668\n",
      "Loss for  19362 th iteration => 0.016859160745\n",
      "Loss for  19363 th iteration => 0.0168582288077\n",
      "Loss for  19364 th iteration => 0.0168572848921\n",
      "Loss for  19365 th iteration => 0.0168561409239\n",
      "Loss for  19366 th iteration => 0.0168554180581\n",
      "Loss for  19367 th iteration => 0.0168541205092\n",
      "Loss for  19368 th iteration => 0.0168534276546\n",
      "Loss for  19369 th iteration => 0.016852120924\n",
      "Loss for  19370 th iteration => 0.0168512141555\n",
      "Loss for  19371 th iteration => 0.0168504390641\n",
      "Loss for  19372 th iteration => 0.016849220261\n",
      "Loss for  19373 th iteration => 0.016848423149\n",
      "Loss for  19374 th iteration => 0.0168472885704\n",
      "Loss for  19375 th iteration => 0.0168467224103\n",
      "Loss for  19376 th iteration => 0.0168454247695\n",
      "Loss for  19377 th iteration => 0.0168444005527\n",
      "Loss for  19378 th iteration => 0.0168435511852\n",
      "Loss for  19379 th iteration => 0.0168423148425\n",
      "Loss for  19380 th iteration => 0.0168414174918\n",
      "Loss for  19381 th iteration => 0.0168404894335\n",
      "Loss for  19382 th iteration => 0.0168395315967\n",
      "Loss for  19383 th iteration => 0.0168384195578\n",
      "Loss for  19384 th iteration => 0.0168376595456\n",
      "Loss for  19385 th iteration => 0.0168364283687\n",
      "Loss for  19386 th iteration => 0.0168356460525\n",
      "Loss for  19387 th iteration => 0.0168344519565\n",
      "Loss for  19388 th iteration => 0.0168339870262\n",
      "Loss for  19389 th iteration => 0.0168325631563\n",
      "Loss for  19390 th iteration => 0.0168317009699\n",
      "Loss for  19391 th iteration => 0.0168306937858\n",
      "Loss for  19392 th iteration => 0.0168295830413\n",
      "Loss for  19393 th iteration => 0.0168285717257\n",
      "Loss for  19394 th iteration => 0.0168277861782\n",
      "Loss for  19395 th iteration => 0.0168266897979\n",
      "Loss for  19396 th iteration => 0.0168256735218\n",
      "Loss for  19397 th iteration => 0.0168248519347\n",
      "Loss for  19398 th iteration => 0.0168239373354\n",
      "Loss for  19399 th iteration => 0.016822716058\n",
      "Loss for  19400 th iteration => 0.0168218258213\n",
      "Loss for  19401 th iteration => 0.0168208290461\n",
      "Loss for  19402 th iteration => 0.0168197472122\n",
      "Loss for  19403 th iteration => 0.0168192602014\n",
      "Loss for  19404 th iteration => 0.0168177609775\n",
      "Loss for  19405 th iteration => 0.0168170833202\n",
      "Loss for  19406 th iteration => 0.0168157892774\n",
      "Loss for  19407 th iteration => 0.0168148118354\n",
      "Loss for  19408 th iteration => 0.0168138594396\n",
      "Loss for  19409 th iteration => 0.0168130005439\n",
      "Loss for  19410 th iteration => 0.0168119797295\n",
      "Loss for  19411 th iteration => 0.0168112658899\n",
      "Loss for  19412 th iteration => 0.0168098940483\n",
      "Loss for  19413 th iteration => 0.0168091554113\n",
      "Loss for  19414 th iteration => 0.0168080110502\n",
      "Loss for  19415 th iteration => 0.0168070502567\n",
      "Loss for  19416 th iteration => 0.0168061872678\n",
      "Loss for  19417 th iteration => 0.0168052656239\n",
      "Loss for  19418 th iteration => 0.0168041755838\n",
      "Loss for  19419 th iteration => 0.0168031497943\n",
      "Loss for  19420 th iteration => 0.0168023100508\n",
      "Loss for  19421 th iteration => 0.0168010786338\n",
      "Loss for  19422 th iteration => 0.0168001860238\n",
      "Loss for  19423 th iteration => 0.0167992600077\n",
      "Loss for  19424 th iteration => 0.0167983083814\n",
      "Loss for  19425 th iteration => 0.0167972007873\n",
      "Loss for  19426 th iteration => 0.0167964451281\n",
      "Loss for  19427 th iteration => 0.0167952191649\n",
      "Loss for  19428 th iteration => 0.016794441466\n",
      "Loss for  19429 th iteration => 0.0167932480403\n",
      "Loss for  19430 th iteration => 0.0167927877515\n",
      "Loss for  19431 th iteration => 0.0167913689076\n",
      "Loss for  19432 th iteration => 0.0167905175655\n",
      "Loss for  19433 th iteration => 0.0167895077031\n",
      "Loss for  19434 th iteration => 0.0167884113136\n",
      "Loss for  19435 th iteration => 0.0167873973688\n",
      "Loss for  19436 th iteration => 0.0167863984584\n",
      "Loss for  19437 th iteration => 0.0167856937636\n",
      "Loss for  19438 th iteration => 0.0167843958179\n",
      "Loss for  19439 th iteration => 0.0167837564677\n",
      "Loss for  19440 th iteration => 0.0167827645604\n",
      "Loss for  19441 th iteration => 0.0167815421291\n",
      "Loss for  19442 th iteration => 0.016780764116\n",
      "Loss for  19443 th iteration => 0.0167795646563\n",
      "Loss for  19444 th iteration => 0.0167787859018\n",
      "Loss for  19445 th iteration => 0.0167779415058\n",
      "Loss for  19446 th iteration => 0.0167766219228\n",
      "Loss for  19447 th iteration => 0.0167759530726\n",
      "Loss for  19448 th iteration => 0.0167746601144\n",
      "Loss for  19449 th iteration => 0.0167737178559\n",
      "Loss for  19450 th iteration => 0.0167729900532\n",
      "Loss for  19451 th iteration => 0.0167717175295\n",
      "Loss for  19452 th iteration => 0.0167710187877\n",
      "Loss for  19453 th iteration => 0.0167698009861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  19454 th iteration => 0.0167690783601\n",
      "Loss for  19455 th iteration => 0.016767926861\n",
      "Loss for  19456 th iteration => 0.0167669825267\n",
      "Loss for  19457 th iteration => 0.016766050327\n",
      "Loss for  19458 th iteration => 0.0167649369994\n",
      "Loss for  19459 th iteration => 0.0167644769013\n",
      "Loss for  19460 th iteration => 0.0167629759134\n",
      "Loss for  19461 th iteration => 0.0167622979748\n",
      "Loss for  19462 th iteration => 0.0167610260265\n",
      "Loss for  19463 th iteration => 0.0167600282862\n",
      "Loss for  19464 th iteration => 0.0167591213911\n",
      "Loss for  19465 th iteration => 0.016758219971\n",
      "Loss for  19466 th iteration => 0.0167572630806\n",
      "Loss for  19467 th iteration => 0.016756500166\n",
      "Loss for  19468 th iteration => 0.0167551753188\n",
      "Loss for  19469 th iteration => 0.0167544049783\n",
      "Loss for  19470 th iteration => 0.0167533029978\n",
      "Loss for  19471 th iteration => 0.0167523150085\n",
      "Loss for  19472 th iteration => 0.0167514998444\n",
      "Loss for  19473 th iteration => 0.0167505438453\n",
      "Loss for  19474 th iteration => 0.0167494887233\n",
      "Loss for  19475 th iteration => 0.0167484433352\n",
      "Loss for  19476 th iteration => 0.0167476336182\n",
      "Loss for  19477 th iteration => 0.016746390046\n",
      "Loss for  19478 th iteration => 0.0167455239688\n",
      "Loss for  19479 th iteration => 0.0167445787817\n",
      "Loss for  19480 th iteration => 0.0167436570483\n",
      "Loss for  19481 th iteration => 0.0167425415008\n",
      "Loss for  19482 th iteration => 0.0167417969389\n",
      "Loss for  19483 th iteration => 0.0167405725615\n",
      "Loss for  19484 th iteration => 0.0167398063273\n",
      "Loss for  19485 th iteration => 0.0167386184089\n",
      "Loss for  19486 th iteration => 0.016738161793\n",
      "Loss for  19487 th iteration => 0.0167367533854\n",
      "Loss for  19488 th iteration => 0.016735901752\n",
      "Loss for  19489 th iteration => 0.0167349027924\n",
      "Loss for  19490 th iteration => 0.0167338089384\n",
      "Loss for  19491 th iteration => 0.0167328065271\n",
      "Loss for  19492 th iteration => 0.01673203139\n",
      "Loss for  19493 th iteration => 0.0167309438108\n",
      "Loss for  19494 th iteration => 0.0167299443089\n",
      "Loss for  19495 th iteration => 0.0167291161635\n",
      "Loss for  19496 th iteration => 0.0167282317047\n",
      "Loss for  19497 th iteration => 0.0167270078412\n",
      "Loss for  19498 th iteration => 0.0167261455785\n",
      "Loss for  19499 th iteration => 0.0167251402899\n",
      "Loss for  19500 th iteration => 0.0167240829262\n",
      "Loss for  19501 th iteration => 0.0167235907358\n",
      "Loss for  19502 th iteration => 0.0167221176782\n",
      "Loss for  19503 th iteration => 0.0167214435161\n",
      "Loss for  19504 th iteration => 0.0167201607722\n",
      "Loss for  19505 th iteration => 0.0167192105318\n",
      "Loss for  19506 th iteration => 0.0167182437844\n",
      "Loss for  19507 th iteration => 0.0167174238224\n",
      "Loss for  19508 th iteration => 0.0167163829137\n",
      "Loss for  19509 th iteration => 0.0167154090251\n",
      "Loss for  19510 th iteration => 0.0167145560614\n",
      "Loss for  19511 th iteration => 0.0167134460995\n",
      "Loss for  19512 th iteration => 0.016712571923\n",
      "Loss for  19513 th iteration => 0.0167114841649\n",
      "Loss for  19514 th iteration => 0.0167106311046\n",
      "Loss for  19515 th iteration => 0.0167098372452\n",
      "Loss for  19516 th iteration => 0.0167085368447\n",
      "Loss for  19517 th iteration => 0.016707864732\n",
      "Loss for  19518 th iteration => 0.0167065935962\n",
      "Loss for  19519 th iteration => 0.016705614256\n",
      "Loss for  19520 th iteration => 0.016704695323\n",
      "Loss for  19521 th iteration => 0.016703824073\n",
      "Loss for  19522 th iteration => 0.016702842191\n",
      "Loss for  19523 th iteration => 0.0167021151649\n",
      "Loss for  19524 th iteration => 0.0167007732835\n",
      "Loss for  19525 th iteration => 0.0167000341024\n",
      "Loss for  19526 th iteration => 0.0166989121515\n",
      "Loss for  19527 th iteration => 0.0166979581827\n",
      "Loss for  19528 th iteration => 0.0166971116172\n",
      "Loss for  19529 th iteration => 0.0166962002458\n",
      "Loss for  19530 th iteration => 0.0166951192292\n",
      "Loss for  19531 th iteration => 0.0166941138747\n",
      "Loss for  19532 th iteration => 0.0166932751368\n",
      "Loss for  19533 th iteration => 0.0166920657702\n",
      "Loss for  19534 th iteration => 0.0166911783856\n",
      "Loss for  19535 th iteration => 0.0166902746792\n",
      "Loss for  19536 th iteration => 0.0166893226621\n",
      "Loss for  19537 th iteration => 0.0166882315817\n",
      "Loss for  19538 th iteration => 0.016687493301\n",
      "Loss for  19539 th iteration => 0.0166862751004\n",
      "Loss for  19540 th iteration => 0.0166855154145\n",
      "Loss for  19541 th iteration => 0.0166843222411\n",
      "Loss for  19542 th iteration => 0.0166838924315\n",
      "Loss for  19543 th iteration => 0.0166824591742\n",
      "Loss for  19544 th iteration => 0.0166816565259\n",
      "Loss for  19545 th iteration => 0.0166806196651\n",
      "Loss for  19546 th iteration => 0.0166795756736\n",
      "Loss for  19547 th iteration => 0.0166787773896\n",
      "Loss for  19548 th iteration => 0.0166775945049\n",
      "Loss for  19549 th iteration => 0.0166768882266\n",
      "Loss for  19550 th iteration => 0.0166756172031\n",
      "Loss for  19551 th iteration => 0.0166749389891\n",
      "Loss for  19552 th iteration => 0.0166736925128\n",
      "Loss for  19553 th iteration => 0.016673007996\n",
      "Loss for  19554 th iteration => 0.0166718376327\n",
      "Loss for  19555 th iteration => 0.0166709378886\n",
      "Loss for  19556 th iteration => 0.0166699839805\n",
      "Loss for  19557 th iteration => 0.0166692291716\n",
      "Loss for  19558 th iteration => 0.0166679729741\n",
      "Loss for  19559 th iteration => 0.0166671482692\n",
      "Loss for  19560 th iteration => 0.0166661354328\n",
      "Loss for  19561 th iteration => 0.0166650775065\n",
      "Loss for  19562 th iteration => 0.0166643426383\n",
      "Loss for  19563 th iteration => 0.0166631023299\n",
      "Loss for  19564 th iteration => 0.0166623960725\n",
      "Loss for  19565 th iteration => 0.0166611289511\n",
      "Loss for  19566 th iteration => 0.0166604715208\n",
      "Loss for  19567 th iteration => 0.0166595229506\n",
      "Loss for  19568 th iteration => 0.0166583020301\n",
      "Loss for  19569 th iteration => 0.0166575516304\n",
      "Loss for  19570 th iteration => 0.016656352944\n",
      "Loss for  19571 th iteration => 0.0166555845193\n",
      "Loss for  19572 th iteration => 0.0166547582873\n",
      "Loss for  19573 th iteration => 0.0166534594303\n",
      "Loss for  19574 th iteration => 0.0166527985972\n",
      "Loss for  19575 th iteration => 0.0166515182659\n",
      "Loss for  19576 th iteration => 0.0166506051565\n",
      "Loss for  19577 th iteration => 0.0166498723496\n",
      "Loss for  19578 th iteration => 0.0166486339112\n",
      "Loss for  19579 th iteration => 0.0166479292496\n",
      "Loss for  19580 th iteration => 0.0166467122847\n",
      "Loss for  19581 th iteration => 0.0166460502769\n",
      "Loss for  19582 th iteration => 0.0166448633608\n",
      "Loss for  19583 th iteration => 0.0166439863762\n",
      "Loss for  19584 th iteration => 0.0166430120986\n",
      "Loss for  19585 th iteration => 0.0166419452321\n",
      "Loss for  19586 th iteration => 0.0166414714426\n",
      "Loss for  19587 th iteration => 0.0166400077883\n",
      "Loss for  19588 th iteration => 0.0166393392441\n",
      "Loss for  19589 th iteration => 0.01663807164\n",
      "Loss for  19590 th iteration => 0.0166371309196\n",
      "Loss for  19591 th iteration => 0.0166361753238\n",
      "Loss for  19592 th iteration => 0.0166353563235\n",
      "Loss for  19593 th iteration => 0.016634330766\n",
      "Loss for  19594 th iteration => 0.0166333572158\n",
      "Loss for  19595 th iteration => 0.0166325259394\n",
      "Loss for  19596 th iteration => 0.0166314131848\n",
      "Loss for  19597 th iteration => 0.0166305611671\n",
      "Loss for  19598 th iteration => 0.0166294701165\n",
      "Loss for  19599 th iteration => 0.016628631097\n",
      "Loss for  19600 th iteration => 0.0166278446113\n",
      "Loss for  19601 th iteration => 0.0166265568915\n",
      "Loss for  19602 th iteration => 0.016625891206\n",
      "Loss for  19603 th iteration => 0.0166246283239\n",
      "Loss for  19604 th iteration => 0.0166236724112\n",
      "Loss for  19605 th iteration => 0.0166227449699\n",
      "Loss for  19606 th iteration => 0.0166218988181\n",
      "Loss for  19607 th iteration => 0.0166209027859\n",
      "Loss for  19608 th iteration => 0.0166199192736\n",
      "Loss for  19609 th iteration => 0.0166190841954\n",
      "Loss for  19610 th iteration => 0.0166179783609\n",
      "Loss for  19611 th iteration => 0.0166171226986\n",
      "Loss for  19612 th iteration => 0.0166160384038\n",
      "Loss for  19613 th iteration => 0.0166152074118\n",
      "Loss for  19614 th iteration => 0.0166144151365\n",
      "Loss for  19615 th iteration => 0.0166131273863\n",
      "Loss for  19616 th iteration => 0.0166124649213\n",
      "Loss for  19617 th iteration => 0.0166112043117\n",
      "Loss for  19618 th iteration => 0.0166102380283\n",
      "Loss for  19619 th iteration => 0.0166093319876\n",
      "Loss for  19620 th iteration => 0.0166084669954\n",
      "Loss for  19621 th iteration => 0.016607496979\n",
      "Loss for  19622 th iteration => 0.0166067779212\n",
      "Loss for  19623 th iteration => 0.0166054506063\n",
      "Loss for  19624 th iteration => 0.0166047221237\n",
      "Loss for  19625 th iteration => 0.0166036086245\n",
      "Loss for  19626 th iteration => 0.0166026713211\n",
      "Loss for  19627 th iteration => 0.0166018211824\n",
      "Loss for  19628 th iteration => 0.0166009417415\n",
      "Loss for  19629 th iteration => 0.0165998469042\n",
      "Loss for  19630 th iteration => 0.016598880649\n",
      "Loss for  19631 th iteration => 0.0165980216339\n",
      "Loss for  19632 th iteration => 0.0165968487029\n",
      "Loss for  19633 th iteration => 0.0165959523905\n",
      "Loss for  19634 th iteration => 0.016595081729\n",
      "Loss for  19635 th iteration => 0.0165941158587\n",
      "Loss for  19636 th iteration => 0.0165930483622\n",
      "Loss for  19637 th iteration => 0.0165920566641\n",
      "Loss for  19638 th iteration => 0.0165913271234\n",
      "Loss for  19639 th iteration => 0.0165902173473\n",
      "Loss for  19640 th iteration => 0.0165892798548\n",
      "Loss for  19641 th iteration => 0.0165884365338\n",
      "Loss for  19642 th iteration => 0.0165875552844\n",
      "Loss for  19643 th iteration => 0.016586460903\n",
      "Loss for  19644 th iteration => 0.016585497754\n",
      "Loss for  19645 th iteration => 0.0165846382476\n",
      "Loss for  19646 th iteration => 0.0165834647438\n",
      "Loss for  19647 th iteration => 0.0165825719831\n",
      "Loss for  19648 th iteration => 0.016581703926\n",
      "Loss for  19649 th iteration => 0.0165807381222\n",
      "Loss for  19650 th iteration => 0.0165796685392\n",
      "Loss for  19651 th iteration => 0.0165786821414\n",
      "Loss for  19652 th iteration => 0.0165779549952\n",
      "Loss for  19653 th iteration => 0.0165768455175\n",
      "Loss for  19654 th iteration => 0.016575911198\n",
      "Loss for  19655 th iteration => 0.0165750702906\n",
      "Loss for  19656 th iteration => 0.0165741943436\n",
      "Loss for  19657 th iteration => 0.0165733067379\n",
      "Loss for  19658 th iteration => 0.0165721586619\n",
      "Loss for  19659 th iteration => 0.0165712846078\n",
      "Loss for  19660 th iteration => 0.016570120775\n",
      "Loss for  19661 th iteration => 0.0165691854517\n",
      "Loss for  19662 th iteration => 0.0165681337807\n",
      "Loss for  19663 th iteration => 0.0165675418804\n",
      "Loss for  19664 th iteration => 0.0165661863211\n",
      "Loss for  19665 th iteration => 0.0165653996989\n",
      "Loss for  19666 th iteration => 0.0165645739053\n",
      "Loss for  19667 th iteration => 0.016563471425\n",
      "Loss for  19668 th iteration => 0.0165626255494\n",
      "Loss for  19669 th iteration => 0.0165616072822\n",
      "Loss for  19670 th iteration => 0.0165610044175\n",
      "Loss for  19671 th iteration => 0.0165597610385\n",
      "Loss for  19672 th iteration => 0.0165588153228\n",
      "Loss for  19673 th iteration => 0.0165579442613\n",
      "Loss for  19674 th iteration => 0.0165567779136\n",
      "Loss for  19675 th iteration => 0.0165558429072\n",
      "Loss for  19676 th iteration => 0.0165548005628\n",
      "Loss for  19677 th iteration => 0.0165542025693\n",
      "Loss for  19678 th iteration => 0.0165528570207\n",
      "Loss for  19679 th iteration => 0.0165526283478\n",
      "Loss for  19680 th iteration => 0.0165510422036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  19681 th iteration => 0.01655032616\n",
      "Loss for  19682 th iteration => 0.0165492112173\n",
      "Loss for  19683 th iteration => 0.0165482894323\n",
      "Loss for  19684 th iteration => 0.0165474300122\n",
      "Loss for  19685 th iteration => 0.0165465820722\n",
      "Loss for  19686 th iteration => 0.0165456716779\n",
      "Loss for  19687 th iteration => 0.0165445557679\n",
      "Loss for  19688 th iteration => 0.0165436588813\n",
      "Loss for  19689 th iteration => 0.0165425232364\n",
      "Loss for  19690 th iteration => 0.0165415750936\n",
      "Loss for  19691 th iteration => 0.0165405392924\n",
      "Loss for  19692 th iteration => 0.0165399348704\n",
      "Loss for  19693 th iteration => 0.0165385950669\n",
      "Loss for  19694 th iteration => 0.0165380727347\n",
      "Loss for  19695 th iteration => 0.0165370106094\n",
      "Loss for  19696 th iteration => 0.0165358984329\n",
      "Loss for  19697 th iteration => 0.0165350682904\n",
      "Loss for  19698 th iteration => 0.0165339772128\n",
      "Loss for  19699 th iteration => 0.0165331605027\n",
      "Loss for  19700 th iteration => 0.0165323816165\n",
      "Loss for  19701 th iteration => 0.016531098792\n",
      "Loss for  19702 th iteration => 0.0165304503394\n",
      "Loss for  19703 th iteration => 0.0165291827983\n",
      "Loss for  19704 th iteration => 0.016528254003\n",
      "Loss for  19705 th iteration => 0.0165273274257\n",
      "Loss for  19706 th iteration => 0.0165265042169\n",
      "Loss for  19707 th iteration => 0.0165255043043\n",
      "Loss for  19708 th iteration => 0.0165245323138\n",
      "Loss for  19709 th iteration => 0.0165237209688\n",
      "Loss for  19710 th iteration => 0.0165226131856\n",
      "Loss for  19711 th iteration => 0.0165217817631\n",
      "Loss for  19712 th iteration => 0.0165206949834\n",
      "Loss for  19713 th iteration => 0.0165198806579\n",
      "Loss for  19714 th iteration => 0.0165191029588\n",
      "Loss for  19715 th iteration => 0.0165178193522\n",
      "Loss for  19716 th iteration => 0.0165171747419\n",
      "Loss for  19717 th iteration => 0.0165159028746\n",
      "Loss for  19718 th iteration => 0.0165149773549\n",
      "Loss for  19719 th iteration => 0.0165140527681\n",
      "Loss for  19720 th iteration => 0.0165132345064\n",
      "Loss for  19721 th iteration => 0.0165122322959\n",
      "Loss for  19722 th iteration => 0.0165112687037\n",
      "Loss for  19723 th iteration => 0.0165107121508\n",
      "Loss for  19724 th iteration => 0.016509369459\n",
      "Loss for  19725 th iteration => 0.0165085349422\n",
      "Loss for  19726 th iteration => 0.0165074540999\n",
      "Loss for  19727 th iteration => 0.0165066000967\n",
      "Loss for  19728 th iteration => 0.0165055610968\n",
      "Loss for  19729 th iteration => 0.0165049904495\n",
      "Loss for  19730 th iteration => 0.0165037626303\n",
      "Loss for  19731 th iteration => 0.0165027782807\n",
      "Loss for  19732 th iteration => 0.0165019559017\n",
      "Loss for  19733 th iteration => 0.0165007983363\n",
      "Loss for  19734 th iteration => 0.0164998365977\n",
      "Loss for  19735 th iteration => 0.0164990592014\n",
      "Loss for  19736 th iteration => 0.0164980192941\n",
      "Loss for  19737 th iteration => 0.0164970806739\n",
      "Loss for  19738 th iteration => 0.016496501793\n",
      "Loss for  19739 th iteration => 0.0164951810645\n",
      "Loss for  19740 th iteration => 0.0164943489719\n",
      "Loss for  19741 th iteration => 0.0164932688866\n",
      "Loss for  19742 th iteration => 0.016492417322\n",
      "Loss for  19743 th iteration => 0.0164913583911\n",
      "Loss for  19744 th iteration => 0.0164908135949\n",
      "Loss for  19745 th iteration => 0.016489562354\n",
      "Loss for  19746 th iteration => 0.0164886187184\n",
      "Loss for  19747 th iteration => 0.0164877587765\n",
      "Loss for  19748 th iteration => 0.0164866422538\n",
      "Loss for  19749 th iteration => 0.0164856448262\n",
      "Loss for  19750 th iteration => 0.0164849065561\n",
      "Loss for  19751 th iteration => 0.0164838306676\n",
      "Loss for  19752 th iteration => 0.0164829157105\n",
      "Loss for  19753 th iteration => 0.0164823154429\n",
      "Loss for  19754 th iteration => 0.0164810162413\n",
      "Loss for  19755 th iteration => 0.0164801864241\n",
      "Loss for  19756 th iteration => 0.0164791072391\n",
      "Loss for  19757 th iteration => 0.0164782579677\n",
      "Loss for  19758 th iteration => 0.0164771991344\n",
      "Loss for  19759 th iteration => 0.0164763802833\n",
      "Loss for  19760 th iteration => 0.0164756087987\n",
      "Loss for  19761 th iteration => 0.0164743149483\n",
      "Loss for  19762 th iteration => 0.016473690972\n",
      "Loss for  19763 th iteration => 0.0164724582915\n",
      "Loss for  19764 th iteration => 0.0164714852204\n",
      "Loss for  19765 th iteration => 0.0164706277356\n",
      "Loss for  19766 th iteration => 0.016469717891\n",
      "Loss for  19767 th iteration => 0.0164688302721\n",
      "Loss for  19768 th iteration => 0.0164680477791\n",
      "Loss for  19769 th iteration => 0.016466817017\n",
      "Loss for  19770 th iteration => 0.0164660295019\n",
      "Loss for  19771 th iteration => 0.0164650020977\n",
      "Loss for  19772 th iteration => 0.0164640159783\n",
      "Loss for  19773 th iteration => 0.0164632298887\n",
      "Loss for  19774 th iteration => 0.016462327603\n",
      "Loss for  19775 th iteration => 0.0164612779173\n",
      "Loss for  19776 th iteration => 0.0164603040435\n",
      "Loss for  19777 th iteration => 0.016459479191\n",
      "Loss for  19778 th iteration => 0.0164583300149\n",
      "Loss for  19779 th iteration => 0.0164574412816\n",
      "Loss for  19780 th iteration => 0.0164565763277\n",
      "Loss for  19781 th iteration => 0.0164556319687\n",
      "Loss for  19782 th iteration => 0.0164545843149\n",
      "Loss for  19783 th iteration => 0.0164536184617\n",
      "Loss for  19784 th iteration => 0.0164528763955\n",
      "Loss for  19785 th iteration => 0.0164518066014\n",
      "Loss for  19786 th iteration => 0.0164508655947\n",
      "Loss for  19787 th iteration => 0.0164500275214\n",
      "Loss for  19788 th iteration => 0.016449184022\n",
      "Loss for  19789 th iteration => 0.0164480856968\n",
      "Loss for  19790 th iteration => 0.0164471631705\n",
      "Loss for  19791 th iteration => 0.0164462900397\n",
      "Loss for  19792 th iteration => 0.0164451769277\n",
      "Loss for  19793 th iteration => 0.016444257848\n",
      "Loss for  19794 th iteration => 0.016443437234\n",
      "Loss for  19795 th iteration => 0.0164424515661\n",
      "Loss for  19796 th iteration => 0.0164414322243\n",
      "Loss for  19797 th iteration => 0.0164404423542\n",
      "Loss for  19798 th iteration => 0.016439741162\n",
      "Loss for  19799 th iteration => 0.0164386335011\n",
      "Loss for  19800 th iteration => 0.0164377331482\n",
      "Loss for  19801 th iteration => 0.0164368487963\n",
      "Loss for  19802 th iteration => 0.016436058118\n",
      "Loss for  19803 th iteration => 0.0164349160739\n",
      "Loss for  19804 th iteration => 0.016434040043\n",
      "Loss for  19805 th iteration => 0.0164331234285\n",
      "Loss for  19806 th iteration => 0.0164320427805\n",
      "Loss for  19807 th iteration => 0.0164310966381\n",
      "Loss for  19808 th iteration => 0.0164303162441\n",
      "Loss for  19809 th iteration => 0.016429293339\n",
      "Loss for  19810 th iteration => 0.0164283094606\n",
      "Loss for  19811 th iteration => 0.0164275292973\n",
      "Loss for  19812 th iteration => 0.0164266498121\n",
      "Loss for  19813 th iteration => 0.0164254964638\n",
      "Loss for  19814 th iteration => 0.0164246434349\n",
      "Loss for  19815 th iteration => 0.0164236890801\n",
      "Loss for  19816 th iteration => 0.0164226417191\n",
      "Loss for  19817 th iteration => 0.0164219484181\n",
      "Loss for  19818 th iteration => 0.0164209630982\n",
      "Loss for  19819 th iteration => 0.0164199830723\n",
      "Loss for  19820 th iteration => 0.0164189514886\n",
      "Loss for  19821 th iteration => 0.0164181916487\n",
      "Loss for  19822 th iteration => 0.0164169965752\n",
      "Loss for  19823 th iteration => 0.0164161601823\n",
      "Loss for  19824 th iteration => 0.0164152451607\n",
      "Loss for  19825 th iteration => 0.0164143584686\n",
      "Loss for  19826 th iteration => 0.0164132797498\n",
      "Loss for  19827 th iteration => 0.0164125794449\n",
      "Loss for  19828 th iteration => 0.0164113861153\n",
      "Loss for  19829 th iteration => 0.0164106661433\n",
      "Loss for  19830 th iteration => 0.0164094933673\n",
      "Loss for  19831 th iteration => 0.0164088070326\n",
      "Loss for  19832 th iteration => 0.0164079235943\n",
      "Loss for  19833 th iteration => 0.0164067522655\n",
      "Loss for  19834 th iteration => 0.0164060212264\n",
      "Loss for  19835 th iteration => 0.0164048581817\n",
      "Loss for  19836 th iteration => 0.0164041200765\n",
      "Loss for  19837 th iteration => 0.0164030549777\n",
      "Loss for  19838 th iteration => 0.0164021908213\n",
      "Loss for  19839 th iteration => 0.0164012569042\n",
      "Loss for  19840 th iteration => 0.0164002021907\n",
      "Loss for  19841 th iteration => 0.0163991893463\n",
      "Loss for  19842 th iteration => 0.0163982955195\n",
      "Loss for  19843 th iteration => 0.0163975771818\n",
      "Loss for  19844 th iteration => 0.0163964056466\n",
      "Loss for  19845 th iteration => 0.0163957100432\n",
      "Loss for  19846 th iteration => 0.016394841402\n",
      "Loss for  19847 th iteration => 0.0163936676786\n",
      "Loss for  19848 th iteration => 0.0163929418858\n",
      "Loss for  19849 th iteration => 0.0163917764679\n",
      "Loss for  19850 th iteration => 0.016391043588\n",
      "Loss for  19851 th iteration => 0.0163899529054\n",
      "Loss for  19852 th iteration => 0.0163891389838\n",
      "Loss for  19853 th iteration => 0.0163881578773\n",
      "Loss for  19854 th iteration => 0.0163871425951\n",
      "Loss for  19855 th iteration => 0.0163863605897\n",
      "Loss for  19856 th iteration => 0.016385244571\n",
      "Loss for  19857 th iteration => 0.0163845272988\n",
      "Loss for  19858 th iteration => 0.0163833574179\n",
      "Loss for  19859 th iteration => 0.0163826209206\n",
      "Loss for  19860 th iteration => 0.016381485318\n",
      "Loss for  19861 th iteration => 0.0163810416351\n",
      "Loss for  19862 th iteration => 0.0163797111295\n",
      "Loss for  19863 th iteration => 0.0163788633318\n",
      "Loss for  19864 th iteration => 0.0163779293867\n",
      "Loss for  19865 th iteration => 0.0163768793096\n",
      "Loss for  19866 th iteration => 0.016375911454\n",
      "Loss for  19867 th iteration => 0.0163751695329\n",
      "Loss for  19868 th iteration => 0.0163741192689\n",
      "Loss for  19869 th iteration => 0.016373176729\n",
      "Loss for  19870 th iteration => 0.0163723592202\n",
      "Loss for  19871 th iteration => 0.0163715229\n",
      "Loss for  19872 th iteration => 0.016370352317\n",
      "Loss for  19873 th iteration => 0.0163695304598\n",
      "Loss for  19874 th iteration => 0.0163685561277\n",
      "Loss for  19875 th iteration => 0.0163675426117\n",
      "Loss for  19876 th iteration => 0.0163668112388\n",
      "Loss for  19877 th iteration => 0.0163658780528\n",
      "Loss for  19878 th iteration => 0.0163648688649\n",
      "Loss for  19879 th iteration => 0.0163638803882\n",
      "Loss for  19880 th iteration => 0.0163630884772\n",
      "Loss for  19881 th iteration => 0.0163619352871\n",
      "Loss for  19882 th iteration => 0.0163610667329\n",
      "Loss for  19883 th iteration => 0.0163602029885\n",
      "Loss for  19884 th iteration => 0.0163592762362\n",
      "Loss for  19885 th iteration => 0.0163582318061\n",
      "Loss for  19886 th iteration => 0.0163572892981\n",
      "Loss for  19887 th iteration => 0.0163565438634\n",
      "Loss for  19888 th iteration => 0.0163554964298\n",
      "Loss for  19889 th iteration => 0.0163545582035\n",
      "Loss for  19890 th iteration => 0.0163537355543\n",
      "Loss for  19891 th iteration => 0.0163529017008\n",
      "Loss for  19892 th iteration => 0.0163518110227\n",
      "Loss for  19893 th iteration => 0.0163509061804\n",
      "Loss for  19894 th iteration => 0.0163500340062\n",
      "Loss for  19895 th iteration => 0.0163489414827\n",
      "Loss for  19896 th iteration => 0.0163480198265\n",
      "Loss for  19897 th iteration => 0.0163472283595\n",
      "Loss for  19898 th iteration => 0.0163462326003\n",
      "Loss for  19899 th iteration => 0.0163452435986\n",
      "Loss for  19900 th iteration => 0.0163444915626\n",
      "Loss for  19901 th iteration => 0.0163435971065\n",
      "Loss for  19902 th iteration => 0.0163424749333\n",
      "Loss for  19903 th iteration => 0.016341612598\n",
      "Loss for  19904 th iteration => 0.0163406838346\n",
      "Loss for  19905 th iteration => 0.0163396326215\n",
      "Loss for  19906 th iteration => 0.0163389568431\n",
      "Loss for  19907 th iteration => 0.0163379739765\n",
      "Loss for  19908 th iteration => 0.016337008267\n",
      "Loss for  19909 th iteration => 0.0163359858524\n",
      "Loss for  19910 th iteration => 0.0163349931393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  19911 th iteration => 0.0163340320619\n",
      "Loss for  19912 th iteration => 0.0163332027854\n",
      "Loss for  19913 th iteration => 0.0163322991566\n",
      "Loss for  19914 th iteration => 0.0163314449772\n",
      "Loss for  19915 th iteration => 0.0163306532541\n",
      "Loss for  19916 th iteration => 0.0163294532031\n",
      "Loss for  19917 th iteration => 0.0163286708633\n",
      "Loss for  19918 th iteration => 0.0163276654399\n",
      "Loss for  19919 th iteration => 0.0163266930213\n",
      "Loss for  19920 th iteration => 0.0163259222866\n",
      "Loss for  19921 th iteration => 0.0163250415558\n",
      "Loss for  19922 th iteration => 0.0163239926448\n",
      "Loss for  19923 th iteration => 0.0163230539457\n",
      "Loss for  19924 th iteration => 0.0163222205821\n",
      "Loss for  19925 th iteration => 0.0163211028934\n",
      "Loss for  19926 th iteration => 0.0163202112543\n",
      "Loss for  19927 th iteration => 0.0163193906966\n",
      "Loss for  19928 th iteration => 0.0163184291693\n",
      "Loss for  19929 th iteration => 0.016317413619\n",
      "Loss for  19930 th iteration => 0.016316701426\n",
      "Loss for  19931 th iteration => 0.0163157745677\n",
      "Loss for  19932 th iteration => 0.0163146807947\n",
      "Loss for  19933 th iteration => 0.016313797655\n",
      "Loss for  19934 th iteration => 0.0163128949448\n",
      "Loss for  19935 th iteration => 0.0163118303566\n",
      "Loss for  19936 th iteration => 0.0163114235723\n",
      "Loss for  19937 th iteration => 0.016309969939\n",
      "Loss for  19938 th iteration => 0.0163093673685\n",
      "Loss for  19939 th iteration => 0.0163081054866\n",
      "Loss for  19940 th iteration => 0.0163072437882\n",
      "Loss for  19941 th iteration => 0.0163062763406\n",
      "Loss for  19942 th iteration => 0.0163055482807\n",
      "Loss for  19943 th iteration => 0.0163044973148\n",
      "Loss for  19944 th iteration => 0.0163036088402\n",
      "Loss for  19945 th iteration => 0.0163030116085\n",
      "Loss for  19946 th iteration => 0.0163017446811\n",
      "Loss for  19947 th iteration => 0.0163009279178\n",
      "Loss for  19948 th iteration => 0.0162998761835\n",
      "Loss for  19949 th iteration => 0.0162990410466\n",
      "Loss for  19950 th iteration => 0.0162980085281\n",
      "Loss for  19951 th iteration => 0.0162971963664\n",
      "Loss for  19952 th iteration => 0.0162964553189\n",
      "Loss for  19953 th iteration => 0.0162951782564\n",
      "Loss for  19954 th iteration => 0.0162945784768\n",
      "Loss for  19955 th iteration => 0.016293351406\n",
      "Loss for  19956 th iteration => 0.016292425682\n",
      "Loss for  19957 th iteration => 0.0162915492919\n",
      "Loss for  19958 th iteration => 0.0162907097111\n",
      "Loss for  19959 th iteration => 0.0162897771704\n",
      "Loss for  19960 th iteration => 0.0162890724199\n",
      "Loss for  19961 th iteration => 0.0162878184596\n",
      "Loss for  19962 th iteration => 0.01628709995\n",
      "Loss for  19963 th iteration => 0.016286039451\n",
      "Loss for  19964 th iteration => 0.0162851319917\n",
      "Loss for  19965 th iteration => 0.0162842811587\n",
      "Loss for  19966 th iteration => 0.0162834881977\n",
      "Loss for  19967 th iteration => 0.0162823817573\n",
      "Loss for  19968 th iteration => 0.0162815105058\n",
      "Loss for  19969 th iteration => 0.0162806183641\n",
      "Loss for  19970 th iteration => 0.0162795609424\n",
      "Loss for  19971 th iteration => 0.0162786210234\n",
      "Loss for  19972 th iteration => 0.0162778658496\n",
      "Loss for  19973 th iteration => 0.0162768476687\n",
      "Loss for  19974 th iteration => 0.0162758986248\n",
      "Loss for  19975 th iteration => 0.0162751048718\n",
      "Loss for  19976 th iteration => 0.0162742680748\n",
      "Loss for  19977 th iteration => 0.0162731181313\n",
      "Loss for  19978 th iteration => 0.0162723010157\n",
      "Loss for  19979 th iteration => 0.0162713410387\n",
      "Loss for  19980 th iteration => 0.0162703384016\n",
      "Loss for  19981 th iteration => 0.0162696112048\n",
      "Loss for  19982 th iteration => 0.0162686932254\n",
      "Loss for  19983 th iteration => 0.0162676922946\n",
      "Loss for  19984 th iteration => 0.0162667209955\n",
      "Loss for  19985 th iteration => 0.0162659306415\n",
      "Loss for  19986 th iteration => 0.016264800967\n",
      "Loss for  19987 th iteration => 0.016263931773\n",
      "Loss for  19988 th iteration => 0.0162630899466\n",
      "Loss for  19989 th iteration => 0.0162621604123\n",
      "Loss for  19990 th iteration => 0.0162611435732\n",
      "Loss for  19991 th iteration => 0.0162601931432\n",
      "Loss for  19992 th iteration => 0.0162594784296\n",
      "Loss for  19993 th iteration => 0.0162584195848\n",
      "Loss for  19994 th iteration => 0.0162575175522\n",
      "Loss for  19995 th iteration => 0.016256666051\n",
      "Loss for  19996 th iteration => 0.0162558815026\n",
      "Loss for  19997 th iteration => 0.0162547714589\n",
      "Loss for  19998 th iteration => 0.0162539109406\n",
      "Loss for  19999 th iteration => 0.0162530134233\n",
      "Loss for  20000 th iteration => 0.0162519651449\n",
      "Loss for  20001 th iteration => 0.0162510236335\n",
      "Loss for  20002 th iteration => 0.016250277646\n",
      "Loss for  20003 th iteration => 0.0162492557375\n",
      "Loss for  20004 th iteration => 0.0162483174307\n",
      "Loss for  20005 th iteration => 0.0162475163124\n",
      "Loss for  20006 th iteration => 0.0162466924418\n",
      "Loss for  20007 th iteration => 0.0162455377483\n",
      "Loss for  20008 th iteration => 0.0162447323551\n",
      "Loss for  20009 th iteration => 0.0162437661704\n",
      "Loss for  20010 th iteration => 0.0162427766779\n",
      "Loss for  20011 th iteration => 0.0162420387514\n",
      "Loss for  20012 th iteration => 0.01624113956\n",
      "Loss for  20013 th iteration => 0.0162401266264\n",
      "Loss for  20014 th iteration => 0.0162391743085\n",
      "Loss for  20015 th iteration => 0.0162383704058\n",
      "Loss for  20016 th iteration => 0.0162372561656\n",
      "Loss for  20017 th iteration => 0.01623637954\n",
      "Loss for  20018 th iteration => 0.0162355540622\n",
      "Loss for  20019 th iteration => 0.0162346136968\n",
      "Loss for  20020 th iteration => 0.0162336089077\n",
      "Loss for  20021 th iteration => 0.0162326527488\n",
      "Loss for  20022 th iteration => 0.0162319548807\n",
      "Loss for  20023 th iteration => 0.0162308847429\n",
      "Loss for  20024 th iteration => 0.016230000848\n",
      "Loss for  20025 th iteration => 0.0162291320769\n",
      "Loss for  20026 th iteration => 0.0162283730363\n",
      "Loss for  20027 th iteration => 0.0162272455804\n",
      "Loss for  20028 th iteration => 0.0162264093556\n",
      "Loss for  20029 th iteration => 0.0162254930217\n",
      "Loss for  20030 th iteration => 0.0162244642649\n",
      "Loss for  20031 th iteration => 0.0162235115103\n",
      "Loss for  20032 th iteration => 0.0162227865243\n",
      "Loss for  20033 th iteration => 0.016221749166\n",
      "Loss for  20034 th iteration => 0.0162208331027\n",
      "Loss for  20035 th iteration => 0.0162200099179\n",
      "Loss for  20036 th iteration => 0.0162192133155\n",
      "Loss for  20037 th iteration => 0.0162180431663\n",
      "Loss for  20038 th iteration => 0.0162172600001\n",
      "Loss for  20039 th iteration => 0.0162162771806\n",
      "Loss for  20040 th iteration => 0.0162153110638\n",
      "Loss for  20041 th iteration => 0.0162145490103\n",
      "Loss for  20042 th iteration => 0.0162136823809\n",
      "Loss for  20043 th iteration => 0.0162126463424\n",
      "Loss for  20044 th iteration => 0.0162117239019\n",
      "Loss for  20045 th iteration => 0.0162108956456\n",
      "Loss for  20046 th iteration => 0.0162098052085\n",
      "Loss for  20047 th iteration => 0.0162089133557\n",
      "Loss for  20048 th iteration => 0.0162081137402\n",
      "Loss for  20049 th iteration => 0.0162071531\n",
      "Loss for  20050 th iteration => 0.0162061673707\n",
      "Loss for  20051 th iteration => 0.016205198754\n",
      "Loss for  20052 th iteration => 0.0162045264238\n",
      "Loss for  20053 th iteration => 0.0162034363601\n",
      "Loss for  20054 th iteration => 0.0162025790677\n",
      "Loss for  20055 th iteration => 0.0162016819134\n",
      "Loss for  20056 th iteration => 0.0162009614668\n",
      "Loss for  20057 th iteration => 0.0161999855825\n",
      "Loss for  20058 th iteration => 0.0161990223898\n",
      "Loss for  20059 th iteration => 0.0161980686195\n",
      "Loss for  20060 th iteration => 0.0161970732278\n",
      "Loss for  20061 th iteration => 0.01619609312\n",
      "Loss for  20062 th iteration => 0.0161951538734\n",
      "Loss for  20063 th iteration => 0.0161943243116\n",
      "Loss for  20064 th iteration => 0.0161934491791\n",
      "Loss for  20065 th iteration => 0.0161925801097\n",
      "Loss for  20066 th iteration => 0.0161918343079\n",
      "Loss for  20067 th iteration => 0.0161906307363\n",
      "Loss for  20068 th iteration => 0.0161898874584\n",
      "Loss for  20069 th iteration => 0.0161888704867\n",
      "Loss for  20070 th iteration => 0.0161879449652\n",
      "Loss for  20071 th iteration => 0.016187136794\n",
      "Loss for  20072 th iteration => 0.0161863249385\n",
      "Loss for  20073 th iteration => 0.0161852480984\n",
      "Loss for  20074 th iteration => 0.0161843729199\n",
      "Loss for  20075 th iteration => 0.0161835030894\n",
      "Loss for  20076 th iteration => 0.0161824495686\n",
      "Loss for  20077 th iteration => 0.0161815301298\n",
      "Loss for  20078 th iteration => 0.0161807718477\n",
      "Loss for  20079 th iteration => 0.0161797755919\n",
      "Loss for  20080 th iteration => 0.0161788297796\n",
      "Loss for  20081 th iteration => 0.0161780502077\n",
      "Loss for  20082 th iteration => 0.0161772204674\n",
      "Loss for  20083 th iteration => 0.0161760848483\n",
      "Loss for  20084 th iteration => 0.0161752783912\n",
      "Loss for  20085 th iteration => 0.0161743268148\n",
      "Loss for  20086 th iteration => 0.0161733406215\n",
      "Loss for  20087 th iteration => 0.016172611206\n",
      "Loss for  20088 th iteration => 0.0161717206748\n",
      "Loss for  20089 th iteration => 0.0161707117582\n",
      "Loss for  20090 th iteration => 0.0161697734644\n",
      "Loss for  20091 th iteration => 0.0161689688358\n",
      "Loss for  20092 th iteration => 0.0161678711459\n",
      "Loss for  20093 th iteration => 0.0161669963958\n",
      "Loss for  20094 th iteration => 0.0161661834972\n",
      "Loss for  20095 th iteration => 0.0161652441266\n",
      "Loss for  20096 th iteration => 0.0161642531789\n",
      "Loss for  20097 th iteration => 0.0161632978311\n",
      "Loss for  20098 th iteration => 0.016162617265\n",
      "Loss for  20099 th iteration => 0.01616154352\n",
      "Loss for  20100 th iteration => 0.0161606808253\n",
      "Loss for  20101 th iteration => 0.0161597972236\n",
      "Loss for  20102 th iteration => 0.0161590709152\n",
      "Loss for  20103 th iteration => 0.0161579280001\n",
      "Loss for  20104 th iteration => 0.0161571249436\n",
      "Loss for  20105 th iteration => 0.016156188913\n",
      "Loss for  20106 th iteration => 0.016155191331\n",
      "Loss for  20107 th iteration => 0.0161542268741\n",
      "Loss for  20108 th iteration => 0.0161535311317\n",
      "Loss for  20109 th iteration => 0.0161524782408\n",
      "Loss for  20110 th iteration => 0.0161515951363\n",
      "Loss for  20111 th iteration => 0.0161507419685\n",
      "Loss for  20112 th iteration => 0.0161499897965\n",
      "Loss for  20113 th iteration => 0.0161488010666\n",
      "Loss for  20114 th iteration => 0.0161480538219\n",
      "Loss for  20115 th iteration => 0.0161470489249\n",
      "Loss for  20116 th iteration => 0.0161461221403\n",
      "Loss for  20117 th iteration => 0.0161453216207\n",
      "Loss for  20118 th iteration => 0.0161445120986\n",
      "Loss for  20119 th iteration => 0.0161434408623\n",
      "Loss for  20120 th iteration => 0.0161425709728\n",
      "Loss for  20121 th iteration => 0.0161417038109\n",
      "Loss for  20122 th iteration => 0.0161406593272\n",
      "Loss for  20123 th iteration => 0.0161397420498\n",
      "Loss for  20124 th iteration => 0.0161389885159\n",
      "Loss for  20125 th iteration => 0.0161379956409\n",
      "Loss for  20126 th iteration => 0.0161370571375\n",
      "Loss for  20127 th iteration => 0.016136274046\n",
      "Loss for  20128 th iteration => 0.0161354568927\n",
      "Loss for  20129 th iteration => 0.0161343218986\n",
      "Loss for  20130 th iteration => 0.0161335254401\n",
      "Loss for  20131 th iteration => 0.0161325720899\n",
      "Loss for  20132 th iteration => 0.0161315982371\n",
      "Loss for  20133 th iteration => 0.0161308587825\n",
      "Loss for  20134 th iteration => 0.0161299888474\n",
      "Loss for  20135 th iteration => 0.016128970724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  20136 th iteration => 0.0161280522719\n",
      "Loss for  20137 th iteration => 0.0161272358943\n",
      "Loss for  20138 th iteration => 0.0161261583058\n",
      "Loss for  20139 th iteration => 0.0161252754539\n",
      "Loss for  20140 th iteration => 0.0161244799359\n",
      "Loss for  20141 th iteration => 0.0161235314205\n",
      "Loss for  20142 th iteration => 0.0161225573786\n",
      "Loss for  20143 th iteration => 0.0161215943801\n",
      "Loss for  20144 th iteration => 0.0161209330541\n",
      "Loss for  20145 th iteration => 0.0161198483662\n",
      "Loss for  20146 th iteration => 0.016119007021\n",
      "Loss for  20147 th iteration => 0.0161181016241\n",
      "Loss for  20148 th iteration => 0.0161174080214\n",
      "Loss for  20149 th iteration => 0.0161162461128\n",
      "Loss for  20150 th iteration => 0.0161154725158\n",
      "Loss for  20151 th iteration => 0.0161145152021\n",
      "Loss for  20152 th iteration => 0.0161135450805\n",
      "Loss for  20153 th iteration => 0.0161125663347\n",
      "Loss for  20154 th iteration => 0.0161116951186\n",
      "Loss for  20155 th iteration => 0.0161109828258\n",
      "Loss for  20156 th iteration => 0.0161098499564\n",
      "Loss for  20157 th iteration => 0.0161091632374\n",
      "Loss for  20158 th iteration => 0.0161083300443\n",
      "Loss for  20159 th iteration => 0.0161071651872\n",
      "Loss for  20160 th iteration => 0.0161064864307\n",
      "Loss for  20161 th iteration => 0.0161053404623\n",
      "Loss for  20162 th iteration => 0.01610464422\n",
      "Loss for  20163 th iteration => 0.0161035426222\n",
      "Loss for  20164 th iteration => 0.0161029095922\n",
      "Loss for  20165 th iteration => 0.0161018155586\n",
      "Loss for  20166 th iteration => 0.016100979686\n",
      "Loss for  20167 th iteration => 0.0161000861575\n",
      "Loss for  20168 th iteration => 0.0160990971736\n",
      "Loss for  20169 th iteration => 0.0160983629873\n",
      "Loss for  20170 th iteration => 0.0160972540783\n",
      "Loss for  20171 th iteration => 0.01609654367\n",
      "Loss for  20172 th iteration => 0.016095412459\n",
      "Loss for  20173 th iteration => 0.0160947401501\n",
      "Loss for  20174 th iteration => 0.0160938953314\n",
      "Loss for  20175 th iteration => 0.0160927328652\n",
      "Loss for  20176 th iteration => 0.0160920552176\n",
      "Loss for  20177 th iteration => 0.0160909114292\n",
      "Loss for  20178 th iteration => 0.0160902164919\n",
      "Loss for  20179 th iteration => 0.0160891421626\n",
      "Loss for  20180 th iteration => 0.0160884586605\n",
      "Loss for  20181 th iteration => 0.0160874172557\n",
      "Loss for  20182 th iteration => 0.0160865333086\n",
      "Loss for  20183 th iteration => 0.0160856900331\n",
      "Loss for  20184 th iteration => 0.0160846797361\n",
      "Loss for  20185 th iteration => 0.0160839454551\n",
      "Loss for  20186 th iteration => 0.0160828401242\n",
      "Loss for  20187 th iteration => 0.0160821293726\n",
      "Loss for  20188 th iteration => 0.0160810019741\n",
      "Loss for  20189 th iteration => 0.0160803376954\n",
      "Loss for  20190 th iteration => 0.0160794873475\n",
      "Loss for  20191 th iteration => 0.0160783256912\n",
      "Loss for  20192 th iteration => 0.0160776506684\n",
      "Loss for  20193 th iteration => 0.0160765075153\n",
      "Loss for  20194 th iteration => 0.0160758153645\n",
      "Loss for  20195 th iteration => 0.0160747598683\n",
      "Loss for  20196 th iteration => 0.0160740410383\n",
      "Loss for  20197 th iteration => 0.0160730372697\n",
      "Loss for  20198 th iteration => 0.0160721200026\n",
      "Loss for  20199 th iteration => 0.0160713123742\n",
      "Loss for  20200 th iteration => 0.0160702889708\n",
      "Loss for  20201 th iteration => 0.0160695527994\n",
      "Loss for  20202 th iteration => 0.016068452779\n",
      "Loss for  20203 th iteration => 0.0160677399317\n",
      "Loss for  20204 th iteration => 0.0160666263116\n",
      "Loss for  20205 th iteration => 0.0160657426389\n",
      "Loss for  20206 th iteration => 0.0160650856864\n",
      "Loss for  20207 th iteration => 0.0160639270514\n",
      "Loss for  20208 th iteration => 0.0160632526916\n",
      "Loss for  20209 th iteration => 0.0160621209586\n",
      "Loss for  20210 th iteration => 0.0160612864088\n",
      "Loss for  20211 th iteration => 0.0160606022006\n",
      "Loss for  20212 th iteration => 0.0160594697802\n",
      "Loss for  20213 th iteration => 0.0160587790857\n",
      "Loss for  20214 th iteration => 0.016057676647\n",
      "Loss for  20215 th iteration => 0.0160569955696\n",
      "Loss for  20216 th iteration => 0.0160559473557\n",
      "Loss for  20217 th iteration => 0.0160550831859\n",
      "Loss for  20218 th iteration => 0.0160542160008\n",
      "Loss for  20219 th iteration => 0.0160531886074\n",
      "Loss for  20220 th iteration => 0.0160522961579\n",
      "Loss for  20221 th iteration => 0.0160515673017\n",
      "Loss for  20222 th iteration => 0.0160505630093\n",
      "Loss for  20223 th iteration => 0.0160496598572\n",
      "Loss for  20224 th iteration => 0.016048843346\n",
      "Loss for  20225 th iteration => 0.0160480849814\n",
      "Loss for  20226 th iteration => 0.016046978687\n",
      "Loss for  20227 th iteration => 0.0160461682183\n",
      "Loss for  20228 th iteration => 0.0160452603605\n",
      "Loss for  20229 th iteration => 0.0160442668693\n",
      "Loss for  20230 th iteration => 0.0160433295295\n",
      "Loss for  20231 th iteration => 0.0160426205695\n",
      "Loss for  20232 th iteration => 0.0160416021474\n",
      "Loss for  20233 th iteration => 0.0160407131981\n",
      "Loss for  20234 th iteration => 0.0160398890522\n",
      "Loss for  20235 th iteration => 0.0160391274793\n",
      "Loss for  20236 th iteration => 0.0160379736973\n",
      "Loss for  20237 th iteration => 0.0160372199414\n",
      "Loss for  20238 th iteration => 0.0160362430734\n",
      "Loss for  20239 th iteration => 0.0160353165381\n",
      "Loss for  20240 th iteration => 0.0160345302588\n",
      "Loss for  20241 th iteration => 0.0160337431901\n",
      "Loss for  20242 th iteration => 0.0160326643965\n",
      "Loss for  20243 th iteration => 0.0160318305172\n",
      "Loss for  20244 th iteration => 0.0160309485146\n",
      "Loss for  20245 th iteration => 0.0160299411066\n",
      "Loss for  20246 th iteration => 0.0160290203462\n",
      "Loss for  20247 th iteration => 0.016028291319\n",
      "Loss for  20248 th iteration => 0.0160272955135\n",
      "Loss for  20249 th iteration => 0.0160263879124\n",
      "Loss for  20250 th iteration => 0.0160255870822\n",
      "Loss for  20251 th iteration => 0.0160248062213\n",
      "Loss for  20252 th iteration => 0.0160236718813\n",
      "Loss for  20253 th iteration => 0.0160229025999\n",
      "Loss for  20254 th iteration => 0.0160219438671\n",
      "Loss for  20255 th iteration => 0.0160210030854\n",
      "Loss for  20256 th iteration => 0.0160202352073\n",
      "Loss for  20257 th iteration => 0.0160194316772\n",
      "Loss for  20258 th iteration => 0.0160183704241\n",
      "Loss for  20259 th iteration => 0.0160175229333\n",
      "Loss for  20260 th iteration => 0.0160166570837\n",
      "Loss for  20261 th iteration => 0.0160156431688\n",
      "Loss for  20262 th iteration => 0.0160147321298\n",
      "Loss for  20263 th iteration => 0.0160139915255\n",
      "Loss for  20264 th iteration => 0.0160130099249\n",
      "Loss for  20265 th iteration => 0.0160120919454\n",
      "Loss for  20266 th iteration => 0.0160113039446\n",
      "Loss for  20267 th iteration => 0.016010514039\n",
      "Loss for  20268 th iteration => 0.0160093914388\n",
      "Loss for  20269 th iteration => 0.016008614206\n",
      "Loss for  20270 th iteration => 0.0160076661019\n",
      "Loss for  20271 th iteration => 0.0160067184554\n",
      "Loss for  20272 th iteration => 0.0160059594152\n",
      "Loss for  20273 th iteration => 0.0160051493161\n",
      "Loss for  20274 th iteration => 0.0160040975589\n",
      "Loss for  20275 th iteration => 0.0160032443692\n",
      "Loss for  20276 th iteration => 0.0160023868371\n",
      "Loss for  20277 th iteration => 0.016001372403\n",
      "Loss for  20278 th iteration => 0.0160004655401\n",
      "Loss for  20279 th iteration => 0.015999720219\n",
      "Loss for  20280 th iteration => 0.0159987460249\n",
      "Loss for  20281 th iteration => 0.0159978243519\n",
      "Loss for  20282 th iteration => 0.0159970407037\n",
      "Loss for  20283 th iteration => 0.0159962500323\n",
      "Loss for  20284 th iteration => 0.0159951329484\n",
      "Loss for  20285 th iteration => 0.0159943538825\n",
      "Loss for  20286 th iteration => 0.0159934103421\n",
      "Loss for  20287 th iteration => 0.0159924617936\n",
      "Loss for  20288 th iteration => 0.0159917038612\n",
      "Loss for  20289 th iteration => 0.0159908951849\n",
      "Loss for  20290 th iteration => 0.0159898464277\n",
      "Loss for  20291 th iteration => 0.0159889939271\n",
      "Loss for  20292 th iteration => 0.0159881383856\n",
      "Loss for  20293 th iteration => 0.0159871282701\n",
      "Loss for  20294 th iteration => 0.0159862210995\n",
      "Loss for  20295 th iteration => 0.015985476604\n",
      "Loss for  20296 th iteration => 0.0159845043228\n",
      "Loss for  20297 th iteration => 0.0159835843568\n",
      "Loss for  20298 th iteration => 0.0159827982093\n",
      "Loss for  20299 th iteration => 0.0159820134625\n",
      "Loss for  20300 th iteration => 0.015980896867\n",
      "Loss for  20301 th iteration => 0.0159801209095\n",
      "Loss for  20302 th iteration => 0.0159791770331\n",
      "Loss for  20303 th iteration => 0.015978232398\n",
      "Loss for  20304 th iteration => 0.0159774693254\n",
      "Loss for  20305 th iteration => 0.0159766685273\n",
      "Loss for  20306 th iteration => 0.0159756175263\n",
      "Loss for  20307 th iteration => 0.0159747708696\n",
      "Loss for  20308 th iteration => 0.0159739122119\n",
      "Loss for  20309 th iteration => 0.015972910323\n",
      "Loss for  20310 th iteration => 0.0159719992192\n",
      "Loss for  20311 th iteration => 0.0159712600262\n",
      "Loss for  20312 th iteration => 0.0159702852192\n",
      "Loss for  20313 th iteration => 0.0159693713221\n",
      "Loss for  20314 th iteration => 0.0159685771372\n",
      "Loss for  20315 th iteration => 0.0159678037209\n",
      "Loss for  20316 th iteration => 0.0159666835525\n",
      "Loss for  20317 th iteration => 0.0159659146933\n",
      "Loss for  20318 th iteration => 0.0159649665234\n",
      "Loss for  20319 th iteration => 0.0159640296896\n",
      "Loss for  20320 th iteration => 0.0159632564273\n",
      "Loss for  20321 th iteration => 0.0159624687202\n",
      "Loss for  20322 th iteration => 0.0159614112444\n",
      "Loss for  20323 th iteration => 0.0159605745893\n",
      "Loss for  20324 th iteration => 0.015959708695\n",
      "Loss for  20325 th iteration => 0.015958718189\n",
      "Loss for  20326 th iteration => 0.0159578002204\n",
      "Loss for  20327 th iteration => 0.0159570699451\n",
      "Loss for  20328 th iteration => 0.0159560890265\n",
      "Loss for  20329 th iteration => 0.0159551847203\n",
      "Loss for  20330 th iteration => 0.0159543780226\n",
      "Loss for  20331 th iteration => 0.015953620304\n",
      "Loss for  20332 th iteration => 0.0159524932832\n",
      "Loss for  20333 th iteration => 0.0159517347425\n",
      "Loss for  20334 th iteration => 0.0159507790836\n",
      "Loss for  20335 th iteration => 0.0159498537347\n",
      "Loss for  20336 th iteration => 0.0159489164993\n",
      "Loss for  20337 th iteration => 0.0159482766163\n",
      "Loss for  20338 th iteration => 0.0159472183514\n",
      "Loss for  20339 th iteration => 0.0159463868723\n",
      "Loss for  20340 th iteration => 0.0159455179869\n",
      "Loss for  20341 th iteration => 0.0159445564619\n",
      "Loss for  20342 th iteration => 0.0159438169735\n",
      "Loss for  20343 th iteration => 0.0159427486175\n",
      "Loss for  20344 th iteration => 0.0159420316679\n",
      "Loss for  20345 th iteration => 0.0159409421682\n",
      "Loss for  20346 th iteration => 0.0159402469697\n",
      "Loss for  20347 th iteration => 0.0159392384499\n",
      "Loss for  20348 th iteration => 0.0159384691019\n",
      "Loss for  20349 th iteration => 0.0159375272745\n",
      "Loss for  20350 th iteration => 0.0159365901035\n",
      "Loss for  20351 th iteration => 0.0159358142141\n",
      "Loss for  20352 th iteration => 0.0159347518111\n",
      "Loss for  20353 th iteration => 0.0159339631202\n",
      "Loss for  20354 th iteration => 0.01593313565\n",
      "Loss for  20355 th iteration => 0.0159322657913\n",
      "Loss for  20356 th iteration => 0.0159312785367\n",
      "Loss for  20357 th iteration => 0.015930365927\n",
      "Loss for  20358 th iteration => 0.0159296396375\n",
      "Loss for  20359 th iteration => 0.0159286600465\n",
      "Loss for  20360 th iteration => 0.0159277612524\n",
      "Loss for  20361 th iteration => 0.0159269557501\n",
      "Loss for  20362 th iteration => 0.0159262035541\n",
      "Loss for  20363 th iteration => 0.0159250736159\n",
      "Loss for  20364 th iteration => 0.0159243247904\n",
      "Loss for  20365 th iteration => 0.0159233647903\n",
      "Loss for  20366 th iteration => 0.0159224500011\n",
      "Loss for  20367 th iteration => 0.0159216591133\n",
      "Loss for  20368 th iteration => 0.0159209010078\n",
      "Loss for  20369 th iteration => 0.0159198233536\n",
      "Loss for  20370 th iteration => 0.0159190171529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  20371 th iteration => 0.0159181288898\n",
      "Loss for  20372 th iteration => 0.0159171616329\n",
      "Loss for  20373 th iteration => 0.0159162341407\n",
      "Loss for  20374 th iteration => 0.0159155266032\n",
      "Loss for  20375 th iteration => 0.0159145311448\n",
      "Loss for  20376 th iteration => 0.0159136515235\n",
      "Loss for  20377 th iteration => 0.0159128261917\n",
      "Loss for  20378 th iteration => 0.0159118658086\n",
      "Loss for  20379 th iteration => 0.0159111333606\n",
      "Loss for  20380 th iteration => 0.015910083954\n",
      "Loss for  20381 th iteration => 0.0159093351763\n",
      "Loss for  20382 th iteration => 0.0159083028321\n",
      "Loss for  20383 th iteration => 0.0159075382796\n",
      "Loss for  20384 th iteration => 0.0159066114071\n",
      "Loss for  20385 th iteration => 0.0159058113174\n",
      "Loss for  20386 th iteration => 0.0159049200043\n",
      "Loss for  20387 th iteration => 0.0159039510235\n",
      "Loss for  20388 th iteration => 0.0159030100785\n",
      "Loss for  20389 th iteration => 0.0159021060735\n",
      "Loss for  20390 th iteration => 0.0159014735537\n",
      "Loss for  20391 th iteration => 0.0159003086102\n",
      "Loss for  20392 th iteration => 0.0158996996579\n",
      "Loss for  20393 th iteration => 0.0158988244863\n",
      "Loss for  20394 th iteration => 0.0158977547354\n",
      "Loss for  20395 th iteration => 0.0158970283808\n",
      "Loss for  20396 th iteration => 0.0158959760825\n",
      "Loss for  20397 th iteration => 0.0158952335824\n",
      "Loss for  20398 th iteration => 0.0158941999134\n",
      "Loss for  20399 th iteration => 0.0158937349207\n",
      "Loss for  20400 th iteration => 0.0158925211869\n",
      "Loss for  20401 th iteration => 0.0158917361291\n",
      "Loss for  20402 th iteration => 0.0158908325417\n",
      "Loss for  20403 th iteration => 0.0158898796457\n",
      "Loss for  20404 th iteration => 0.0158889318642\n",
      "Loss for  20405 th iteration => 0.0158880356778\n",
      "Loss for  20406 th iteration => 0.0158872330914\n",
      "Loss for  20407 th iteration => 0.0158863744573\n",
      "Loss for  20408 th iteration => 0.015885533548\n",
      "Loss for  20409 th iteration => 0.0158845830473\n",
      "Loss for  20410 th iteration => 0.0158838579737\n",
      "Loss for  20411 th iteration => 0.0158828071512\n",
      "Loss for  20412 th iteration => 0.0158820658504\n",
      "Loss for  20413 th iteration => 0.015881031982\n",
      "Loss for  20414 th iteration => 0.015880275005\n",
      "Loss for  20415 th iteration => 0.0158793366217\n",
      "Loss for  20416 th iteration => 0.0158785649947\n",
      "Loss for  20417 th iteration => 0.0158776505817\n",
      "Loss for  20418 th iteration => 0.0158767022412\n",
      "Loss for  20419 th iteration => 0.0158757471758\n",
      "Loss for  20420 th iteration => 0.0158748649792\n",
      "Loss for  20421 th iteration => 0.0158742215664\n",
      "Loss for  20422 th iteration => 0.0158730735566\n",
      "Loss for  20423 th iteration => 0.0158724527998\n",
      "Loss for  20424 th iteration => 0.0158715903072\n",
      "Loss for  20425 th iteration => 0.015870518982\n",
      "Loss for  20426 th iteration => 0.015869800239\n",
      "Loss for  20427 th iteration => 0.0158687462653\n",
      "Loss for  20428 th iteration => 0.0158680114685\n",
      "Loss for  20429 th iteration => 0.015866974267\n",
      "Loss for  20430 th iteration => 0.0158662810647\n",
      "Loss for  20431 th iteration => 0.0158654956176\n",
      "Loss for  20432 th iteration => 0.0158643806313\n",
      "Loss for  20433 th iteration => 0.0158637158125\n",
      "Loss for  20434 th iteration => 0.0158626162111\n",
      "Loss for  20435 th iteration => 0.0158617324208\n",
      "Loss for  20436 th iteration => 0.015860875456\n",
      "Loss for  20437 th iteration => 0.0158599420547\n",
      "Loss for  20438 th iteration => 0.015859317959\n",
      "Loss for  20439 th iteration => 0.0158581583858\n",
      "Loss for  20440 th iteration => 0.0158573736177\n",
      "Loss for  20441 th iteration => 0.0158566566026\n",
      "Loss for  20442 th iteration => 0.0158556037579\n",
      "Loss for  20443 th iteration => 0.015854870744\n",
      "Loss for  20444 th iteration => 0.0158538361361\n",
      "Loss for  20445 th iteration => 0.015853404122\n",
      "Loss for  20446 th iteration => 0.0158521515247\n",
      "Loss for  20447 th iteration => 0.0158514100421\n",
      "Loss for  20448 th iteration => 0.0158504708418\n",
      "Loss for  20449 th iteration => 0.0158495458575\n",
      "Loss for  20450 th iteration => 0.0158487880241\n",
      "Loss for  20451 th iteration => 0.0158477316913\n",
      "Loss for  20452 th iteration => 0.0158470761951\n",
      "Loss for  20453 th iteration => 0.0158459459743\n",
      "Loss for  20454 th iteration => 0.0158453118902\n",
      "Loss for  20455 th iteration => 0.0158441616004\n",
      "Loss for  20456 th iteration => 0.0158435852527\n",
      "Loss for  20457 th iteration => 0.0158426839841\n",
      "Loss for  20458 th iteration => 0.0158416220248\n",
      "Loss for  20459 th iteration => 0.0158409008715\n",
      "Loss for  20460 th iteration => 0.0158398557996\n",
      "Loss for  20461 th iteration => 0.0158391190264\n",
      "Loss for  20462 th iteration => 0.0158381382307\n",
      "Loss for  20463 th iteration => 0.0158374474239\n",
      "Loss for  20464 th iteration => 0.0158364606456\n",
      "Loss for  20465 th iteration => 0.015835586124\n",
      "Loss for  20466 th iteration => 0.015834780927\n",
      "Loss for  20467 th iteration => 0.0158337649695\n",
      "Loss for  20468 th iteration => 0.0158329038814\n",
      "Loss for  20469 th iteration => 0.0158321329561\n",
      "Loss for  20470 th iteration => 0.015831216111\n",
      "Loss for  20471 th iteration => 0.015830279864\n",
      "Loss for  20472 th iteration => 0.0158295484647\n",
      "Loss for  20473 th iteration => 0.0158287343646\n",
      "Loss for  20474 th iteration => 0.0158276763775\n",
      "Loss for  20475 th iteration => 0.0158268807168\n",
      "Loss for  20476 th iteration => 0.0158259859137\n",
      "Loss for  20477 th iteration => 0.0158250374629\n",
      "Loss for  20478 th iteration => 0.0158241333405\n",
      "Loss for  20479 th iteration => 0.015823499319\n",
      "Loss for  20480 th iteration => 0.0158224584352\n",
      "Loss for  20481 th iteration => 0.015821641508\n",
      "Loss for  20482 th iteration => 0.0158207814067\n",
      "Loss for  20483 th iteration => 0.0158198534143\n",
      "Loss for  20484 th iteration => 0.0158190901941\n",
      "Loss for  20485 th iteration => 0.0158180736793\n",
      "Loss for  20486 th iteration => 0.0158173321026\n",
      "Loss for  20487 th iteration => 0.0158162952786\n",
      "Loss for  20488 th iteration => 0.0158155783735\n",
      "Loss for  20489 th iteration => 0.015814820796\n",
      "Loss for  20490 th iteration => 0.0158136580239\n",
      "Loss for  20491 th iteration => 0.0158130436613\n",
      "Loss for  20492 th iteration => 0.0158118980088\n",
      "Loss for  20493 th iteration => 0.0158112677859\n",
      "Loss for  20494 th iteration => 0.0158101755775\n",
      "Loss for  20495 th iteration => 0.0158093406241\n",
      "Loss for  20496 th iteration => 0.0158086903387\n",
      "Loss for  20497 th iteration => 0.0158075797513\n",
      "Loss for  20498 th iteration => 0.015806923489\n",
      "Loss for  20499 th iteration => 0.0158058682013\n",
      "Loss for  20500 th iteration => 0.0158051719426\n",
      "Loss for  20501 th iteration => 0.0158041868671\n",
      "Loss for  20502 th iteration => 0.0158033236461\n",
      "Loss for  20503 th iteration => 0.0158025036318\n",
      "Loss for  20504 th iteration => 0.0158014908391\n",
      "Loss for  20505 th iteration => 0.015800643696\n",
      "Loss for  20506 th iteration => 0.0157999192561\n",
      "Loss for  20507 th iteration => 0.0157989591051\n",
      "Loss for  20508 th iteration => 0.0157980751118\n",
      "Loss for  20509 th iteration => 0.0157972727112\n",
      "Loss for  20510 th iteration => 0.0157963104277\n",
      "Loss for  20511 th iteration => 0.0157956603782\n",
      "Loss for  20512 th iteration => 0.0157945522257\n",
      "Loss for  20513 th iteration => 0.0157938961665\n",
      "Loss for  20514 th iteration => 0.0157927992682\n",
      "Loss for  20515 th iteration => 0.0157919468224\n",
      "Loss for  20516 th iteration => 0.0157912927829\n",
      "Loss for  20517 th iteration => 0.015790173713\n",
      "Loss for  20518 th iteration => 0.0157895406011\n",
      "Loss for  20519 th iteration => 0.0157884019175\n",
      "Loss for  20520 th iteration => 0.0157878316074\n",
      "Loss for  20521 th iteration => 0.0157869346679\n",
      "Loss for  20522 th iteration => 0.0157858751423\n",
      "Loss for  20523 th iteration => 0.0157851640735\n",
      "Loss for  20524 th iteration => 0.0157841210929\n",
      "Loss for  20525 th iteration => 0.0157833947199\n",
      "Loss for  20526 th iteration => 0.015782420413\n",
      "Loss for  20527 th iteration => 0.0157817309188\n",
      "Loss for  20528 th iteration => 0.0157807531885\n",
      "Loss for  20529 th iteration => 0.0157798837683\n",
      "Loss for  20530 th iteration => 0.0157790838713\n",
      "Loss for  20531 th iteration => 0.0157780730426\n",
      "Loss for  20532 th iteration => 0.0157772209762\n",
      "Loss for  20533 th iteration => 0.0157764534576\n",
      "Loss for  20534 th iteration => 0.0157755438248\n",
      "Loss for  20535 th iteration => 0.0157746142625\n",
      "Loss for  20536 th iteration => 0.0157738870356\n",
      "Loss for  20537 th iteration => 0.015773080139\n",
      "Loss for  20538 th iteration => 0.0157720260416\n",
      "Loss for  20539 th iteration => 0.0157712403065\n",
      "Loss for  20540 th iteration => 0.0157703463188\n",
      "Loss for  20541 th iteration => 0.0157694085256\n",
      "Loss for  20542 th iteration => 0.015768502136\n",
      "Loss for  20543 th iteration => 0.0157678881791\n",
      "Loss for  20544 th iteration => 0.0157668377663\n",
      "Loss for  20545 th iteration => 0.0157660442\n",
      "Loss for  20546 th iteration => 0.0157651713103\n",
      "Loss for  20547 th iteration => 0.0157642627701\n",
      "Loss for  20548 th iteration => 0.0157634970458\n",
      "Loss for  20549 th iteration => 0.015762495437\n",
      "Loss for  20550 th iteration => 0.0157617509889\n",
      "Loss for  20551 th iteration => 0.0157607294143\n",
      "Loss for  20552 th iteration => 0.0157600055007\n",
      "Loss for  20553 th iteration => 0.0157590372246\n",
      "Loss for  20554 th iteration => 0.0157582928406\n",
      "Loss for  20555 th iteration => 0.0157573609448\n",
      "Loss for  20556 th iteration => 0.015756458469\n",
      "Loss for  20557 th iteration => 0.0157556828786\n",
      "Loss for  20558 th iteration => 0.0157546559952\n",
      "Loss for  20559 th iteration => 0.0157538470606\n",
      "Loss for  20560 th iteration => 0.0157531111031\n",
      "Loss for  20561 th iteration => 0.0157521841213\n",
      "Loss for  20562 th iteration => 0.0157512726524\n",
      "Loss for  20563 th iteration => 0.0157505431436\n",
      "Loss for  20564 th iteration => 0.0157497029694\n",
      "Loss for  20565 th iteration => 0.0157486772106\n",
      "Loss for  20566 th iteration => 0.0157478677675\n",
      "Loss for  20567 th iteration => 0.0157470050671\n",
      "Loss for  20568 th iteration => 0.0157460363124\n",
      "Loss for  20569 th iteration => 0.0157453608011\n",
      "Loss for  20570 th iteration => 0.0157445107207\n",
      "Loss for  20571 th iteration => 0.015743495512\n",
      "Loss for  20572 th iteration => 0.0157426785336\n",
      "Loss for  20573 th iteration => 0.0157418209172\n",
      "Loss for  20574 th iteration => 0.0157408635265\n",
      "Loss for  20575 th iteration => 0.0157399826422\n",
      "Loss for  20576 th iteration => 0.015739339213\n",
      "Loss for  20577 th iteration => 0.0157383232424\n",
      "Loss for  20578 th iteration => 0.0157375029048\n",
      "Loss for  20579 th iteration => 0.0157366640745\n",
      "Loss for  20580 th iteration => 0.0157359323443\n",
      "Loss for  20581 th iteration => 0.01573482817\n",
      "Loss for  20582 th iteration => 0.0157340994115\n",
      "Loss for  20583 th iteration => 0.0157331594424\n",
      "Loss for  20584 th iteration => 0.0157322702391\n",
      "Loss for  20585 th iteration => 0.0157314936948\n",
      "Loss for  20586 th iteration => 0.0157307455092\n",
      "Loss for  20587 th iteration => 0.0157296587844\n",
      "Loss for  20588 th iteration => 0.0157289156762\n",
      "Loss for  20589 th iteration => 0.0157279875375\n",
      "Loss for  20590 th iteration => 0.015727090702\n",
      "Loss for  20591 th iteration => 0.0157261566387\n",
      "Loss for  20592 th iteration => 0.0157253375101\n",
      "Loss for  20593 th iteration => 0.0157246895877\n",
      "Loss for  20594 th iteration => 0.0157235949567\n",
      "Loss for  20595 th iteration => 0.0157229412344\n",
      "Loss for  20596 th iteration => 0.0157218926134\n",
      "Loss for  20597 th iteration => 0.0157212168617\n",
      "Loss for  20598 th iteration => 0.01572022741\n",
      "Loss for  20599 th iteration => 0.0157193892553\n",
      "Loss for  20600 th iteration => 0.0157185603536\n",
      "Loss for  20601 th iteration => 0.0157175702957\n",
      "Loss for  20602 th iteration => 0.0157167158206\n",
      "Loss for  20603 th iteration => 0.0157160248205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  20604 th iteration => 0.0157150475378\n",
      "Loss for  20605 th iteration => 0.0157142011414\n",
      "Loss for  20606 th iteration => 0.0157133774956\n",
      "Loss for  20607 th iteration => 0.0157124418627\n",
      "Loss for  20608 th iteration => 0.0157117949757\n",
      "Loss for  20609 th iteration => 0.0157107019036\n",
      "Loss for  20610 th iteration => 0.0157100491665\n",
      "Loss for  20611 th iteration => 0.0157089627152\n",
      "Loss for  20612 th iteration => 0.0157083090926\n",
      "Loss for  20613 th iteration => 0.0157074878965\n",
      "Loss for  20614 th iteration => 0.0157063860079\n",
      "Loss for  20615 th iteration => 0.0157057537171\n",
      "Loss for  20616 th iteration => 0.0157046325078\n",
      "Loss for  20617 th iteration => 0.0157040200976\n",
      "Loss for  20618 th iteration => 0.0157029217269\n",
      "Loss for  20619 th iteration => 0.0157023104866\n",
      "Loss for  20620 th iteration => 0.0157012568181\n",
      "Loss for  20621 th iteration => 0.0157004890966\n",
      "Loss for  20622 th iteration => 0.0156995901454\n",
      "Loss for  20623 th iteration => 0.0156986941861\n",
      "Loss for  20624 th iteration => 0.0156980465558\n",
      "Loss for  20625 th iteration => 0.015696957083\n",
      "Loss for  20626 th iteration => 0.0156963035851\n",
      "Loss for  20627 th iteration => 0.0156952207528\n",
      "Loss for  20628 th iteration => 0.0156945615621\n",
      "Loss for  20629 th iteration => 0.0156935363172\n",
      "Loss for  20630 th iteration => 0.0156928322265\n",
      "Loss for  20631 th iteration => 0.0156918762013\n",
      "Loss for  20632 th iteration => 0.0156910120972\n",
      "Loss for  20633 th iteration => 0.0156902142547\n",
      "Loss for  20634 th iteration => 0.0156892111287\n",
      "Loss for  20635 th iteration => 0.0156883712852\n",
      "Loss for  20636 th iteration => 0.0156876651127\n",
      "Loss for  20637 th iteration => 0.0156867082006\n",
      "Loss for  20638 th iteration => 0.0156858487741\n",
      "Loss for  20639 th iteration => 0.0156850433766\n",
      "Loss for  20640 th iteration => 0.0156841068907\n",
      "Loss for  20641 th iteration => 0.0156834550174\n",
      "Loss for  20642 th iteration => 0.0156823731193\n",
      "Loss for  20643 th iteration => 0.0156817154916\n",
      "Loss for  20644 th iteration => 0.015680640109\n",
      "Loss for  20645 th iteration => 0.0156799822935\n",
      "Loss for  20646 th iteration => 0.0156791715106\n",
      "Loss for  20647 th iteration => 0.0156780648291\n",
      "Loss for  20648 th iteration => 0.0156774434302\n",
      "Loss for  20649 th iteration => 0.0156763177033\n",
      "Loss for  20650 th iteration => 0.0156757159028\n",
      "Loss for  20651 th iteration => 0.0156746199979\n",
      "Loss for  20652 th iteration => 0.0156740053662\n",
      "Loss for  20653 th iteration => 0.0156729603908\n",
      "Loss for  20654 th iteration => 0.0156721911254\n",
      "Loss for  20655 th iteration => 0.0156712990376\n",
      "Loss for  20656 th iteration => 0.0156704075574\n",
      "Loss for  20657 th iteration => 0.0156697554691\n",
      "Loss for  20658 th iteration => 0.0156686766074\n",
      "Loss for  20659 th iteration => 0.0156680187372\n",
      "Loss for  20660 th iteration => 0.0156669464215\n",
      "Loss for  20661 th iteration => 0.0156662829424\n",
      "Loss for  20662 th iteration => 0.0156652676366\n",
      "Loss for  20663 th iteration => 0.0156645608332\n",
      "Loss for  20664 th iteration => 0.0156636128365\n",
      "Loss for  20665 th iteration => 0.0156627477662\n",
      "Loss for  20666 th iteration => 0.0156619562234\n",
      "Loss for  20667 th iteration => 0.0156609580623\n",
      "Loss for  20668 th iteration => 0.015660115715\n",
      "Loss for  20669 th iteration => 0.0156594169841\n",
      "Loss for  20670 th iteration => 0.0156584580249\n",
      "Loss for  20671 th iteration => 0.015657607611\n",
      "Loss for  20672 th iteration => 0.0156567986114\n",
      "Loss for  20673 th iteration => 0.0156558714867\n",
      "Loss for  20674 th iteration => 0.0156552156567\n",
      "Loss for  20675 th iteration => 0.0156541438322\n",
      "Loss for  20676 th iteration => 0.0156534823242\n",
      "Loss for  20677 th iteration => 0.015652416931\n",
      "Loss for  20678 th iteration => 0.0156517510371\n",
      "Loss for  20679 th iteration => 0.0156509536339\n",
      "Loss for  20680 th iteration => 0.0156498450884\n",
      "Loss for  20681 th iteration => 0.0156492316066\n",
      "Loss for  20682 th iteration => 0.0156481042238\n",
      "Loss for  20683 th iteration => 0.0156475101267\n",
      "Loss for  20684 th iteration => 0.0156464088065\n",
      "Loss for  20685 th iteration => 0.0156458093276\n",
      "Loss for  20686 th iteration => 0.0156447546574\n",
      "Loss for  20687 th iteration => 0.0156440019213\n",
      "Loss for  20688 th iteration => 0.0156430987768\n",
      "Loss for  20689 th iteration => 0.0156422200684\n",
      "Loss for  20690 th iteration => 0.0156415645029\n",
      "Loss for  20691 th iteration => 0.0156404952078\n",
      "Loss for  20692 th iteration => 0.0156398339295\n",
      "Loss for  20693 th iteration => 0.0156387711042\n",
      "Loss for  20694 th iteration => 0.0156381042846\n",
      "Loss for  20695 th iteration => 0.0156370898583\n",
      "Loss for  20696 th iteration => 0.0156363974669\n",
      "Loss for  20697 th iteration => 0.0156354405174\n",
      "Loss for  20698 th iteration => 0.0156345911747\n",
      "Loss for  20699 th iteration => 0.0156337893781\n",
      "Loss for  20700 th iteration => 0.0156328082324\n",
      "Loss for  20701 th iteration => 0.0156319520085\n",
      "Loss for  20702 th iteration => 0.0156312756698\n",
      "Loss for  20703 th iteration => 0.0156302998347\n",
      "Loss for  20704 th iteration => 0.0156294730017\n",
      "Loss for  20705 th iteration => 0.0156286459507\n",
      "Loss for  20706 th iteration => 0.0156277348318\n",
      "Loss for  20707 th iteration => 0.0156270758904\n",
      "Loss for  20708 th iteration => 0.0156260132372\n",
      "Loss for  20709 th iteration => 0.0156253486805\n",
      "Loss for  20710 th iteration => 0.0156242923895\n",
      "Loss for  20711 th iteration => 0.0156236223867\n",
      "Loss for  20712 th iteration => 0.0156226465493\n",
      "Loss for  20713 th iteration => 0.0156218858253\n",
      "Loss for  20714 th iteration => 0.0156209992455\n",
      "Loss for  20715 th iteration => 0.0156200841209\n",
      "Loss for  20716 th iteration => 0.0156193501642\n",
      "Loss for  20717 th iteration => 0.0156183223522\n",
      "Loss for  20718 th iteration => 0.0156175126291\n",
      "Loss for  20719 th iteration => 0.0156167787911\n",
      "Loss for  20720 th iteration => 0.0156158626117\n",
      "Loss for  20721 th iteration => 0.0156149805638\n",
      "Loss for  20722 th iteration => 0.0156142157736\n",
      "Loss for  20723 th iteration => 0.0156135026314\n",
      "Loss for  20724 th iteration => 0.015612434935\n",
      "Loss for  20725 th iteration => 0.0156116959394\n",
      "Loss for  20726 th iteration => 0.0156107982391\n",
      "Loss for  20727 th iteration => 0.015609897925\n",
      "Loss for  20728 th iteration => 0.0156089782148\n",
      "Loss for  20729 th iteration => 0.0156083368112\n",
      "Loss for  20730 th iteration => 0.0156073341163\n",
      "Loss for  20731 th iteration => 0.0156065375892\n",
      "Loss for  20732 th iteration => 0.0156056882385\n",
      "Loss for  20733 th iteration => 0.0156047786876\n",
      "Loss for  20734 th iteration => 0.015604099478\n",
      "Loss for  20735 th iteration => 0.0156030635318\n",
      "Loss for  20736 th iteration => 0.0156023694734\n",
      "Loss for  20737 th iteration => 0.0156013490347\n",
      "Loss for  20738 th iteration => 0.0156006406317\n",
      "Loss for  20739 th iteration => 0.015599672973\n",
      "Loss for  20740 th iteration => 0.0155990272032\n",
      "Loss for  20741 th iteration => 0.0155980401056\n",
      "Loss for  20742 th iteration => 0.0155972252637\n",
      "Loss for  20743 th iteration => 0.0155964052669\n",
      "Loss for  20744 th iteration => 0.0155954485488\n",
      "Loss for  20745 th iteration => 0.0155945829284\n",
      "Loss for  20746 th iteration => 0.0155938781408\n",
      "Loss for  20747 th iteration => 0.0155929408886\n",
      "Loss for  20748 th iteration => 0.0155920834349\n",
      "Loss for  20749 th iteration => 0.0155912970898\n",
      "Loss for  20750 th iteration => 0.0155903620573\n",
      "Loss for  20751 th iteration => 0.015589677152\n",
      "Loss for  20752 th iteration => 0.0155886500725\n",
      "Loss for  20753 th iteration => 0.015587950538\n",
      "Loss for  20754 th iteration => 0.0155869387383\n",
      "Loss for  20755 th iteration => 0.0155862250736\n",
      "Loss for  20756 th iteration => 0.0155852929706\n",
      "Loss for  20757 th iteration => 0.0155845886289\n",
      "Loss for  20758 th iteration => 0.0155836621373\n",
      "Loss for  20759 th iteration => 0.0155827911381\n",
      "Loss for  20760 th iteration => 0.0155820293545\n",
      "Loss for  20761 th iteration => 0.0155810311858\n",
      "Loss for  20762 th iteration => 0.0155802058593\n",
      "Loss for  20763 th iteration => 0.015579454689\n",
      "Loss for  20764 th iteration => 0.0155785660405\n",
      "Loss for  20765 th iteration => 0.015577664239\n",
      "Loss for  20766 th iteration => 0.0155769356733\n",
      "Loss for  20767 th iteration => 0.0155761777918\n",
      "Loss for  20768 th iteration => 0.0155751185422\n",
      "Loss for  20769 th iteration => 0.0155743864035\n",
      "Loss for  20770 th iteration => 0.0155734766001\n",
      "Loss for  20771 th iteration => 0.0155725985302\n",
      "Loss for  20772 th iteration => 0.0155718329892\n",
      "Loss for  20773 th iteration => 0.0155708976556\n",
      "Loss for  20774 th iteration => 0.0155702499595\n",
      "Loss for  20775 th iteration => 0.0155691884446\n",
      "Loss for  20776 th iteration => 0.0155685352556\n",
      "Loss for  20777 th iteration => 0.0155674860915\n",
      "Loss for  20778 th iteration => 0.0155666413913\n",
      "Loss for  20779 th iteration => 0.015566019129\n",
      "Loss for  20780 th iteration => 0.0155649184579\n",
      "Loss for  20781 th iteration => 0.015564315374\n",
      "Loss for  20782 th iteration => 0.0155631987598\n",
      "Loss for  20783 th iteration => 0.0155624331445\n",
      "Loss for  20784 th iteration => 0.0155617594881\n",
      "Loss for  20785 th iteration => 0.0155607271843\n",
      "Loss for  20786 th iteration => 0.0155600390188\n",
      "Loss for  20787 th iteration => 0.0155590218678\n",
      "Loss for  20788 th iteration => 0.0155583544154\n",
      "Loss for  20789 th iteration => 0.0155576038873\n",
      "Loss for  20790 th iteration => 0.0155565385867\n",
      "Loss for  20791 th iteration => 0.015555891569\n",
      "Loss for  20792 th iteration => 0.0155548325875\n",
      "Loss for  20793 th iteration => 0.0155541801531\n",
      "Loss for  20794 th iteration => 0.0155531484068\n",
      "Loss for  20795 th iteration => 0.0155522865906\n",
      "Loss for  20796 th iteration => 0.0155516713476\n",
      "Loss for  20797 th iteration => 0.0155505670621\n",
      "Loss for  20798 th iteration => 0.0155499706961\n",
      "Loss for  20799 th iteration => 0.0155488645665\n",
      "Loss for  20800 th iteration => 0.0155480912984\n",
      "Loss for  20801 th iteration => 0.0155474143023\n",
      "Loss for  20802 th iteration => 0.0155463884819\n",
      "Loss for  20803 th iteration => 0.0155456971811\n",
      "Loss for  20804 th iteration => 0.0155446863015\n",
      "Loss for  20805 th iteration => 0.0155440323117\n",
      "Loss for  20806 th iteration => 0.0155432690678\n",
      "Loss for  20807 th iteration => 0.0155422057484\n",
      "Loss for  20808 th iteration => 0.0155415599962\n",
      "Loss for  20809 th iteration => 0.0155405029233\n",
      "Loss for  20810 th iteration => 0.015539851817\n",
      "Loss for  20811 th iteration => 0.0155388329436\n",
      "Loss for  20812 th iteration => 0.0155379593643\n",
      "Loss for  20813 th iteration => 0.015537349011\n",
      "Loss for  20814 th iteration => 0.0155362431704\n",
      "Loss for  20815 th iteration => 0.0155356514441\n",
      "Loss for  20816 th iteration => 0.0155345562623\n",
      "Loss for  20817 th iteration => 0.0155339894219\n",
      "Loss for  20818 th iteration => 0.0155329235868\n",
      "Loss for  20819 th iteration => 0.0155322090608\n",
      "Loss for  20820 th iteration => 0.0155312892374\n",
      "Loss for  20821 th iteration => 0.0155304396031\n",
      "Loss for  20822 th iteration => 0.0155299420684\n",
      "Loss for  20823 th iteration => 0.0155287479802\n",
      "Loss for  20824 th iteration => 0.0155281017645\n",
      "Loss for  20825 th iteration => 0.015527047827\n",
      "Loss for  20826 th iteration => 0.0155263961987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  20827 th iteration => 0.0155253510503\n",
      "Loss for  20828 th iteration => 0.015524516196\n",
      "Loss for  20829 th iteration => 0.0155237236193\n",
      "Loss for  20830 th iteration => 0.015522945845\n",
      "Loss for  20831 th iteration => 0.0155220941359\n",
      "Loss for  20832 th iteration => 0.0155211696735\n",
      "Loss for  20833 th iteration => 0.0155204789399\n",
      "Loss for  20834 th iteration => 0.0155196980854\n",
      "Loss for  20835 th iteration => 0.015518664817\n",
      "Loss for  20836 th iteration => 0.015517920857\n",
      "Loss for  20837 th iteration => 0.015517033375\n",
      "Loss for  20838 th iteration => 0.0155161476376\n",
      "Loss for  20839 th iteration => 0.0155152491533\n",
      "Loss for  20840 th iteration => 0.0155146738351\n",
      "Loss for  20841 th iteration => 0.0155136320354\n",
      "Loss for  20842 th iteration => 0.0155128926244\n",
      "Loss for  20843 th iteration => 0.0155120130026\n",
      "Loss for  20844 th iteration => 0.0155111438126\n",
      "Loss for  20845 th iteration => 0.0155104167175\n",
      "Loss for  20846 th iteration => 0.0155094325796\n",
      "Loss for  20847 th iteration => 0.0155087248927\n",
      "Loss for  20848 th iteration => 0.015507722547\n",
      "Loss for  20849 th iteration => 0.0155070335875\n",
      "Loss for  20850 th iteration => 0.0155060475184\n",
      "Loss for  20851 th iteration => 0.0155054103663\n",
      "Loss for  20852 th iteration => 0.0155044200361\n",
      "Loss for  20853 th iteration => 0.0155036372227\n",
      "Loss for  20854 th iteration => 0.0155027908979\n",
      "Loss for  20855 th iteration => 0.0155018765045\n",
      "Loss for  20856 th iteration => 0.015501010355\n",
      "Loss for  20857 th iteration => 0.0155003970341\n",
      "Loss for  20858 th iteration => 0.0154993954335\n",
      "Loss for  20859 th iteration => 0.0154986199338\n",
      "Loss for  20860 th iteration => 0.0154977786147\n",
      "Loss for  20861 th iteration => 0.015496891849\n",
      "Loss for  20862 th iteration => 0.0154961675005\n",
      "Loss for  20863 th iteration => 0.0154951838793\n",
      "Loss for  20864 th iteration => 0.0154944787314\n",
      "Loss for  20865 th iteration => 0.0154934770994\n",
      "Loss for  20866 th iteration => 0.015492790477\n",
      "Loss for  20867 th iteration => 0.015491820521\n",
      "Loss for  20868 th iteration => 0.015491154715\n",
      "Loss for  20869 th iteration => 0.0154901954375\n",
      "Loss for  20870 th iteration => 0.0154893854708\n",
      "Loss for  20871 th iteration => 0.0154885687108\n",
      "Loss for  20872 th iteration => 0.0154876347354\n",
      "Loss for  20873 th iteration => 0.0154867911885\n",
      "Loss for  20874 th iteration => 0.0154861524664\n",
      "Loss for  20875 th iteration => 0.0154851785881\n",
      "Loss for  20876 th iteration => 0.0154843792846\n",
      "Loss for  20877 th iteration => 0.0154835641058\n",
      "Loss for  20878 th iteration => 0.0154826665519\n",
      "Loss for  20879 th iteration => 0.0154819435603\n",
      "Loss for  20880 th iteration => 0.0154809617953\n",
      "Loss for  20881 th iteration => 0.0154802578317\n",
      "Loss for  20882 th iteration => 0.0154792582193\n",
      "Loss for  20883 th iteration => 0.0154785726136\n",
      "Loss for  20884 th iteration => 0.0154776145352\n",
      "Loss for  20885 th iteration => 0.0154769298004\n",
      "Loss for  20886 th iteration => 0.0154759919384\n",
      "Loss for  20887 th iteration => 0.0154751643039\n",
      "Loss for  20888 th iteration => 0.0154743677094\n",
      "Loss for  20889 th iteration => 0.0154734215065\n",
      "Loss for  20890 th iteration => 0.0154725926812\n",
      "Loss for  20891 th iteration => 0.0154719387538\n",
      "Loss for  20892 th iteration => 0.0154709825021\n",
      "Loss for  20893 th iteration => 0.0154701693337\n",
      "Loss for  20894 th iteration => 0.0154693743265\n",
      "Loss for  20895 th iteration => 0.0154686508744\n",
      "Loss for  20896 th iteration => 0.0154675962015\n",
      "Loss for  20897 th iteration => 0.0154668840165\n",
      "Loss for  20898 th iteration => 0.0154659779109\n",
      "Loss for  20899 th iteration => 0.0154651205769\n",
      "Loss for  20900 th iteration => 0.0154643579379\n",
      "Loss for  20901 th iteration => 0.0154634234772\n",
      "Loss for  20902 th iteration => 0.0154627670935\n",
      "Loss for  20903 th iteration => 0.0154617387108\n",
      "Loss for  20904 th iteration => 0.0154610683233\n",
      "Loss for  20905 th iteration => 0.0154600545647\n",
      "Loss for  20906 th iteration => 0.0154593706507\n",
      "Loss for  20907 th iteration => 0.0154584303212\n",
      "Loss for  20908 th iteration => 0.015457764536\n",
      "Loss for  20909 th iteration => 0.0154568231065\n",
      "Loss for  20910 th iteration => 0.0154559980776\n",
      "Loss for  20911 th iteration => 0.0154552140251\n",
      "Loss for  20912 th iteration => 0.0154542578871\n",
      "Loss for  20913 th iteration => 0.0154534269134\n",
      "Loss for  20914 th iteration => 0.0154527099251\n",
      "Loss for  20915 th iteration => 0.0154518111986\n",
      "Loss for  20916 th iteration => 0.0154509500497\n",
      "Loss for  20917 th iteration => 0.0154501938111\n",
      "Loss for  20918 th iteration => 0.015449259625\n",
      "Loss for  20919 th iteration => 0.0154486024243\n",
      "Loss for  20920 th iteration => 0.0154475778831\n",
      "Loss for  20921 th iteration => 0.0154469067726\n",
      "Loss for  20922 th iteration => 0.0154458967576\n",
      "Loss for  20923 th iteration => 0.015445212212\n",
      "Loss for  20924 th iteration => 0.0154442767194\n",
      "Loss for  20925 th iteration => 0.0154436080658\n",
      "Loss for  20926 th iteration => 0.0154426720585\n",
      "Loss for  20927 th iteration => 0.0154418451491\n",
      "Loss for  20928 th iteration => 0.0154410655414\n",
      "Loss for  20929 th iteration => 0.0154401093138\n",
      "Loss for  20930 th iteration => 0.0154392801978\n",
      "Loss for  20931 th iteration => 0.0154385641736\n",
      "Loss for  20932 th iteration => 0.01543766711\n",
      "Loss for  20933 th iteration => 0.0154368077692\n",
      "Loss for  20934 th iteration => 0.0154360523581\n",
      "Loss for  20935 th iteration => 0.0154351209499\n",
      "Loss for  20936 th iteration => 0.0154344634757\n",
      "Loss for  20937 th iteration => 0.0154334422184\n",
      "Loss for  20938 th iteration => 0.0154327709159\n",
      "Loss for  20939 th iteration => 0.0154317640999\n",
      "Loss for  20940 th iteration => 0.0154310794414\n",
      "Loss for  20941 th iteration => 0.0154301456973\n",
      "Loss for  20942 th iteration => 0.0154294797228\n",
      "Loss for  20943 th iteration => 0.0154285436418\n",
      "Loss for  20944 th iteration => 0.0154277202586\n",
      "Loss for  20945 th iteration => 0.0154269397394\n",
      "Loss for  20946 th iteration => 0.015425987532\n",
      "Loss for  20947 th iteration => 0.0154251564856\n",
      "Loss for  20948 th iteration => 0.0154244460875\n",
      "Loss for  20949 th iteration => 0.0154235460652\n",
      "Loss for  20950 th iteration => 0.0154226930769\n",
      "Loss for  20951 th iteration => 0.0154219339882\n",
      "Loss for  20952 th iteration => 0.0154210073798\n",
      "Loss for  20953 th iteration => 0.0154203500717\n",
      "Loss for  20954 th iteration => 0.0154193316458\n",
      "Loss for  20955 th iteration => 0.0154186605807\n",
      "Loss for  20956 th iteration => 0.0154176565219\n",
      "Loss for  20957 th iteration => 0.01541697217\n",
      "Loss for  20958 th iteration => 0.0154160376758\n",
      "Loss for  20959 th iteration => 0.0154153788632\n",
      "Loss for  20960 th iteration => 0.0154144382661\n",
      "Loss for  20961 th iteration => 0.0154136227779\n",
      "Loss for  20962 th iteration => 0.0154128370179\n",
      "Loss for  20963 th iteration => 0.0154118921525\n",
      "Loss for  20964 th iteration => 0.0154110561147\n",
      "Loss for  20965 th iteration => 0.0154103551083\n",
      "Loss for  20966 th iteration => 0.0154094483927\n",
      "Loss for  20967 th iteration => 0.0154086054275\n",
      "Loss for  20968 th iteration => 0.0154078390213\n",
      "Loss for  20969 th iteration => 0.0154069188443\n",
      "Loss for  20970 th iteration => 0.0154062620575\n",
      "Loss for  20971 th iteration => 0.0154052460961\n",
      "Loss for  20972 th iteration => 0.0154045756153\n",
      "Loss for  20973 th iteration => 0.0154035739552\n",
      "Loss for  20974 th iteration => 0.0154028902486\n",
      "Loss for  20975 th iteration => 0.0154019529839\n",
      "Loss for  20976 th iteration => 0.0154013049535\n",
      "Loss for  20977 th iteration => 0.0154003562516\n",
      "Loss for  20978 th iteration => 0.0153995521858\n",
      "Loss for  20979 th iteration => 0.0153987576883\n",
      "Loss for  20980 th iteration => 0.0153978228479\n",
      "Loss for  20981 th iteration => 0.0153969793465\n",
      "Loss for  20982 th iteration => 0.0153962907709\n",
      "Loss for  20983 th iteration => 0.0153953743471\n",
      "Loss for  20984 th iteration => 0.0153945443667\n",
      "Loss for  20985 th iteration => 0.0153937677046\n",
      "Loss for  20986 th iteration => 0.0153928552744\n",
      "Loss for  20987 th iteration => 0.0153921992959\n",
      "Loss for  20988 th iteration => 0.015391185501\n",
      "Loss for  20989 th iteration => 0.0153905158847\n",
      "Loss for  20990 th iteration => 0.0153895163324\n",
      "Loss for  20991 th iteration => 0.0153888335448\n",
      "Loss for  20992 th iteration => 0.0153878918766\n",
      "Loss for  20993 th iteration => 0.0153872575491\n",
      "Loss for  20994 th iteration => 0.0153862978458\n",
      "Loss for  20995 th iteration => 0.0153855080477\n",
      "Loss for  20996 th iteration => 0.0153847019911\n",
      "Loss for  20997 th iteration => 0.0153837793408\n",
      "Loss for  20998 th iteration => 0.0153829263814\n",
      "Loss for  20999 th iteration => 0.0153822526867\n",
      "Loss for  21000 th iteration => 0.0153813241226\n",
      "Loss for  21001 th iteration => 0.0153805095142\n",
      "Loss for  21002 th iteration => 0.0153797202267\n",
      "Loss for  21003 th iteration => 0.0153788166022\n",
      "Loss for  21004 th iteration => 0.0153781616633\n",
      "Loss for  21005 th iteration => 0.0153771497933\n",
      "Loss for  21006 th iteration => 0.0153764812674\n",
      "Loss for  21007 th iteration => 0.0153754835868\n",
      "Loss for  21008 th iteration => 0.0153748019386\n",
      "Loss for  21009 th iteration => 0.0153738545486\n",
      "Loss for  21010 th iteration => 0.0153732362774\n",
      "Loss for  21011 th iteration => 0.0153722632376\n",
      "Loss for  21012 th iteration => 0.0153714899994\n",
      "Loss for  21013 th iteration => 0.0153706701091\n",
      "Loss for  21014 th iteration => 0.0153697613943\n",
      "Loss for  21015 th iteration => 0.0153688973695\n",
      "Loss for  21016 th iteration => 0.0153682405284\n",
      "Loss for  21017 th iteration => 0.0153672978646\n",
      "Loss for  21018 th iteration => 0.0153665005497\n",
      "Loss for  21019 th iteration => 0.0153656967284\n",
      "Loss for  21020 th iteration => 0.0153648080135\n",
      "Loss for  21021 th iteration => 0.0153643365084\n",
      "Loss for  21022 th iteration => 0.0153631558863\n",
      "Loss for  21023 th iteration => 0.0153624870018\n",
      "Loss for  21024 th iteration => 0.0153614925302\n",
      "Loss for  21025 th iteration => 0.0153608104296\n",
      "Loss for  21026 th iteration => 0.0153598413741\n",
      "Loss for  21027 th iteration => 0.015358974391\n",
      "Loss for  21028 th iteration => 0.0153582485739\n",
      "Loss for  21029 th iteration => 0.0153575026147\n",
      "Loss for  21030 th iteration => 0.0153566582891\n",
      "Loss for  21031 th iteration => 0.0153557721027\n",
      "Loss for  21032 th iteration => 0.0153548875623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  21033 th iteration => 0.015354259642\n",
      "Loss for  21034 th iteration => 0.0153532909121\n",
      "Loss for  21035 th iteration => 0.0153525226956\n",
      "Loss for  21036 th iteration => 0.015351692634\n",
      "Loss for  21037 th iteration => 0.0153508290128\n",
      "Loss for  21038 th iteration => 0.0153503397813\n",
      "Loss for  21039 th iteration => 0.0153491712071\n",
      "Loss for  21040 th iteration => 0.0153485016466\n",
      "Loss for  21041 th iteration => 0.0153475107818\n",
      "Loss for  21042 th iteration => 0.0153468280373\n",
      "Loss for  21043 th iteration => 0.0153458568467\n",
      "Loss for  21044 th iteration => 0.0153450004625\n",
      "Loss for  21045 th iteration => 0.0153442588004\n",
      "Loss for  21046 th iteration => 0.0153435397416\n",
      "Loss for  21047 th iteration => 0.0153426713482\n",
      "Loss for  21048 th iteration => 0.0153418073831\n",
      "Loss for  21049 th iteration => 0.0153409026998\n",
      "Loss for  21050 th iteration => 0.0153403032069\n",
      "Loss for  21051 th iteration => 0.0153393088934\n",
      "Loss for  21052 th iteration => 0.0153385692894\n",
      "Loss for  21053 th iteration => 0.015337713463\n",
      "Loss for  21054 th iteration => 0.0153368744253\n",
      "Loss for  21055 th iteration => 0.0153363679094\n",
      "Loss for  21056 th iteration => 0.0153352111289\n",
      "Loss for  21057 th iteration => 0.0153345410228\n",
      "Loss for  21058 th iteration => 0.0153335536273\n",
      "Loss for  21059 th iteration => 0.0153328703694\n",
      "Loss for  21060 th iteration => 0.0153318970156\n",
      "Loss for  21061 th iteration => 0.0153310512079\n",
      "Loss for  21062 th iteration => 0.0153302938216\n",
      "Loss for  21063 th iteration => 0.0153296013523\n",
      "Loss for  21064 th iteration => 0.015328709192\n",
      "Loss for  21065 th iteration => 0.0153278674941\n",
      "Loss for  21066 th iteration => 0.015327139163\n",
      "Loss for  21067 th iteration => 0.0153263912149\n",
      "Loss for  21068 th iteration => 0.015325362439\n",
      "Loss for  21069 th iteration => 0.0153246593911\n",
      "Loss for  21070 th iteration => 0.0153237704482\n",
      "Loss for  21071 th iteration => 0.0153229308004\n",
      "Loss for  21072 th iteration => 0.0153221897796\n",
      "Loss for  21073 th iteration => 0.0153214743258\n",
      "Loss for  21074 th iteration => 0.0153204462579\n",
      "Loss for  21075 th iteration => 0.01531974455\n",
      "Loss for  21076 th iteration => 0.0153188525808\n",
      "Loss for  21077 th iteration => 0.0153180179921\n",
      "Loss for  21078 th iteration => 0.0153172573684\n",
      "Loss for  21079 th iteration => 0.0153163376517\n",
      "Loss for  21080 th iteration => 0.0153157570028\n",
      "Loss for  21081 th iteration => 0.0153146828938\n",
      "Loss for  21082 th iteration => 0.0153140970386\n",
      "Loss for  21083 th iteration => 0.0153130288009\n",
      "Loss for  21084 th iteration => 0.0153124378914\n",
      "Loss for  21085 th iteration => 0.0153114408871\n",
      "Loss for  21086 th iteration => 0.0153107743411\n",
      "Loss for  21087 th iteration => 0.0153098523607\n",
      "Loss for  21088 th iteration => 0.0153090478025\n",
      "Loss for  21089 th iteration => 0.0153082622298\n",
      "Loss for  21090 th iteration => 0.0153073251064\n",
      "Loss for  21091 th iteration => 0.0153065173404\n",
      "Loss for  21092 th iteration => 0.0153058550095\n",
      "Loss for  21093 th iteration => 0.0153049266168\n",
      "Loss for  21094 th iteration => 0.0153041312797\n",
      "Loss for  21095 th iteration => 0.0153033343593\n",
      "Loss for  21096 th iteration => 0.0153024360077\n",
      "Loss for  21097 th iteration => 0.0153018547601\n",
      "Loss for  21098 th iteration => 0.015300784137\n",
      "Loss for  21099 th iteration => 0.0153001976913\n",
      "Loss for  21100 th iteration => 0.0152991329304\n",
      "Loss for  21101 th iteration => 0.0152985414383\n",
      "Loss for  21102 th iteration => 0.0152975294873\n",
      "Loss for  21103 th iteration => 0.0152968989296\n",
      "Loss for  21104 th iteration => 0.0152959438899\n",
      "Loss for  21105 th iteration => 0.0152951752378\n",
      "Loss for  21106 th iteration => 0.0152943566899\n",
      "Loss for  21107 th iteration => 0.0152934547171\n",
      "Loss for  21108 th iteration => 0.0152927935646\n",
      "Loss for  21109 th iteration => 0.0152920094191\n",
      "Loss for  21110 th iteration => 0.0152910378934\n",
      "Loss for  21111 th iteration => 0.0152902875905\n",
      "Loss for  21112 th iteration => 0.0152894491624\n",
      "Loss for  21113 th iteration => 0.0152885731617\n",
      "Loss for  21114 th iteration => 0.0152877092459\n",
      "Loss for  21115 th iteration => 0.0152869097687\n",
      "Loss for  21116 th iteration => 0.015286323812\n",
      "Loss for  21117 th iteration => 0.0152852614266\n",
      "Loss for  21118 th iteration => 0.0152846704209\n",
      "Loss for  21119 th iteration => 0.0152836381279\n",
      "Loss for  21120 th iteration => 0.0152830532766\n",
      "Loss for  21121 th iteration => 0.0152820555482\n",
      "Loss for  21122 th iteration => 0.0152813322868\n",
      "Loss for  21123 th iteration => 0.0152804713657\n",
      "Loss for  21124 th iteration => 0.0152796144686\n",
      "Loss for  21125 th iteration => 0.0152788995473\n",
      "Loss for  21126 th iteration => 0.0152781717199\n",
      "Loss for  21127 th iteration => 0.015277158559\n",
      "Loss for  21128 th iteration => 0.0152764526215\n",
      "Loss for  21129 th iteration => 0.0152755728224\n",
      "Loss for  21130 th iteration => 0.0152747366832\n",
      "Loss for  21131 th iteration => 0.0152739855765\n",
      "Loss for  21132 th iteration => 0.01527307227\n",
      "Loss for  21133 th iteration => 0.0152724860553\n",
      "Loss for  21134 th iteration => 0.0152714266899\n",
      "Loss for  21135 th iteration => 0.0152708353904\n",
      "Loss for  21136 th iteration => 0.0152697817629\n",
      "Loss for  21137 th iteration => 0.0152691855281\n",
      "Loss for  21138 th iteration => 0.0152682020614\n",
      "Loss for  21139 th iteration => 0.0152675329882\n",
      "Loss for  21140 th iteration => 0.0152666214968\n",
      "Loss for  21141 th iteration => 0.0152658169512\n",
      "Loss for  21142 th iteration => 0.0152650393538\n",
      "Loss for  21143 th iteration => 0.0152641127783\n",
      "Loss for  21144 th iteration => 0.0152632987487\n",
      "Loss for  21145 th iteration => 0.0152626478313\n",
      "Loss for  21146 th iteration => 0.0152617160996\n",
      "Loss for  21147 th iteration => 0.0152609344689\n",
      "Loss for  21148 th iteration => 0.0152601319399\n",
      "Loss for  21149 th iteration => 0.0152592470692\n",
      "Loss for  21150 th iteration => 0.0152586609646\n",
      "Loss for  21151 th iteration => 0.0152576043289\n",
      "Loss for  21152 th iteration => 0.0152570131347\n",
      "Loss for  21153 th iteration => 0.0152559622421\n",
      "Loss for  21154 th iteration => 0.0152553661077\n",
      "Loss for  21155 th iteration => 0.0152543601506\n",
      "Loss for  21156 th iteration => 0.0152537414455\n",
      "Loss for  21157 th iteration => 0.0152527826378\n",
      "Loss for  21158 th iteration => 0.0152520280244\n",
      "Loss for  21159 th iteration => 0.0152512035457\n",
      "Loss for  21160 th iteration => 0.0152503177264\n",
      "Loss for  21161 th iteration => 0.0152496403133\n",
      "Loss for  21162 th iteration => 0.0152488849871\n",
      "Loss for  21163 th iteration => 0.0152478976496\n",
      "Loss for  21164 th iteration => 0.0152471733256\n",
      "Loss for  21165 th iteration => 0.0152463171147\n",
      "Loss for  21166 th iteration => 0.0152454655546\n",
      "Loss for  21167 th iteration => 0.0152445969245\n",
      "Loss for  21168 th iteration => 0.015243806107\n",
      "Loss for  21169 th iteration => 0.0152432159151\n",
      "Loss for  21170 th iteration => 0.0152421668432\n",
      "Loss for  21171 th iteration => 0.0152415716981\n",
      "Loss for  21172 th iteration => 0.0152405393804\n",
      "Loss for  21173 th iteration => 0.0152399778994\n",
      "Loss for  21174 th iteration => 0.0152389649794\n",
      "Loss for  21175 th iteration => 0.0152382669902\n",
      "Loss for  21176 th iteration => 0.0152373889969\n",
      "Loss for  21177 th iteration => 0.0152365592091\n",
      "Loss for  21178 th iteration => 0.0152358140572\n",
      "Loss for  21179 th iteration => 0.0152351286471\n",
      "Loss for  21180 th iteration => 0.0152340895629\n",
      "Loss for  21181 th iteration => 0.0152334195428\n",
      "Loss for  21182 th iteration => 0.0152325121001\n",
      "Loss for  21183 th iteration => 0.0152317135569\n",
      "Loss for  21184 th iteration => 0.0152309331471\n",
      "Loss for  21185 th iteration => 0.0152300443725\n",
      "Loss for  21186 th iteration => 0.0152294544081\n",
      "Loss for  21187 th iteration => 0.015228407834\n",
      "Loss for  21188 th iteration => 0.0152278128726\n",
      "Loss for  21189 th iteration => 0.0152267719399\n",
      "Loss for  21190 th iteration => 0.0152261721289\n",
      "Loss for  21191 th iteration => 0.0152251828561\n",
      "Loss for  21192 th iteration => 0.015224548122\n",
      "Loss for  21193 th iteration => 0.0152236105577\n",
      "Loss for  21194 th iteration => 0.015222841975\n",
      "Loss for  21195 th iteration => 0.0152220366999\n",
      "Loss for  21196 th iteration => 0.0152211466481\n",
      "Loss for  21197 th iteration => 0.0152203016788\n",
      "Loss for  21198 th iteration => 0.0152196946839\n",
      "Loss for  21199 th iteration => 0.0152187273624\n",
      "Loss for  21200 th iteration => 0.0152179911325\n",
      "Loss for  21201 th iteration => 0.015217151553\n",
      "Loss for  21202 th iteration => 0.0152162963163\n",
      "Loss for  21203 th iteration => 0.0152154917929\n",
      "Loss for  21204 th iteration => 0.0152148457979\n",
      "Loss for  21205 th iteration => 0.0152139288485\n",
      "Loss for  21206 th iteration => 0.0152131387015\n",
      "Loss for  21207 th iteration => 0.0152123642018\n",
      "Loss for  21208 th iteration => 0.0152114494909\n",
      "Loss for  21209 th iteration => 0.0152106738891\n",
      "Loss for  21210 th iteration => 0.0152099466076\n",
      "Loss for  21211 th iteration => 0.015209103349\n",
      "Loss for  21212 th iteration => 0.015208245254\n",
      "Loss for  21213 th iteration => 0.0152075312742\n",
      "Loss for  21214 th iteration => 0.0152065721161\n",
      "Loss for  21215 th iteration => 0.0152057965994\n",
      "Loss for  21216 th iteration => 0.0152051072801\n",
      "Loss for  21217 th iteration => 0.0152042241806\n",
      "Loss for  21218 th iteration => 0.0152034083558\n",
      "Loss for  21219 th iteration => 0.0152026502902\n",
      "Loss for  21220 th iteration => 0.0152017577526\n",
      "Loss for  21221 th iteration => 0.0152011675782\n",
      "Loss for  21222 th iteration => 0.0152001272011\n",
      "Loss for  21223 th iteration => 0.0151995321042\n",
      "Loss for  21224 th iteration => 0.0151984972857\n",
      "Loss for  21225 th iteration => 0.015197897412\n",
      "Loss for  21226 th iteration => 0.0151969198727\n",
      "Loss for  21227 th iteration => 0.0151962715243\n",
      "Loss for  21228 th iteration => 0.0151953526428\n",
      "Loss for  21229 th iteration => 0.0151945723482\n",
      "Loss for  21230 th iteration => 0.015193783872\n",
      "Loss for  21231 th iteration => 0.0151928824625\n",
      "Loss for  21232 th iteration => 0.0151920529528\n",
      "Loss for  21233 th iteration => 0.0151914384404\n",
      "Loss for  21234 th iteration => 0.015190483783\n",
      "Loss for  21235 th iteration => 0.0151897417661\n",
      "Loss for  21236 th iteration => 0.0151889131367\n",
      "Loss for  21237 th iteration => 0.0151880579712\n",
      "Loss for  21238 th iteration => 0.0151874687731\n",
      "Loss for  21239 th iteration => 0.0151864301919\n",
      "Loss for  21240 th iteration => 0.0151858360499\n",
      "Loss for  21241 th iteration => 0.0151848030505\n",
      "Loss for  21242 th iteration => 0.0151842041105\n",
      "Loss for  21243 th iteration => 0.0151831945703\n",
      "Loss for  21244 th iteration => 0.0151826147624\n",
      "Loss for  21245 th iteration => 0.0151816305393\n",
      "Loss for  21246 th iteration => 0.0151809179025\n",
      "Loss for  21247 th iteration => 0.0151800649631\n",
      "Loss for  21248 th iteration => 0.0151792240964\n",
      "Loss for  21249 th iteration => 0.0151785048751\n",
      "Loss for  21250 th iteration => 0.0151778089229\n",
      "Loss for  21251 th iteration => 0.0151767828496\n",
      "Loss for  21252 th iteration => 0.0151761136903\n",
      "Loss for  21253 th iteration => 0.015175215943\n",
      "Loss for  21254 th iteration => 0.0151744215059\n",
      "Loss for  21255 th iteration => 0.0151736475784\n",
      "Loss for  21256 th iteration => 0.0151727692817\n",
      "Loss for  21257 th iteration => 0.0151721762774\n",
      "Loss for  21258 th iteration => 0.0151711448047\n",
      "Loss for  21259 th iteration => 0.0151705469405\n",
      "Loss for  21260 th iteration => 0.0151695209578\n",
      "Loss for  21261 th iteration => 0.0151689183778\n",
      "Loss for  21262 th iteration => 0.0151679393049\n",
      "Loss for  21263 th iteration => 0.0151673103204\n",
      "Loss for  21264 th iteration => 0.0151663775859\n",
      "Loss for  21265 th iteration => 0.0151656178361\n",
      "Loss for  21266 th iteration => 0.0151648143396\n",
      "Loss for  21267 th iteration => 0.0151639371234\n",
      "Loss for  21268 th iteration => 0.0151630870524\n",
      "Loss for  21269 th iteration => 0.0151624989781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  21270 th iteration => 0.0151615234432\n",
      "Loss for  21271 th iteration => 0.0151608089369\n",
      "Loss for  21272 th iteration => 0.0151599583699\n",
      "Loss for  21273 th iteration => 0.0151591218841\n",
      "Loss for  21274 th iteration => 0.0151584443704\n",
      "Loss for  21275 th iteration => 0.0151576991823\n",
      "Loss for  21276 th iteration => 0.015156771888\n",
      "Loss for  21277 th iteration => 0.0151560048632\n",
      "Loss for  21278 th iteration => 0.0151552183387\n",
      "Loss for  21279 th iteration => 0.0151543221181\n",
      "Loss for  21280 th iteration => 0.0151535072705\n",
      "Loss for  21281 th iteration => 0.0151526657917\n",
      "Loss for  21282 th iteration => 0.0151520923316\n",
      "Loss for  21283 th iteration => 0.0151510334609\n",
      "Loss for  21284 th iteration => 0.0151504758633\n",
      "Loss for  21285 th iteration => 0.0151494244779\n",
      "Loss for  21286 th iteration => 0.0151486965839\n",
      "Loss for  21287 th iteration => 0.0151480320357\n",
      "Loss for  21288 th iteration => 0.0151470783202\n",
      "Loss for  21289 th iteration => 0.015146401608\n",
      "Loss for  21290 th iteration => 0.0151454606025\n",
      "Loss for  21291 th iteration => 0.0151447959712\n",
      "Loss for  21292 th iteration => 0.0151441108163\n",
      "Loss for  21293 th iteration => 0.0151430806832\n",
      "Loss for  21294 th iteration => 0.0151424872765\n",
      "Loss for  21295 th iteration => 0.0151414626281\n",
      "Loss for  21296 th iteration => 0.0151408645083\n",
      "Loss for  21297 th iteration => 0.0151398694629\n",
      "Loss for  21298 th iteration => 0.0151390625745\n",
      "Loss for  21299 th iteration => 0.0151383123611\n",
      "Loss for  21300 th iteration => 0.0151375695608\n",
      "Loss for  21301 th iteration => 0.0151367544385\n",
      "Loss for  21302 th iteration => 0.0151358865406\n",
      "Loss for  21303 th iteration => 0.0151352097148\n",
      "Loss for  21304 th iteration => 0.0151344816811\n",
      "Loss for  21305 th iteration => 0.0151334866812\n",
      "Loss for  21306 th iteration => 0.0151327971438\n",
      "Loss for  21307 th iteration => 0.0151319275574\n",
      "Loss for  21308 th iteration => 0.0151311155956\n",
      "Loss for  21309 th iteration => 0.0151303670026\n",
      "Loss for  21310 th iteration => 0.0151294844984\n",
      "Loss for  21311 th iteration => 0.0151288924601\n",
      "Loss for  21312 th iteration => 0.0151278691384\n",
      "Loss for  21313 th iteration => 0.0151272723427\n",
      "Loss for  21314 th iteration => 0.0151262543966\n",
      "Loss for  21315 th iteration => 0.0151256529852\n",
      "Loss for  21316 th iteration => 0.0151246880135\n",
      "Loss for  21317 th iteration => 0.0151240476713\n",
      "Loss for  21318 th iteration => 0.0151231341141\n",
      "Loss for  21319 th iteration => 0.0151223656912\n",
      "Loss for  21320 th iteration => 0.0151215787141\n",
      "Loss for  21321 th iteration => 0.0151206986809\n",
      "Loss for  21322 th iteration => 0.0151198588172\n",
      "Loss for  21323 th iteration => 0.015119267116\n",
      "Loss for  21324 th iteration => 0.0151183031353\n",
      "Loss for  21325 th iteration => 0.0151175874497\n",
      "Loss for  21326 th iteration => 0.0151167460133\n",
      "Loss for  21327 th iteration => 0.0151159113518\n",
      "Loss for  21328 th iteration => 0.0151153212129\n",
      "Loss for  21329 th iteration => 0.0151142986927\n",
      "Loss for  21330 th iteration => 0.0151137037562\n",
      "Loss for  21331 th iteration => 0.0151126866552\n",
      "Loss for  21332 th iteration => 0.0151120870633\n",
      "Loss for  21333 th iteration => 0.0151110881086\n",
      "Loss for  21334 th iteration => 0.0151103458178\n",
      "Loss for  21335 th iteration => 0.015109681438\n",
      "Loss for  21336 th iteration => 0.0151087221201\n",
      "Loss for  21337 th iteration => 0.0151080738572\n",
      "Loss for  21338 th iteration => 0.0151070994425\n",
      "Loss for  21339 th iteration => 0.0151064667172\n",
      "Loss for  21340 th iteration => 0.0151055445746\n",
      "Loss for  21341 th iteration => 0.0151048917576\n",
      "Loss for  21342 th iteration => 0.0151039914354\n",
      "Loss for  21343 th iteration => 0.015103215593\n",
      "Loss for  21344 th iteration => 0.0151024368642\n",
      "Loss for  21345 th iteration => 0.0151015522599\n",
      "Loss for  21346 th iteration => 0.0151007408177\n",
      "Loss for  21347 th iteration => 0.0151001451395\n",
      "Loss for  21348 th iteration => 0.0150991988958\n",
      "Loss for  21349 th iteration => 0.0150984652918\n",
      "Loss for  21350 th iteration => 0.0150976553415\n",
      "Loss for  21351 th iteration => 0.0150968155379\n",
      "Loss for  21352 th iteration => 0.0150961357551\n",
      "Loss for  21353 th iteration => 0.0150951943619\n",
      "Loss for  21354 th iteration => 0.0150945309382\n",
      "Loss for  21355 th iteration => 0.0150935742129\n",
      "Loss for  21356 th iteration => 0.0150929265648\n",
      "Loss for  21357 th iteration => 0.0150919659308\n",
      "Loss for  21358 th iteration => 0.0150914100403\n",
      "Loss for  21359 th iteration => 0.0150904162288\n",
      "Loss for  21360 th iteration => 0.015089735724\n",
      "Loss for  21361 th iteration => 0.0150888650855\n",
      "Loss for  21362 th iteration => 0.0150880643484\n",
      "Loss for  21363 th iteration => 0.0150873125338\n",
      "Loss for  21364 th iteration => 0.0150864434322\n",
      "Loss for  21365 th iteration => 0.0150858451553\n",
      "Loss for  21366 th iteration => 0.0150848370617\n",
      "Loss for  21367 th iteration => 0.015084234137\n",
      "Loss for  21368 th iteration => 0.0150832312996\n",
      "Loss for  21369 th iteration => 0.0150826238663\n",
      "Loss for  21370 th iteration => 0.0150816672548\n",
      "Loss for  21371 th iteration => 0.015081034499\n",
      "Loss for  21372 th iteration => 0.0150801213395\n",
      "Loss for  21373 th iteration => 0.0150793626091\n",
      "Loss for  21374 th iteration => 0.0150785739458\n",
      "Loss for  21375 th iteration => 0.0150777049021\n",
      "Loss for  21376 th iteration => 0.015076857946\n",
      "Loss for  21377 th iteration => 0.0150762870199\n",
      "Loss for  21378 th iteration => 0.0150753103403\n",
      "Loss for  21379 th iteration => 0.015074617336\n",
      "Loss for  21380 th iteration => 0.015073761315\n",
      "Loss for  21381 th iteration => 0.0150729505468\n",
      "Loss for  21382 th iteration => 0.0150722606723\n",
      "Loss for  21383 th iteration => 0.0150715430056\n",
      "Loss for  21384 th iteration => 0.0150706086734\n",
      "Loss for  21385 th iteration => 0.0150698691128\n",
      "Loss for  21386 th iteration => 0.0150690709066\n",
      "Loss for  21387 th iteration => 0.0150681994341\n",
      "Loss for  21388 th iteration => 0.0150673872454\n",
      "Loss for  21389 th iteration => 0.0150665562272\n",
      "Loss for  21390 th iteration => 0.0150659917523\n",
      "Loss for  21391 th iteration => 0.0150649422708\n",
      "Loss for  21392 th iteration => 0.0150643931195\n",
      "Loss for  21393 th iteration => 0.0150633471624\n",
      "Loss for  21394 th iteration => 0.015062627739\n",
      "Loss for  21395 th iteration => 0.0150619794013\n",
      "Loss for  21396 th iteration => 0.0150610273991\n",
      "Loss for  21397 th iteration => 0.0150603672441\n",
      "Loss for  21398 th iteration => 0.0150594275894\n",
      "Loss for  21399 th iteration => 0.0150587702421\n",
      "Loss for  21400 th iteration => 0.0150580919787\n",
      "Loss for  21401 th iteration => 0.0150570830305\n",
      "Loss for  21402 th iteration => 0.015056486541\n",
      "Loss for  21403 th iteration => 0.0150554828911\n",
      "Loss for  21404 th iteration => 0.015054881853\n",
      "Loss for  21405 th iteration => 0.0150538946183\n",
      "Loss for  21406 th iteration => 0.0150531164821\n",
      "Loss for  21407 th iteration => 0.0150523425902\n",
      "Loss for  21408 th iteration => 0.0150516495064\n",
      "Loss for  21409 th iteration => 0.0150508009249\n",
      "Loss for  21410 th iteration => 0.0150499861896\n",
      "Loss for  21411 th iteration => 0.0150492578189\n",
      "Loss for  21412 th iteration => 0.015048392346\n",
      "Loss for  21413 th iteration => 0.0150477378221\n",
      "Loss for  21414 th iteration => 0.0150467949182\n",
      "Loss for  21415 th iteration => 0.0150461286298\n",
      "Loss for  21416 th iteration => 0.0150451980193\n",
      "Loss for  21417 th iteration => 0.0150445203769\n",
      "Loss for  21418 th iteration => 0.0150436226179\n",
      "Loss for  21419 th iteration => 0.0150430314379\n",
      "Loss for  21420 th iteration => 0.0150420914453\n",
      "Loss for  21421 th iteration => 0.0150413645856\n",
      "Loss for  21422 th iteration => 0.0150405586671\n",
      "Loss for  21423 th iteration => 0.0150397157555\n",
      "Loss for  21424 th iteration => 0.0150388457811\n",
      "Loss for  21425 th iteration => 0.0150381074727\n",
      "Loss for  21426 th iteration => 0.0150374441158\n",
      "Loss for  21427 th iteration => 0.0150364994828\n",
      "Loss for  21428 th iteration => 0.0150358515445\n",
      "Loss for  21429 th iteration => 0.015034901825\n",
      "Loss for  21430 th iteration => 0.0150345025877\n",
      "Loss for  21431 th iteration => 0.0150333694768\n",
      "Loss for  21432 th iteration => 0.015032709477\n",
      "Loss for  21433 th iteration => 0.0150318295395\n",
      "Loss for  21434 th iteration => 0.0150310505126\n",
      "Loss for  21435 th iteration => 0.0150302882099\n",
      "Loss for  21436 th iteration => 0.0150294085336\n",
      "Loss for  21437 th iteration => 0.0150286024409\n",
      "Loss for  21438 th iteration => 0.0150280143738\n",
      "Loss for  21439 th iteration => 0.0150270735012\n",
      "Loss for  21440 th iteration => 0.0150263517626\n",
      "Loss for  21441 th iteration => 0.0150255429756\n",
      "Loss for  21442 th iteration => 0.0150247184562\n",
      "Loss for  21443 th iteration => 0.0150240377054\n",
      "Loss for  21444 th iteration => 0.0150231126691\n",
      "Loss for  21445 th iteration => 0.0150224478052\n",
      "Loss for  21446 th iteration => 0.0150215078811\n",
      "Loss for  21447 th iteration => 0.0150208583367\n",
      "Loss for  21448 th iteration => 0.0150199065318\n",
      "Loss for  21449 th iteration => 0.0150195079322\n",
      "Loss for  21450 th iteration => 0.015018382316\n",
      "Loss for  21451 th iteration => 0.0150177247742\n",
      "Loss for  21452 th iteration => 0.0150168451439\n",
      "Loss for  21453 th iteration => 0.0150160693369\n",
      "Loss for  21454 th iteration => 0.0150153065874\n",
      "Loss for  21455 th iteration => 0.0150144359291\n",
      "Loss for  21456 th iteration => 0.0150136229869\n",
      "Loss for  21457 th iteration => 0.0150128466263\n",
      "Loss for  21458 th iteration => 0.0150122520065\n",
      "Loss for  21459 th iteration => 0.0150112558319\n",
      "Loss for  21460 th iteration => 0.0150106568035\n",
      "Loss for  21461 th iteration => 0.015009691684\n",
      "Loss for  21462 th iteration => 0.0150090981184\n",
      "Loss for  21463 th iteration => 0.0150081592888\n",
      "Loss for  21464 th iteration => 0.0150074427525\n",
      "Loss for  21465 th iteration => 0.0150066254488\n",
      "Loss for  21466 th iteration => 0.015005791302\n",
      "Loss for  21467 th iteration => 0.0150049312191\n",
      "Loss for  21468 th iteration => 0.0150042012116\n",
      "Loss for  21469 th iteration => 0.0150035539262\n",
      "Loss for  21470 th iteration => 0.015002613065\n",
      "Loss for  21471 th iteration => 0.0150019544017\n",
      "Loss for  21472 th iteration => 0.0150010254323\n",
      "Loss for  21473 th iteration => 0.0150003852344\n",
      "Loss for  21474 th iteration => 0.0149996978131\n",
      "Loss for  21475 th iteration => 0.0149986963417\n",
      "Loss for  21476 th iteration => 0.0149981047874\n",
      "Loss for  21477 th iteration => 0.0149971084489\n",
      "Loss for  21478 th iteration => 0.0149965124894\n",
      "Loss for  21479 th iteration => 0.0149955470374\n",
      "Loss for  21480 th iteration => 0.0149947482133\n",
      "Loss for  21481 th iteration => 0.0149940168584\n",
      "Loss for  21482 th iteration => 0.0149932811925\n",
      "Loss for  21483 th iteration => 0.0149924853934\n",
      "Loss for  21484 th iteration => 0.0149916338588\n",
      "Loss for  21485 th iteration => 0.0149907889392\n",
      "Loss for  21486 th iteration => 0.0149902403995\n",
      "Loss for  21487 th iteration => 0.014989257366\n",
      "Loss for  21488 th iteration => 0.0149885906963\n",
      "Loss for  21489 th iteration => 0.0149877244141\n",
      "Loss for  21490 th iteration => 0.014986943802\n",
      "Loss for  21491 th iteration => 0.0149862314119\n",
      "Loss for  21492 th iteration => 0.0149855591043\n",
      "Loss for  21493 th iteration => 0.0149845985629\n",
      "Loss for  21494 th iteration => 0.0149839052334\n",
      "Loss for  21495 th iteration => 0.014983076646\n",
      "Loss for  21496 th iteration => 0.0149822542514\n",
      "Loss for  21497 th iteration => 0.0149815531876\n",
      "Loss for  21498 th iteration => 0.0149806309136\n",
      "Loss for  21499 th iteration => 0.0149800586321\n",
      "Loss for  21500 th iteration => 0.0149790348859\n",
      "Loss for  21501 th iteration => 0.0149784776694\n",
      "Loss for  21502 th iteration => 0.0149774406568\n",
      "Loss for  21503 th iteration => 0.0149767221512\n",
      "Loss for  21504 th iteration => 0.0149758685228\n",
      "Loss for  21505 th iteration => 0.0149751375356\n",
      "Loss for  21506 th iteration => 0.0149744952911\n",
      "Loss for  21507 th iteration => 0.0149735554771\n",
      "Loss for  21508 th iteration => 0.0149729021095\n",
      "Loss for  21509 th iteration => 0.0149720208468\n",
      "Loss for  21510 th iteration => 0.014971549706\n",
      "Loss for  21511 th iteration => 0.0149705062045\n",
      "Loss for  21512 th iteration => 0.0149698045032\n",
      "Loss for  21513 th iteration => 0.0149689871543\n",
      "Loss for  21514 th iteration => 0.0149681563801\n",
      "Loss for  21515 th iteration => 0.0149674665638\n",
      "Loss for  21516 th iteration => 0.0149665330196\n",
      "Loss for  21517 th iteration => 0.0149657756428\n",
      "Loss for  21518 th iteration => 0.0149649387315\n",
      "Loss for  21519 th iteration => 0.0149643772857\n",
      "Loss for  21520 th iteration => 0.0149633466784\n",
      "Loss for  21521 th iteration => 0.0149627996495\n",
      "Loss for  21522 th iteration => 0.0149617879005\n",
      "Loss for  21523 th iteration => 0.0149610583638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  21524 th iteration => 0.0149604218628\n",
      "Loss for  21525 th iteration => 0.0149594791427\n",
      "Loss for  21526 th iteration => 0.0149588314521\n",
      "Loss for  21527 th iteration => 0.0149579004272\n",
      "Loss for  21528 th iteration => 0.0149572706373\n",
      "Loss for  21529 th iteration => 0.0149565864598\n",
      "Loss for  21530 th iteration => 0.0149555862816\n",
      "Loss for  21531 th iteration => 0.0149550024442\n",
      "Loss for  21532 th iteration => 0.0149540073296\n",
      "Loss for  21533 th iteration => 0.0149534191448\n",
      "Loss for  21534 th iteration => 0.014952449013\n",
      "Loss for  21535 th iteration => 0.0149516646735\n",
      "Loss for  21536 th iteration => 0.0149509305222\n",
      "Loss for  21537 th iteration => 0.0149502081067\n",
      "Loss for  21538 th iteration => 0.0149494070273\n",
      "Loss for  21539 th iteration => 0.0149485691572\n",
      "Loss for  21540 th iteration => 0.0149478866282\n",
      "Loss for  21541 th iteration => 0.0149472029392\n",
      "Loss for  21542 th iteration => 0.014946208788\n",
      "Loss for  21543 th iteration => 0.0149455622622\n",
      "Loss for  21544 th iteration => 0.0149446844455\n",
      "Loss for  21545 th iteration => 0.0149439243772\n",
      "Loss for  21546 th iteration => 0.014943158764\n",
      "Loss for  21547 th iteration => 0.0149423161997\n",
      "Loss for  21548 th iteration => 0.0149417357127\n",
      "Loss for  21549 th iteration => 0.0149407397715\n",
      "Loss for  21550 th iteration => 0.0149401548485\n",
      "Loss for  21551 th iteration => 0.0149391639216\n",
      "Loss for  21552 th iteration => 0.0149385746949\n",
      "Loss for  21553 th iteration => 0.0149376067015\n",
      "Loss for  21554 th iteration => 0.0149370381022\n",
      "Loss for  21555 th iteration => 0.0149360876416\n",
      "Loss for  21556 th iteration => 0.0149353994083\n",
      "Loss for  21557 th iteration => 0.0149345671727\n",
      "Loss for  21558 th iteration => 0.0149337634932\n",
      "Loss for  21559 th iteration => 0.0149330453273\n",
      "Loss for  21560 th iteration => 0.0149321897141\n",
      "Loss for  21561 th iteration => 0.0149315533765\n",
      "Loss for  21562 th iteration => 0.0149306163405\n",
      "Loss for  21563 th iteration => 0.0149299687778\n",
      "Loss for  21564 th iteration => 0.0149290434726\n",
      "Loss for  21565 th iteration => 0.0149283850778\n",
      "Loss for  21566 th iteration => 0.0149274841648\n",
      "Loss for  21567 th iteration => 0.014926721691\n",
      "Loss for  21568 th iteration => 0.0149261411687\n",
      "Loss for  21569 th iteration => 0.0149251486621\n",
      "Loss for  21570 th iteration => 0.0149245638149\n",
      "Loss for  21571 th iteration => 0.0149235887499\n",
      "Loss for  21572 th iteration => 0.014922827247\n",
      "Loss for  21573 th iteration => 0.0149220639436\n",
      "Loss for  21574 th iteration => 0.0149213846892\n",
      "Loss for  21575 th iteration => 0.0149205460943\n",
      "Loss for  21576 th iteration => 0.0149197519528\n",
      "Loss for  21577 th iteration => 0.0149190268747\n",
      "Loss for  21578 th iteration => 0.0149181780448\n",
      "Loss for  21579 th iteration => 0.0149175403508\n",
      "Loss for  21580 th iteration => 0.014916607543\n",
      "Loss for  21581 th iteration => 0.0149159586857\n",
      "Loss for  21582 th iteration => 0.0149150375448\n",
      "Loss for  21583 th iteration => 0.0149143779155\n",
      "Loss for  21584 th iteration => 0.0149134755581\n",
      "Loss for  21585 th iteration => 0.0149127183169\n",
      "Loss for  21586 th iteration => 0.01491214174\n",
      "Loss for  21587 th iteration => 0.0149111481717\n",
      "Loss for  21588 th iteration => 0.0149105672793\n",
      "Loss for  21589 th iteration => 0.0149095858222\n",
      "Loss for  21590 th iteration => 0.0149088311152\n",
      "Loss for  21591 th iteration => 0.0149080645605\n",
      "Loss for  21592 th iteration => 0.0149073969672\n",
      "Loss for  21593 th iteration => 0.0149065493513\n",
      "Loss for  21594 th iteration => 0.0149057673628\n",
      "Loss for  21595 th iteration => 0.0149050327775\n",
      "Loss for  21596 th iteration => 0.0149041918607\n",
      "Loss for  21597 th iteration => 0.0149035531306\n",
      "Loss for  21598 th iteration => 0.0149026242208\n",
      "Loss for  21599 th iteration => 0.0149019743832\n",
      "Loss for  21600 th iteration => 0.0149010570824\n",
      "Loss for  21601 th iteration => 0.014900396527\n",
      "Loss for  21602 th iteration => 0.0148994919758\n",
      "Loss for  21603 th iteration => 0.0148987405367\n",
      "Loss for  21604 th iteration => 0.0148981678837\n",
      "Loss for  21605 th iteration => 0.0148971732639\n",
      "Loss for  21606 th iteration => 0.0148965963034\n",
      "Loss for  21607 th iteration => 0.0148956079334\n",
      "Loss for  21608 th iteration => 0.0148948612635\n",
      "Loss for  21609 th iteration => 0.0148940896633\n",
      "Loss for  21610 th iteration => 0.0148934358867\n",
      "Loss for  21611 th iteration => 0.0148925771096\n",
      "Loss for  21612 th iteration => 0.0148918093749\n",
      "Loss for  21613 th iteration => 0.0148910631965\n",
      "Loss for  21614 th iteration => 0.0148902310882\n",
      "Loss for  21615 th iteration => 0.0148895916021\n",
      "Loss for  21616 th iteration => 0.0148886663008\n",
      "Loss for  21617 th iteration => 0.014888015758\n",
      "Loss for  21618 th iteration => 0.014887102013\n",
      "Loss for  21619 th iteration => 0.0148864408016\n",
      "Loss for  21620 th iteration => 0.0148855385087\n",
      "Loss for  21621 th iteration => 0.0148851361937\n",
      "Loss for  21622 th iteration => 0.0148840406815\n",
      "Loss for  21623 th iteration => 0.0148833935249\n",
      "Loss for  21624 th iteration => 0.0148825373904\n",
      "Loss for  21625 th iteration => 0.0148817654206\n",
      "Loss for  21626 th iteration => 0.0148810326075\n",
      "Loss for  21627 th iteration => 0.0148801661596\n",
      "Loss for  21628 th iteration => 0.0148793446978\n",
      "Loss for  21629 th iteration => 0.0148787493553\n",
      "Loss for  21630 th iteration => 0.0148778353678\n",
      "Loss for  21631 th iteration => 0.014877125527\n",
      "Loss for  21632 th iteration => 0.0148763246768\n",
      "Loss for  21633 th iteration => 0.0148755252763\n",
      "Loss for  21634 th iteration => 0.0148750417397\n",
      "Loss for  21635 th iteration => 0.0148739738924\n",
      "Loss for  21636 th iteration => 0.0148733280996\n",
      "Loss for  21637 th iteration => 0.0148724125049\n",
      "Loss for  21638 th iteration => 0.0148717558742\n",
      "Loss for  21639 th iteration => 0.0148708516099\n",
      "Loss for  21640 th iteration => 0.0148701845242\n",
      "Loss for  21641 th iteration => 0.0148693090703\n",
      "Loss for  21642 th iteration => 0.0148687302804\n",
      "Loss for  21643 th iteration => 0.0148678090922\n",
      "Loss for  21644 th iteration => 0.0148671047339\n",
      "Loss for  21645 th iteration => 0.0148663076185\n",
      "Loss for  21646 th iteration => 0.0148655005959\n",
      "Loss for  21647 th iteration => 0.0148646331086\n",
      "Loss for  21648 th iteration => 0.0148639252617\n",
      "Loss for  21649 th iteration => 0.0148632615054\n",
      "Loss for  21650 th iteration => 0.0148623541195\n",
      "Loss for  21651 th iteration => 0.0148617046953\n",
      "Loss for  21652 th iteration => 0.0148607845646\n",
      "Loss for  21653 th iteration => 0.0148603869141\n",
      "Loss for  21654 th iteration => 0.014859274862\n",
      "Loss for  21655 th iteration => 0.0148586553353\n",
      "Loss for  21656 th iteration => 0.0148577671539\n",
      "Loss for  21657 th iteration => 0.0148570366021\n",
      "Loss for  21658 th iteration => 0.0148562581366\n",
      "Loss for  21659 th iteration => 0.0148554285235\n",
      "Loss for  21660 th iteration => 0.0148546165751\n",
      "Loss for  21661 th iteration => 0.0148538543205\n",
      "Loss for  21662 th iteration => 0.014853293395\n",
      "Loss for  21663 th iteration => 0.0148522963758\n",
      "Loss for  21664 th iteration => 0.014851731215\n",
      "Loss for  21665 th iteration => 0.0148507487347\n",
      "Loss for  21666 th iteration => 0.0148500516013\n",
      "Loss for  21667 th iteration => 0.0148493878066\n",
      "Loss for  21668 th iteration => 0.0148484831039\n",
      "Loss for  21669 th iteration => 0.0148478337725\n",
      "Loss for  21670 th iteration => 0.014846915531\n",
      "Loss for  21671 th iteration => 0.0148462801387\n",
      "Loss for  21672 th iteration => 0.0148453819269\n",
      "Loss for  21673 th iteration => 0.0148447902318\n",
      "Loss for  21674 th iteration => 0.0148438769997\n",
      "Loss for  21675 th iteration => 0.0148431743581\n",
      "Loss for  21676 th iteration => 0.0148423707657\n",
      "Loss for  21677 th iteration => 0.0148415611439\n",
      "Loss for  21678 th iteration => 0.0148408632553\n",
      "Loss for  21679 th iteration => 0.0148399987627\n",
      "Loss for  21680 th iteration => 0.0148394349646\n",
      "Loss for  21681 th iteration => 0.01483844354\n",
      "Loss for  21682 th iteration => 0.0148378754914\n",
      "Loss for  21683 th iteration => 0.0148368888698\n",
      "Loss for  21684 th iteration => 0.0148363166971\n",
      "Loss for  21685 th iteration => 0.0148353716917\n",
      "Loss for  21686 th iteration => 0.0148347819058\n",
      "Loss for  21687 th iteration => 0.0148338708495\n",
      "Loss for  21688 th iteration => 0.01483316757\n",
      "Loss for  21689 th iteration => 0.0148323686594\n",
      "Loss for  21690 th iteration => 0.0148315558912\n",
      "Loss for  21691 th iteration => 0.0148308651533\n",
      "Loss for  21692 th iteration => 0.0148300175018\n",
      "Loss for  21693 th iteration => 0.0148293799688\n",
      "Loss for  21694 th iteration => 0.0148284651245\n",
      "Loss for  21695 th iteration => 0.0148278169304\n",
      "Loss for  21696 th iteration => 0.0148269132319\n",
      "Loss for  21697 th iteration => 0.0148262547536\n",
      "Loss for  21698 th iteration => 0.0148253767585\n",
      "Loss for  21699 th iteration => 0.0148246121921\n",
      "Loss for  21700 th iteration => 0.0148240496748\n",
      "Loss for  21701 th iteration => 0.0148230602345\n",
      "Loss for  21702 th iteration => 0.0148224935414\n",
      "Loss for  21703 th iteration => 0.0148215253153\n",
      "Loss for  21704 th iteration => 0.0148207799051\n",
      "Loss for  21705 th iteration => 0.0148200157844\n",
      "Loss for  21706 th iteration => 0.0148193587001\n",
      "Loss for  21707 th iteration => 0.0148185165387\n",
      "Loss for  21708 th iteration => 0.0148177496199\n",
      "Loss for  21709 th iteration => 0.014817015976\n",
      "Loss for  21710 th iteration => 0.0148161912861\n",
      "Loss for  21711 th iteration => 0.0148155559019\n",
      "Loss for  21712 th iteration => 0.0148146416946\n",
      "Loss for  21713 th iteration => 0.0148139956323\n",
      "Loss for  21714 th iteration => 0.0148130925878\n",
      "Loss for  21715 th iteration => 0.0148124362241\n",
      "Loss for  21716 th iteration => 0.014811546861\n",
      "Loss for  21717 th iteration => 0.014810795148\n",
      "Loss for  21718 th iteration => 0.0148102372632\n",
      "Loss for  21719 th iteration => 0.0148092459726\n",
      "Loss for  21720 th iteration => 0.0148086838968\n",
      "Loss for  21721 th iteration => 0.0148077009543\n",
      "Loss for  21722 th iteration => 0.0148069796666\n",
      "Loss for  21723 th iteration => 0.0148061878067\n",
      "Loss for  21724 th iteration => 0.0148055724462\n",
      "Loss for  21725 th iteration => 0.0148046914545\n",
      "Loss for  21726 th iteration => 0.0148039660165\n",
      "Loss for  21727 th iteration => 0.0148031937853\n",
      "Loss for  21728 th iteration => 0.0148023899939\n",
      "Loss for  21729 th iteration => 0.0148017565504\n",
      "Loss for  21730 th iteration => 0.0148008431844\n",
      "Loss for  21731 th iteration => 0.0148001990541\n",
      "Loss for  21732 th iteration => 0.0147992968593\n",
      "Loss for  21733 th iteration => 0.0147986424186\n",
      "Loss for  21734 th iteration => 0.0147977510081\n",
      "Loss for  21735 th iteration => 0.0147971135364\n",
      "Loss for  21736 th iteration => 0.0147964616391\n",
      "Loss for  21737 th iteration => 0.014795466949\n",
      "Loss for  21738 th iteration => 0.0147949109203\n",
      "Loss for  21739 th iteration => 0.0147939210211\n",
      "Loss for  21740 th iteration => 0.0147933608739\n",
      "Loss for  21741 th iteration => 0.0147923986896\n",
      "Loss for  21742 th iteration => 0.0147916415361\n",
      "Loss for  21743 th iteration => 0.0147909016987\n",
      "Loss for  21744 th iteration => 0.0147902238433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  21745 th iteration => 0.0147894074613\n",
      "Loss for  21746 th iteration => 0.0147886218643\n",
      "Loss for  21747 th iteration => 0.0147879129344\n",
      "Loss for  21748 th iteration => 0.0147872789487\n",
      "Loss for  21749 th iteration => 0.0147862785687\n",
      "Loss for  21750 th iteration => 0.0147856751171\n",
      "Loss for  21751 th iteration => 0.0147847837291\n",
      "Loss for  21752 th iteration => 0.0147840739186\n",
      "Loss for  21753 th iteration => 0.0147832876254\n",
      "Loss for  21754 th iteration => 0.014782478054\n",
      "Loss for  21755 th iteration => 0.0147817109641\n",
      "Loss for  21756 th iteration => 0.0147811160763\n",
      "Loss for  21757 th iteration => 0.0147802261496\n",
      "Loss for  21758 th iteration => 0.0147795116173\n",
      "Loss for  21759 th iteration => 0.0147787399016\n",
      "Loss for  21760 th iteration => 0.0147779137814\n",
      "Loss for  21761 th iteration => 0.0147771320027\n",
      "Loss for  21762 th iteration => 0.0147765125992\n",
      "Loss for  21763 th iteration => 0.0147756413116\n",
      "Loss for  21764 th iteration => 0.0147749122937\n",
      "Loss for  21765 th iteration => 0.0147741493102\n",
      "Loss for  21766 th iteration => 0.0147733147433\n",
      "Loss for  21767 th iteration => 0.0147725054206\n",
      "Loss for  21768 th iteration => 0.0147719571288\n",
      "Loss for  21769 th iteration => 0.0147710136135\n",
      "Loss for  21770 th iteration => 0.0147703583391\n",
      "Loss for  21771 th iteration => 0.0147695205396\n",
      "Loss for  21772 th iteration => 0.014768762127\n",
      "Loss for  21773 th iteration => 0.0147680570461\n",
      "Loss for  21774 th iteration => 0.0147674244623\n",
      "Loss for  21775 th iteration => 0.0147664699741\n",
      "Loss for  21776 th iteration => 0.0147658216667\n",
      "Loss for  21777 th iteration => 0.0147649872951\n",
      "Loss for  21778 th iteration => 0.0147642215225\n",
      "Loss for  21779 th iteration => 0.014763503203\n",
      "Loss for  21780 th iteration => 0.0147626409775\n",
      "Loss for  21781 th iteration => 0.014761895882\n",
      "Loss for  21782 th iteration => 0.0147612324017\n",
      "Loss for  21783 th iteration => 0.0147604074912\n",
      "Loss for  21784 th iteration => 0.0147596362496\n",
      "Loss for  21785 th iteration => 0.0147589178065\n",
      "Loss for  21786 th iteration => 0.0147580550286\n",
      "Loss for  21787 th iteration => 0.0147572760358\n",
      "Loss for  21788 th iteration => 0.0147566891075\n",
      "Loss for  21789 th iteration => 0.0147557866108\n",
      "Loss for  21790 th iteration => 0.0147550943772\n",
      "Loss for  21791 th iteration => 0.0147542959333\n",
      "Loss for  21792 th iteration => 0.0147535021949\n",
      "Loss for  21793 th iteration => 0.0147528435703\n",
      "Loss for  21794 th iteration => 0.0147521661261\n",
      "Loss for  21795 th iteration => 0.0147512513103\n",
      "Loss for  21796 th iteration => 0.0147505674202\n",
      "Loss for  21797 th iteration => 0.0147497709441\n",
      "Loss for  21798 th iteration => 0.0147489713355\n",
      "Loss for  21799 th iteration => 0.0147482891818\n",
      "Loss for  21800 th iteration => 0.0147474040247\n",
      "Loss for  21801 th iteration => 0.014746683436\n",
      "Loss for  21802 th iteration => 0.0147459909741\n",
      "Loss for  21803 th iteration => 0.0147451974879\n",
      "Loss for  21804 th iteration => 0.0147443987557\n",
      "Loss for  21805 th iteration => 0.0147437102595\n",
      "Loss for  21806 th iteration => 0.0147428298985\n",
      "Loss for  21807 th iteration => 0.014742071032\n",
      "Loss for  21808 th iteration => 0.0147414590396\n",
      "Loss for  21809 th iteration => 0.0147405841133\n",
      "Loss for  21810 th iteration => 0.0147398681697\n",
      "Loss for  21811 th iteration => 0.0147390959542\n",
      "Loss for  21812 th iteration => 0.0147382841081\n",
      "Loss for  21813 th iteration => 0.0147375260572\n",
      "Loss for  21814 th iteration => 0.0147369309916\n",
      "Loss for  21815 th iteration => 0.0147360490298\n",
      "Loss for  21816 th iteration => 0.0147353368895\n",
      "Loss for  21817 th iteration => 0.0147345705967\n",
      "Loss for  21818 th iteration => 0.0147337490089\n",
      "Loss for  21819 th iteration => 0.0147329717525\n",
      "Loss for  21820 th iteration => 0.0147323563179\n",
      "Loss for  21821 th iteration => 0.0147314890259\n",
      "Loss for  21822 th iteration => 0.0147307662068\n",
      "Loss for  21823 th iteration => 0.0147300050121\n",
      "Loss for  21824 th iteration => 0.0147291786336\n",
      "Loss for  21825 th iteration => 0.0147285322046\n",
      "Loss for  21826 th iteration => 0.0147278475431\n",
      "Loss for  21827 th iteration => 0.0147268975588\n",
      "Loss for  21828 th iteration => 0.0147262580076\n",
      "Loss for  21829 th iteration => 0.0147254131079\n",
      "Loss for  21830 th iteration => 0.0147246710294\n",
      "Loss for  21831 th iteration => 0.0147239274293\n",
      "Loss for  21832 th iteration => 0.0147230996367\n",
      "Loss for  21833 th iteration => 0.0147225537037\n",
      "Loss for  21834 th iteration => 0.0147215682899\n",
      "Loss for  21835 th iteration => 0.0147210182391\n",
      "Loss for  21836 th iteration => 0.0147200374747\n",
      "Loss for  21837 th iteration => 0.0147194834277\n",
      "Loss for  21838 th iteration => 0.0147185214448\n",
      "Loss for  21839 th iteration => 0.0147178294626\n",
      "Loss for  21840 th iteration => 0.0147171826662\n",
      "Loss for  21841 th iteration => 0.0147162887069\n",
      "Loss for  21842 th iteration => 0.0147156554673\n",
      "Loss for  21843 th iteration => 0.014714748828\n",
      "Loss for  21844 th iteration => 0.0147141286488\n",
      "Loss for  21845 th iteration => 0.0147132380118\n",
      "Loss for  21846 th iteration => 0.0147126691652\n",
      "Loss for  21847 th iteration => 0.0147117571791\n",
      "Loss for  21848 th iteration => 0.0147110836578\n",
      "Loss for  21849 th iteration => 0.014710275104\n",
      "Loss for  21850 th iteration => 0.0147095006783\n",
      "Loss for  21851 th iteration => 0.0147087918153\n",
      "Loss for  21852 th iteration => 0.0147079521985\n",
      "Loss for  21853 th iteration => 0.0147074008625\n",
      "Loss for  21854 th iteration => 0.0147064239816\n",
      "Loss for  21855 th iteration => 0.0147058686003\n",
      "Loss for  21856 th iteration => 0.014704896291\n",
      "Loss for  21857 th iteration => 0.0147043369846\n",
      "Loss for  21858 th iteration => 0.0147033916278\n",
      "Loss for  21859 th iteration => 0.0147026865029\n",
      "Loss for  21860 th iteration => 0.0147020475439\n",
      "Loss for  21861 th iteration => 0.0147011490146\n",
      "Loss for  21862 th iteration => 0.0147005234094\n",
      "Loss for  21863 th iteration => 0.0146996123944\n",
      "Loss for  21864 th iteration => 0.0146989996518\n",
      "Loss for  21865 th iteration => 0.0146981195706\n",
      "Loss for  21866 th iteration => 0.0146975282644\n",
      "Loss for  21867 th iteration => 0.0146966412196\n",
      "Loss for  21868 th iteration => 0.0146959465924\n",
      "Loss for  21869 th iteration => 0.0146951616381\n",
      "Loss for  21870 th iteration => 0.0146943691385\n",
      "Loss for  21871 th iteration => 0.0146935592372\n",
      "Loss for  21872 th iteration => 0.014692824045\n",
      "Loss for  21873 th iteration => 0.0146922682032\n",
      "Loss for  21874 th iteration => 0.0146912990117\n",
      "Loss for  21875 th iteration => 0.0146907392204\n",
      "Loss for  21876 th iteration => 0.0146897744968\n",
      "Loss for  21877 th iteration => 0.0146892266041\n",
      "Loss for  21878 th iteration => 0.0146884637402\n",
      "Loss for  21879 th iteration => 0.014687574484\n",
      "Loss for  21880 th iteration => 0.0146869422791\n",
      "Loss for  21881 th iteration => 0.0146860402381\n",
      "Loss for  21882 th iteration => 0.014685421199\n",
      "Loss for  21883 th iteration => 0.0146845068523\n",
      "Loss for  21884 th iteration => 0.0146839004926\n",
      "Loss for  21885 th iteration => 0.0146830298266\n",
      "Loss for  21886 th iteration => 0.0146824194053\n",
      "Loss for  21887 th iteration => 0.0146815539847\n",
      "Loss for  21888 th iteration => 0.0146808415067\n",
      "Loss for  21889 th iteration => 0.0146800769237\n",
      "Loss for  21890 th iteration => 0.0146792736221\n",
      "Loss for  21891 th iteration => 0.0146784737381\n",
      "Loss for  21892 th iteration => 0.0146777371745\n",
      "Loss for  21893 th iteration => 0.0146771770369\n",
      "Loss for  21894 th iteration => 0.0146762152306\n",
      "Loss for  21895 th iteration => 0.0146756512012\n",
      "Loss for  21896 th iteration => 0.0146746973241\n",
      "Loss for  21897 th iteration => 0.0146740127635\n",
      "Loss for  21898 th iteration => 0.0146733714871\n",
      "Loss for  21899 th iteration => 0.0146724810503\n",
      "Loss for  21900 th iteration => 0.0146718531131\n",
      "Loss for  21901 th iteration => 0.0146709502016\n",
      "Loss for  21902 th iteration => 0.0146703351145\n",
      "Loss for  21903 th iteration => 0.0146694302415\n",
      "Loss for  21904 th iteration => 0.0146689014311\n",
      "Loss for  21905 th iteration => 0.0146679576236\n",
      "Loss for  21906 th iteration => 0.0146673255452\n",
      "Loss for  21907 th iteration => 0.0146664837794\n",
      "Loss for  21908 th iteration => 0.0146657521539\n",
      "Loss for  21909 th iteration => 0.0146650087378\n",
      "Loss for  21910 th iteration => 0.0146641996317\n",
      "Loss for  21911 th iteration => 0.0146636389341\n",
      "Loss for  21912 th iteration => 0.0146626802546\n",
      "Loss for  21913 th iteration => 0.014662115608\n",
      "Loss for  21914 th iteration => 0.0146611613969\n",
      "Loss for  21915 th iteration => 0.0146605929199\n",
      "Loss for  21916 th iteration => 0.014659652739\n",
      "Loss for  21917 th iteration => 0.0146589583392\n",
      "Loss for  21918 th iteration => 0.0146583233521\n",
      "Loss for  21919 th iteration => 0.0146574298345\n",
      "Loss for  21920 th iteration => 0.014656808014\n",
      "Loss for  21921 th iteration => 0.0146559021868\n",
      "Loss for  21922 th iteration => 0.0146552930482\n",
      "Loss for  21923 th iteration => 0.014654395494\n",
      "Loss for  21924 th iteration => 0.0146538521455\n",
      "Loss for  21925 th iteration => 0.0146529254147\n",
      "Loss for  21926 th iteration => 0.0146522799517\n",
      "Loss for  21927 th iteration => 0.0146514541198\n",
      "Loss for  21928 th iteration => 0.0146507102309\n",
      "Loss for  21929 th iteration => 0.0146499816378\n",
      "Loss for  21930 th iteration => 0.0146491698643\n",
      "Loss for  21931 th iteration => 0.0146486049821\n",
      "Loss for  21932 th iteration => 0.0146476535548\n",
      "Loss for  21933 th iteration => 0.0146470847779\n",
      "Loss for  21934 th iteration => 0.0146461377607\n",
      "Loss for  21935 th iteration => 0.0146455652067\n",
      "Loss for  21936 th iteration => 0.0146446359087\n",
      "Loss for  21937 th iteration => 0.0146439349641\n",
      "Loss for  21938 th iteration => 0.0146433049709\n",
      "Loss for  21939 th iteration => 0.0146424096257\n",
      "Loss for  21940 th iteration => 0.0146417926567\n",
      "Loss for  21941 th iteration => 0.0146408851377\n",
      "Loss for  21942 th iteration => 0.0146402807121\n",
      "Loss for  21943 th iteration => 0.0146393867133\n",
      "Loss for  21944 th iteration => 0.0146388375428\n",
      "Loss for  21945 th iteration => 0.01463791925\n",
      "Loss for  21946 th iteration => 0.0146372689104\n",
      "Loss for  21947 th iteration => 0.0146364505802\n",
      "Loss for  21948 th iteration => 0.0146357027327\n",
      "Loss for  21949 th iteration => 0.0146349807322\n",
      "Loss for  21950 th iteration => 0.0146341701656\n",
      "Loss for  21951 th iteration => 0.0146336014573\n",
      "Loss for  21952 th iteration => 0.0146326569036\n",
      "Loss for  21953 th iteration => 0.014632084348\n",
      "Loss for  21954 th iteration => 0.0146311441536\n",
      "Loss for  21955 th iteration => 0.0146305678675\n",
      "Loss for  21956 th iteration => 0.0146296471811\n",
      "Loss for  21957 th iteration => 0.0146289423374\n",
      "Loss for  21958 th iteration => 0.0146283162713\n",
      "Loss for  21959 th iteration => 0.0146274201299\n",
      "Loss for  21960 th iteration => 0.0146268069698\n",
      "Loss for  21961 th iteration => 0.0146258987672\n",
      "Loss for  21962 th iteration => 0.0146252980356\n",
      "Loss for  21963 th iteration => 0.0146244046324\n",
      "Loss for  21964 th iteration => 0.0146238565422\n",
      "Loss for  21965 th iteration => 0.0146229398452\n",
      "Loss for  21966 th iteration => 0.0146222913656\n",
      "Loss for  21967 th iteration => 0.0146214738592\n",
      "Loss for  21968 th iteration => 0.0146207286278\n",
      "Loss for  21969 th iteration => 0.0146200067028\n",
      "Loss for  21970 th iteration => 0.0146192003811\n",
      "Loss for  21971 th iteration => 0.0146186281806\n",
      "Loss for  21972 th iteration => 0.0146176901492\n",
      "Loss for  21973 th iteration => 0.0146171141433\n",
      "Loss for  21974 th iteration => 0.0146161804262\n",
      "Loss for  21975 th iteration => 0.0146156007307\n",
      "Loss for  21976 th iteration => 0.0146146868167\n",
      "Loss for  21977 th iteration => 0.0146139802026\n",
      "Loss for  21978 th iteration => 0.0146133571798\n",
      "Loss for  21979 th iteration => 0.0146124610962\n",
      "Loss for  21980 th iteration => 0.0146118508805\n",
      "Loss for  21981 th iteration => 0.0146109428296\n",
      "Loss for  21982 th iteration => 0.0146103449464\n",
      "Loss for  21983 th iteration => 0.0146094498179\n",
      "Loss for  21984 th iteration => 0.0146089082674\n",
      "Loss for  21985 th iteration => 0.0146079877527\n",
      "Loss for  21986 th iteration => 0.0146073464609\n",
      "Loss for  21987 th iteration => 0.0146065244957\n",
      "Loss for  21988 th iteration => 0.0146057870792\n",
      "Loss for  21989 th iteration => 0.0146050600749\n",
      "Loss for  21990 th iteration => 0.0146042603717\n",
      "Loss for  21991 th iteration => 0.0146036849926\n",
      "Loss for  21992 th iteration => 0.0146027531543\n",
      "Loss for  21993 th iteration => 0.0146021740068\n",
      "Loss for  21994 th iteration => 0.0146012464431\n",
      "Loss for  21995 th iteration => 0.0146006636424\n",
      "Loss for  21996 th iteration => 0.0145997550067\n",
      "Loss for  21997 th iteration => 0.0145990483386\n",
      "Loss for  21998 th iteration => 0.0145984276216\n",
      "Loss for  21999 th iteration => 0.0145975323076\n",
      "Loss for  22000 th iteration => 0.0145969243146\n",
      "Loss for  22001 th iteration => 0.014596017112\n",
      "Loss for  22002 th iteration => 0.0145954213709\n",
      "Loss for  22003 th iteration => 0.0145945277745\n",
      "Loss for  22004 th iteration => 0.0145938155453\n",
      "Loss for  22005 th iteration => 0.0145932126262\n",
      "Loss for  22006 th iteration => 0.0145923112157\n",
      "Loss for  22007 th iteration => 0.0145916987989\n",
      "Loss for  22008 th iteration => 0.0145908075305\n",
      "Loss for  22009 th iteration => 0.0145900832728\n",
      "Loss for  22010 th iteration => 0.0145893226315\n",
      "Loss for  22011 th iteration => 0.0145887592244\n",
      "Loss for  22012 th iteration => 0.0145878717066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  22013 th iteration => 0.0145871994741\n",
      "Loss for  22014 th iteration => 0.0145864194664\n",
      "Loss for  22015 th iteration => 0.0145856547488\n",
      "Loss for  22016 th iteration => 0.014584858406\n",
      "Loss for  22017 th iteration => 0.0145842750585\n",
      "Loss for  22018 th iteration => 0.0145834023293\n",
      "Loss for  22019 th iteration => 0.0145827188054\n",
      "Loss for  22020 th iteration => 0.0145819450415\n",
      "Loss for  22021 th iteration => 0.0145811649427\n",
      "Loss for  22022 th iteration => 0.0145804967025\n",
      "Loss for  22023 th iteration => 0.014579848969\n",
      "Loss for  22024 th iteration => 0.0145789396448\n",
      "Loss for  22025 th iteration => 0.0145782931625\n",
      "Loss for  22026 th iteration => 0.0145774820201\n",
      "Loss for  22027 th iteration => 0.0145767397694\n",
      "Loss for  22028 th iteration => 0.0145760232371\n",
      "Loss for  22029 th iteration => 0.0145752093983\n",
      "Loss for  22030 th iteration => 0.0145744314406\n",
      "Loss for  22031 th iteration => 0.0145737098257\n",
      "Loss for  22032 th iteration => 0.0145731320678\n",
      "Loss for  22033 th iteration => 0.0145722089799\n",
      "Loss for  22034 th iteration => 0.0145716276207\n",
      "Loss for  22035 th iteration => 0.0145707172852\n",
      "Loss for  22036 th iteration => 0.0145700185661\n",
      "Loss for  22037 th iteration => 0.0145693991697\n",
      "Loss for  22038 th iteration => 0.0145685085228\n",
      "Loss for  22039 th iteration => 0.014567901674\n",
      "Loss for  22040 th iteration => 0.0145669993054\n",
      "Loss for  22041 th iteration => 0.0145664045379\n",
      "Loss for  22042 th iteration => 0.0145655089727\n",
      "Loss for  22043 th iteration => 0.0145648041872\n",
      "Loss for  22044 th iteration => 0.0145642057328\n",
      "Loss for  22045 th iteration => 0.0145633056906\n",
      "Loss for  22046 th iteration => 0.0145626978552\n",
      "Loss for  22047 th iteration => 0.0145618076288\n",
      "Loss for  22048 th iteration => 0.0145611907493\n",
      "Loss for  22049 th iteration => 0.0145603350528\n",
      "Loss for  22050 th iteration => 0.0145597870926\n",
      "Loss for  22051 th iteration => 0.0145588897916\n",
      "Loss for  22052 th iteration => 0.0145582332186\n",
      "Loss for  22053 th iteration => 0.0145574432198\n",
      "Loss for  22054 th iteration => 0.0145566860856\n",
      "Loss for  22055 th iteration => 0.0145558537353\n",
      "Loss for  22056 th iteration => 0.014555166609\n",
      "Loss for  22057 th iteration => 0.0145545396561\n",
      "Loss for  22058 th iteration => 0.0145536597292\n",
      "Loss for  22059 th iteration => 0.0145530451683\n",
      "Loss for  22060 th iteration => 0.0145521536669\n",
      "Loss for  22061 th iteration => 0.0145515618487\n",
      "Loss for  22062 th iteration => 0.0145508823671\n",
      "Loss for  22063 th iteration => 0.0145499681962\n",
      "Loss for  22064 th iteration => 0.0145493766894\n",
      "Loss for  22065 th iteration => 0.0145484726728\n",
      "Loss for  22066 th iteration => 0.0145478717971\n",
      "Loss for  22067 th iteration => 0.0145469775827\n",
      "Loss for  22068 th iteration => 0.014546367674\n",
      "Loss for  22069 th iteration => 0.0145455000847\n",
      "Loss for  22070 th iteration => 0.0145447871833\n",
      "Loss for  22071 th iteration => 0.0145442148255\n",
      "Loss for  22072 th iteration => 0.0145432921004\n",
      "Loss for  22073 th iteration => 0.0145427161437\n",
      "Loss for  22074 th iteration => 0.014541798962\n",
      "Loss for  22075 th iteration => 0.0145410875072\n",
      "Loss for  22076 th iteration => 0.0145403331003\n",
      "Loss for  22077 th iteration => 0.0145397259616\n",
      "Loss for  22078 th iteration => 0.0145388845021\n",
      "Loss for  22079 th iteration => 0.0145381804834\n",
      "Loss for  22080 th iteration => 0.0145374347242\n",
      "Loss for  22081 th iteration => 0.0145366444326\n",
      "Loss for  22082 th iteration => 0.0145360627932\n",
      "Loss for  22083 th iteration => 0.014535151483\n",
      "Loss for  22084 th iteration => 0.0145345602692\n",
      "Loss for  22085 th iteration => 0.0145336589711\n",
      "Loss for  22086 th iteration => 0.0145330585223\n",
      "Loss for  22087 th iteration => 0.0145321668881\n",
      "Loss for  22088 th iteration => 0.0145315649169\n",
      "Loss for  22089 th iteration => 0.0145309203491\n",
      "Loss for  22090 th iteration => 0.0145299909468\n",
      "Loss for  22091 th iteration => 0.014529424012\n",
      "Loss for  22092 th iteration => 0.0145284988233\n",
      "Loss for  22093 th iteration => 0.0145279282844\n",
      "Loss for  22094 th iteration => 0.0145270071873\n",
      "Loss for  22095 th iteration => 0.0145264331549\n",
      "Loss for  22096 th iteration => 0.0145255451998\n",
      "Loss for  22097 th iteration => 0.0145249707088\n",
      "Loss for  22098 th iteration => 0.0145240999919\n",
      "Loss for  22099 th iteration => 0.0145234275291\n",
      "Loss for  22100 th iteration => 0.0145226535972\n",
      "Loss for  22101 th iteration => 0.0145218884063\n",
      "Loss for  22102 th iteration => 0.0145210709397\n",
      "Loss for  22103 th iteration => 0.0145203776087\n",
      "Loss for  22104 th iteration => 0.0145197869849\n",
      "Loss for  22105 th iteration => 0.0145188880354\n",
      "Loss for  22106 th iteration => 0.0145182881891\n",
      "Loss for  22107 th iteration => 0.01451739889\n",
      "Loss for  22108 th iteration => 0.0145167901527\n",
      "Loss for  22109 th iteration => 0.0145159570938\n",
      "Loss for  22110 th iteration => 0.0145154038311\n",
      "Loss for  22111 th iteration => 0.0145145197428\n",
      "Loss for  22112 th iteration => 0.0145138599346\n",
      "Loss for  22113 th iteration => 0.0145130811042\n",
      "Loss for  22114 th iteration => 0.0145123184055\n",
      "Loss for  22115 th iteration => 0.0145116412097\n",
      "Loss for  22116 th iteration => 0.0145107975217\n",
      "Loss for  22117 th iteration => 0.0145100941073\n",
      "Loss for  22118 th iteration => 0.0145094290555\n",
      "Loss for  22119 th iteration => 0.0145086507345\n",
      "Loss for  22120 th iteration => 0.0145078906321\n",
      "Loss for  22121 th iteration => 0.0145072061993\n",
      "Loss for  22122 th iteration => 0.0145063755565\n",
      "Loss for  22123 th iteration => 0.0145055990967\n",
      "Loss for  22124 th iteration => 0.0145048954202\n",
      "Loss for  22125 th iteration => 0.0145042977506\n",
      "Loss for  22126 th iteration => 0.0145034090044\n",
      "Loss for  22127 th iteration => 0.0145028023632\n",
      "Loss for  22128 th iteration => 0.0145019230098\n",
      "Loss for  22129 th iteration => 0.0145013274608\n",
      "Loss for  22130 th iteration => 0.0145006800903\n",
      "Loss for  22131 th iteration => 0.0144997488139\n",
      "Loss for  22132 th iteration => 0.014499189975\n",
      "Loss for  22133 th iteration => 0.0144982628409\n",
      "Loss for  22134 th iteration => 0.0144977004584\n",
      "Loss for  22135 th iteration => 0.0144967882378\n",
      "Loss for  22136 th iteration => 0.0144960758782\n",
      "Loss for  22137 th iteration => 0.0144953318215\n",
      "Loss for  22138 th iteration => 0.0144947190682\n",
      "Loss for  22139 th iteration => 0.0144938910899\n",
      "Loss for  22140 th iteration => 0.0144931839906\n",
      "Loss for  22141 th iteration => 0.0144924492042\n",
      "Loss for  22142 th iteration => 0.0144916650457\n",
      "Loss for  22143 th iteration => 0.0144910787027\n",
      "Loss for  22144 th iteration => 0.0144901811125\n",
      "Loss for  22145 th iteration => 0.0144895854477\n",
      "Loss for  22146 th iteration => 0.0144886976086\n",
      "Loss for  22147 th iteration => 0.0144880929548\n",
      "Loss for  22148 th iteration => 0.0144872145257\n",
      "Loss for  22149 th iteration => 0.014486604931\n",
      "Loss for  22150 th iteration => 0.014485976128\n",
      "Loss for  22151 th iteration => 0.0144850419525\n",
      "Loss for  22152 th iteration => 0.0144844889027\n",
      "Loss for  22153 th iteration => 0.0144835588884\n",
      "Loss for  22154 th iteration => 0.0144830022757\n",
      "Loss for  22155 th iteration => 0.0144820797982\n",
      "Loss for  22156 th iteration => 0.0144813938817\n",
      "Loss for  22157 th iteration => 0.0144806141431\n",
      "Loss for  22158 th iteration => 0.0144800468953\n",
      "Loss for  22159 th iteration => 0.0144791764425\n",
      "Loss for  22160 th iteration => 0.0144785145808\n",
      "Loss for  22161 th iteration => 0.0144777375877\n",
      "Loss for  22162 th iteration => 0.0144769850949\n",
      "Loss for  22163 th iteration => 0.0144762042818\n",
      "Loss for  22164 th iteration => 0.0144756649462\n",
      "Loss for  22165 th iteration => 0.0144747658584\n",
      "Loss for  22166 th iteration => 0.01447413367\n",
      "Loss for  22167 th iteration => 0.0144733263115\n",
      "Loss for  22168 th iteration => 0.0144726046816\n",
      "Loss for  22169 th iteration => 0.0144718856681\n",
      "Loss for  22170 th iteration => 0.0144711125605\n",
      "Loss for  22171 th iteration => 0.0144705589052\n",
      "Loss for  22172 th iteration => 0.0144696322012\n",
      "Loss for  22173 th iteration => 0.0144690749718\n",
      "Loss for  22174 th iteration => 0.0144681523216\n",
      "Loss for  22175 th iteration => 0.0144675916269\n",
      "Loss for  22176 th iteration => 0.01446669386\n",
      "Loss for  22177 th iteration => 0.014465963003\n",
      "Loss for  22178 th iteration => 0.0144652491067\n",
      "Loss for  22179 th iteration => 0.0144646066235\n",
      "Loss for  22180 th iteration => 0.0144638133869\n",
      "Loss for  22181 th iteration => 0.0144630787958\n",
      "Loss for  22182 th iteration => 0.0144623765344\n",
      "Loss for  22183 th iteration => 0.0144615845678\n",
      "Loss for  22184 th iteration => 0.0144609932512\n",
      "Loss for  22185 th iteration => 0.0144601066642\n",
      "Loss for  22186 th iteration => 0.01445950624\n",
      "Loss for  22187 th iteration => 0.0144586291835\n",
      "Loss for  22188 th iteration => 0.014458019979\n",
      "Loss for  22189 th iteration => 0.0144571521175\n",
      "Loss for  22190 th iteration => 0.0144565454267\n",
      "Loss for  22191 th iteration => 0.01445591527\n",
      "Loss for  22192 th iteration => 0.0144549849454\n",
      "Loss for  22193 th iteration => 0.0144544341728\n",
      "Loss for  22194 th iteration => 0.0144535079365\n",
      "Loss for  22195 th iteration => 0.0144529536651\n",
      "Loss for  22196 th iteration => 0.0144520407653\n",
      "Loss for  22197 th iteration => 0.0144513460042\n",
      "Loss for  22198 th iteration => 0.0144505829336\n",
      "Loss for  22199 th iteration => 0.0144500026187\n",
      "Loss for  22200 th iteration => 0.0144491503956\n",
      "Loss for  22201 th iteration => 0.0144484773119\n",
      "Loss for  22202 th iteration => 0.0144477167215\n",
      "Loss for  22203 th iteration => 0.0144469593148\n",
      "Loss for  22204 th iteration => 0.0144461889117\n",
      "Loss for  22205 th iteration => 0.014445640568\n",
      "Loss for  22206 th iteration => 0.0144447557234\n",
      "Loss for  22207 th iteration => 0.0144441162174\n",
      "Loss for  22208 th iteration => 0.0144433214282\n",
      "Loss for  22209 th iteration => 0.014442594122\n",
      "Loss for  22210 th iteration => 0.0144418860524\n",
      "Loss for  22211 th iteration => 0.0144411119353\n",
      "Loss for  22212 th iteration => 0.0144405610092\n",
      "Loss for  22213 th iteration => 0.014439637599\n",
      "Loss for  22214 th iteration => 0.0144390831523\n",
      "Loss for  22215 th iteration => 0.0144381637362\n",
      "Loss for  22216 th iteration => 0.0144376058765\n",
      "Loss for  22217 th iteration => 0.014436713572\n",
      "Loss for  22218 th iteration => 0.0144359821433\n",
      "Loss for  22219 th iteration => 0.0144352732914\n",
      "Loss for  22220 th iteration => 0.0144346320733\n",
      "Loss for  22221 th iteration => 0.0144338428611\n",
      "Loss for  22222 th iteration => 0.0144331110374\n",
      "Loss for  22223 th iteration => 0.0144324113134\n",
      "Loss for  22224 th iteration => 0.0144316251141\n",
      "Loss for  22225 th iteration => 0.0144310315831\n",
      "Loss for  22226 th iteration => 0.014430153183\n",
      "Loss for  22227 th iteration => 0.0144295506949\n",
      "Loss for  22228 th iteration => 0.0144286816698\n",
      "Loss for  22229 th iteration => 0.0144280705478\n",
      "Loss for  22230 th iteration => 0.0144272105665\n",
      "Loss for  22231 th iteration => 0.0144266011029\n",
      "Loss for  22232 th iteration => 0.0144259768585\n",
      "Loss for  22233 th iteration => 0.0144250495011\n",
      "Loss for  22234 th iteration => 0.0144245017986\n",
      "Loss for  22235 th iteration => 0.0144235784789\n",
      "Loss for  22236 th iteration => 0.0144230273214\n",
      "Loss for  22237 th iteration => 0.0144221169658\n",
      "Loss for  22238 th iteration => 0.014421427592\n",
      "Loss for  22239 th iteration => 0.0144206622253\n",
      "Loss for  22240 th iteration => 0.0144199532479\n",
      "Loss for  22241 th iteration => 0.014419352465\n",
      "Loss for  22242 th iteration => 0.0144184744274\n",
      "Loss for  22243 th iteration => 0.0144178848986\n",
      "Loss for  22244 th iteration => 0.0144170005675\n",
      "Loss for  22245 th iteration => 0.0144163255688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  22246 th iteration => 0.0144157276877\n",
      "Loss for  22247 th iteration => 0.0144148567296\n",
      "Loss for  22248 th iteration => 0.0144142500989\n",
      "Loss for  22249 th iteration => 0.0144133883027\n",
      "Loss for  22250 th iteration => 0.0144127732408\n",
      "Loss for  22251 th iteration => 0.0144119253147\n",
      "Loss for  22252 th iteration => 0.0144112206956\n",
      "Loss for  22253 th iteration => 0.0144106755555\n",
      "Loss for  22254 th iteration => 0.0144097523936\n",
      "Loss for  22255 th iteration => 0.0144092037948\n",
      "Loss for  22256 th iteration => 0.0144082845576\n",
      "Loss for  22257 th iteration => 0.0144077326054\n",
      "Loss for  22258 th iteration => 0.0144068384124\n",
      "Loss for  22259 th iteration => 0.0144061486946\n",
      "Loss for  22260 th iteration => 0.0144055439923\n",
      "Loss for  22261 th iteration => 0.0144046724003\n",
      "Loss for  22262 th iteration => 0.0144040791221\n",
      "Loss for  22263 th iteration => 0.0144031968753\n",
      "Loss for  22264 th iteration => 0.0144026145888\n",
      "Loss for  22265 th iteration => 0.0144017487913\n",
      "Loss for  22266 th iteration => 0.0144012146544\n",
      "Loss for  22267 th iteration => 0.0144003238259\n",
      "Loss for  22268 th iteration => 0.0143996998266\n",
      "Loss for  22269 th iteration => 0.0143988977682\n",
      "Loss for  22270 th iteration => 0.0143981872244\n",
      "Loss for  22271 th iteration => 0.0143974706445\n",
      "Loss for  22272 th iteration => 0.0143966924626\n",
      "Loss for  22273 th iteration => 0.014396146548\n",
      "Loss for  22274 th iteration => 0.014395226958\n",
      "Loss for  22275 th iteration => 0.0143946775608\n",
      "Loss for  22276 th iteration => 0.0143937619211\n",
      "Loss for  22277 th iteration => 0.0143932091472\n",
      "Loss for  22278 th iteration => 0.0143922998531\n",
      "Loss for  22279 th iteration => 0.0143916341653\n",
      "Loss for  22280 th iteration => 0.0143910223643\n",
      "Loss for  22281 th iteration => 0.0143901605875\n",
      "Loss for  22282 th iteration => 0.0143895603509\n",
      "Loss for  22283 th iteration => 0.0143886877819\n",
      "Loss for  22284 th iteration => 0.0143880986754\n",
      "Loss for  22285 th iteration => 0.0143872157332\n",
      "Loss for  22286 th iteration => 0.014386543474\n",
      "Loss for  22287 th iteration => 0.0143859541303\n",
      "Loss for  22288 th iteration => 0.01438508055\n",
      "Loss for  22289 th iteration => 0.0143844825314\n",
      "Loss for  22290 th iteration => 0.0143836180344\n",
      "Loss for  22291 th iteration => 0.0143830116563\n",
      "Loss for  22292 th iteration => 0.0143821571088\n",
      "Loss for  22293 th iteration => 0.0143814638901\n",
      "Loss for  22294 th iteration => 0.0143809170874\n",
      "Loss for  22295 th iteration => 0.0143800014911\n",
      "Loss for  22296 th iteration => 0.0143794512758\n",
      "Loss for  22297 th iteration => 0.0143785395538\n",
      "Loss for  22298 th iteration => 0.0143779860303\n",
      "Loss for  22299 th iteration => 0.0143770921413\n",
      "Loss for  22300 th iteration => 0.0143764119419\n",
      "Loss for  22301 th iteration => 0.014375807103\n",
      "Loss for  22302 th iteration => 0.01437494163\n",
      "Loss for  22303 th iteration => 0.0143743481071\n",
      "Loss for  22304 th iteration => 0.0143734720804\n",
      "Loss for  22305 th iteration => 0.0143728894453\n",
      "Loss for  22306 th iteration => 0.0143720141391\n",
      "Loss for  22307 th iteration => 0.0143713361774\n",
      "Loss for  22308 th iteration => 0.0143707450774\n",
      "Loss for  22309 th iteration => 0.0143698763045\n",
      "Loss for  22310 th iteration => 0.0143692766925\n",
      "Loss for  22311 th iteration => 0.0143684168354\n",
      "Loss for  22312 th iteration => 0.014367809023\n",
      "Loss for  22313 th iteration => 0.0143669668179\n",
      "Loss for  22314 th iteration => 0.0143662670033\n",
      "Loss for  22315 th iteration => 0.0143657199003\n",
      "Loss for  22316 th iteration => 0.0143648076791\n",
      "Loss for  22317 th iteration => 0.0143642572205\n",
      "Loss for  22318 th iteration => 0.0143633488118\n",
      "Loss for  22319 th iteration => 0.0143627951005\n",
      "Loss for  22320 th iteration => 0.0143619129896\n",
      "Loss for  22321 th iteration => 0.014361369001\n",
      "Loss for  22322 th iteration => 0.0143604969066\n",
      "Loss for  22323 th iteration => 0.0143598625265\n",
      "Loss for  22324 th iteration => 0.0143590797161\n",
      "Loss for  22325 th iteration => 0.0143583582435\n",
      "Loss for  22326 th iteration => 0.014357661445\n",
      "Loss for  22327 th iteration => 0.0143568838302\n",
      "Loss for  22328 th iteration => 0.0143562993779\n",
      "Loss for  22329 th iteration => 0.0143554267571\n",
      "Loss for  22330 th iteration => 0.0143548336605\n",
      "Loss for  22331 th iteration => 0.0143539700901\n",
      "Loss for  22332 th iteration => 0.0143533686627\n",
      "Loss for  22333 th iteration => 0.0143525138217\n",
      "Loss for  22334 th iteration => 0.0143519094133\n",
      "Loss for  22335 th iteration => 0.0143512938927\n",
      "Loss for  22336 th iteration => 0.0143503763608\n",
      "Loss for  22337 th iteration => 0.0143498338274\n",
      "Loss for  22338 th iteration => 0.0143489202056\n",
      "Loss for  22339 th iteration => 0.0143483743284\n",
      "Loss for  22340 th iteration => 0.0143474670737\n",
      "Loss for  22341 th iteration => 0.0143467979496\n",
      "Loss for  22342 th iteration => 0.0143460265596\n",
      "Loss for  22343 th iteration => 0.0143453336424\n",
      "Loss for  22344 th iteration => 0.0143447346131\n",
      "Loss for  22345 th iteration => 0.0143438699357\n",
      "Loss for  22346 th iteration => 0.0143432817865\n",
      "Loss for  22347 th iteration => 0.0143424069727\n",
      "Loss for  22348 th iteration => 0.0143418536063\n",
      "Loss for  22349 th iteration => 0.0143411656685\n",
      "Loss for  22350 th iteration => 0.0143402959408\n",
      "Loss for  22351 th iteration => 0.0143397029499\n",
      "Loss for  22352 th iteration => 0.0143388422438\n",
      "Loss for  22353 th iteration => 0.0143382409489\n",
      "Loss for  22354 th iteration => 0.0143373889442\n",
      "Loss for  22355 th iteration => 0.0143367796513\n",
      "Loss for  22356 th iteration => 0.0143359549699\n",
      "Loss for  22357 th iteration => 0.0143352433227\n",
      "Loss for  22358 th iteration => 0.0143347012761\n",
      "Loss for  22359 th iteration => 0.0143337902073\n",
      "Loss for  22360 th iteration => 0.014333244862\n",
      "Loss for  22361 th iteration => 0.0143323448046\n",
      "Loss for  22362 th iteration => 0.0143316632556\n",
      "Loss for  22363 th iteration => 0.0143309085525\n",
      "Loss for  22364 th iteration => 0.0143303456668\n",
      "Loss for  22365 th iteration => 0.0143294972455\n",
      "Loss for  22366 th iteration => 0.0143288470952\n",
      "Loss for  22367 th iteration => 0.0143280848603\n",
      "Loss for  22368 th iteration => 0.0143273506608\n",
      "Loss for  22369 th iteration => 0.0143267046961\n",
      "Loss for  22370 th iteration => 0.0143260770146\n",
      "Loss for  22371 th iteration => 0.0143251774223\n",
      "Loss for  22372 th iteration => 0.0143245783991\n",
      "Loss for  22373 th iteration => 0.0143237651375\n",
      "Loss for  22374 th iteration => 0.0143230819547\n",
      "Loss for  22375 th iteration => 0.0143223518133\n",
      "Loss for  22376 th iteration => 0.0143215876327\n",
      "Loss for  22377 th iteration => 0.0143209384453\n",
      "Loss for  22378 th iteration => 0.0143203297455\n",
      "Loss for  22379 th iteration => 0.0143194585817\n",
      "Loss for  22380 th iteration => 0.01431882969\n",
      "Loss for  22381 th iteration => 0.0143180543302\n",
      "Loss for  22382 th iteration => 0.0143173392509\n",
      "Loss for  22383 th iteration => 0.0143165169641\n",
      "Loss for  22384 th iteration => 0.0143158540074\n",
      "Loss for  22385 th iteration => 0.0143150821214\n",
      "Loss for  22386 th iteration => 0.0143143907107\n",
      "Loss for  22387 th iteration => 0.0143137910608\n",
      "Loss for  22388 th iteration => 0.0143129334297\n",
      "Loss for  22389 th iteration => 0.0143123443568\n",
      "Loss for  22390 th iteration => 0.0143115125225\n",
      "Loss for  22391 th iteration => 0.0143110894579\n",
      "Loss for  22392 th iteration => 0.0143101138942\n",
      "Loss for  22393 th iteration => 0.0143095061314\n",
      "Loss for  22394 th iteration => 0.0143087046423\n",
      "Loss for  22395 th iteration => 0.0143080125047\n",
      "Loss for  22396 th iteration => 0.0143072943505\n",
      "Loss for  22397 th iteration => 0.0143065210012\n",
      "Loss for  22398 th iteration => 0.014305883044\n",
      "Loss for  22399 th iteration => 0.0143050504454\n",
      "Loss for  22400 th iteration => 0.0143043926577\n",
      "Loss for  22401 th iteration => 0.0143037585808\n",
      "Loss for  22402 th iteration => 0.0143029909416\n",
      "Loss for  22403 th iteration => 0.0143022707009\n",
      "Loss for  22404 th iteration => 0.014301454226\n",
      "Loss for  22405 th iteration => 0.0143007887474\n",
      "Loss for  22406 th iteration => 0.0143000222012\n",
      "Loss for  22407 th iteration => 0.0142993317065\n",
      "Loss for  22408 th iteration => 0.014298733264\n",
      "Loss for  22409 th iteration => 0.0142978774873\n",
      "Loss for  22410 th iteration => 0.0142972895196\n",
      "Loss for  22411 th iteration => 0.0142964610219\n",
      "Loss for  22412 th iteration => 0.0142960387116\n",
      "Loss for  22413 th iteration => 0.0142950647033\n",
      "Loss for  22414 th iteration => 0.0142944559835\n",
      "Loss for  22415 th iteration => 0.0142936580877\n",
      "Loss for  22416 th iteration => 0.0142929657328\n",
      "Loss for  22417 th iteration => 0.0142922504396\n",
      "Loss for  22418 th iteration => 0.014291477591\n",
      "Loss for  22419 th iteration => 0.0142908417839\n",
      "Loss for  22420 th iteration => 0.0142900099896\n",
      "Loss for  22421 th iteration => 0.0142893542164\n",
      "Loss for  22422 th iteration => 0.0142887219906\n",
      "Loss for  22423 th iteration => 0.014287955101\n",
      "Loss for  22424 th iteration => 0.0142872350737\n",
      "Loss for  22425 th iteration => 0.0142864208951\n",
      "Loss for  22426 th iteration => 0.0142857563482\n",
      "Loss for  22427 th iteration => 0.0142849917034\n",
      "Loss for  22428 th iteration => 0.0142843040849\n",
      "Loss for  22429 th iteration => 0.0142837060859\n",
      "Loss for  22430 th iteration => 0.0142828528995\n",
      "Loss for  22431 th iteration => 0.0142822652913\n",
      "Loss for  22432 th iteration => 0.0142814392084\n",
      "Loss for  22433 th iteration => 0.0142810199183\n",
      "Loss for  22434 th iteration => 0.0142800437885\n",
      "Loss for  22435 th iteration => 0.0142794394591\n",
      "Loss for  22436 th iteration => 0.0142786398536\n",
      "Loss for  22437 th iteration => 0.014277952503\n",
      "Loss for  22438 th iteration => 0.0142772348925\n",
      "Loss for  22439 th iteration => 0.0142764676433\n",
      "Loss for  22440 th iteration => 0.0142758289301\n",
      "Loss for  22441 th iteration => 0.0142750019105\n",
      "Loss for  22442 th iteration => 0.0142743438738\n",
      "Loss for  22443 th iteration => 0.014273719013\n",
      "Loss for  22444 th iteration => 0.0142729474086\n",
      "Loss for  22445 th iteration => 0.0142722318277\n",
      "Loss for  22446 th iteration => 0.0142714171706\n",
      "Loss for  22447 th iteration => 0.0142707562793\n",
      "Loss for  22448 th iteration => 0.014269990822\n",
      "Loss for  22449 th iteration => 0.0142693076207\n",
      "Loss for  22450 th iteration => 0.0142687094515\n",
      "Loss for  22451 th iteration => 0.0142678594455\n",
      "Loss for  22452 th iteration => 0.0142672715973\n",
      "Loss for  22453 th iteration => 0.0142664472166\n",
      "Loss for  22454 th iteration => 0.0142660326896\n",
      "Loss for  22455 th iteration => 0.0142650515931\n",
      "Loss for  22456 th iteration => 0.0142644558297\n",
      "Loss for  22457 th iteration => 0.0142636503719\n",
      "Loss for  22458 th iteration => 0.0142629721022\n",
      "Loss for  22459 th iteration => 0.0142622481301\n",
      "Loss for  22460 th iteration => 0.0142614904602\n",
      "Loss for  22461 th iteration => 0.0142608448924\n",
      "Loss for  22462 th iteration => 0.0142600257516\n",
      "Loss for  22463 th iteration => 0.0142593620939\n",
      "Loss for  22464 th iteration => 0.0142587489174\n",
      "Loss for  22465 th iteration => 0.014257968316\n",
      "Loss for  22466 th iteration => 0.0142572644119\n",
      "Loss for  22467 th iteration => 0.014256573417\n",
      "Loss for  22468 th iteration => 0.0142558026972\n",
      "Loss for  22469 th iteration => 0.0142550300708\n",
      "Loss for  22470 th iteration => 0.0142543551214\n",
      "Loss for  22471 th iteration => 0.0142537548681\n",
      "Loss for  22472 th iteration => 0.0142529097526\n",
      "Loss for  22473 th iteration => 0.0142523199207\n",
      "Loss for  22474 th iteration => 0.014251465102\n",
      "Loss for  22475 th iteration => 0.0142508951832\n",
      "Loss for  22476 th iteration => 0.0142502438225\n",
      "Loss for  22477 th iteration => 0.0142493654875\n",
      "Loss for  22478 th iteration => 0.0142487993586\n",
      "Loss for  22479 th iteration => 0.0142479297435\n",
      "Loss for  22480 th iteration => 0.0142473555887\n",
      "Loss for  22481 th iteration => 0.0142464943846\n",
      "Loss for  22482 th iteration => 0.0142459124994\n",
      "Loss for  22483 th iteration => 0.0142450734616\n",
      "Loss for  22484 th iteration => 0.0142443891474\n",
      "Loss for  22485 th iteration => 0.014243844743\n",
      "Loss for  22486 th iteration => 0.0142429539636\n",
      "Loss for  22487 th iteration => 0.0142424064093\n",
      "Loss for  22488 th iteration => 0.0142415192159\n",
      "Loss for  22489 th iteration => 0.0142409686096\n",
      "Loss for  22490 th iteration => 0.0142401015618\n",
      "Loss for  22491 th iteration => 0.0142394319418\n",
      "Loss for  22492 th iteration => 0.0142388309579\n",
      "Loss for  22493 th iteration => 0.0142379893998\n",
      "Loss for  22494 th iteration => 0.0142373989093\n",
      "Loss for  22495 th iteration => 0.0142365475776\n",
      "Loss for  22496 th iteration => 0.0142359671778\n",
      "Loss for  22497 th iteration => 0.0142351195021\n",
      "Loss for  22498 th iteration => 0.0142344385778\n",
      "Loss for  22499 th iteration => 0.0142338754804\n",
      "Loss for  22500 th iteration => 0.0142330057646\n",
      "Loss for  22501 th iteration => 0.0142324346837\n",
      "Loss for  22502 th iteration => 0.0142315733347\n",
      "Loss for  22503 th iteration => 0.0142309945641\n",
      "Loss for  22504 th iteration => 0.01423015254\n",
      "Loss for  22505 th iteration => 0.0142294733756\n",
      "Loss for  22506 th iteration => 0.0142289304781\n",
      "Loss for  22507 th iteration => 0.0142280411231\n",
      "Loss for  22508 th iteration => 0.0142274950915\n",
      "Loss for  22509 th iteration => 0.0142266093048\n",
      "Loss for  22510 th iteration => 0.0142260602363\n",
      "Loss for  22511 th iteration => 0.0142251903929\n",
      "Loss for  22512 th iteration => 0.0142245275616\n",
      "Loss for  22513 th iteration => 0.0142239253212\n",
      "Loss for  22514 th iteration => 0.0142230879864\n",
      "Loss for  22515 th iteration => 0.0142224961871\n",
      "Loss for  22516 th iteration => 0.0142216491274\n",
      "Loss for  22517 th iteration => 0.0142210673688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  22518 th iteration => 0.0142202193286\n",
      "Loss for  22519 th iteration => 0.014219542031\n",
      "Loss for  22520 th iteration => 0.0142189821987\n",
      "Loss for  22521 th iteration => 0.0142181121379\n",
      "Loss for  22522 th iteration => 0.0142175443578\n",
      "Loss for  22523 th iteration => 0.0142166826264\n",
      "Loss for  22524 th iteration => 0.0142161071911\n",
      "Loss for  22525 th iteration => 0.0142152613939\n",
      "Loss for  22526 th iteration => 0.0142145879825\n",
      "Loss for  22527 th iteration => 0.0142140466684\n",
      "Loss for  22528 th iteration => 0.0142131586494\n",
      "Loss for  22529 th iteration => 0.0142126142147\n",
      "Loss for  22530 th iteration => 0.0142117297486\n",
      "Loss for  22531 th iteration => 0.0142111822901\n",
      "Loss for  22532 th iteration => 0.0142103089688\n",
      "Loss for  22533 th iteration => 0.0142096537193\n",
      "Loss for  22534 th iteration => 0.0142090499366\n",
      "Loss for  22535 th iteration => 0.0142082170938\n",
      "Loss for  22536 th iteration => 0.014207623708\n",
      "Loss for  22537 th iteration => 0.0142067811814\n",
      "Loss for  22538 th iteration => 0.0142061977938\n",
      "Loss for  22539 th iteration => 0.0142053490637\n",
      "Loss for  22540 th iteration => 0.0142046757585\n",
      "Loss for  22541 th iteration => 0.0142041193638\n",
      "Loss for  22542 th iteration => 0.0142032487753\n",
      "Loss for  22543 th iteration => 0.0142026844635\n",
      "Loss for  22544 th iteration => 0.0142018221721\n",
      "Loss for  22545 th iteration => 0.0142012502344\n",
      "Loss for  22546 th iteration => 0.0142004000558\n",
      "Loss for  22547 th iteration => 0.0141997328637\n",
      "Loss for  22548 th iteration => 0.0141991931891\n",
      "Loss for  22549 th iteration => 0.0141983064387\n",
      "Loss for  22550 th iteration => 0.0141977636553\n",
      "Loss for  22551 th iteration => 0.0141968804442\n",
      "Loss for  22552 th iteration => 0.0141963346484\n",
      "Loss for  22553 th iteration => 0.0141954573237\n",
      "Loss for  22554 th iteration => 0.0141948102632\n",
      "Loss for  22555 th iteration => 0.014194204727\n",
      "Loss for  22556 th iteration => 0.0141933765721\n",
      "Loss for  22557 th iteration => 0.0141927813947\n",
      "Loss for  22558 th iteration => 0.0141919435913\n",
      "Loss for  22559 th iteration => 0.0141913583758\n",
      "Loss for  22560 th iteration => 0.0141905113072\n",
      "Loss for  22561 th iteration => 0.0141899525326\n",
      "Loss for  22562 th iteration => 0.0141893009034\n",
      "Loss for  22563 th iteration => 0.0141884274114\n",
      "Loss for  22564 th iteration => 0.0141878687552\n",
      "Loss for  22565 th iteration => 0.0141870036576\n",
      "Loss for  22566 th iteration => 0.0141864372813\n",
      "Loss for  22567 th iteration => 0.0141855802785\n",
      "Loss for  22568 th iteration => 0.0141850064687\n",
      "Loss for  22569 th iteration => 0.0141841769839\n",
      "Loss for  22570 th iteration => 0.0141834942481\n",
      "Loss for  22571 th iteration => 0.0141829537848\n",
      "Loss for  22572 th iteration => 0.0141820710824\n",
      "Loss for  22573 th iteration => 0.0141815275793\n",
      "Loss for  22574 th iteration => 0.0141806483416\n",
      "Loss for  22575 th iteration => 0.0141801018935\n",
      "Loss for  22576 th iteration => 0.0141792446905\n",
      "Loss for  22577 th iteration => 0.0141785800588\n",
      "Loss for  22578 th iteration => 0.0141779805296\n",
      "Loss for  22579 th iteration => 0.0141771497845\n",
      "Loss for  22580 th iteration => 0.0141765603724\n",
      "Loss for  22581 th iteration => 0.0141757202108\n",
      "Loss for  22582 th iteration => 0.014175140525\n",
      "Loss for  22583 th iteration => 0.014174304588\n",
      "Loss for  22584 th iteration => 0.0141736241838\n",
      "Loss for  22585 th iteration => 0.0141730708322\n",
      "Loss for  22586 th iteration => 0.0141722033095\n",
      "Loss for  22587 th iteration => 0.0141716422248\n",
      "Loss for  22588 th iteration => 0.0141707828093\n",
      "Loss for  22589 th iteration => 0.0141702142777\n",
      "Loss for  22590 th iteration => 0.01416937468\n",
      "Loss for  22591 th iteration => 0.0141687025764\n",
      "Loss for  22592 th iteration => 0.0141681643844\n",
      "Loss for  22593 th iteration => 0.0141672822796\n",
      "Loss for  22594 th iteration => 0.014166741045\n",
      "Loss for  22595 th iteration => 0.0141658624069\n",
      "Loss for  22596 th iteration => 0.0141653182244\n",
      "Loss for  22597 th iteration => 0.0141644519796\n",
      "Loss for  22598 th iteration => 0.0141638014631\n",
      "Loss for  22599 th iteration => 0.0141631978857\n",
      "Loss for  22600 th iteration => 0.01416237405\n",
      "Loss for  22601 th iteration => 0.0141617806048\n",
      "Loss for  22602 th iteration => 0.0141609473363\n",
      "Loss for  22603 th iteration => 0.0141603636331\n",
      "Loss for  22604 th iteration => 0.01415952453\n",
      "Loss for  22605 th iteration => 0.0141588509787\n",
      "Loss for  22606 th iteration => 0.0141583026415\n",
      "Loss for  22607 th iteration => 0.0141574329766\n",
      "Loss for  22608 th iteration => 0.0141568768982\n",
      "Loss for  22609 th iteration => 0.0141560153478\n",
      "Loss for  22610 th iteration => 0.0141554518139\n",
      "Loss for  22611 th iteration => 0.0141546028397\n",
      "Loss for  22612 th iteration => 0.0141539408045\n",
      "Loss for  22613 th iteration => 0.0141534047854\n",
      "Loss for  22614 th iteration => 0.0141525233702\n",
      "Loss for  22615 th iteration => 0.0141519843073\n",
      "Loss for  22616 th iteration => 0.0141511063592\n",
      "Loss for  22617 th iteration => 0.014150564347\n",
      "Loss for  22618 th iteration => 0.0141496897648\n",
      "Loss for  22619 th iteration => 0.0141491587306\n",
      "Loss for  22620 th iteration => 0.0141484577263\n",
      "Loss for  22621 th iteration => 0.014147639502\n",
      "Loss for  22622 th iteration => 0.014147043286\n",
      "Loss for  22623 th iteration => 0.0141462154747\n",
      "Loss for  22624 th iteration => 0.0141456291562\n",
      "Loss for  22625 th iteration => 0.0141447921376\n",
      "Loss for  22626 th iteration => 0.0141442153322\n",
      "Loss for  22627 th iteration => 0.0141433880586\n",
      "Loss for  22628 th iteration => 0.0141427048913\n",
      "Loss for  22629 th iteration => 0.0141421548954\n",
      "Loss for  22630 th iteration => 0.0141412900732\n",
      "Loss for  22631 th iteration => 0.0141407324896\n",
      "Loss for  22632 th iteration => 0.0141398756242\n",
      "Loss for  22633 th iteration => 0.0141393107349\n",
      "Loss for  22634 th iteration => 0.0141384771726\n",
      "Loss for  22635 th iteration => 0.0141378043692\n",
      "Loss for  22636 th iteration => 0.0141372678559\n",
      "Loss for  22637 th iteration => 0.0141363901406\n",
      "Loss for  22638 th iteration => 0.0141358506378\n",
      "Loss for  22639 th iteration => 0.0141349763307\n",
      "Loss for  22640 th iteration => 0.0141344339316\n",
      "Loss for  22641 th iteration => 0.0141335745478\n",
      "Loss for  22642 th iteration => 0.0141329248801\n",
      "Loss for  22643 th iteration => 0.0141323229078\n",
      "Loss for  22644 th iteration => 0.0141315036551\n",
      "Loss for  22645 th iteration => 0.0141309116353\n",
      "Loss for  22646 th iteration => 0.0141300831203\n",
      "Loss for  22647 th iteration => 0.0141295006683\n",
      "Loss for  22648 th iteration => 0.0141286685796\n",
      "Loss for  22649 th iteration => 0.0141279941771\n",
      "Loss for  22650 th iteration => 0.0141274502711\n",
      "Loss for  22651 th iteration => 0.0141265822062\n",
      "Loss for  22652 th iteration => 0.0141260306785\n",
      "Loss for  22653 th iteration => 0.0141251706043\n",
      "Loss for  22654 th iteration => 0.0141246117367\n",
      "Loss for  22655 th iteration => 0.0141237654291\n",
      "Loss for  22656 th iteration => 0.0141231052825\n",
      "Loss for  22657 th iteration => 0.01412257131\n",
      "Loss for  22658 th iteration => 0.0141216938863\n",
      "Loss for  22659 th iteration => 0.0141211569143\n",
      "Loss for  22660 th iteration => 0.0141202829085\n",
      "Loss for  22661 th iteration => 0.0141197430303\n",
      "Loss for  22662 th iteration => 0.0141188723426\n",
      "Loss for  22663 th iteration => 0.0141183385666\n",
      "Loss for  22664 th iteration => 0.0141176448204\n",
      "Loss for  22665 th iteration => 0.0141168324827\n",
      "Loss for  22666 th iteration => 0.0141162363704\n",
      "Loss for  22667 th iteration => 0.0141154145865\n",
      "Loss for  22668 th iteration => 0.0141148282276\n",
      "Loss for  22669 th iteration => 0.0141139973725\n",
      "Loss for  22670 th iteration => 0.0141134203874\n",
      "Loss for  22671 th iteration => 0.014112598658\n",
      "Loss for  22672 th iteration => 0.0141119163458\n",
      "Loss for  22673 th iteration => 0.0141113717024\n",
      "Loss for  22674 th iteration => 0.0141105075324\n",
      "Loss for  22675 th iteration => 0.0141099553976\n",
      "Loss for  22676 th iteration => 0.0141090990843\n",
      "Loss for  22677 th iteration => 0.0141085397367\n",
      "Loss for  22678 th iteration => 0.0141077058863\n",
      "Loss for  22679 th iteration => 0.0141070372391\n",
      "Loss for  22680 th iteration => 0.0141065031101\n",
      "Loss for  22681 th iteration => 0.0141056290175\n",
      "Loss for  22682 th iteration => 0.0141050919354\n",
      "Loss for  22683 th iteration => 0.0141042212103\n",
      "Loss for  22684 th iteration => 0.0141036812673\n",
      "Loss for  22685 th iteration => 0.0141028231613\n",
      "Loss for  22686 th iteration => 0.0141021808659\n",
      "Loss for  22687 th iteration => 0.0141015780074\n",
      "Loss for  22688 th iteration => 0.0141007657294\n",
      "Loss for  22689 th iteration => 0.0141001727035\n",
      "Loss for  22690 th iteration => 0.014099351276\n",
      "Loss for  22691 th iteration => 0.0140987677022\n",
      "Loss for  22692 th iteration => 0.0140979401279\n",
      "Loss for  22693 th iteration => 0.0140972677596\n",
      "Loss for  22694 th iteration => 0.0140967298162\n",
      "Loss for  22695 th iteration => 0.0140958617715\n",
      "Loss for  22696 th iteration => 0.0140953162869\n",
      "Loss for  22697 th iteration => 0.0140944561489\n",
      "Loss for  22698 th iteration => 0.0140939034019\n",
      "Loss for  22699 th iteration => 0.0140930547917\n",
      "Loss for  22700 th iteration => 0.014092409363\n",
      "Loss for  22701 th iteration => 0.0140916920448\n",
      "Loss for  22702 th iteration => 0.0140911584947\n",
      "Loss for  22703 th iteration => 0.0140903274971\n",
      "Loss for  22704 th iteration => 0.0140897107414\n",
      "Loss for  22705 th iteration => 0.0140889619126\n",
      "Loss for  22706 th iteration => 0.0140882649243\n",
      "Loss for  22707 th iteration => 0.0140875953181\n",
      "Loss for  22708 th iteration => 0.0140868656785\n",
      "Loss for  22709 th iteration => 0.0140862662198\n",
      "Loss for  22710 th iteration => 0.0140854536017\n",
      "Loss for  22711 th iteration => 0.0140848638712\n",
      "Loss for  22712 th iteration => 0.0140840422035\n",
      "Loss for  22713 th iteration => 0.0140834618236\n",
      "Loss for  22714 th iteration => 0.014082642575\n",
      "Loss for  22715 th iteration => 0.014081943409\n",
      "Loss for  22716 th iteration => 0.0140812451308\n",
      "Loss for  22717 th iteration => 0.0140806984115\n",
      "Loss for  22718 th iteration => 0.0140798758235\n",
      "Loss for  22719 th iteration => 0.0140792556329\n",
      "Loss for  22720 th iteration => 0.0140785055869\n",
      "Loss for  22721 th iteration => 0.0140778248948\n",
      "Loss for  22722 th iteration => 0.0140770456185\n",
      "Loss for  22723 th iteration => 0.0140765969606\n",
      "Loss for  22724 th iteration => 0.0140756844108\n",
      "Loss for  22725 th iteration => 0.0140751513722\n",
      "Loss for  22726 th iteration => 0.014074322158\n",
      "Loss for  22727 th iteration => 0.0140737077324\n",
      "Loss for  22728 th iteration => 0.0140729588866\n",
      "Loss for  22729 th iteration => 0.0140722806591\n",
      "Loss for  22730 th iteration => 0.0140717474939\n",
      "Loss for  22731 th iteration => 0.0140708806736\n",
      "Loss for  22732 th iteration => 0.0140702782505\n",
      "Loss for  22733 th iteration => 0.0140694717941\n",
      "Loss for  22734 th iteration => 0.0140688790066\n",
      "Loss for  22735 th iteration => 0.0140680635881\n",
      "Loss for  22736 th iteration => 0.0140674800618\n",
      "Loss for  22737 th iteration => 0.0140666714001\n",
      "Loss for  22738 th iteration => 0.0140659695912\n",
      "Loss for  22739 th iteration => 0.0140652765907\n",
      "Loss for  22740 th iteration => 0.0140645741555\n",
      "Loss for  22741 th iteration => 0.0140640358905\n",
      "Loss for  22742 th iteration => 0.0140631746537\n",
      "Loss for  22743 th iteration => 0.0140626293328\n",
      "Loss for  22744 th iteration => 0.0140617911699\n",
      "Loss for  22745 th iteration => 0.0140611306707\n",
      "Loss for  22746 th iteration => 0.0140606008354\n",
      "Loss for  22747 th iteration => 0.0140597314067\n",
      "Loss for  22748 th iteration => 0.0140591986824\n",
      "Loss for  22749 th iteration => 0.01405833255\n",
      "Loss for  22750 th iteration => 0.0140577970269\n",
      "Loss for  22751 th iteration => 0.0140569377947\n",
      "Loss for  22752 th iteration => 0.0140563115575\n",
      "Loss for  22753 th iteration => 0.0140557005919\n",
      "Loss for  22754 th iteration => 0.0140549055162\n",
      "Loss for  22755 th iteration => 0.0140543041807\n",
      "Loss for  22756 th iteration => 0.0140535001458\n",
      "Loss for  22757 th iteration => 0.0140529080674\n",
      "Loss for  22758 th iteration => 0.0140520954341\n",
      "Loss for  22759 th iteration => 0.014051512248\n",
      "Loss for  22760 th iteration => 0.0140507433725\n",
      "Loss for  22761 th iteration => 0.0140501841513\n",
      "Loss for  22762 th iteration => 0.0140493799459\n",
      "Loss for  22763 th iteration => 0.0140487472228\n",
      "Loss for  22764 th iteration => 0.0140480155918\n",
      "Loss for  22765 th iteration => 0.0140473121992\n",
      "Loss for  22766 th iteration => 0.0140466503331\n",
      "Loss for  22767 th iteration => 0.0140458967476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  22768 th iteration => 0.0140451915396\n",
      "Loss for  22769 th iteration => 0.0140445092396\n",
      "Loss for  22770 th iteration => 0.0140439720268\n",
      "Loss for  22771 th iteration => 0.014043113322\n",
      "Loss for  22772 th iteration => 0.0140425733473\n",
      "Loss for  22773 th iteration => 0.014041724349\n",
      "Loss for  22774 th iteration => 0.0140410655175\n",
      "Loss for  22775 th iteration => 0.0140403483514\n",
      "Loss for  22776 th iteration => 0.014039789867\n",
      "Loss for  22777 th iteration => 0.0140389877689\n",
      "Loss for  22778 th iteration => 0.0140383554642\n",
      "Loss for  22779 th iteration => 0.0140376262478\n",
      "Loss for  22780 th iteration => 0.0140369229391\n",
      "Loss for  22781 th iteration => 0.0140362727816\n",
      "Loss for  22782 th iteration => 0.0140357109842\n",
      "Loss for  22783 th iteration => 0.0140348152057\n",
      "Loss for  22784 th iteration => 0.01403427602\n",
      "Loss for  22785 th iteration => 0.0140334542664\n",
      "Loss for  22786 th iteration => 0.0140328429774\n",
      "Loss for  22787 th iteration => 0.014032092413\n",
      "Loss for  22788 th iteration => 0.0140314118148\n",
      "Loss for  22789 th iteration => 0.0140307296683\n",
      "Loss for  22790 th iteration => 0.0140300096098\n",
      "Loss for  22791 th iteration => 0.0140294743684\n",
      "Loss for  22792 th iteration => 0.014028616364\n",
      "Loss for  22793 th iteration => 0.0140280782922\n",
      "Loss for  22794 th iteration => 0.0140272235216\n",
      "Loss for  22795 th iteration => 0.0140266827087\n",
      "Loss for  22796 th iteration => 0.0140258374044\n",
      "Loss for  22797 th iteration => 0.0140251874797\n",
      "Loss for  22798 th iteration => 0.0140244569566\n",
      "Loss for  22799 th iteration => 0.0140237944209\n",
      "Loss for  22800 th iteration => 0.0140232029741\n",
      "Loss for  22801 th iteration => 0.0140223952755\n",
      "Loss for  22802 th iteration => 0.0140218128545\n",
      "Loss for  22803 th iteration => 0.0140209967789\n",
      "Loss for  22804 th iteration => 0.0140204230251\n",
      "Loss for  22805 th iteration => 0.0140196555786\n",
      "Loss for  22806 th iteration => 0.0140190954387\n",
      "Loss for  22807 th iteration => 0.0140182974983\n",
      "Loss for  22808 th iteration => 0.0140176653642\n",
      "Loss for  22809 th iteration => 0.0140169385054\n",
      "Loss for  22810 th iteration => 0.014016237166\n",
      "Loss for  22811 th iteration => 0.0140155786228\n",
      "Loss for  22812 th iteration => 0.0140148282168\n",
      "Loss for  22813 th iteration => 0.0140141275416\n",
      "Loss for  22814 th iteration => 0.0140136033301\n",
      "Loss for  22815 th iteration => 0.0140127764289\n",
      "Loss for  22816 th iteration => 0.0140121723944\n",
      "Loss for  22817 th iteration => 0.014011424317\n",
      "Loss for  22818 th iteration => 0.0140107476117\n",
      "Loss for  22819 th iteration => 0.0140099610558\n",
      "Loss for  22820 th iteration => 0.0140093350089\n",
      "Loss for  22821 th iteration => 0.014008730179\n",
      "Loss for  22822 th iteration => 0.0140079384318\n",
      "Loss for  22823 th iteration => 0.0140073428455\n",
      "Loss for  22824 th iteration => 0.0140065425085\n",
      "Loss for  22825 th iteration => 0.0140059558035\n",
      "Loss for  22826 th iteration => 0.014005153618\n",
      "Loss for  22827 th iteration => 0.0140047653328\n",
      "Loss for  22828 th iteration => 0.014003805947\n",
      "Loss for  22829 th iteration => 0.0140032702726\n",
      "Loss for  22830 th iteration => 0.0140024508359\n",
      "Loss for  22831 th iteration => 0.0140018433584\n",
      "Loss for  22832 th iteration => 0.0140010948157\n",
      "Loss for  22833 th iteration => 0.0140004183136\n",
      "Loss for  22834 th iteration => 0.013999737909\n",
      "Loss for  22835 th iteration => 0.0139990120615\n",
      "Loss for  22836 th iteration => 0.0139982958616\n",
      "Loss for  22837 th iteration => 0.0139976254961\n",
      "Loss for  22838 th iteration => 0.0139970834964\n",
      "Loss for  22839 th iteration => 0.0139962386586\n",
      "Loss for  22840 th iteration => 0.0139956939826\n",
      "Loss for  22841 th iteration => 0.0139948544713\n",
      "Loss for  22842 th iteration => 0.0139942093054\n",
      "Loss for  22843 th iteration => 0.0139934797777\n",
      "Loss for  22844 th iteration => 0.0139928220416\n",
      "Loss for  22845 th iteration => 0.0139922317548\n",
      "Loss for  22846 th iteration => 0.0139914290115\n",
      "Loss for  22847 th iteration => 0.0139908475988\n",
      "Loss for  22848 th iteration => 0.0139900366224\n",
      "Loss for  22849 th iteration => 0.0139894637299\n",
      "Loss for  22850 th iteration => 0.013988693956\n",
      "Loss for  22851 th iteration => 0.0139881489279\n",
      "Loss for  22852 th iteration => 0.0139873413616\n",
      "Loss for  22853 th iteration => 0.01398672543\n",
      "Loss for  22854 th iteration => 0.0139859878666\n",
      "Loss for  22855 th iteration => 0.0139853037858\n",
      "Loss for  22856 th iteration => 0.0139846334932\n",
      "Loss for  22857 th iteration => 0.0139838995015\n",
      "Loss for  22858 th iteration => 0.0139831879644\n",
      "Loss for  22859 th iteration => 0.0139825245584\n",
      "Loss for  22860 th iteration => 0.013981979417\n",
      "Loss for  22861 th iteration => 0.0139811406484\n",
      "Loss for  22862 th iteration => 0.0139805928732\n",
      "Loss for  22863 th iteration => 0.0139797571286\n",
      "Loss for  22864 th iteration => 0.013979206805\n",
      "Loss for  22865 th iteration => 0.0139783971694\n",
      "Loss for  22866 th iteration => 0.0139778591268\n",
      "Loss for  22867 th iteration => 0.0139770479465\n",
      "Loss for  22868 th iteration => 0.0139764373249\n",
      "Loss for  22869 th iteration => 0.0139756978006\n",
      "Loss for  22870 th iteration => 0.0139750173703\n",
      "Loss for  22871 th iteration => 0.0139743467551\n",
      "Loss for  22872 th iteration => 0.0139736162697\n",
      "Loss for  22873 th iteration => 0.0139728907176\n",
      "Loss for  22874 th iteration => 0.0139723903807\n",
      "Loss for  22875 th iteration => 0.0139715408218\n",
      "Loss for  22876 th iteration => 0.0139709703942\n",
      "Loss for  22877 th iteration => 0.0139701900342\n",
      "Loss for  22878 th iteration => 0.0139695522448\n",
      "Loss for  22879 th iteration => 0.0139688383769\n",
      "Loss for  22880 th iteration => 0.0139681488469\n",
      "Loss for  22881 th iteration => 0.0139676073848\n",
      "Loss for  22882 th iteration => 0.0139667675563\n",
      "Loss for  22883 th iteration => 0.0139662233728\n",
      "Loss for  22884 th iteration => 0.0139653866609\n",
      "Loss for  22885 th iteration => 0.0139648398431\n",
      "Loss for  22886 th iteration => 0.0139640061549\n",
      "Loss for  22887 th iteration => 0.0139634567881\n",
      "Loss for  22888 th iteration => 0.0139626412251\n",
      "Loss for  22889 th iteration => 0.0139619931569\n",
      "Loss for  22890 th iteration => 0.0139614072687\n",
      "Loss for  22891 th iteration => 0.0139606059808\n",
      "Loss for  22892 th iteration => 0.0139600290092\n",
      "Loss for  22893 th iteration => 0.0139592194449\n",
      "Loss for  22894 th iteration => 0.0139586510362\n",
      "Loss for  22895 th iteration => 0.0139578468685\n",
      "Loss for  22896 th iteration => 0.0139571738033\n",
      "Loss for  22897 th iteration => 0.01395650527\n",
      "Loss for  22898 th iteration => 0.0139559323285\n",
      "Loss for  22899 th iteration => 0.0139551574874\n",
      "Loss for  22900 th iteration => 0.0139545168679\n",
      "Loss for  22901 th iteration => 0.0139538088333\n",
      "Loss for  22902 th iteration => 0.013953109827\n",
      "Loss for  22903 th iteration => 0.0139523794941\n",
      "Loss for  22904 th iteration => 0.0139517310701\n",
      "Loss for  22905 th iteration => 0.0139511846271\n",
      "Loss for  22906 th iteration => 0.0139503530766\n",
      "Loss for  22907 th iteration => 0.0139498040336\n",
      "Loss for  22908 th iteration => 0.0139489754704\n",
      "Loss for  22909 th iteration => 0.0139484239119\n",
      "Loss for  22910 th iteration => 0.0139476081938\n",
      "Loss for  22911 th iteration => 0.0139469636748\n",
      "Loss for  22912 th iteration => 0.0139463802453\n",
      "Loss for  22913 th iteration => 0.0139455794624\n",
      "Loss for  22914 th iteration => 0.0139450048639\n",
      "Loss for  22915 th iteration => 0.0139441958864\n",
      "Loss for  22916 th iteration => 0.0139436297675\n",
      "Loss for  22917 th iteration => 0.0139428219241\n",
      "Loss for  22918 th iteration => 0.0139421508165\n",
      "Loss for  22919 th iteration => 0.0139414858825\n",
      "Loss for  22920 th iteration => 0.0139409177043\n",
      "Loss for  22921 th iteration => 0.0139401407315\n",
      "Loss for  22922 th iteration => 0.0139395054398\n",
      "Loss for  22923 th iteration => 0.0139387947148\n",
      "Loss for  22924 th iteration => 0.0139380975044\n",
      "Loss for  22925 th iteration => 0.0139373653101\n",
      "Loss for  22926 th iteration => 0.0139367260812\n",
      "Loss for  22927 th iteration => 0.0139361776094\n",
      "Loss for  22928 th iteration => 0.0139353509761\n",
      "Loss for  22929 th iteration => 0.0139347999346\n",
      "Loss for  22930 th iteration => 0.0139339762561\n",
      "Loss for  22931 th iteration => 0.013933422729\n",
      "Loss for  22932 th iteration => 0.0139326055862\n",
      "Loss for  22933 th iteration => 0.0139319660988\n",
      "Loss for  22934 th iteration => 0.0139313845082\n",
      "Loss for  22935 th iteration => 0.0139305848277\n",
      "Loss for  22936 th iteration => 0.0139300119959\n",
      "Loss for  22937 th iteration => 0.0139292041895\n",
      "Loss for  22938 th iteration => 0.0139286397673\n",
      "Loss for  22939 th iteration => 0.0139278277707\n",
      "Loss for  22940 th iteration => 0.013927160438\n",
      "Loss for  22941 th iteration => 0.0139264960842\n",
      "Loss for  22942 th iteration => 0.0139259365703\n",
      "Loss for  22943 th iteration => 0.0139251535951\n",
      "Loss for  22944 th iteration => 0.0139245274411\n",
      "Loss for  22945 th iteration => 0.0139238102455\n",
      "Loss for  22946 th iteration => 0.013923120091\n",
      "Loss for  22947 th iteration => 0.0139224660572\n",
      "Loss for  22948 th iteration => 0.0139217621072\n",
      "Loss for  22949 th iteration => 0.0139212115277\n",
      "Loss for  22950 th iteration => 0.0139203898224\n",
      "Loss for  22951 th iteration => 0.0139198366706\n",
      "Loss for  22952 th iteration => 0.0139190179231\n",
      "Loss for  22953 th iteration => 0.0139184622833\n",
      "Loss for  22954 th iteration => 0.0139176464037\n",
      "Loss for  22955 th iteration => 0.0139170883587\n",
      "Loss for  22956 th iteration => 0.0139162988101\n",
      "Loss for  22957 th iteration => 0.0139157509087\n",
      "Loss for  22958 th iteration => 0.0139149601889\n",
      "Loss for  22959 th iteration => 0.0139143426743\n",
      "Loss for  22960 th iteration => 0.0139136206747\n",
      "Loss for  22961 th iteration => 0.0139129362324\n",
      "Loss for  22962 th iteration => 0.0139122802904\n",
      "Loss for  22963 th iteration => 0.0139115486769\n",
      "Loss for  22964 th iteration => 0.0139108290317\n",
      "Loss for  22965 th iteration => 0.0139103396647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  22966 th iteration => 0.0139094898925\n",
      "Loss for  22967 th iteration => 0.0139089330596\n",
      "Loss for  22968 th iteration => 0.0139081498881\n",
      "Loss for  22969 th iteration => 0.0139075282408\n",
      "Loss for  22970 th iteration => 0.0139068090404\n",
      "Loss for  22971 th iteration => 0.0139061381458\n",
      "Loss for  22972 th iteration => 0.0139055897337\n",
      "Loss for  22973 th iteration => 0.0139047688131\n",
      "Loss for  22974 th iteration => 0.0139042178075\n",
      "Loss for  22975 th iteration => 0.0139033998663\n",
      "Loss for  22976 th iteration => 0.0139028463518\n",
      "Loss for  22977 th iteration => 0.0139020313\n",
      "Loss for  22978 th iteration => 0.0139014753593\n",
      "Loss for  22979 th iteration => 0.0139006690363\n",
      "Loss for  22980 th iteration => 0.0139000259807\n",
      "Loss for  22981 th iteration => 0.0138994521405\n",
      "Loss for  22982 th iteration => 0.0138986509902\n",
      "Loss for  22983 th iteration => 0.013898085734\n",
      "Loss for  22984 th iteration => 0.0138972766246\n",
      "Loss for  22985 th iteration => 0.0138967196081\n",
      "Loss for  22986 th iteration => 0.0138959105387\n",
      "Loss for  22987 th iteration => 0.0138952543246\n",
      "Loss for  22988 th iteration => 0.0138945747117\n",
      "Loss for  22989 th iteration => 0.0138940350207\n",
      "Loss for  22990 th iteration => 0.0138932378692\n",
      "Loss for  22991 th iteration => 0.0138926326012\n",
      "Loss for  22992 th iteration => 0.0138919001781\n",
      "Loss for  22993 th iteration => 0.0138912319383\n",
      "Loss for  22994 th iteration => 0.0138905616598\n",
      "Loss for  22995 th iteration => 0.0138898714773\n",
      "Loss for  22996 th iteration => 0.0138893189518\n",
      "Loss for  22997 th iteration => 0.0138885053229\n",
      "Loss for  22998 th iteration => 0.0138879502774\n",
      "Loss for  22999 th iteration => 0.0138871395498\n",
      "Loss for  23000 th iteration => 0.0138865820673\n",
      "Loss for  23001 th iteration => 0.0138857741525\n",
      "Loss for  23002 th iteration => 0.0138852143147\n",
      "Loss for  23003 th iteration => 0.0138844257186\n",
      "Loss for  23004 th iteration => 0.0138837670684\n",
      "Loss for  23005 th iteration => 0.0138832020547\n",
      "Loss for  23006 th iteration => 0.0138823954017\n",
      "Loss for  23007 th iteration => 0.0138818387506\n",
      "Loss for  23008 th iteration => 0.0138810243517\n",
      "Loss for  23009 th iteration => 0.0138804757242\n",
      "Loss for  23010 th iteration => 0.0138796734224\n",
      "Loss for  23011 th iteration => 0.0138790022802\n",
      "Loss for  23012 th iteration => 0.0138783489557\n",
      "Loss for  23013 th iteration => 0.0138777772173\n",
      "Loss for  23014 th iteration => 0.0138770145775\n",
      "Loss for  23015 th iteration => 0.0138763787356\n",
      "Loss for  23016 th iteration => 0.0138756793639\n",
      "Loss for  23017 th iteration => 0.0138749871491\n",
      "Loss for  23018 th iteration => 0.0138742619376\n",
      "Loss for  23019 th iteration => 0.0138736325958\n",
      "Loss for  23020 th iteration => 0.0138730767792\n",
      "Loss for  23021 th iteration => 0.0138722696449\n",
      "Loss for  23022 th iteration => 0.013871711398\n",
      "Loss for  23023 th iteration => 0.0138709070694\n",
      "Loss for  23024 th iteration => 0.0138703464734\n",
      "Loss for  23025 th iteration => 0.0138695484209\n",
      "Loss for  23026 th iteration => 0.0138689042248\n",
      "Loss for  23027 th iteration => 0.0138683372056\n",
      "Loss for  23028 th iteration => 0.0138675353792\n",
      "Loss for  23029 th iteration => 0.0138669767335\n",
      "Loss for  23030 th iteration => 0.0138661671494\n",
      "Loss for  23031 th iteration => 0.0138656165387\n",
      "Loss for  23032 th iteration => 0.0138648068626\n",
      "Loss for  23033 th iteration => 0.0138641507589\n",
      "Loss for  23034 th iteration => 0.0138634794999\n",
      "Loss for  23035 th iteration => 0.0138629407704\n",
      "Loss for  23036 th iteration => 0.0138621479892\n",
      "Loss for  23037 th iteration => 0.0138615450673\n",
      "Loss for  23038 th iteration => 0.0138608156435\n",
      "Loss for  23039 th iteration => 0.0138601510949\n",
      "Loss for  23040 th iteration => 0.0138594824843\n",
      "Loss for  23041 th iteration => 0.0138588001718\n",
      "Loss for  23042 th iteration => 0.0138582433999\n",
      "Loss for  23043 th iteration => 0.0138574399835\n",
      "Loss for  23044 th iteration => 0.01385688076\n",
      "Loss for  23045 th iteration => 0.0138560801719\n",
      "Loss for  23046 th iteration => 0.0138555185785\n",
      "Loss for  23047 th iteration => 0.0138547207316\n",
      "Loss for  23048 th iteration => 0.0138541568486\n",
      "Loss for  23049 th iteration => 0.0138533726365\n",
      "Loss for  23050 th iteration => 0.0138527173878\n",
      "Loss for  23051 th iteration => 0.0138521573415\n",
      "Loss for  23052 th iteration => 0.0138513518052\n",
      "Loss for  23053 th iteration => 0.0138507999514\n",
      "Loss for  23054 th iteration => 0.0138499868316\n",
      "Loss for  23055 th iteration => 0.0138494428362\n",
      "Loss for  23056 th iteration => 0.0138486383758\n",
      "Loss for  23057 th iteration => 0.0138479727975\n",
      "Loss for  23058 th iteration => 0.0138473188641\n",
      "Loss for  23059 th iteration => 0.0138467598403\n",
      "Loss for  23060 th iteration => 0.0138459899152\n",
      "Loss for  23061 th iteration => 0.0138453678994\n",
      "Loss for  23062 th iteration => 0.0138446601425\n",
      "Loss for  23063 th iteration => 0.0138439784485\n",
      "Loss for  23064 th iteration => 0.0138432480848\n",
      "Loss for  23065 th iteration => 0.0138426319216\n",
      "Loss for  23066 th iteration => 0.0138420724257\n",
      "Loss for  23067 th iteration => 0.0138412749051\n",
      "Loss for  23068 th iteration => 0.0138407130377\n",
      "Loss for  23069 th iteration => 0.0138399182597\n",
      "Loss for  23070 th iteration => 0.013839354101\n",
      "Loss for  23071 th iteration => 0.0138385619806\n",
      "Loss for  23072 th iteration => 0.0138380163581\n",
      "Loss for  23073 th iteration => 0.0138373684149\n",
      "Loss for  23074 th iteration => 0.0138365681076\n",
      "Loss for  23075 th iteration => 0.0138360138204\n",
      "Loss for  23076 th iteration => 0.0138352057551\n",
      "Loss for  23077 th iteration => 0.013834659502\n",
      "Loss for  23078 th iteration => 0.0138338452849\n",
      "Loss for  23079 th iteration => 0.0138332001657\n",
      "Loss for  23080 th iteration => 0.0138324943403\n",
      "Loss for  23081 th iteration => 0.0138318235139\n",
      "Loss for  23082 th iteration => 0.0138313282779\n",
      "Loss for  23083 th iteration => 0.0138304688405\n",
      "Loss for  23084 th iteration => 0.0138299672813\n",
      "Loss for  23085 th iteration => 0.0138291144926\n",
      "Loss for  23086 th iteration => 0.0138286068529\n",
      "Loss for  23087 th iteration => 0.0138277966481\n",
      "Loss for  23088 th iteration => 0.0138273531034\n",
      "Loss for  23089 th iteration => 0.0138264780323\n",
      "Loss for  23090 th iteration => 0.0138259615461\n",
      "Loss for  23091 th iteration => 0.0138251584955\n",
      "Loss for  23092 th iteration => 0.0138245717279\n",
      "Loss for  23093 th iteration => 0.0138238380616\n",
      "Loss for  23094 th iteration => 0.0138231836121\n",
      "Loss for  23095 th iteration => 0.0138225167543\n",
      "Loss for  23096 th iteration => 0.0138218220058\n",
      "Loss for  23097 th iteration => 0.0138213330518\n",
      "Loss for  23098 th iteration => 0.0138204714959\n",
      "Loss for  23099 th iteration => 0.0138199133057\n",
      "Loss for  23100 th iteration => 0.0138191121871\n",
      "Loss for  23101 th iteration => 0.0138185620462\n",
      "Loss for  23102 th iteration => 0.0138177584011\n",
      "Loss for  23103 th iteration => 0.0138171041482\n",
      "Loss for  23104 th iteration => 0.0138164105133\n",
      "Loss for  23105 th iteration => 0.0138157276029\n",
      "Loss for  23106 th iteration => 0.0138150625067\n",
      "Loss for  23107 th iteration => 0.0138143708237\n",
      "Loss for  23108 th iteration => 0.013813874181\n",
      "Loss for  23109 th iteration => 0.0138130195601\n",
      "Loss for  23110 th iteration => 0.0138125169094\n",
      "Loss for  23111 th iteration => 0.0138117091296\n",
      "Loss for  23112 th iteration => 0.0138112609574\n",
      "Loss for  23113 th iteration => 0.0138103931332\n",
      "Loss for  23114 th iteration => 0.0138098729909\n",
      "Loss for  23115 th iteration => 0.0138090762259\n",
      "Loss for  23116 th iteration => 0.013808486747\n",
      "Loss for  23117 th iteration => 0.0138077584315\n",
      "Loss for  23118 th iteration => 0.0138071021893\n",
      "Loss for  23119 th iteration => 0.0138064398727\n",
      "Loss for  23120 th iteration => 0.0138058770429\n",
      "Loss for  23121 th iteration => 0.0138051409031\n",
      "Loss for  23122 th iteration => 0.013804504935\n",
      "Loss for  23123 th iteration => 0.0138037488964\n",
      "Loss for  23124 th iteration => 0.0138031202946\n",
      "Loss for  23125 th iteration => 0.0138024287325\n",
      "Loss for  23126 th iteration => 0.0138017420634\n",
      "Loss for  23127 th iteration => 0.0138010013087\n",
      "Loss for  23128 th iteration => 0.0138003679383\n",
      "Loss for  23129 th iteration => 0.0137996564222\n",
      "Loss for  23130 th iteration => 0.0137989952417\n",
      "Loss for  23131 th iteration => 0.0137983303579\n",
      "Loss for  23132 th iteration => 0.0137977834045\n",
      "Loss for  23133 th iteration => 0.0137970090671\n",
      "Loss for  23134 th iteration => 0.0137964030019\n",
      "Loss for  23135 th iteration => 0.0137957148385\n",
      "Loss for  23136 th iteration => 0.0137952204869\n",
      "Loss for  23137 th iteration => 0.0137943487484\n",
      "Loss for  23138 th iteration => 0.0137938354154\n",
      "Loss for  23139 th iteration => 0.0137930349845\n",
      "Loss for  23140 th iteration => 0.013792452067\n",
      "Loss for  23141 th iteration => 0.0137917203324\n",
      "Loss for  23142 th iteration => 0.0137910704052\n",
      "Loss for  23143 th iteration => 0.0137904048154\n",
      "Loss for  23144 th iteration => 0.0137896903948\n",
      "Loss for  23145 th iteration => 0.0137890968586\n",
      "Loss for  23146 th iteration => 0.0137884709366\n",
      "Loss for  23147 th iteration => 0.0137877104626\n",
      "Loss for  23148 th iteration => 0.0137870896563\n",
      "Loss for  23149 th iteration => 0.0137863931003\n",
      "Loss for  23150 th iteration => 0.0137857134286\n",
      "Loss for  23151 th iteration => 0.0137849725802\n",
      "Loss for  23152 th iteration => 0.0137843425446\n",
      "Loss for  23153 th iteration => 0.0137836306849\n",
      "Loss for  23154 th iteration => 0.0137829730811\n",
      "Loss for  23155 th iteration => 0.0137823042457\n",
      "Loss for  23156 th iteration => 0.0137817650457\n",
      "Loss for  23157 th iteration => 0.0137809857797\n",
      "Loss for  23158 th iteration => 0.0137803879603\n",
      "Loss for  23159 th iteration => 0.0137796898945\n",
      "Loss for  23160 th iteration => 0.0137792097546\n",
      "Loss for  23161 th iteration => 0.0137783289246\n",
      "Loss for  23162 th iteration => 0.0137778280149\n",
      "Loss for  23163 th iteration => 0.0137770179581\n",
      "Loss for  23164 th iteration => 0.0137764479874\n",
      "Loss for  23165 th iteration => 0.0137757061096\n",
      "Loss for  23166 th iteration => 0.0137750696361\n",
      "Loss for  23167 th iteration => 0.0137743934022\n",
      "Loss for  23168 th iteration => 0.0137736929259\n",
      "Loss for  23169 th iteration => 0.0137730855422\n",
      "Loss for  23170 th iteration => 0.0137724755423\n",
      "Loss for  23171 th iteration => 0.013771706123\n",
      "Loss for  23172 th iteration => 0.0137710975529\n",
      "Loss for  23173 th iteration => 0.0137703915971\n",
      "Loss for  23174 th iteration => 0.0137697221937\n",
      "Loss for  23175 th iteration => 0.0137689793204\n",
      "Loss for  23176 th iteration => 0.0137683545112\n",
      "Loss for  23177 th iteration => 0.0137676404165\n",
      "Loss for  23178 th iteration => 0.0137669882419\n",
      "Loss for  23179 th iteration => 0.0137663124319\n",
      "Loss for  23180 th iteration => 0.0137657849194\n",
      "Loss for  23181 th iteration => 0.0137649968197\n",
      "Loss for  23182 th iteration => 0.0137644110902\n",
      "Loss for  23183 th iteration => 0.0137636981233\n",
      "Loss for  23184 th iteration => 0.013763237313\n",
      "Loss for  23185 th iteration => 0.0137623432825\n",
      "Loss for  23186 th iteration => 0.0137618588405\n",
      "Loss for  23187 th iteration => 0.0137610351465\n",
      "Loss for  23188 th iteration => 0.0137604820706\n",
      "Loss for  23189 th iteration => 0.0137597261339\n",
      "Loss for  23190 th iteration => 0.0137591069677\n",
      "Loss for  23191 th iteration => 0.0137584162676\n",
      "Loss for  23192 th iteration => 0.0137577334969\n",
      "Loss for  23193 th iteration => 0.0137571076677\n",
      "Loss for  23194 th iteration => 0.0137565180368\n",
      "Loss for  23195 th iteration => 0.0137557362598\n",
      "Loss for  23196 th iteration => 0.0137551432835\n",
      "Loss for  23197 th iteration => 0.0137544245942\n",
      "Loss for  23198 th iteration => 0.0137537702099\n",
      "Loss for  23199 th iteration => 0.0137531121186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  23200 th iteration => 0.0137524169736\n",
      "Loss for  23201 th iteration => 0.0137516961318\n",
      "Loss for  23202 th iteration => 0.0137510534703\n",
      "Loss for  23203 th iteration => 0.0137503647321\n",
      "Loss for  23204 th iteration => 0.013749856141\n",
      "Loss for  23205 th iteration => 0.0137490524619\n",
      "Loss for  23206 th iteration => 0.0137484848782\n",
      "Loss for  23207 th iteration => 0.0137477394101\n",
      "Loss for  23208 th iteration => 0.0137471195086\n",
      "Loss for  23209 th iteration => 0.013746389605\n",
      "Loss for  23210 th iteration => 0.0137459302043\n",
      "Loss for  23211 th iteration => 0.0137450843845\n",
      "Loss for  23212 th iteration => 0.0137445565648\n",
      "Loss for  23213 th iteration => 0.0137437782908\n",
      "Loss for  23214 th iteration => 0.0137431845858\n",
      "Loss for  23215 th iteration => 0.0137424713468\n",
      "Loss for  23216 th iteration => 0.0137418142327\n",
      "Loss for  23217 th iteration => 0.0137411635752\n",
      "Loss for  23218 th iteration => 0.0137404783857\n",
      "Loss for  23219 th iteration => 0.0137399068799\n",
      "Loss for  23220 th iteration => 0.0137391345288\n",
      "Loss for  23221 th iteration => 0.0137385708112\n",
      "Loss for  23222 th iteration => 0.0137377912567\n",
      "Loss for  23223 th iteration => 0.0137372350079\n",
      "Loss for  23224 th iteration => 0.0137364625523\n",
      "Loss for  23225 th iteration => 0.0137358066401\n",
      "Loss for  23226 th iteration => 0.0137351296164\n",
      "Loss for  23227 th iteration => 0.0137344471036\n",
      "Loss for  23228 th iteration => 0.0137338269521\n",
      "Loss for  23229 th iteration => 0.013733235266\n",
      "Loss for  23230 th iteration => 0.0137325167134\n",
      "Loss for  23231 th iteration => 0.0137318732489\n",
      "Loss for  23232 th iteration => 0.0137311344302\n",
      "Loss for  23233 th iteration => 0.0137305247877\n",
      "Loss for  23234 th iteration => 0.0137299832954\n",
      "Loss for  23235 th iteration => 0.0137291891889\n",
      "Loss for  23236 th iteration => 0.0137286454336\n",
      "Loss for  23237 th iteration => 0.0137278539464\n",
      "Loss for  23238 th iteration => 0.0137273080043\n",
      "Loss for  23239 th iteration => 0.0137265190557\n",
      "Loss for  23240 th iteration => 0.0137259790921\n",
      "Loss for  23241 th iteration => 0.0137253377873\n",
      "Loss for  23242 th iteration => 0.0137245755941\n",
      "Loss for  23243 th iteration => 0.0137240044685\n",
      "Loss for  23244 th iteration => 0.0137232349204\n",
      "Loss for  23245 th iteration => 0.0137226714159\n",
      "Loss for  23246 th iteration => 0.0137218948245\n",
      "Loss for  23247 th iteration => 0.0137213386264\n",
      "Loss for  23248 th iteration => 0.0137205656214\n",
      "Loss for  23249 th iteration => 0.0137199204667\n",
      "Loss for  23250 th iteration => 0.0137192461404\n",
      "Loss for  23251 th iteration => 0.0137187345062\n",
      "Loss for  23252 th iteration => 0.0137179395753\n",
      "Loss for  23253 th iteration => 0.0137173693121\n",
      "Loss for  23254 th iteration => 0.0137166322335\n",
      "Loss for  23255 th iteration => 0.0137160074856\n",
      "Loss for  23256 th iteration => 0.0137152585912\n",
      "Loss for  23257 th iteration => 0.0137146558253\n",
      "Loss for  23258 th iteration => 0.0137141129713\n",
      "Loss for  23259 th iteration => 0.0137133232255\n",
      "Loss for  23260 th iteration => 0.0137127781278\n",
      "Loss for  23261 th iteration => 0.0137119909806\n",
      "Loss for  23262 th iteration => 0.013711443715\n",
      "Loss for  23263 th iteration => 0.0137106590859\n",
      "Loss for  23264 th iteration => 0.0137101120339\n",
      "Loss for  23265 th iteration => 0.0137094791401\n",
      "Loss for  23266 th iteration => 0.0137087194116\n",
      "Loss for  23267 th iteration => 0.0137081488137\n",
      "Loss for  23268 th iteration => 0.0137073817645\n",
      "Loss for  23269 th iteration => 0.0137068187525\n",
      "Loss for  23270 th iteration => 0.0137060446926\n",
      "Loss for  23271 th iteration => 0.0137054889533\n",
      "Loss for  23272 th iteration => 0.0137047152608\n",
      "Loss for  23273 th iteration => 0.01370408066\n",
      "Loss for  23274 th iteration => 0.0137033907941\n",
      "Loss for  23275 th iteration => 0.0137029019175\n",
      "Loss for  23276 th iteration => 0.0137020871295\n",
      "Loss for  23277 th iteration => 0.0137015398303\n",
      "Loss for  23278 th iteration => 0.0137007826914\n",
      "Loss for  23279 th iteration => 0.0137001793584\n",
      "Loss for  23280 th iteration => 0.0136994774998\n",
      "Loss for  23281 th iteration => 0.0136988341213\n",
      "Loss for  23282 th iteration => 0.0136981380857\n",
      "Loss for  23283 th iteration => 0.0136976423919\n",
      "Loss for  23284 th iteration => 0.0136968405306\n",
      "Loss for  23285 th iteration => 0.0136962796073\n",
      "Loss for  23286 th iteration => 0.0136955421347\n",
      "Loss for  23287 th iteration => 0.0136949184309\n",
      "Loss for  23288 th iteration => 0.0136942429204\n",
      "Loss for  23289 th iteration => 0.0136935588295\n",
      "Loss for  23290 th iteration => 0.0136929470134\n",
      "Loss for  23291 th iteration => 0.0136923527263\n",
      "Loss for  23292 th iteration => 0.0136915882304\n",
      "Loss for  23293 th iteration => 0.0136909917234\n",
      "Loss for  23294 th iteration => 0.0136902874243\n",
      "Loss for  23295 th iteration => 0.013689632627\n",
      "Loss for  23296 th iteration => 0.0136888964934\n",
      "Loss for  23297 th iteration => 0.0136882811913\n",
      "Loss for  23298 th iteration => 0.0136875724237\n",
      "Loss for  23299 th iteration => 0.0136869311253\n",
      "Loss for  23300 th iteration => 0.01368624891\n",
      "Loss for  23301 th iteration => 0.0136857463982\n",
      "Loss for  23302 th iteration => 0.0136849471763\n",
      "Loss for  23303 th iteration => 0.0136843892914\n",
      "Loss for  23304 th iteration => 0.0136836523439\n",
      "Loss for  23305 th iteration => 0.0136832323658\n",
      "Loss for  23306 th iteration => 0.0136823170335\n",
      "Loss for  23307 th iteration => 0.0136818707235\n",
      "Loss for  23308 th iteration => 0.0136810226026\n",
      "Loss for  23309 th iteration => 0.0136805107246\n",
      "Loss for  23310 th iteration => 0.0136797273296\n",
      "Loss for  23311 th iteration => 0.0136791523349\n",
      "Loss for  23312 th iteration => 0.0136784312368\n",
      "Loss for  23313 th iteration => 0.0136777955209\n",
      "Loss for  23314 th iteration => 0.013677134346\n",
      "Loss for  23315 th iteration => 0.0136764661032\n",
      "Loss for  23316 th iteration => 0.0136758959956\n",
      "Loss for  23317 th iteration => 0.0136751346288\n",
      "Loss for  23318 th iteration => 0.013674572004\n",
      "Loss for  23319 th iteration => 0.0136738037238\n",
      "Loss for  23320 th iteration => 0.0136732482722\n",
      "Loss for  23321 th iteration => 0.0136724844326\n",
      "Loss for  23322 th iteration => 0.0136718396892\n",
      "Loss for  23323 th iteration => 0.0136711633472\n",
      "Loss for  23324 th iteration => 0.0136704930175\n",
      "Loss for  23325 th iteration => 0.0136698616996\n",
      "Loss for  23326 th iteration => 0.0136692989824\n",
      "Loss for  23327 th iteration => 0.0136685626079\n",
      "Loss for  23328 th iteration => 0.0136679479173\n",
      "Loss for  23329 th iteration => 0.0136671976639\n",
      "Loss for  23330 th iteration => 0.013666602961\n",
      "Loss for  23331 th iteration => 0.0136660615391\n",
      "Loss for  23332 th iteration => 0.0136652794756\n",
      "Loss for  23333 th iteration => 0.0136647358812\n",
      "Loss for  23334 th iteration => 0.0136639563385\n",
      "Loss for  23335 th iteration => 0.0136634106459\n",
      "Loss for  23336 th iteration => 0.0136626335455\n",
      "Loss for  23337 th iteration => 0.0136620865599\n",
      "Loss for  23338 th iteration => 0.013661457939\n",
      "Loss for  23339 th iteration => 0.0136607090735\n",
      "Loss for  23340 th iteration => 0.0136601366631\n",
      "Loss for  23341 th iteration => 0.0136593806808\n",
      "Loss for  23342 th iteration => 0.013658815648\n",
      "Loss for  23343 th iteration => 0.0136580528524\n",
      "Loss for  23344 th iteration => 0.0136574948908\n",
      "Loss for  23345 th iteration => 0.013656728044\n",
      "Loss for  23346 th iteration => 0.0136561024316\n",
      "Loss for  23347 th iteration => 0.0136554098496\n",
      "Loss for  23348 th iteration => 0.0136547869841\n",
      "Loss for  23349 th iteration => 0.0136542457454\n",
      "Loss for  23350 th iteration => 0.0136534658555\n",
      "Loss for  23351 th iteration => 0.0136529191939\n",
      "Loss for  23352 th iteration => 0.0136521450307\n",
      "Loss for  23353 th iteration => 0.0136515931677\n",
      "Loss for  23354 th iteration => 0.013650836061\n",
      "Loss for  23355 th iteration => 0.0136502296825\n",
      "Loss for  23356 th iteration => 0.0136496876091\n",
      "Loss for  23357 th iteration => 0.0136489093122\n",
      "Loss for  23358 th iteration => 0.0136483651109\n",
      "Loss for  23359 th iteration => 0.0136475892871\n",
      "Loss for  23360 th iteration => 0.013647043031\n",
      "Loss for  23361 th iteration => 0.0136462696031\n",
      "Loss for  23362 th iteration => 0.0136457213638\n",
      "Loss for  23363 th iteration => 0.0136449643082\n",
      "Loss for  23364 th iteration => 0.0136443361942\n",
      "Loss for  23365 th iteration => 0.0136437717205\n",
      "Loss for  23366 th iteration => 0.0136430111169\n",
      "Loss for  23367 th iteration => 0.0136424537746\n",
      "Loss for  23368 th iteration => 0.0136416865954\n",
      "Loss for  23369 th iteration => 0.0136411360841\n",
      "Loss for  23370 th iteration => 0.0136403762409\n",
      "Loss for  23371 th iteration => 0.0136397294385\n",
      "Loss for  23372 th iteration => 0.0136390719762\n",
      "Loss for  23373 th iteration => 0.0136385580566\n",
      "Loss for  23374 th iteration => 0.0136377789975\n",
      "Loss for  23375 th iteration => 0.0136372099045\n",
      "Loss for  23376 th iteration => 0.013636485277\n",
      "Loss for  23377 th iteration => 0.0136358670427\n",
      "Loss for  23378 th iteration => 0.0136351233613\n",
      "Loss for  23379 th iteration => 0.0136345322836\n",
      "Loss for  23380 th iteration => 0.0136339903088\n",
      "Loss for  23381 th iteration => 0.013633214872\n",
      "Loss for  23382 th iteration => 0.0136326707849\n",
      "Loss for  23383 th iteration => 0.013631897804\n",
      "Loss for  23384 th iteration => 0.0136313516774\n",
      "Loss for  23385 th iteration => 0.0136305810755\n",
      "Loss for  23386 th iteration => 0.0136300350651\n",
      "Loss for  23387 th iteration => 0.0136294099962\n",
      "Loss for  23388 th iteration => 0.0136286630784\n",
      "Loss for  23389 th iteration => 0.0136280947368\n",
      "Loss for  23390 th iteration => 0.0136273408813\n",
      "Loss for  23391 th iteration => 0.0136267797352\n",
      "Loss for  23392 th iteration => 0.0136260192403\n",
      "Loss for  23393 th iteration => 0.0136254649886\n",
      "Loss for  23394 th iteration => 0.0136247002785\n",
      "Loss for  23395 th iteration => 0.0136240779483\n",
      "Loss for  23396 th iteration => 0.013623387973\n",
      "Loss for  23397 th iteration => 0.0136227678532\n",
      "Loss for  23398 th iteration => 0.0136222310506\n",
      "Loss for  23399 th iteration => 0.0136214527611\n",
      "Loss for  23400 th iteration => 0.0136209106488\n",
      "Loss for  23401 th iteration => 0.0136201379691\n",
      "Loss for  23402 th iteration => 0.0136195907652\n",
      "Loss for  23403 th iteration => 0.0136188334073\n",
      "Loss for  23404 th iteration => 0.0136182337839\n",
      "Loss for  23405 th iteration => 0.0136176916693\n",
      "Loss for  23406 th iteration => 0.0136169194542\n",
      "Loss for  23407 th iteration => 0.0136163752611\n",
      "Loss for  23408 th iteration => 0.0136156054656\n",
      "Loss for  23409 th iteration => 0.0136150592659\n",
      "Loss for  23410 th iteration => 0.013614291814\n",
      "Loss for  23411 th iteration => 0.0136137436783\n",
      "Loss for  23412 th iteration => 0.0136129887898\n",
      "Loss for  23413 th iteration => 0.0136123660453\n",
      "Loss for  23414 th iteration => 0.0136118039681\n",
      "Loss for  23415 th iteration => 0.0136110471055\n",
      "Loss for  23416 th iteration => 0.0136104920155\n",
      "Loss for  23417 th iteration => 0.0136097287146\n",
      "Loss for  23418 th iteration => 0.0136091803156\n",
      "Loss for  23419 th iteration => 0.0136084210839\n",
      "Loss for  23420 th iteration => 0.0136077827805\n",
      "Loss for  23421 th iteration => 0.0136071185183\n",
      "Loss for  23422 th iteration => 0.0136066215717\n",
      "Loss for  23423 th iteration => 0.0136058311014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  23424 th iteration => 0.013605279963\n",
      "Loss for  23425 th iteration => 0.0136045429533\n",
      "Loss for  23426 th iteration => 0.0136039401045\n",
      "Loss for  23427 th iteration => 0.0136031901043\n",
      "Loss for  23428 th iteration => 0.0136026079048\n",
      "Loss for  23429 th iteration => 0.0136020663311\n",
      "Loss for  23430 th iteration => 0.0136012965053\n",
      "Loss for  23431 th iteration => 0.0136007528608\n",
      "Loss for  23432 th iteration => 0.0135999854458\n",
      "Loss for  23433 th iteration => 0.0135994398022\n",
      "Loss for  23434 th iteration => 0.0135986747221\n",
      "Loss for  23435 th iteration => 0.0135981271497\n",
      "Loss for  23436 th iteration => 0.0135973894158\n",
      "Loss for  23437 th iteration => 0.0135969451142\n",
      "Loss for  23438 th iteration => 0.0135961089184\n",
      "Loss for  23439 th iteration => 0.0135955537802\n",
      "Loss for  23440 th iteration => 0.0135948248237\n",
      "Loss for  23441 th iteration => 0.013594213836\n",
      "Loss for  23442 th iteration => 0.0135935399788\n",
      "Loss for  23443 th iteration => 0.0135928754279\n",
      "Loss for  23444 th iteration => 0.0135922544035\n",
      "Loss for  23445 th iteration => 0.0135915506023\n",
      "Loss for  23446 th iteration => 0.0135908924106\n",
      "Loss for  23447 th iteration => 0.0135902199726\n",
      "Loss for  23448 th iteration => 0.0135896145005\n",
      "Loss for  23449 th iteration => 0.0135890380651\n",
      "Loss for  23450 th iteration => 0.0135883292645\n",
      "Loss for  23451 th iteration => 0.0135877044951\n",
      "Loss for  23452 th iteration => 0.0135869724514\n",
      "Loss for  23453 th iteration => 0.0135863729766\n",
      "Loss for  23454 th iteration => 0.0135856705515\n",
      "Loss for  23455 th iteration => 0.013585066814\n",
      "Loss for  23456 th iteration => 0.0135845242919\n",
      "Loss for  23457 th iteration => 0.0135837588312\n",
      "Loss for  23458 th iteration => 0.0135832143469\n",
      "Loss for  23459 th iteration => 0.0135824511813\n",
      "Loss for  23460 th iteration => 0.0135819081511\n",
      "Loss for  23461 th iteration => 0.0135812905147\n",
      "Loss for  23462 th iteration => 0.0135806155389\n",
      "Loss for  23463 th iteration => 0.0135799933089\n",
      "Loss for  23464 th iteration => 0.0135792402971\n",
      "Loss for  23465 th iteration => 0.0135786872948\n",
      "Loss for  23466 th iteration => 0.0135779277683\n",
      "Loss for  23467 th iteration => 0.0135773815327\n",
      "Loss for  23468 th iteration => 0.0135766157792\n",
      "Loss for  23469 th iteration => 0.01357607602\n",
      "Loss for  23470 th iteration => 0.013575321943\n",
      "Loss for  23471 th iteration => 0.013574686858\n",
      "Loss for  23472 th iteration => 0.0135740216422\n",
      "Loss for  23473 th iteration => 0.013573526458\n",
      "Loss for  23474 th iteration => 0.0135727398037\n",
      "Loss for  23475 th iteration => 0.0135721918422\n",
      "Loss for  23476 th iteration => 0.0135714572484\n",
      "Loss for  23477 th iteration => 0.0135708616808\n",
      "Loss for  23478 th iteration => 0.0135701150271\n",
      "Loss for  23479 th iteration => 0.0135695337748\n",
      "Loss for  23480 th iteration => 0.0135688505862\n",
      "Loss for  23481 th iteration => 0.0135683602694\n",
      "Loss for  23482 th iteration => 0.0135675741599\n",
      "Loss for  23483 th iteration => 0.0135670254937\n",
      "Loss for  23484 th iteration => 0.0135662969657\n",
      "Loss for  23485 th iteration => 0.0135656922088\n",
      "Loss for  23486 th iteration => 0.0135650190242\n",
      "Loss for  23487 th iteration => 0.0135643776255\n",
      "Loss for  23488 th iteration => 0.0135638673843\n",
      "Loss for  23489 th iteration => 0.0135630736103\n",
      "Loss for  23490 th iteration => 0.013562518885\n",
      "Loss for  23491 th iteration => 0.0135617640781\n",
      "Loss for  23492 th iteration => 0.0135612161373\n",
      "Loss for  23493 th iteration => 0.0135604550849\n",
      "Loss for  23494 th iteration => 0.0135599136384\n",
      "Loss for  23495 th iteration => 0.0135591648883\n",
      "Loss for  23496 th iteration => 0.0135585226229\n",
      "Loss for  23497 th iteration => 0.0135578645542\n",
      "Loss for  23498 th iteration => 0.0135571988935\n",
      "Loss for  23499 th iteration => 0.0135565810521\n",
      "Loss for  23500 th iteration => 0.0135560247499\n",
      "Loss for  23501 th iteration => 0.0135553011587\n",
      "Loss for  23502 th iteration => 0.0135546996214\n",
      "Loss for  23503 th iteration => 0.0135539567005\n",
      "Loss for  23504 th iteration => 0.0135533771045\n",
      "Loss for  23505 th iteration => 0.0135528356325\n",
      "Loss for  23506 th iteration => 0.0135520749795\n",
      "Loss for  23507 th iteration => 0.0135515315219\n",
      "Loss for  23508 th iteration => 0.0135507731877\n",
      "Loss for  23509 th iteration => 0.0135502278141\n",
      "Loss for  23510 th iteration => 0.013549471725\n",
      "Loss for  23511 th iteration => 0.0135489245037\n",
      "Loss for  23512 th iteration => 0.0135481936876\n",
      "Loss for  23513 th iteration => 0.0135477484526\n",
      "Loss for  23514 th iteration => 0.0135469226921\n",
      "Loss for  23515 th iteration => 0.0135463719319\n",
      "Loss for  23516 th iteration => 0.0135456469075\n",
      "Loss for  23517 th iteration => 0.0135450422877\n",
      "Loss for  23518 th iteration => 0.0135443703936\n",
      "Loss for  23519 th iteration => 0.0135437141429\n",
      "Loss for  23520 th iteration => 0.0135430931696\n",
      "Loss for  23521 th iteration => 0.0135424007412\n",
      "Loss for  23522 th iteration => 0.0135417447769\n",
      "Loss for  23523 th iteration => 0.0135410799462\n",
      "Loss for  23524 th iteration => 0.0135404687497\n",
      "Loss for  23525 th iteration => 0.0135399078919\n",
      "Loss for  23526 th iteration => 0.0135391919394\n",
      "Loss for  23527 th iteration => 0.0135385846165\n",
      "Loss for  23528 th iteration => 0.0135378486316\n",
      "Loss for  23529 th iteration => 0.0135372629587\n",
      "Loss for  23530 th iteration => 0.0135365556413\n",
      "Loss for  23531 th iteration => 0.0135359594981\n",
      "Loss for  23532 th iteration => 0.01353541788\n",
      "Loss for  23533 th iteration => 0.0135346607345\n",
      "Loss for  23534 th iteration => 0.0135341172221\n",
      "Loss for  23535 th iteration => 0.0135333622979\n",
      "Loss for  23536 th iteration => 0.0135328169588\n",
      "Loss for  23537 th iteration => 0.0135320878993\n",
      "Loss for  23538 th iteration => 0.0135316459484\n",
      "Loss for  23539 th iteration => 0.0135308186114\n",
      "Loss for  23540 th iteration => 0.0135302701692\n",
      "Loss for  23541 th iteration => 0.0135295455384\n",
      "Loss for  23542 th iteration => 0.0135289438895\n",
      "Loss for  23543 th iteration => 0.0135282717427\n",
      "Loss for  23544 th iteration => 0.0135276190973\n",
      "Loss for  23545 th iteration => 0.0135269972436\n",
      "Loss for  23546 th iteration => 0.0135263070284\n",
      "Loss for  23547 th iteration => 0.01352564996\n",
      "Loss for  23548 th iteration => 0.0135249894408\n",
      "Loss for  23549 th iteration => 0.0135243786913\n",
      "Loss for  23550 th iteration => 0.0135238219469\n",
      "Loss for  23551 th iteration => 0.013523104627\n",
      "Loss for  23552 th iteration => 0.0135224989616\n",
      "Loss for  23553 th iteration => 0.0135217644765\n",
      "Loss for  23554 th iteration => 0.0135211805235\n",
      "Loss for  23555 th iteration => 0.0135204743937\n",
      "Loss for  23556 th iteration => 0.0135198813502\n",
      "Loss for  23557 th iteration => 0.0135193417391\n",
      "Loss for  23558 th iteration => 0.0135185855993\n",
      "Loss for  23559 th iteration => 0.0135180441109\n",
      "Loss for  23560 th iteration => 0.0135172901734\n",
      "Loss for  23561 th iteration => 0.0135167468749\n",
      "Loss for  23562 th iteration => 0.0135160179871\n",
      "Loss for  23563 th iteration => 0.0135155820308\n",
      "Loss for  23564 th iteration => 0.0135147490257\n",
      "Loss for  23565 th iteration => 0.0135142086545\n",
      "Loss for  23566 th iteration => 0.0135134787119\n",
      "Loss for  23567 th iteration => 0.0135128856557\n",
      "Loss for  23568 th iteration => 0.0135122076812\n",
      "Loss for  23569 th iteration => 0.0135115641342\n",
      "Loss for  23570 th iteration => 0.0135109359526\n",
      "Loss for  23571 th iteration => 0.0135102519758\n",
      "Loss for  23572 th iteration => 0.0135095914886\n",
      "Loss for  23573 th iteration => 0.0135089375498\n",
      "Loss for  23574 th iteration => 0.0135083234316\n",
      "Loss for  23575 th iteration => 0.0135077758141\n",
      "Loss for  23576 th iteration => 0.0135070521527\n",
      "Loss for  23577 th iteration => 0.0135064556248\n",
      "Loss for  23578 th iteration => 0.0135057801937\n",
      "Loss for  23579 th iteration => 0.0135051469801\n",
      "Loss for  23580 th iteration => 0.013504436674\n",
      "Loss for  23581 th iteration => 0.0135038492109\n",
      "Loss for  23582 th iteration => 0.0135033113978\n",
      "Loss for  23583 th iteration => 0.0135025564152\n",
      "Loss for  23584 th iteration => 0.0135020167118\n",
      "Loss for  23585 th iteration => 0.0135012639446\n",
      "Loss for  23586 th iteration => 0.0135007224182\n",
      "Loss for  23587 th iteration => 0.0134999717953\n",
      "Loss for  23588 th iteration => 0.0134994298902\n",
      "Loss for  23589 th iteration => 0.0134988255354\n",
      "Loss for  23590 th iteration => 0.0134980808926\n",
      "Loss for  23591 th iteration => 0.0134975346449\n",
      "Loss for  23592 th iteration => 0.0134967838041\n",
      "Loss for  23593 th iteration => 0.0134962440001\n",
      "Loss for  23594 th iteration => 0.0134954872392\n",
      "Loss for  23595 th iteration => 0.0134949535987\n",
      "Loss for  23596 th iteration => 0.0134941966207\n",
      "Loss for  23597 th iteration => 0.0134935927843\n",
      "Loss for  23598 th iteration => 0.0134929081759\n",
      "Loss for  23599 th iteration => 0.0134923035549\n",
      "Loss for  23600 th iteration => 0.0134917730641\n",
      "Loss for  23601 th iteration => 0.0134910129041\n",
      "Loss for  23602 th iteration => 0.0134904775944\n",
      "Loss for  23603 th iteration => 0.013489722539\n",
      "Loss for  23604 th iteration => 0.0134891826146\n",
      "Loss for  23605 th iteration => 0.0134884367841\n",
      "Loss for  23606 th iteration => 0.0134878537139\n",
      "Loss for  23607 th iteration => 0.0134873177047\n",
      "Loss for  23608 th iteration => 0.0134865638692\n",
      "Loss for  23609 th iteration => 0.0134860259607\n",
      "Loss for  23610 th iteration => 0.0134852743492\n",
      "Loss for  23611 th iteration => 0.0134847346088\n",
      "Loss for  23612 th iteration => 0.0134839851503\n",
      "Loss for  23613 th iteration => 0.0134834436439\n",
      "Loss for  23614 th iteration => 0.0134826985682\n",
      "Loss for  23615 th iteration => 0.0134820912185\n",
      "Loss for  23616 th iteration => 0.013481544309\n",
      "Loss for  23617 th iteration => 0.0134807971557\n",
      "Loss for  23618 th iteration => 0.0134802566345\n",
      "Loss for  23619 th iteration => 0.0134795036134\n",
      "Loss for  23620 th iteration => 0.0134789692022\n",
      "Loss for  23621 th iteration => 0.0134782115476\n",
      "Loss for  23622 th iteration => 0.0134776129354\n",
      "Loss for  23623 th iteration => 0.0134769260332\n",
      "Loss for  23624 th iteration => 0.0134763255591\n",
      "Loss for  23625 th iteration => 0.0134757975866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  23626 th iteration => 0.013475037883\n",
      "Loss for  23627 th iteration => 0.0134745051296\n",
      "Loss for  23628 th iteration => 0.0134737504912\n",
      "Loss for  23629 th iteration => 0.0134732131598\n",
      "Loss for  23630 th iteration => 0.0134724643151\n",
      "Loss for  23631 th iteration => 0.0134718865629\n",
      "Loss for  23632 th iteration => 0.0134713525682\n",
      "Loss for  23633 th iteration => 0.0134705996946\n",
      "Loss for  23634 th iteration => 0.0134700638129\n",
      "Loss for  23635 th iteration => 0.0134693131494\n",
      "Loss for  23636 th iteration => 0.0134687754476\n",
      "Loss for  23637 th iteration => 0.0134680269237\n",
      "Loss for  23638 th iteration => 0.0134674874674\n",
      "Loss for  23639 th iteration => 0.013466741014\n",
      "Loss for  23640 th iteration => 0.0134662069864\n",
      "Loss for  23641 th iteration => 0.0134656014812\n",
      "Loss for  23642 th iteration => 0.0134648577004\n",
      "Loss for  23643 th iteration => 0.0134643167568\n",
      "Loss for  23644 th iteration => 0.0134635670244\n",
      "Loss for  23645 th iteration => 0.0134630322747\n",
      "Loss for  23646 th iteration => 0.013462276862\n",
      "Loss for  23647 th iteration => 0.0134617480325\n",
      "Loss for  23648 th iteration => 0.0134610001451\n",
      "Loss for  23649 th iteration => 0.0134603888812\n",
      "Loss for  23650 th iteration => 0.0134597176764\n",
      "Loss for  23651 th iteration => 0.0134591103358\n",
      "Loss for  23652 th iteration => 0.0134585812111\n",
      "Loss for  23653 th iteration => 0.0134578258765\n",
      "Loss for  23654 th iteration => 0.0134572920972\n",
      "Loss for  23655 th iteration => 0.0134565416987\n",
      "Loss for  23656 th iteration => 0.0134560034647\n",
      "Loss for  23657 th iteration => 0.0134552669539\n",
      "Loss for  23658 th iteration => 0.0134546809566\n",
      "Loss for  23659 th iteration => 0.0134541475267\n",
      "Loss for  23660 th iteration => 0.0134533973254\n",
      "Loss for  23661 th iteration => 0.0134528620513\n",
      "Loss for  23662 th iteration => 0.0134521140142\n",
      "Loss for  23663 th iteration => 0.0134515769617\n",
      "Loss for  23664 th iteration => 0.0134508310194\n",
      "Loss for  23665 th iteration => 0.013450292253\n",
      "Loss for  23666 th iteration => 0.0134495546327\n",
      "Loss for  23667 th iteration => 0.0134489463321\n",
      "Loss for  23668 th iteration => 0.0134484022567\n",
      "Loss for  23669 th iteration => 0.0134476586089\n",
      "Loss for  23670 th iteration => 0.0134471207265\n",
      "Loss for  23671 th iteration => 0.0134463713977\n",
      "Loss for  23672 th iteration => 0.0134458394355\n",
      "Loss for  23673 th iteration => 0.0134450885378\n",
      "Loss for  23674 th iteration => 0.0134444902286\n",
      "Loss for  23675 th iteration => 0.0134438090222\n",
      "Loss for  23676 th iteration => 0.0134432084808\n",
      "Loss for  23677 th iteration => 0.0134426834363\n",
      "Loss for  23678 th iteration => 0.0134419269701\n",
      "Loss for  23679 th iteration => 0.0134413972738\n",
      "Loss for  23680 th iteration => 0.01344064574\n",
      "Loss for  23681 th iteration => 0.0134401115912\n",
      "Loss for  23682 th iteration => 0.0134393672934\n",
      "Loss for  23683 th iteration => 0.0134387902857\n",
      "Loss for  23684 th iteration => 0.0134382593685\n",
      "Loss for  23685 th iteration => 0.0134375095971\n",
      "Loss for  23686 th iteration => 0.0134369768361\n",
      "Loss for  23687 th iteration => 0.0134362292274\n",
      "Loss for  23688 th iteration => 0.0134356946882\n",
      "Loss for  23689 th iteration => 0.0134349491731\n",
      "Loss for  23690 th iteration => 0.01343441292\n",
      "Loss for  23691 th iteration => 0.0134336694307\n",
      "Loss for  23692 th iteration => 0.0134331360524\n",
      "Loss for  23693 th iteration => 0.0134325350702\n",
      "Loss for  23694 th iteration => 0.0134317966098\n",
      "Loss for  23695 th iteration => 0.0134312564719\n",
      "Loss for  23696 th iteration => 0.0134305122067\n",
      "Loss for  23697 th iteration => 0.0134299781132\n",
      "Loss for  23698 th iteration => 0.01342922831\n",
      "Loss for  23699 th iteration => 0.0134286999919\n",
      "Loss for  23700 th iteration => 0.0134279568943\n",
      "Loss for  23701 th iteration => 0.0134273523971\n",
      "Loss for  23702 th iteration => 0.0134266804381\n",
      "Loss for  23703 th iteration => 0.0134260752555\n",
      "Loss for  23704 th iteration => 0.0134255503277\n",
      "Loss for  23705 th iteration => 0.0134247969344\n",
      "Loss for  23706 th iteration => 0.0134242674516\n",
      "Loss for  23707 th iteration => 0.0134235188915\n",
      "Loss for  23708 th iteration => 0.0134229850505\n",
      "Loss for  23709 th iteration => 0.013422249135\n",
      "Loss for  23710 th iteration => 0.0134216669269\n",
      "Loss for  23711 th iteration => 0.0134211369833\n",
      "Loss for  23712 th iteration => 0.0134203894417\n",
      "Loss for  23713 th iteration => 0.0134198576871\n",
      "Loss for  23714 th iteration => 0.0134191122728\n",
      "Loss for  23715 th iteration => 0.0134185787719\n",
      "Loss for  23716 th iteration => 0.0134178354166\n",
      "Loss for  23717 th iteration => 0.013417300233\n",
      "Loss for  23718 th iteration => 0.0134165626067\n",
      "Loss for  23719 th iteration => 0.0134159621503\n",
      "Loss for  23720 th iteration => 0.0134154175292\n",
      "Loss for  23721 th iteration => 0.0134146806496\n",
      "Loss for  23722 th iteration => 0.0134141421034\n",
      "Loss for  23723 th iteration => 0.0134133996544\n",
      "Loss for  23724 th iteration => 0.0134128669144\n",
      "Loss for  23725 th iteration => 0.0134121194472\n",
      "Loss for  23726 th iteration => 0.0134115324084\n",
      "Loss for  23727 th iteration => 0.0134108459488\n",
      "Loss for  23728 th iteration => 0.0134102490881\n",
      "Loss for  23729 th iteration => 0.0134097290894\n",
      "Loss for  23730 th iteration => 0.0134089736921\n",
      "Loss for  23731 th iteration => 0.0134084491209\n",
      "Loss for  23732 th iteration => 0.0134076985738\n",
      "Loss for  23733 th iteration => 0.0134071696267\n",
      "Loss for  23734 th iteration => 0.0134064237301\n",
      "Loss for  23735 th iteration => 0.0134058929886\n",
      "Loss for  23736 th iteration => 0.0134053340427\n",
      "Loss for  23737 th iteration => 0.0134045853882\n",
      "Loss for  23738 th iteration => 0.0134040575839\n",
      "Loss for  23739 th iteration => 0.0134033110951\n",
      "Loss for  23740 th iteration => 0.0134027815076\n",
      "Loss for  23741 th iteration => 0.0134020371156\n",
      "Loss for  23742 th iteration => 0.013401505809\n",
      "Loss for  23743 th iteration => 0.0134007634463\n",
      "Loss for  23744 th iteration => 0.0134002304833\n",
      "Loss for  23745 th iteration => 0.0133994993767\n",
      "Loss for  23746 th iteration => 0.0133988958134\n",
      "Loss for  23747 th iteration => 0.0133983525073\n",
      "Loss for  23748 th iteration => 0.0133976175734\n",
      "Loss for  23749 th iteration => 0.0133970802339\n",
      "Loss for  23750 th iteration => 0.0133963398344\n",
      "Loss for  23751 th iteration => 0.0133958081957\n",
      "Loss for  23752 th iteration => 0.0133950675584\n",
      "Loss for  23753 th iteration => 0.0133944793468\n",
      "Loss for  23754 th iteration => 0.0133937971267\n",
      "Loss for  23755 th iteration => 0.0133931973843\n",
      "Loss for  23756 th iteration => 0.013392678382\n",
      "Loss for  23757 th iteration => 0.0133919251517\n",
      "Loss for  23758 th iteration => 0.0133914016501\n",
      "Loss for  23759 th iteration => 0.0133906531948\n",
      "Loss for  23760 th iteration => 0.0133901253884\n",
      "Loss for  23761 th iteration => 0.0133893844423\n",
      "Loss for  23762 th iteration => 0.0133888108875\n",
      "Loss for  23763 th iteration => 0.0133881508893\n",
      "Loss for  23764 th iteration => 0.013387667238\n",
      "Loss for  23765 th iteration => 0.0133869046603\n",
      "Loss for  23766 th iteration => 0.0133863699262\n",
      "Loss for  23767 th iteration => 0.0133856577424\n",
      "Loss for  23768 th iteration => 0.0133850739766\n",
      "Loss for  23769 th iteration => 0.0133844101545\n",
      "Loss for  23770 th iteration => 0.0133837793619\n",
      "Loss for  23771 th iteration => 0.0133831619153\n",
      "Loss for  23772 th iteration => 0.0133825123436\n",
      "Loss for  23773 th iteration => 0.013381970108\n",
      "Loss for  23774 th iteration => 0.013381237176\n",
      "Loss for  23775 th iteration => 0.013380700847\n",
      "Loss for  23776 th iteration => 0.0133799625067\n",
      "Loss for  23777 th iteration => 0.0133794318202\n",
      "Loss for  23778 th iteration => 0.0133786973206\n",
      "Loss for  23779 th iteration => 0.0133780907061\n",
      "Loss for  23780 th iteration => 0.0133774298554\n",
      "Loss for  23781 th iteration => 0.0133768026778\n",
      "Loss for  23782 th iteration => 0.0133761642108\n",
      "Loss for  23783 th iteration => 0.0133756696071\n",
      "Loss for  23784 th iteration => 0.0133749151784\n",
      "Loss for  23785 th iteration => 0.0133743767568\n",
      "Loss for  23786 th iteration => 0.013373665519\n",
      "Loss for  23787 th iteration => 0.0133730913868\n",
      "Loss for  23788 th iteration => 0.0133723763635\n",
      "Loss for  23789 th iteration => 0.0133719695327\n",
      "Loss for  23790 th iteration => 0.0133711334709\n",
      "Loss for  23791 th iteration => 0.0133706745914\n",
      "Loss for  23792 th iteration => 0.0133698898822\n",
      "Loss for  23793 th iteration => 0.0133693810214\n",
      "Loss for  23794 th iteration => 0.0133686456163\n",
      "Loss for  23795 th iteration => 0.0133680887952\n",
      "Loss for  23796 th iteration => 0.013367400692\n",
      "Loss for  23797 th iteration => 0.0133668032644\n",
      "Loss for  23798 th iteration => 0.0133662810216\n",
      "Loss for  23799 th iteration => 0.0133655330042\n",
      "Loss for  23800 th iteration => 0.013364985192\n",
      "Loss for  23801 th iteration => 0.0133642609998\n",
      "Loss for  23802 th iteration => 0.0133637190699\n",
      "Loss for  23803 th iteration => 0.0133629894909\n",
      "Loss for  23804 th iteration => 0.0133624531808\n",
      "Loss for  23805 th iteration => 0.0133617254598\n",
      "Loss for  23806 th iteration => 0.0133611253563\n",
      "Loss for  23807 th iteration => 0.0133604610961\n",
      "Loss for  23808 th iteration => 0.0133598405793\n",
      "Loss for  23809 th iteration => 0.0133591966277\n",
      "Loss for  23810 th iteration => 0.0133585789762\n",
      "Loss for  23811 th iteration => 0.0133580654485\n",
      "Loss for  23812 th iteration => 0.0133573131979\n",
      "Loss for  23813 th iteration => 0.0133567954889\n",
      "Loss for  23814 th iteration => 0.0133560545956\n",
      "Loss for  23815 th iteration => 0.0133554822239\n",
      "Loss for  23816 th iteration => 0.0133549607496\n",
      "Loss for  23817 th iteration => 0.0133542170198\n",
      "Loss for  23818 th iteration => 0.0133536937978\n",
      "Loss for  23819 th iteration => 0.0133529521245\n",
      "Loss for  23820 th iteration => 0.0133524272175\n",
      "Loss for  23821 th iteration => 0.0133516875347\n",
      "Loss for  23822 th iteration => 0.0133511610042\n",
      "Loss for  23823 th iteration => 0.0133504232473\n",
      "Loss for  23824 th iteration => 0.0133498951536\n",
      "Loss for  23825 th iteration => 0.0133491943922\n",
      "Loss for  23826 th iteration => 0.0133486822662\n",
      "Loss for  23827 th iteration => 0.0133479519027\n",
      "Loss for  23828 th iteration => 0.0133473934194\n",
      "Loss for  23829 th iteration => 0.0133467087628\n",
      "Loss for  23830 th iteration => 0.0133461059331\n",
      "Loss for  23831 th iteration => 0.0133454649901\n",
      "Loss for  23832 th iteration => 0.0133448227889\n",
      "Loss for  23833 th iteration => 0.0133441688321\n",
      "Loss for  23834 th iteration => 0.0133435410507\n",
      "Loss for  23835 th iteration => 0.0133429119372\n",
      "Loss for  23836 th iteration => 0.0133424135006\n",
      "Loss for  23837 th iteration => 0.0133416688232\n",
      "Loss for  23838 th iteration => 0.0133411270088\n",
      "Loss for  23839 th iteration => 0.0133404250877\n",
      "Loss for  23840 th iteration => 0.0133398485225\n",
      "Loss for  23841 th iteration => 0.0133391257537\n",
      "Loss for  23842 th iteration => 0.013338566963\n",
      "Loss for  23843 th iteration => 0.0133378919847\n",
      "Loss for  23844 th iteration => 0.0133374358783\n",
      "Loss for  23845 th iteration => 0.013336653854\n",
      "Loss for  23846 th iteration => 0.0133361491961\n",
      "Loss for  23847 th iteration => 0.0133354150615\n",
      "Loss for  23848 th iteration => 0.0133348638333\n",
      "Loss for  23849 th iteration => 0.0133341756256\n",
      "Loss for  23850 th iteration => 0.0133335857148\n",
      "Loss for  23851 th iteration => 0.0133330621792\n",
      "Loss for  23852 th iteration => 0.0133323196701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  23853 th iteration => 0.0133317723977\n",
      "Loss for  23854 th iteration => 0.0133310539237\n",
      "Loss for  23855 th iteration => 0.0133305123824\n",
      "Loss for  23856 th iteration => 0.0133297886657\n",
      "Loss for  23857 th iteration => 0.0133292525977\n",
      "Loss for  23858 th iteration => 0.0133285272941\n",
      "Loss for  23859 th iteration => 0.0133279326161\n",
      "Loss for  23860 th iteration => 0.0133272689257\n",
      "Loss for  23861 th iteration => 0.0133266543377\n",
      "Loss for  23862 th iteration => 0.0133260104539\n",
      "Loss for  23863 th iteration => 0.0133253975416\n",
      "Loss for  23864 th iteration => 0.0133248881358\n",
      "Loss for  23865 th iteration => 0.0133241378812\n",
      "Loss for  23866 th iteration => 0.0133236243926\n",
      "Loss for  23867 th iteration => 0.0133228824551\n",
      "Loss for  23868 th iteration => 0.0133223174352\n",
      "Loss for  23869 th iteration => 0.0133216599008\n",
      "Loss for  23870 th iteration => 0.0133211909509\n",
      "Loss for  23871 th iteration => 0.0133204246351\n",
      "Loss for  23872 th iteration => 0.0133199073642\n",
      "Loss for  23873 th iteration => 0.0133191887102\n",
      "Loss for  23874 th iteration => 0.0133186250918\n",
      "Loss for  23875 th iteration => 0.0133179521444\n",
      "Loss for  23876 th iteration => 0.0133173441077\n",
      "Loss for  23877 th iteration => 0.0133167149559\n",
      "Loss for  23878 th iteration => 0.0133160866622\n",
      "Loss for  23879 th iteration => 0.0133155372731\n",
      "Loss for  23880 th iteration => 0.013314823991\n",
      "Loss for  23881 th iteration => 0.0133142802382\n",
      "Loss for  23882 th iteration => 0.0133135618041\n",
      "Loss for  23883 th iteration => 0.0133130234325\n",
      "Loss for  23884 th iteration => 0.013312300094\n",
      "Loss for  23885 th iteration => 0.0133117668542\n",
      "Loss for  23886 th iteration => 0.0133110544302\n",
      "Loss for  23887 th iteration => 0.013310436752\n",
      "Loss for  23888 th iteration => 0.0133097989768\n",
      "Loss for  23889 th iteration => 0.0133091880712\n",
      "Loss for  23890 th iteration => 0.0133086832771\n",
      "Loss for  23891 th iteration => 0.0133079313752\n",
      "Loss for  23892 th iteration => 0.0133074224642\n",
      "Loss for  23893 th iteration => 0.0133066776103\n",
      "Loss for  23894 th iteration => 0.0133061121275\n",
      "Loss for  23895 th iteration => 0.0133054260608\n",
      "Loss for  23896 th iteration => 0.0133048493331\n",
      "Loss for  23897 th iteration => 0.0133043326638\n",
      "Loss for  23898 th iteration => 0.0133035935568\n",
      "Loss for  23899 th iteration => 0.0133030752506\n",
      "Loss for  23900 th iteration => 0.0133023380807\n",
      "Loss for  23901 th iteration => 0.0133018181976\n",
      "Loss for  23902 th iteration => 0.0133010829016\n",
      "Loss for  23903 th iteration => 0.0133005636017\n",
      "Loss for  23904 th iteration => 0.0132999614836\n",
      "Loss for  23905 th iteration => 0.0132992607948\n",
      "Loss for  23906 th iteration => 0.0132987073175\n",
      "Loss for  23907 th iteration => 0.0132980013102\n",
      "Loss for  23908 th iteration => 0.0132974533811\n",
      "Loss for  23909 th iteration => 0.0132967423055\n",
      "Loss for  23910 th iteration => 0.0132961996723\n",
      "Loss for  23911 th iteration => 0.0132954837732\n",
      "Loss for  23912 th iteration => 0.0132949461894\n",
      "Loss for  23913 th iteration => 0.0132942337044\n",
      "Loss for  23914 th iteration => 0.0132936382282\n",
      "Loss for  23915 th iteration => 0.0132931434839\n",
      "Loss for  23916 th iteration => 0.0132923843498\n",
      "Loss for  23917 th iteration => 0.013291885283\n",
      "Loss for  23918 th iteration => 0.0132911307392\n",
      "Loss for  23919 th iteration => 0.0132906275372\n",
      "Loss for  23920 th iteration => 0.0132898773936\n",
      "Loss for  23921 th iteration => 0.0132893702398\n",
      "Loss for  23922 th iteration => 0.0132886343701\n",
      "Loss for  23923 th iteration => 0.0132880698426\n",
      "Loss for  23924 th iteration => 0.0132874137831\n",
      "Loss for  23925 th iteration => 0.0132869452532\n",
      "Loss for  23926 th iteration => 0.0132861840696\n",
      "Loss for  23927 th iteration => 0.013285668807\n",
      "Loss for  23928 th iteration => 0.013284953714\n",
      "Loss for  23929 th iteration => 0.0132843936482\n",
      "Loss for  23930 th iteration => 0.0132837227342\n",
      "Loss for  23931 th iteration => 0.0132831197516\n",
      "Loss for  23932 th iteration => 0.0132824911478\n",
      "Loss for  23933 th iteration => 0.0132818710085\n",
      "Loss for  23934 th iteration => 0.0132813186033\n",
      "Loss for  23935 th iteration => 0.013280614771\n",
      "Loss for  23936 th iteration => 0.0132800678454\n",
      "Loss for  23937 th iteration => 0.01327935901\n",
      "Loss for  23938 th iteration => 0.013278817314\n",
      "Loss for  23939 th iteration => 0.0132781054679\n",
      "Loss for  23940 th iteration => 0.013277507379\n",
      "Loss for  23941 th iteration => 0.0132768561362\n",
      "Loss for  23942 th iteration => 0.0132762390437\n",
      "Loss for  23943 th iteration => 0.0132756067031\n",
      "Loss for  23944 th iteration => 0.0132749920077\n",
      "Loss for  23945 th iteration => 0.0132744936432\n",
      "Loss for  23946 th iteration => 0.0132737416244\n",
      "Loss for  23947 th iteration => 0.0132732393383\n",
      "Loss for  23948 th iteration => 0.0132724971359\n",
      "Loss for  23949 th iteration => 0.0132719393032\n",
      "Loss for  23950 th iteration => 0.0132712801792\n",
      "Loss for  23951 th iteration => 0.013270825626\n",
      "Loss for  23952 th iteration => 0.0132700532178\n",
      "Loss for  23953 th iteration => 0.013269552357\n",
      "Loss for  23954 th iteration => 0.0132688256192\n",
      "Loss for  23955 th iteration => 0.0132682803674\n",
      "Loss for  23956 th iteration => 0.0132675974012\n",
      "Loss for  23957 th iteration => 0.0132670096319\n",
      "Loss for  23958 th iteration => 0.0132663685812\n",
      "Loss for  23959 th iteration => 0.0132657610796\n",
      "Loss for  23960 th iteration => 0.0132652517279\n",
      "Loss for  23961 th iteration => 0.0132645168569\n",
      "Loss for  23962 th iteration => 0.0132639635713\n",
      "Loss for  23963 th iteration => 0.0132632639454\n",
      "Loss for  23964 th iteration => 0.0132627159882\n",
      "Loss for  23965 th iteration => 0.0132620115045\n",
      "Loss for  23966 th iteration => 0.0132614686297\n",
      "Loss for  23967 th iteration => 0.0132607672335\n",
      "Loss for  23968 th iteration => 0.013260155934\n",
      "Loss for  23969 th iteration => 0.0132595209412\n",
      "Loss for  23970 th iteration => 0.0132588911623\n",
      "Loss for  23971 th iteration => 0.0132582745484\n",
      "Loss for  23972 th iteration => 0.013257653269\n",
      "Loss for  23973 th iteration => 0.0132571586268\n",
      "Loss for  23974 th iteration => 0.0132564060811\n",
      "Loss for  23975 th iteration => 0.0132558603209\n",
      "Loss for  23976 th iteration => 0.0132551635245\n",
      "Loss for  23977 th iteration => 0.0132545970563\n",
      "Loss for  23978 th iteration => 0.0132540878778\n",
      "Loss for  23979 th iteration => 0.0132533505235\n",
      "Loss for  23980 th iteration => 0.0132528397454\n",
      "Loss for  23981 th iteration => 0.0132521042858\n",
      "Loss for  23982 th iteration => 0.0132515919667\n",
      "Loss for  23983 th iteration => 0.0132508583403\n",
      "Loss for  23984 th iteration => 0.0132503445376\n",
      "Loss for  23985 th iteration => 0.0132496355655\n",
      "Loss for  23986 th iteration => 0.0132492170045\n",
      "Loss for  23987 th iteration => 0.0132484094227\n",
      "Loss for  23988 th iteration => 0.0132479111675\n",
      "Loss for  23989 th iteration => 0.0132471837272\n",
      "Loss for  23990 th iteration => 0.0132466428811\n",
      "Loss for  23991 th iteration => 0.0132459574207\n",
      "Loss for  23992 th iteration => 0.0132453758869\n",
      "Loss for  23993 th iteration => 0.0132447305198\n",
      "Loss for  23994 th iteration => 0.0132441101594\n",
      "Loss for  23995 th iteration => 0.013243503041\n",
      "Loss for  23996 th iteration => 0.0132428531743\n",
      "Loss for  23997 th iteration => 0.0132422267516\n",
      "Loss for  23998 th iteration => 0.0132416176281\n",
      "Loss for  23999 th iteration => 0.0132411217131\n",
      "Loss for  24000 th iteration => 0.0132403733188\n",
      "Loss for  24001 th iteration => 0.0132398734335\n",
      "Loss for  24002 th iteration => 0.0132391334718\n",
      "Loss for  24003 th iteration => 0.0132385743548\n",
      "Loss for  24004 th iteration => 0.0132378940372\n",
      "Loss for  24005 th iteration => 0.0132373129152\n",
      "Loss for  24006 th iteration => 0.0132366543799\n",
      "Loss for  24007 th iteration => 0.0132360669989\n",
      "Loss for  24008 th iteration => 0.0132355595734\n",
      "Loss for  24009 th iteration => 0.013234823953\n",
      "Loss for  24010 th iteration => 0.0132343150137\n",
      "Loss for  24011 th iteration => 0.0132335811968\n",
      "Loss for  24012 th iteration => 0.0132330716162\n",
      "Loss for  24013 th iteration => 0.0132324716501\n",
      "Loss for  24014 th iteration => 0.0132318406983\n",
      "Loss for  24015 th iteration => 0.0132312365851\n",
      "Loss for  24016 th iteration => 0.0132305484169\n",
      "Loss for  24017 th iteration => 0.0132299949866\n",
      "Loss for  24018 th iteration => 0.0132293018781\n",
      "Loss for  24019 th iteration => 0.0132287536118\n",
      "Loss for  24020 th iteration => 0.0132280558022\n",
      "Loss for  24021 th iteration => 0.0132275124591\n",
      "Loss for  24022 th iteration => 0.0132268101825\n",
      "Loss for  24023 th iteration => 0.0132262715269\n",
      "Loss for  24024 th iteration => 0.0132255790675\n",
      "Loss for  24025 th iteration => 0.0132249785501\n",
      "Loss for  24026 th iteration => 0.0132244922576\n",
      "Loss for  24027 th iteration => 0.013223737275\n",
      "Loss for  24028 th iteration => 0.0132232469204\n",
      "Loss for  24029 th iteration => 0.0132224962606\n",
      "Loss for  24030 th iteration => 0.0132220020236\n",
      "Loss for  24031 th iteration => 0.0132212599325\n",
      "Loss for  24032 th iteration => 0.0132207210599\n",
      "Loss for  24033 th iteration => 0.013220023602\n",
      "Loss for  24034 th iteration => 0.0132194632715\n",
      "Loss for  24035 th iteration => 0.0132188077231\n",
      "Loss for  24036 th iteration => 0.0132183476119\n",
      "Loss for  24037 th iteration => 0.013217588844\n",
      "Loss for  24038 th iteration => 0.0132170857154\n",
      "Loss for  24039 th iteration => 0.0132163693617\n",
      "Loss for  24040 th iteration => 0.0132158250458\n",
      "Loss for  24041 th iteration => 0.0132151492935\n",
      "Loss for  24042 th iteration => 0.0132145748676\n",
      "Loss for  24043 th iteration => 0.0132140434614\n",
      "Loss for  24044 th iteration => 0.0132133358737\n",
      "Loss for  24045 th iteration => 0.01321277946\n",
      "Loss for  24046 th iteration => 0.0132120924551\n",
      "Loss for  24047 th iteration => 0.0132115412232\n",
      "Loss for  24048 th iteration => 0.0132108494985\n",
      "Loss for  24049 th iteration => 0.0132103032079\n",
      "Loss for  24050 th iteration => 0.0132096083193\n",
      "Loss for  24051 th iteration => 0.0132090180154\n",
      "Loss for  24052 th iteration => 0.01320837126\n",
      "Loss for  24053 th iteration => 0.013207763017\n",
      "Loss for  24054 th iteration => 0.0132071341017\n",
      "Loss for  24055 th iteration => 0.0132065183768\n",
      "Loss for  24056 th iteration => 0.0132060320961\n",
      "Loss for  24057 th iteration => 0.0132052805356\n",
      "Loss for  24058 th iteration => 0.0132047905266\n",
      "Loss for  24059 th iteration => 0.0132040500915\n",
      "Loss for  24060 th iteration => 0.0132035107752\n",
      "Loss for  24061 th iteration => 0.0132028300683\n",
      "Loss for  24062 th iteration => 0.0132024120589\n",
      "Loss for  24063 th iteration => 0.0132016145424\n",
      "Loss for  24064 th iteration => 0.0132011525142\n",
      "Loss for  24065 th iteration => 0.013200398405\n",
      "Loss for  24066 th iteration => 0.0131998942068\n",
      "Loss for  24067 th iteration => 0.0131991816734\n",
      "Loss for  24068 th iteration => 0.0131986371126\n",
      "Loss for  24069 th iteration => 0.0131979643646\n",
      "Loss for  24070 th iteration => 0.0131973924922\n",
      "Loss for  24071 th iteration => 0.0131968584056\n",
      "Loss for  24072 th iteration => 0.0131961576425\n",
      "Loss for  24073 th iteration => 0.0131955975736\n",
      "Loss for  24074 th iteration => 0.0131949174331\n",
      "Loss for  24075 th iteration => 0.0131943624687\n",
      "Loss for  24076 th iteration => 0.0131936776817\n",
      "Loss for  24077 th iteration => 0.013193127584\n",
      "Loss for  24078 th iteration => 0.0131924394063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  24079 th iteration => 0.0131918471043\n",
      "Loss for  24080 th iteration => 0.0131912054019\n",
      "Loss for  24081 th iteration => 0.0131905954636\n",
      "Loss for  24082 th iteration => 0.0131899712991\n",
      "Loss for  24083 th iteration => 0.0131893530485\n",
      "Loss for  24084 th iteration => 0.0131888720116\n",
      "Loss for  24085 th iteration => 0.0131881183451\n",
      "Loss for  24086 th iteration => 0.0131876336289\n",
      "Loss for  24087 th iteration => 0.0131868926093\n",
      "Loss for  24088 th iteration => 0.0131863563633\n",
      "Loss for  24089 th iteration => 0.013185673172\n",
      "Loss for  24090 th iteration => 0.0131852617042\n",
      "Loss for  24091 th iteration => 0.0131844604638\n",
      "Loss for  24092 th iteration => 0.0131840056359\n",
      "Loss for  24093 th iteration => 0.0131832471514\n",
      "Loss for  24094 th iteration => 0.0131827507934\n",
      "Loss for  24095 th iteration => 0.0131820332518\n",
      "Loss for  24096 th iteration => 0.0131814971529\n",
      "Loss for  24097 th iteration => 0.0131808187819\n",
      "Loss for  24098 th iteration => 0.0131802560627\n",
      "Loss for  24099 th iteration => 0.01317971439\n",
      "Loss for  24100 th iteration => 0.0131790234536\n",
      "Loss for  24101 th iteration => 0.0131784589449\n",
      "Loss for  24102 th iteration => 0.0131777864219\n",
      "Loss for  24103 th iteration => 0.0131772269617\n",
      "Loss for  24104 th iteration => 0.0131765498449\n",
      "Loss for  24105 th iteration => 0.0131759951973\n",
      "Loss for  24106 th iteration => 0.0131753137161\n",
      "Loss for  24107 th iteration => 0.0131747636504\n",
      "Loss for  24108 th iteration => 0.0131740914021\n",
      "Loss for  24109 th iteration => 0.013173484404\n",
      "Loss for  24110 th iteration => 0.0131728604805\n",
      "Loss for  24111 th iteration => 0.0131722408994\n",
      "Loss for  24112 th iteration => 0.0131717665605\n",
      "Loss for  24113 th iteration => 0.0131710093\n",
      "Loss for  24114 th iteration => 0.013170531224\n",
      "Loss for  24115 th iteration => 0.0131697869039\n",
      "Loss for  24116 th iteration => 0.0131692579229\n",
      "Loss for  24117 th iteration => 0.0131685596509\n",
      "Loss for  24118 th iteration => 0.0131680101968\n",
      "Loss for  24119 th iteration => 0.0131673448411\n",
      "Loss for  24120 th iteration => 0.0131669073352\n",
      "Loss for  24121 th iteration => 0.0131661344467\n",
      "Loss for  24122 th iteration => 0.0131656558222\n",
      "Loss for  24123 th iteration => 0.0131649234703\n",
      "Loss for  24124 th iteration => 0.0131644055027\n",
      "Loss for  24125 th iteration => 0.0131637119283\n",
      "Loss for  24126 th iteration => 0.013163165733\n",
      "Loss for  24127 th iteration => 0.0131626109701\n",
      "Loss for  24128 th iteration => 0.013161933148\n",
      "Loss for  24129 th iteration => 0.0131613634578\n",
      "Loss for  24130 th iteration => 0.0131606992608\n",
      "Loss for  24131 th iteration => 0.0131601345859\n",
      "Loss for  24132 th iteration => 0.0131594658257\n",
      "Loss for  24133 th iteration => 0.0131589059318\n",
      "Loss for  24134 th iteration => 0.013158232836\n",
      "Loss for  24135 th iteration => 0.013157677494\n",
      "Loss for  24136 th iteration => 0.0131570095119\n",
      "Loss for  24137 th iteration => 0.0131564075054\n",
      "Loss for  24138 th iteration => 0.0131557816604\n",
      "Loss for  24139 th iteration => 0.0131551631579\n",
      "Loss for  24140 th iteration => 0.0131545820203\n",
      "Loss for  24141 th iteration => 0.0131540425985\n",
      "Loss for  24142 th iteration => 0.0131533696211\n",
      "Loss for  24143 th iteration => 0.0131528013692\n",
      "Loss for  24144 th iteration => 0.0131521207775\n",
      "Loss for  24145 th iteration => 0.0131515564716\n",
      "Loss for  24146 th iteration => 0.0131508963669\n",
      "Loss for  24147 th iteration => 0.0131503170523\n",
      "Loss for  24148 th iteration => 0.0131498184368\n",
      "Loss for  24149 th iteration => 0.0131490896695\n",
      "Loss for  24150 th iteration => 0.0131485896131\n",
      "Loss for  24151 th iteration => 0.0131478625682\n",
      "Loss for  24152 th iteration => 0.0131473611253\n",
      "Loss for  24153 th iteration => 0.0131466357459\n",
      "Loss for  24154 th iteration => 0.0131461384858\n",
      "Loss for  24155 th iteration => 0.0131455304788\n",
      "Loss for  24156 th iteration => 0.0131448734602\n",
      "Loss for  24157 th iteration => 0.0131443044839\n",
      "Loss for  24158 th iteration => 0.0131436428203\n",
      "Loss for  24159 th iteration => 0.0131430787067\n",
      "Loss for  24160 th iteration => 0.0131424126268\n",
      "Loss for  24161 th iteration => 0.0131418531456\n",
      "Loss for  24162 th iteration => 0.0131411828731\n",
      "Loss for  24163 th iteration => 0.0131406277992\n",
      "Loss for  24164 th iteration => 0.0131399584918\n",
      "Loss for  24165 th iteration => 0.0131393674035\n",
      "Loss for  24166 th iteration => 0.0131387486594\n",
      "Loss for  24167 th iteration => 0.0131382679674\n",
      "Loss for  24168 th iteration => 0.0131375398094\n",
      "Loss for  24169 th iteration => 0.0131370225505\n",
      "Loss for  24170 th iteration => 0.0131363304089\n",
      "Loss for  24171 th iteration => 0.0131357809897\n",
      "Loss for  24172 th iteration => 0.0131350899045\n",
      "Loss for  24173 th iteration => 0.0131345392849\n",
      "Loss for  24174 th iteration => 0.0131338685524\n",
      "Loss for  24175 th iteration => 0.0131332987039\n",
      "Loss for  24176 th iteration => 0.0131326767338\n",
      "Loss for  24177 th iteration => 0.0131321910818\n",
      "Loss for  24178 th iteration => 0.0131314714381\n",
      "Loss for  24179 th iteration => 0.0131309472526\n",
      "Loss for  24180 th iteration => 0.0131302655858\n",
      "Loss for  24181 th iteration => 0.0131297045789\n",
      "Loss for  24182 th iteration => 0.013129059193\n",
      "Loss for  24183 th iteration => 0.013128485092\n",
      "Loss for  24184 th iteration => 0.0131279545648\n",
      "Loss for  24185 th iteration => 0.0131272656043\n",
      "Loss for  24186 th iteration => 0.0131267017479\n",
      "Loss for  24187 th iteration => 0.0131260382567\n",
      "Loss for  24188 th iteration => 0.0131254791439\n",
      "Loss for  24189 th iteration => 0.0131248113513\n",
      "Loss for  24190 th iteration => 0.0131242567547\n",
      "Loss for  24191 th iteration => 0.0131235871004\n",
      "Loss for  24192 th iteration => 0.0131229862003\n",
      "Loss for  24193 th iteration => 0.0131223652908\n",
      "Loss for  24194 th iteration => 0.0131217480239\n",
      "Loss for  24195 th iteration => 0.0131211433858\n",
      "Loss for  24196 th iteration => 0.0131205189209\n",
      "Loss for  24197 th iteration => 0.0131200564098\n",
      "Loss for  24198 th iteration => 0.0131192986877\n",
      "Loss for  24199 th iteration => 0.0131187903636\n",
      "Loss for  24200 th iteration => 0.0131180803804\n",
      "Loss for  24201 th iteration => 0.0131175523139\n",
      "Loss for  24202 th iteration => 0.0131168727683\n",
      "Loss for  24203 th iteration => 0.0131164711658\n",
      "Loss for  24204 th iteration => 0.0131156708498\n",
      "Loss for  24205 th iteration => 0.0131152295647\n",
      "Loss for  24206 th iteration => 0.0131144683655\n",
      "Loss for  24207 th iteration => 0.0131139891305\n",
      "Loss for  24208 th iteration => 0.0131132653316\n",
      "Loss for  24209 th iteration => 0.0131127601554\n",
      "Loss for  24210 th iteration => 0.0131121801519\n",
      "Loss for  24211 th iteration => 0.0131115324732\n",
      "Loss for  24212 th iteration => 0.0131109743563\n",
      "Loss for  24213 th iteration => 0.0131103077177\n",
      "Loss for  24214 th iteration => 0.0131097357276\n",
      "Loss for  24215 th iteration => 0.013109083426\n",
      "Loss for  24216 th iteration => 0.0131085162054\n",
      "Loss for  24217 th iteration => 0.0131078595753\n",
      "Loss for  24218 th iteration => 0.0131072968973\n",
      "Loss for  24219 th iteration => 0.0131066361592\n",
      "Loss for  24220 th iteration => 0.0131060778019\n",
      "Loss for  24221 th iteration => 0.0131054202609\n",
      "Loss for  24222 th iteration => 0.0131048173545\n",
      "Loss for  24223 th iteration => 0.0131042015586\n",
      "Loss for  24224 th iteration => 0.0131035829954\n",
      "Loss for  24225 th iteration => 0.0131030080877\n",
      "Loss for  24226 th iteration => 0.0131024760405\n",
      "Loss for  24227 th iteration => 0.0131018042442\n",
      "Loss for  24228 th iteration => 0.013101245745\n",
      "Loss for  24229 th iteration => 0.0131005617995\n",
      "Loss for  24230 th iteration => 0.0131000108626\n",
      "Loss for  24231 th iteration => 0.0130993464515\n",
      "Loss for  24232 th iteration => 0.0130987789423\n",
      "Loss for  24233 th iteration => 0.0130982807552\n",
      "Loss for  24234 th iteration => 0.0130975609325\n",
      "Loss for  24235 th iteration => 0.0130970613751\n",
      "Loss for  24236 th iteration => 0.0130963431988\n",
      "Loss for  24237 th iteration => 0.0130958423238\n",
      "Loss for  24238 th iteration => 0.013095125739\n",
      "Loss for  24239 th iteration => 0.01309462963\n",
      "Loss for  24240 th iteration => 0.0130940241293\n",
      "Loss for  24241 th iteration => 0.0130934227295\n",
      "Loss for  24242 th iteration => 0.0130928159541\n",
      "Loss for  24243 th iteration => 0.0130921681439\n",
      "Loss for  24244 th iteration => 0.0130915994932\n",
      "Loss for  24245 th iteration => 0.0130909473238\n",
      "Loss for  24246 th iteration => 0.0130903832459\n",
      "Loss for  24247 th iteration => 0.0130897269377\n",
      "Loss for  24248 th iteration => 0.0130891672108\n",
      "Loss for  24249 th iteration => 0.0130885069796\n",
      "Loss for  24250 th iteration => 0.0130879513865\n",
      "Loss for  24251 th iteration => 0.0130872986227\n",
      "Loss for  24252 th iteration => 0.0130867020546\n",
      "Loss for  24253 th iteration => 0.0130860937179\n",
      "Loss for  24254 th iteration => 0.0130856087344\n",
      "Loss for  24255 th iteration => 0.0130848934649\n",
      "Loss for  24256 th iteration => 0.0130843740177\n",
      "Loss for  24257 th iteration => 0.0130836926826\n",
      "Loss for  24258 th iteration => 0.0130831496235\n",
      "Loss for  24259 th iteration => 0.0130824603115\n",
      "Loss for  24260 th iteration => 0.0130819182202\n",
      "Loss for  24261 th iteration => 0.0130812481562\n",
      "Loss for  24262 th iteration => 0.0130806879126\n",
      "Loss for  24263 th iteration => 0.0130800605758\n",
      "Loss for  24264 th iteration => 0.0130795862243\n",
      "Loss for  24265 th iteration => 0.0130788638213\n",
      "Loss for  24266 th iteration => 0.0130783530381\n",
      "Loss for  24267 th iteration => 0.0130776665331\n",
      "Loss for  24268 th iteration => 0.0130771209718\n",
      "Loss for  24269 th iteration => 0.0130764687267\n",
      "Loss for  24270 th iteration => 0.0130759101201\n",
      "Loss for  24271 th iteration => 0.0130753675635\n",
      "Loss for  24272 th iteration => 0.0130746997445\n",
      "Loss for  24273 th iteration => 0.0130741329764\n",
      "Loss for  24274 th iteration => 0.0130734821112\n",
      "Loss for  24275 th iteration => 0.0130729198721\n",
      "Loss for  24276 th iteration => 0.0130722649097\n",
      "Loss for  24277 th iteration => 0.0130717069791\n",
      "Loss for  24278 th iteration => 0.0130710506171\n",
      "Loss for  24279 th iteration => 0.013070454638\n",
      "Loss for  24280 th iteration => 0.013069838128\n",
      "Loss for  24281 th iteration => 0.0130692265552\n",
      "Loss for  24282 th iteration => 0.0130686255456\n",
      "Loss for  24283 th iteration => 0.0130679995072\n",
      "Loss for  24284 th iteration => 0.0130674380459\n",
      "Loss for  24285 th iteration => 0.0130668965182\n",
      "Loss for  24286 th iteration => 0.0130662032737\n",
      "Loss for  24287 th iteration => 0.0130656681903\n",
      "Loss for  24288 th iteration => 0.0130649939953\n",
      "Loss for  24289 th iteration => 0.0130644409527\n",
      "Loss for  24290 th iteration => 0.0130638134948\n",
      "Loss for  24291 th iteration => 0.013063352251\n",
      "Loss for  24292 th iteration => 0.0130626194736\n",
      "Loss for  24293 th iteration => 0.013062122209\n",
      "Loss for  24294 th iteration => 0.0130614249232\n",
      "Loss for  24295 th iteration => 0.0130608932792\n",
      "Loss for  24296 th iteration => 0.0130602330671\n",
      "Loss for  24297 th iteration => 0.013059787377\n",
      "Loss for  24298 th iteration => 0.0130590501667\n",
      "Loss for  24299 th iteration => 0.0130585563543\n",
      "Loss for  24300 th iteration => 0.0130578669051\n",
      "Loss for  24301 th iteration => 0.0130573376758\n",
      "Loss for  24302 th iteration => 0.0130566398873\n",
      "Loss for  24303 th iteration => 0.0130561077593\n",
      "Loss for  24304 th iteration => 0.0130554451523\n",
      "Loss for  24305 th iteration => 0.0130548790119\n",
      "Loss for  24306 th iteration => 0.0130542498931\n",
      "Loss for  24307 th iteration => 0.0130536544785\n",
      "Loss for  24308 th iteration => 0.0130530065501\n",
      "Loss for  24309 th iteration => 0.0130524291807\n",
      "Loss for  24310 th iteration => 0.0130517971263\n",
      "Loss for  24311 th iteration => 0.0130512049217\n",
      "Loss for  24312 th iteration => 0.0130505916733\n",
      "Loss for  24313 th iteration => 0.0130501182365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  24314 th iteration => 0.0130493965785\n",
      "Loss for  24315 th iteration => 0.0130488915142\n",
      "Loss for  24316 th iteration => 0.0130482009814\n",
      "Loss for  24317 th iteration => 0.0130476750997\n",
      "Loss for  24318 th iteration => 0.0130469838645\n",
      "Loss for  24319 th iteration => 0.0130466079144\n",
      "Loss for  24320 th iteration => 0.0130457940982\n",
      "Loss for  24321 th iteration => 0.0130453794217\n",
      "Loss for  24322 th iteration => 0.0130446037789\n",
      "Loss for  24323 th iteration => 0.0130441520724\n",
      "Loss for  24324 th iteration => 0.0130434129223\n",
      "Loss for  24325 th iteration => 0.0130429258448\n",
      "Loss for  24326 th iteration => 0.013042221544\n",
      "Loss for  24327 th iteration => 0.0130417007174\n",
      "Loss for  24328 th iteration => 0.0130410296594\n",
      "Loss for  24329 th iteration => 0.0130404913971\n",
      "Loss for  24330 th iteration => 0.0130399336605\n",
      "Loss for  24331 th iteration => 0.0130392855899\n",
      "Loss for  24332 th iteration => 0.0130387143966\n",
      "Loss for  24333 th iteration => 0.0130380744465\n",
      "Loss for  24334 th iteration => 0.0130375076775\n",
      "Loss for  24335 th iteration => 0.0130368637289\n",
      "Loss for  24336 th iteration => 0.0130363011673\n",
      "Loss for  24337 th iteration => 0.0130356534311\n",
      "Loss for  24338 th iteration => 0.0130350948651\n",
      "Loss for  24339 th iteration => 0.0130344558611\n",
      "Loss for  24340 th iteration => 0.0130338557378\n",
      "Loss for  24341 th iteration => 0.0130332496899\n",
      "Loss for  24342 th iteration => 0.0130326350647\n",
      "Loss for  24343 th iteration => 0.0130320572701\n",
      "Loss for  24344 th iteration => 0.0130315406353\n",
      "Loss for  24345 th iteration => 0.0130308360274\n",
      "Loss for  24346 th iteration => 0.0130303186766\n",
      "Loss for  24347 th iteration => 0.0130296331554\n",
      "Loss for  24348 th iteration => 0.013029097803\n",
      "Loss for  24349 th iteration => 0.0130284300821\n",
      "Loss for  24350 th iteration => 0.013027877992\n",
      "Loss for  24351 th iteration => 0.0130272475405\n",
      "Loss for  24352 th iteration => 0.0130267875091\n",
      "Loss for  24353 th iteration => 0.0130260590181\n",
      "Loss for  24354 th iteration => 0.0130255653314\n",
      "Loss for  24355 th iteration => 0.0130248699911\n",
      "Loss for  24356 th iteration => 0.0130243609767\n",
      "Loss for  24357 th iteration => 0.0130237854017\n",
      "Loss for  24358 th iteration => 0.0130231504298\n",
      "Loss for  24359 th iteration => 0.013022594352\n",
      "Loss for  24360 th iteration => 0.0130219473101\n",
      "Loss for  24361 th iteration => 0.0130213740053\n",
      "Loss for  24362 th iteration => 0.0130207393771\n",
      "Loss for  24363 th iteration => 0.0130201704017\n",
      "Loss for  24364 th iteration => 0.0130195318658\n",
      "Loss for  24365 th iteration => 0.0130189670059\n",
      "Loss for  24366 th iteration => 0.0130183247705\n",
      "Loss for  24367 th iteration => 0.0130177638166\n",
      "Loss for  24368 th iteration => 0.0130171292313\n",
      "Loss for  24369 th iteration => 0.0130165241601\n",
      "Loss for  24370 th iteration => 0.0130159260916\n",
      "Loss for  24371 th iteration => 0.0130153068523\n",
      "Loss for  24372 th iteration => 0.0130147395294\n",
      "Loss for  24373 th iteration => 0.0130142180652\n",
      "Loss for  24374 th iteration => 0.0130135163042\n",
      "Loss for  24375 th iteration => 0.0130129994918\n",
      "Loss for  24376 th iteration => 0.0130123164287\n",
      "Loss for  24377 th iteration => 0.0130117819943\n",
      "Loss for  24378 th iteration => 0.0130111163538\n",
      "Loss for  24379 th iteration => 0.0130105655504\n",
      "Loss for  24380 th iteration => 0.0130099393696\n",
      "Loss for  24381 th iteration => 0.0130094801928\n",
      "Loss for  24382 th iteration => 0.0130087536079\n",
      "Loss for  24383 th iteration => 0.0130082615245\n",
      "Loss for  24384 th iteration => 0.0130075673493\n",
      "Loss for  24385 th iteration => 0.0130070646378\n",
      "Loss for  24386 th iteration => 0.0130064850583\n",
      "Loss for  24387 th iteration => 0.0130058574003\n",
      "Loss for  24388 th iteration => 0.0130052969064\n",
      "Loss for  24389 th iteration => 0.013004654666\n",
      "Loss for  24390 th iteration => 0.0130040781894\n",
      "Loss for  24391 th iteration => 0.0130034499082\n",
      "Loss for  24392 th iteration => 0.0130028776918\n",
      "Loss for  24393 th iteration => 0.0130022455688\n",
      "Loss for  24394 th iteration => 0.0130016774008\n",
      "Loss for  24395 th iteration => 0.0130010416419\n",
      "Loss for  24396 th iteration => 0.0130004773152\n",
      "Loss for  24397 th iteration => 0.0129998459391\n",
      "Loss for  24398 th iteration => 0.0129992397288\n",
      "Loss for  24399 th iteration => 0.0129986458411\n",
      "Loss for  24400 th iteration => 0.0129980257219\n",
      "Loss for  24401 th iteration => 0.0129974627938\n",
      "Loss for  24402 th iteration => 0.0129969428009\n",
      "Loss for  24403 th iteration => 0.0129962396402\n",
      "Loss for  24404 th iteration => 0.0129957275431\n",
      "Loss for  24405 th iteration => 0.0129950427788\n",
      "Loss for  24406 th iteration => 0.0129945133535\n",
      "Loss for  24407 th iteration => 0.0129938457198\n",
      "Loss for  24408 th iteration => 0.01299330021\n",
      "Loss for  24409 th iteration => 0.0129926720083\n",
      "Loss for  24410 th iteration => 0.0129922217805\n",
      "Loss for  24411 th iteration => 0.0129914890797\n",
      "Loss for  24412 th iteration => 0.0129910065115\n",
      "Loss for  24413 th iteration => 0.0129903071253\n",
      "Loss for  24414 th iteration => 0.0129899159135\n",
      "Loss for  24415 th iteration => 0.0129891353762\n",
      "Loss for  24416 th iteration => 0.0129886987693\n",
      "Loss for  24417 th iteration => 0.0129879632958\n",
      "Loss for  24418 th iteration => 0.0129874824161\n",
      "Loss for  24419 th iteration => 0.0129867908968\n",
      "Loss for  24420 th iteration => 0.0129862774155\n",
      "Loss for  24421 th iteration => 0.0129855836949\n",
      "Loss for  24422 th iteration => 0.0129850616646\n",
      "Loss for  24423 th iteration => 0.0129844006272\n",
      "Loss for  24424 th iteration => 0.0129838470382\n",
      "Loss for  24425 th iteration => 0.0129832170611\n",
      "Loss for  24426 th iteration => 0.0129826343363\n",
      "Loss for  24427 th iteration => 0.0129819965603\n",
      "Loss for  24428 th iteration => 0.0129814226279\n",
      "Loss for  24429 th iteration => 0.012980799657\n",
      "Loss for  24430 th iteration => 0.0129802119275\n",
      "Loss for  24431 th iteration => 0.012979602665\n",
      "Loss for  24432 th iteration => 0.0129790212115\n",
      "Loss for  24433 th iteration => 0.0129785246567\n",
      "Loss for  24434 th iteration => 0.0129778269492\n",
      "Loss for  24435 th iteration => 0.0129773079666\n",
      "Loss for  24436 th iteration => 0.0129766331944\n",
      "Loss for  24437 th iteration => 0.0129760974997\n",
      "Loss for  24438 th iteration => 0.0129754591129\n",
      "Loss for  24439 th iteration => 0.0129750288103\n",
      "Loss for  24440 th iteration => 0.0129742798353\n",
      "Loss for  24441 th iteration => 0.012973815929\n",
      "Loss for  24442 th iteration => 0.012973100056\n",
      "Loss for  24443 th iteration => 0.0129726041152\n",
      "Loss for  24444 th iteration => 0.0129719197896\n",
      "Loss for  24445 th iteration => 0.0129713933488\n",
      "Loss for  24446 th iteration => 0.0129707442262\n",
      "Loss for  24447 th iteration => 0.0129702927649\n",
      "Loss for  24448 th iteration => 0.012969574746\n",
      "Loss for  24449 th iteration => 0.012969084835\n",
      "Loss for  24450 th iteration => 0.0129683731425\n",
      "Loss for  24451 th iteration => 0.0129678722886\n",
      "Loss for  24452 th iteration => 0.0129671930596\n",
      "Loss for  24453 th iteration => 0.0129666608608\n",
      "Loss for  24454 th iteration => 0.0129660124811\n",
      "Loss for  24455 th iteration => 0.0129654505301\n",
      "Loss for  24456 th iteration => 0.0129648314215\n",
      "Loss for  24457 th iteration => 0.0129642462818\n",
      "Loss for  24458 th iteration => 0.0129636144088\n",
      "Loss for  24459 th iteration => 0.0129630384284\n",
      "Loss for  24460 th iteration => 0.0129624206037\n",
      "Loss for  24461 th iteration => 0.0129618469404\n",
      "Loss for  24462 th iteration => 0.012961352477\n",
      "Loss for  24463 th iteration => 0.0129606533281\n",
      "Loss for  24464 th iteration => 0.012960155988\n",
      "Loss for  24465 th iteration => 0.0129594661755\n",
      "Loss for  24466 th iteration => 0.0129589428519\n",
      "Loss for  24467 th iteration => 0.0129582755671\n",
      "Loss for  24468 th iteration => 0.0129577359856\n",
      "Loss for  24469 th iteration => 0.0129571027673\n",
      "Loss for  24470 th iteration => 0.0129566670995\n",
      "Loss for  24471 th iteration => 0.0129559263645\n",
      "Loss for  24472 th iteration => 0.0129554579828\n",
      "Loss for  24473 th iteration => 0.0129547494692\n",
      "Loss for  24474 th iteration => 0.0129542499195\n",
      "Loss for  24475 th iteration => 0.012953572096\n",
      "Loss for  24476 th iteration => 0.0129530428898\n",
      "Loss for  24477 th iteration => 0.0129523982203\n",
      "Loss for  24478 th iteration => 0.0129519428561\n",
      "Loss for  24479 th iteration => 0.0129512314548\n",
      "Loss for  24480 th iteration => 0.0129507426951\n",
      "Loss for  24481 th iteration => 0.0129500336475\n",
      "Loss for  24482 th iteration => 0.012949533841\n",
      "Loss for  24483 th iteration => 0.0129488565228\n",
      "Loss for  24484 th iteration => 0.0129483260927\n",
      "Loss for  24485 th iteration => 0.0129476789101\n",
      "Loss for  24486 th iteration => 0.0129471194291\n",
      "Loss for  24487 th iteration => 0.0129465008236\n",
      "Loss for  24488 th iteration => 0.0129459205861\n",
      "Loss for  24489 th iteration => 0.0129452906334\n",
      "Loss for  24490 th iteration => 0.0129447162465\n",
      "Loss for  24491 th iteration => 0.0129441000357\n",
      "Loss for  24492 th iteration => 0.0129435260976\n",
      "Loss for  24493 th iteration => 0.0129430318155\n",
      "Loss for  24494 th iteration => 0.0129423357681\n",
      "Loss for  24495 th iteration => 0.0129418386732\n",
      "Loss for  24496 th iteration => 0.0129411535877\n",
      "Loss for  24497 th iteration => 0.0129406315164\n",
      "Loss for  24498 th iteration => 0.0129399661515\n",
      "Loss for  24499 th iteration => 0.0129394281635\n",
      "Loss for  24500 th iteration => 0.012938791612\n",
      "Loss for  24501 th iteration => 0.012938361596\n",
      "Loss for  24502 th iteration => 0.0129376181834\n",
      "Loss for  24503 th iteration => 0.0129371560988\n",
      "Loss for  24504 th iteration => 0.0129364442695\n",
      "Loss for  24505 th iteration => 0.0129359516439\n",
      "Loss for  24506 th iteration => 0.0129352698846\n",
      "Loss for  24507 th iteration => 0.0129347482117\n",
      "Loss for  24508 th iteration => 0.0129340956072\n",
      "Loss for  24509 th iteration => 0.0129336504371\n",
      "Loss for  24510 th iteration => 0.0129329317067\n",
      "Loss for  24511 th iteration => 0.012932455681\n",
      "Loss for  24512 th iteration => 0.0129317403999\n",
      "Loss for  24513 th iteration => 0.0129312503977\n",
      "Loss for  24514 th iteration => 0.0129305663072\n",
      "Loss for  24515 th iteration => 0.0129300462099\n",
      "Loss for  24516 th iteration => 0.0129293917323\n",
      "Loss for  24517 th iteration => 0.0129288430969\n",
      "Loss for  24518 th iteration => 0.0129282166894\n",
      "Loss for  24519 th iteration => 0.0129276474585\n",
      "Loss for  24520 th iteration => 0.0129270158481\n",
      "Loss for  24521 th iteration => 0.0129264465689\n",
      "Loss for  24522 th iteration => 0.0129258284656\n",
      "Loss for  24523 th iteration => 0.0129252554363\n",
      "Loss for  24524 th iteration => 0.0129247620805\n",
      "Loss for  24525 th iteration => 0.0129240683756\n",
      "Loss for  24526 th iteration => 0.0129235722548\n",
      "Loss for  24527 th iteration => 0.0129228896436\n",
      "Loss for  24528 th iteration => 0.0129223728276\n",
      "Loss for  24529 th iteration => 0.0129217053942\n",
      "Loss for  24530 th iteration => 0.0129211729223\n",
      "Loss for  24531 th iteration => 0.0129205269671\n",
      "Loss for  24532 th iteration => 0.012920110422\n",
      "Loss for  24533 th iteration => 0.0129193565802\n",
      "Loss for  24534 th iteration => 0.012918908437\n",
      "Loss for  24535 th iteration => 0.0129181857134\n",
      "Loss for  24536 th iteration => 0.0129177074853\n",
      "Loss for  24537 th iteration => 0.0129170143812\n",
      "Loss for  24538 th iteration => 0.0129165075475\n",
      "Loss for  24539 th iteration => 0.0129158425976\n",
      "Loss for  24540 th iteration => 0.0129153250388\n",
      "Loss for  24541 th iteration => 0.0129147572711\n",
      "Loss for  24542 th iteration => 0.012914145948\n",
      "Loss for  24543 th iteration => 0.0129135642165\n",
      "Loss for  24544 th iteration => 0.0129129577366\n",
      "Loss for  24545 th iteration => 0.0129123799082\n",
      "Loss for  24546 th iteration => 0.0129117699265\n",
      "Loss for  24547 th iteration => 0.0129111958006\n",
      "Loss for  24548 th iteration => 0.0129105825123\n",
      "Loss for  24549 th iteration => 0.012910011893\n",
      "Loss for  24550 th iteration => 0.0129094013784\n",
      "Loss for  24551 th iteration => 0.0129088114733\n",
      "Loss for  24552 th iteration => 0.0129082171746\n",
      "Loss for  24553 th iteration => 0.0129076146189\n",
      "Loss for  24554 th iteration => 0.0129070352549\n",
      "Loss for  24555 th iteration => 0.0129065461757\n",
      "Loss for  24556 th iteration => 0.0129058401153\n",
      "Loss for  24557 th iteration => 0.0129053481504\n",
      "Loss for  24558 th iteration => 0.0129046589952\n",
      "Loss for  24559 th iteration => 0.0129041511532\n",
      "Loss for  24560 th iteration => 0.0129034776857\n",
      "Loss for  24561 th iteration => 0.0129029551632\n",
      "Loss for  24562 th iteration => 0.0129022961921\n",
      "Loss for  24563 th iteration => 0.0129017859486\n",
      "Loss for  24564 th iteration => 0.0129012314153\n",
      "Loss for  24565 th iteration => 0.01290060344\n",
      "Loss for  24566 th iteration => 0.0129000480752\n",
      "Loss for  24567 th iteration => 0.0128994278722\n",
      "Loss for  24568 th iteration => 0.0128989722965\n",
      "Loss for  24569 th iteration => 0.0128982557324\n",
      "Loss for  24570 th iteration => 0.0128977876215\n",
      "Loss for  24571 th iteration => 0.0128970836548\n",
      "Loss for  24572 th iteration => 0.0128966033329\n",
      "Loss for  24573 th iteration => 0.0128959222814\n",
      "Loss for  24574 th iteration => 0.0128954074525\n",
      "Loss for  24575 th iteration => 0.0128947540891\n",
      "Loss for  24576 th iteration => 0.0128942102299\n",
      "Loss for  24577 th iteration => 0.0128935854255\n",
      "Loss for  24578 th iteration => 0.0128930140635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  24579 th iteration => 0.0128924163043\n",
      "Loss for  24580 th iteration => 0.0128918285626\n",
      "Loss for  24581 th iteration => 0.0128912126992\n",
      "Loss for  24582 th iteration => 0.0128906343795\n",
      "Loss for  24583 th iteration => 0.0128900316052\n",
      "Loss for  24584 th iteration => 0.0128894411589\n",
      "Loss for  24585 th iteration => 0.0128888504269\n",
      "Loss for  24586 th iteration => 0.0128882707467\n",
      "Loss for  24587 th iteration => 0.0128877607878\n",
      "Loss for  24588 th iteration => 0.0128870926645\n",
      "Loss for  24589 th iteration => 0.0128865671089\n",
      "Loss for  24590 th iteration => 0.0128859143944\n",
      "Loss for  24591 th iteration => 0.0128853744307\n",
      "Loss for  24592 th iteration => 0.0128847481191\n",
      "Loss for  24593 th iteration => 0.0128843174734\n",
      "Loss for  24594 th iteration => 0.0128835829434\n",
      "Loss for  24595 th iteration => 0.0128831232164\n",
      "Loss for  24596 th iteration => 0.0128824173131\n",
      "Loss for  24597 th iteration => 0.0128819299557\n",
      "Loss for  24598 th iteration => 0.0128812512422\n",
      "Loss for  24599 th iteration => 0.0128807563579\n",
      "Loss for  24600 th iteration => 0.012880179599\n",
      "Loss for  24601 th iteration => 0.0128795733082\n",
      "Loss for  24602 th iteration => 0.0128789899796\n",
      "Loss for  24603 th iteration => 0.0128783913516\n",
      "Loss for  24604 th iteration => 0.0128778118218\n",
      "Loss for  24605 th iteration => 0.0128772097909\n",
      "Loss for  24606 th iteration => 0.012876633863\n",
      "Loss for  24607 th iteration => 0.0128760286209\n",
      "Loss for  24608 th iteration => 0.0128754561021\n",
      "Loss for  24609 th iteration => 0.0128748478366\n",
      "Loss for  24610 th iteration => 0.0128742785384\n",
      "Loss for  24611 th iteration => 0.0128736716119\n",
      "Loss for  24612 th iteration => 0.0128730815865\n",
      "Loss for  24613 th iteration => 0.0128724963059\n",
      "Loss for  24614 th iteration => 0.0128720267808\n",
      "Loss for  24615 th iteration => 0.0128713310748\n",
      "Loss for  24616 th iteration => 0.0128708379067\n",
      "Loss for  24617 th iteration => 0.0128701386114\n",
      "Loss for  24618 th iteration => 0.0128696467478\n",
      "Loss for  24619 th iteration => 0.012868963627\n",
      "Loss for  24620 th iteration => 0.0128684565997\n",
      "Loss for  24621 th iteration => 0.0128677884571\n",
      "Loss for  24622 th iteration => 0.0128672674418\n",
      "Loss for  24623 th iteration => 0.0128666131069\n",
      "Loss for  24624 th iteration => 0.012866106399\n",
      "Loss for  24625 th iteration => 0.0128655567316\n",
      "Loss for  24626 th iteration => 0.0128649302708\n",
      "Loss for  24627 th iteration => 0.0128643798069\n",
      "Loss for  24628 th iteration => 0.012863762682\n",
      "Loss for  24629 th iteration => 0.0128633116334\n",
      "Loss for  24630 th iteration => 0.0128625965885\n",
      "Loss for  24631 th iteration => 0.0128621335707\n",
      "Loss for  24632 th iteration => 0.0128614305617\n",
      "Loss for  24633 th iteration => 0.0128609558848\n",
      "Loss for  24634 th iteration => 0.0128602720245\n",
      "Loss for  24635 th iteration => 0.012859765933\n",
      "Loss for  24636 th iteration => 0.0128591096512\n",
      "Loss for  24637 th iteration => 0.0128585756826\n",
      "Loss for  24638 th iteration => 0.0128579468186\n",
      "Loss for  24639 th iteration => 0.0128573864679\n",
      "Loss for  24640 th iteration => 0.0128567835402\n",
      "Loss for  24641 th iteration => 0.0128562027489\n",
      "Loss for  24642 th iteration => 0.0128555872342\n",
      "Loss for  24643 th iteration => 0.0128550152746\n",
      "Loss for  24644 th iteration => 0.0128544123681\n",
      "Loss for  24645 th iteration => 0.0128538287487\n",
      "Loss for  24646 th iteration => 0.0128532374191\n",
      "Loss for  24647 th iteration => 0.0128526629734\n",
      "Loss for  24648 th iteration => 0.0128521717968\n",
      "Loss for  24649 th iteration => 0.0128514952983\n",
      "Loss for  24650 th iteration => 0.0128509799375\n",
      "Loss for  24651 th iteration => 0.0128503233299\n",
      "Loss for  24652 th iteration => 0.0128497937009\n",
      "Loss for  24653 th iteration => 0.0128491598455\n",
      "Loss for  24654 th iteration => 0.0128487494229\n",
      "Loss for  24655 th iteration => 0.0128480008619\n",
      "Loss for  24656 th iteration => 0.0128475616162\n",
      "Loss for  24657 th iteration => 0.0128468414279\n",
      "Loss for  24658 th iteration => 0.0128463747976\n",
      "Loss for  24659 th iteration => 0.012845681557\n",
      "Loss for  24660 th iteration => 0.0128451889486\n",
      "Loss for  24661 th iteration => 0.0128445212628\n",
      "Loss for  24662 th iteration => 0.0128440225668\n",
      "Loss for  24663 th iteration => 0.0128434509298\n",
      "Loss for  24664 th iteration => 0.0128428506555\n",
      "Loss for  24665 th iteration => 0.0128422683422\n",
      "Loss for  24666 th iteration => 0.0128416753896\n",
      "Loss for  24667 th iteration => 0.012841096698\n",
      "Loss for  24668 th iteration => 0.0128405005122\n",
      "Loss for  24669 th iteration => 0.0128399252505\n",
      "Loss for  24670 th iteration => 0.0128393260183\n",
      "Loss for  24671 th iteration => 0.0128387539987\n",
      "Loss for  24672 th iteration => 0.0128381519031\n",
      "Loss for  24673 th iteration => 0.0128375829418\n",
      "Loss for  24674 th iteration => 0.012836983981\n",
      "Loss for  24675 th iteration => 0.0128363966614\n",
      "Loss for  24676 th iteration => 0.0128358122704\n",
      "Loss for  24677 th iteration => 0.0128352393735\n",
      "Loss for  24678 th iteration => 0.0128347534536\n",
      "Loss for  24679 th iteration => 0.0128340739379\n",
      "Loss for  24680 th iteration => 0.012833557354\n",
      "Loss for  24681 th iteration => 0.0128329052463\n",
      "Loss for  24682 th iteration => 0.0128323738563\n",
      "Loss for  24683 th iteration => 0.0128317363713\n",
      "Loss for  24684 th iteration => 0.0128311913387\n",
      "Loss for  24685 th iteration => 0.012830567318\n",
      "Loss for  24686 th iteration => 0.0128300097814\n",
      "Loss for  24687 th iteration => 0.0128294182916\n",
      "Loss for  24688 th iteration => 0.0128289510342\n",
      "Loss for  24689 th iteration => 0.0128282611669\n",
      "Loss for  24690 th iteration => 0.0128277686605\n",
      "Loss for  24691 th iteration => 0.0128271075513\n",
      "Loss for  24692 th iteration => 0.0128266967822\n",
      "Loss for  24693 th iteration => 0.0128259600349\n",
      "Loss for  24694 th iteration => 0.0128255134362\n",
      "Loss for  24695 th iteration => 0.0128248122923\n",
      "Loss for  24696 th iteration => 0.0128243400142\n",
      "Loss for  24697 th iteration => 0.0128236350446\n",
      "Loss for  24698 th iteration => 0.0128231561312\n",
      "Loss for  24699 th iteration => 0.0128224784443\n",
      "Loss for  24700 th iteration => 0.012821973276\n",
      "Loss for  24701 th iteration => 0.0128213214016\n",
      "Loss for  24702 th iteration => 0.0128207914294\n",
      "Loss for  24703 th iteration => 0.0128201639297\n",
      "Loss for  24704 th iteration => 0.0128196105725\n",
      "Loss for  24705 th iteration => 0.0128190060414\n",
      "Loss for  24706 th iteration => 0.0128184394046\n",
      "Loss for  24707 th iteration => 0.0128178266784\n",
      "Loss for  24708 th iteration => 0.0128172642535\n",
      "Loss for  24709 th iteration => 0.0128167837128\n",
      "Loss for  24710 th iteration => 0.0128160964151\n",
      "Loss for  24711 th iteration => 0.0128156133341\n",
      "Loss for  24712 th iteration => 0.0128149321207\n",
      "Loss for  24713 th iteration => 0.0128144363512\n",
      "Loss for  24714 th iteration => 0.0128137666904\n",
      "Loss for  24715 th iteration => 0.0128132565631\n",
      "Loss for  24716 th iteration => 0.0128126010792\n",
      "Loss for  24717 th iteration => 0.0128120777437\n",
      "Loss for  24718 th iteration => 0.0128114352925\n",
      "Loss for  24719 th iteration => 0.0128109236942\n",
      "Loss for  24720 th iteration => 0.0128103853497\n",
      "Loss for  24721 th iteration => 0.0128097573896\n",
      "Loss for  24722 th iteration => 0.0128092182722\n",
      "Loss for  24723 th iteration => 0.0128085913305\n",
      "Loss for  24724 th iteration => 0.012808051482\n",
      "Loss for  24725 th iteration => 0.0128074340803\n",
      "Loss for  24726 th iteration => 0.0128069813696\n",
      "Loss for  24727 th iteration => 0.0128062773802\n",
      "Loss for  24728 th iteration => 0.0128058138736\n",
      "Loss for  24729 th iteration => 0.0128051307124\n",
      "Loss for  24730 th iteration => 0.0128046367584\n",
      "Loss for  24731 th iteration => 0.0128039772754\n",
      "Loss for  24732 th iteration => 0.0128034573046\n",
      "Loss for  24733 th iteration => 0.0128028233988\n",
      "Loss for  24734 th iteration => 0.012802278854\n",
      "Loss for  24735 th iteration => 0.0128016690954\n",
      "Loss for  24736 th iteration => 0.0128011041181\n",
      "Loss for  24737 th iteration => 0.0128004897965\n",
      "Loss for  24738 th iteration => 0.0127999270201\n",
      "Loss for  24739 th iteration => 0.012799324518\n",
      "Loss for  24740 th iteration => 0.012798750848\n",
      "Loss for  24741 th iteration => 0.012798159159\n",
      "Loss for  24742 th iteration => 0.012797589703\n",
      "Loss for  24743 th iteration => 0.0127971021879\n",
      "Loss for  24744 th iteration => 0.012796430798\n",
      "Loss for  24745 th iteration => 0.0127959260982\n",
      "Loss for  24746 th iteration => 0.0127952683252\n",
      "Loss for  24747 th iteration => 0.0127947502325\n",
      "Loss for  24748 th iteration => 0.0127941056761\n",
      "Loss for  24749 th iteration => 0.0127936002538\n",
      "Loss for  24750 th iteration => 0.0127930667072\n",
      "Loss for  24751 th iteration => 0.0127924369997\n",
      "Loss for  24752 th iteration => 0.0127919026601\n",
      "Loss for  24753 th iteration => 0.0127912739908\n",
      "Loss for  24754 th iteration => 0.0127907389\n",
      "Loss for  24755 th iteration => 0.0127901112254\n",
      "Loss for  24756 th iteration => 0.0127895754246\n",
      "Loss for  24757 th iteration => 0.0127889588861\n",
      "Loss for  24758 th iteration => 0.0127885070547\n",
      "Loss for  24759 th iteration => 0.0127878083097\n",
      "Loss for  24760 th iteration => 0.012787331212\n",
      "Loss for  24761 th iteration => 0.012786657798\n",
      "Loss for  24762 th iteration => 0.0127861549805\n",
      "Loss for  24763 th iteration => 0.0127855068501\n",
      "Loss for  24764 th iteration => 0.0127849797458\n",
      "Loss for  24765 th iteration => 0.0127843554789\n",
      "Loss for  24766 th iteration => 0.0127838054894\n",
      "Loss for  24767 th iteration => 0.0127832036973\n",
      "Loss for  24768 th iteration => 0.0127826365238\n",
      "Loss for  24769 th iteration => 0.012782025067\n",
      "Loss for  24770 th iteration => 0.0127814632066\n",
      "Loss for  24771 th iteration => 0.0127808628677\n",
      "Loss for  24772 th iteration => 0.0127803056572\n",
      "Loss for  24773 th iteration => 0.0127798165556\n",
      "Loss for  24774 th iteration => 0.0127791443674\n",
      "Loss for  24775 th iteration => 0.0127786530015\n",
      "Loss for  24776 th iteration => 0.0127779890577\n",
      "Loss for  24777 th iteration => 0.0127774798259\n",
      "Loss for  24778 th iteration => 0.0127768297125\n",
      "Loss for  24779 th iteration => 0.012776307502\n",
      "Loss for  24780 th iteration => 0.0127756701933\n",
      "Loss for  24781 th iteration => 0.0127751620624\n",
      "Loss for  24782 th iteration => 0.0127746331001\n",
      "Loss for  24783 th iteration => 0.0127740020874\n",
      "Loss for  24784 th iteration => 0.0127734723464\n",
      "Loss for  24785 th iteration => 0.0127728423558\n",
      "Loss for  24786 th iteration => 0.0127723118773\n",
      "Loss for  24787 th iteration => 0.0127716828658\n",
      "Loss for  24788 th iteration => 0.0127711516905\n",
      "Loss for  24789 th iteration => 0.0127705332168\n",
      "Loss for  24790 th iteration => 0.0127700868643\n",
      "Loss for  24791 th iteration => 0.0127693873263\n",
      "Loss for  24792 th iteration => 0.012768912737\n",
      "Loss for  24793 th iteration => 0.0127682397634\n",
      "Loss for  24794 th iteration => 0.0127677401266\n",
      "Loss for  24795 th iteration => 0.0127670917713\n",
      "Loss for  24796 th iteration => 0.0127665685017\n",
      "Loss for  24797 th iteration => 0.0127659433627\n",
      "Loss for  24798 th iteration => 0.012765397844\n",
      "Loss for  24799 th iteration => 0.0127647945503\n",
      "Loss for  24800 th iteration => 0.0127642318838\n",
      "Loss for  24801 th iteration => 0.0127636196813\n",
      "Loss for  24802 th iteration => 0.012763062025\n",
      "Loss for  24803 th iteration => 0.0127624606685\n",
      "Loss for  24804 th iteration => 0.0127619085232\n",
      "Loss for  24805 th iteration => 0.0127614165531\n",
      "Loss for  24806 th iteration => 0.0127607504794\n",
      "Loss for  24807 th iteration => 0.0127602563123\n",
      "Loss for  24808 th iteration => 0.0127595968171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  24809 th iteration => 0.0127590883127\n",
      "Loss for  24810 th iteration => 0.0127584406227\n",
      "Loss for  24811 th iteration => 0.0127579194499\n",
      "Loss for  24812 th iteration => 0.0127572842561\n",
      "Loss for  24813 th iteration => 0.0127567756511\n",
      "Loss for  24814 th iteration => 0.0127562514259\n",
      "Loss for  24815 th iteration => 0.0127556189366\n",
      "Loss for  24816 th iteration => 0.0127550939407\n",
      "Loss for  24817 th iteration => 0.0127544624638\n",
      "Loss for  24818 th iteration => 0.0127539367381\n",
      "Loss for  24819 th iteration => 0.0127533062311\n",
      "Loss for  24820 th iteration => 0.0127527798157\n",
      "Loss for  24821 th iteration => 0.0127521572377\n",
      "Loss for  24822 th iteration => 0.012751720182\n",
      "Loss for  24823 th iteration => 0.0127510139834\n",
      "Loss for  24824 th iteration => 0.0127505501892\n",
      "Loss for  24825 th iteration => 0.0127498694386\n",
      "Loss for  24826 th iteration => 0.0127493810873\n",
      "Loss for  24827 th iteration => 0.01274872447\n",
      "Loss for  24828 th iteration => 0.0127482129616\n",
      "Loss for  24829 th iteration => 0.0127475790901\n",
      "Loss for  24830 th iteration => 0.0127470457941\n",
      "Loss for  24831 th iteration => 0.0127464333116\n",
      "Loss for  24832 th iteration => 0.0127458808187\n",
      "Loss for  24833 th iteration => 0.0127452645914\n",
      "Loss for  24834 th iteration => 0.0127447143588\n",
      "Loss for  24835 th iteration => 0.0127441087715\n",
      "Loss for  24836 th iteration => 0.0127435626517\n",
      "Loss for  24837 th iteration => 0.0127430686721\n",
      "Loss for  24838 th iteration => 0.0127424078404\n",
      "Loss for  24839 th iteration => 0.012741911716\n",
      "Loss for  24840 th iteration => 0.0127412544548\n",
      "Loss for  24841 th iteration => 0.012740750519\n",
      "Loss for  24842 th iteration => 0.0127401014244\n",
      "Loss for  24843 th iteration => 0.0127395850556\n",
      "Loss for  24844 th iteration => 0.0127389482232\n",
      "Loss for  24845 th iteration => 0.0127384408074\n",
      "Loss for  24846 th iteration => 0.0127379213239\n",
      "Loss for  24847 th iteration => 0.0127372873361\n",
      "Loss for  24848 th iteration => 0.0127367670859\n",
      "Loss for  24849 th iteration => 0.0127361341051\n",
      "Loss for  24850 th iteration => 0.0127356131287\n",
      "Loss for  24851 th iteration => 0.0127349811129\n",
      "Loss for  24852 th iteration => 0.01273445945\n",
      "Loss for  24853 th iteration => 0.012733831304\n",
      "Loss for  24854 th iteration => 0.0127334061585\n",
      "Loss for  24855 th iteration => 0.0127326892039\n",
      "Loss for  24856 th iteration => 0.0127322420908\n",
      "Loss for  24857 th iteration => 0.0127315477234\n",
      "Loss for  24858 th iteration => 0.0127310764126\n",
      "Loss for  24859 th iteration => 0.0127304058234\n",
      "Loss for  24860 th iteration => 0.0127299117031\n",
      "Loss for  24861 th iteration => 0.0127292635162\n",
      "Loss for  24862 th iteration => 0.0127287479444\n",
      "Loss for  24863 th iteration => 0.0127281208144\n",
      "Loss for  24864 th iteration => 0.0127275851189\n",
      "Loss for  24865 th iteration => 0.0127269777301\n",
      "Loss for  24866 th iteration => 0.0127264301657\n",
      "Loss for  24867 th iteration => 0.0127258157424\n",
      "Loss for  24868 th iteration => 0.0127252772177\n",
      "Loss for  24869 th iteration => 0.0127247820547\n",
      "Loss for  24870 th iteration => 0.0127241256128\n",
      "Loss for  24871 th iteration => 0.0127236282608\n",
      "Loss for  24872 th iteration => 0.0127229742247\n",
      "Loss for  24873 th iteration => 0.012722474815\n",
      "Loss for  24874 th iteration => 0.0127218280207\n",
      "Loss for  24875 th iteration => 0.0127213209979\n",
      "Loss for  24876 th iteration => 0.0127206782754\n",
      "Loss for  24877 th iteration => 0.0127201592472\n",
      "Loss for  24878 th iteration => 0.0127195283618\n",
      "Loss for  24879 th iteration => 0.0127190138772\n",
      "Loss for  24880 th iteration => 0.0127184993175\n",
      "Loss for  24881 th iteration => 0.012717863848\n",
      "Loss for  24882 th iteration => 0.012717348535\n",
      "Loss for  24883 th iteration => 0.0127167140572\n",
      "Loss for  24884 th iteration => 0.0127161980309\n",
      "Loss for  24885 th iteration => 0.0127155645033\n",
      "Loss for  24886 th iteration => 0.0127150478026\n",
      "Loss for  24887 th iteration => 0.0127144151848\n",
      "Loss for  24888 th iteration => 0.012713916417\n",
      "Loss for  24889 th iteration => 0.0127133564579\n",
      "Loss for  24890 th iteration => 0.0127127675205\n",
      "Loss for  24891 th iteration => 0.0127122075184\n",
      "Loss for  24892 th iteration => 0.0127116158521\n",
      "Loss for  24893 th iteration => 0.0127110587684\n",
      "Loss for  24894 th iteration => 0.0127104645461\n",
      "Loss for  24895 th iteration => 0.0127099102071\n",
      "Loss for  24896 th iteration => 0.0127093135981\n",
      "Loss for  24897 th iteration => 0.0127087618338\n",
      "Loss for  24898 th iteration => 0.012708165209\n",
      "Loss for  24899 th iteration => 0.0127076029718\n",
      "Loss for  24900 th iteration => 0.0127070158913\n",
      "Loss for  24901 th iteration => 0.0127064440673\n",
      "Loss for  24902 th iteration => 0.0127058715871\n",
      "Loss for  24903 th iteration => 0.0127054017028\n",
      "Loss for  24904 th iteration => 0.0127047328124\n",
      "Loss for  24905 th iteration => 0.0127042471155\n",
      "Loss for  24906 th iteration => 0.0127035788514\n",
      "Loss for  24907 th iteration => 0.0127030879348\n",
      "Loss for  24908 th iteration => 0.0127024321873\n",
      "Loss for  24909 th iteration => 0.0127019296792\n",
      "Loss for  24910 th iteration => 0.0127012853572\n",
      "Loss for  24911 th iteration => 0.0127007799018\n",
      "Loss for  24912 th iteration => 0.0127002654713\n",
      "Loss for  24913 th iteration => 0.0126996330951\n",
      "Loss for  24914 th iteration => 0.0126991179412\n",
      "Loss for  24915 th iteration => 0.0126984865249\n",
      "Loss for  24916 th iteration => 0.0126979706868\n",
      "Loss for  24917 th iteration => 0.0126973401899\n",
      "Loss for  24918 th iteration => 0.0126968341824\n",
      "Loss for  24919 th iteration => 0.01269629104\n",
      "Loss for  24920 th iteration => 0.0126956953051\n",
      "Loss for  24921 th iteration => 0.0126951445955\n",
      "Loss for  24922 th iteration => 0.0126945565344\n",
      "Loss for  24923 th iteration => 0.0126940074905\n",
      "Loss for  24924 th iteration => 0.0126934184305\n",
      "Loss for  24925 th iteration => 0.012692861942\n",
      "Loss for  24926 th iteration => 0.0126922702293\n",
      "Loss for  24927 th iteration => 0.0126917165818\n",
      "Loss for  24928 th iteration => 0.0126911223872\n",
      "Loss for  24929 th iteration => 0.0126905714092\n",
      "Loss for  24930 th iteration => 0.0126899749001\n",
      "Loss for  24931 th iteration => 0.0126894264238\n",
      "Loss for  24932 th iteration => 0.0126888335603\n",
      "Loss for  24933 th iteration => 0.0126882777133\n",
      "Loss for  24934 th iteration => 0.0126876875851\n",
      "Loss for  24935 th iteration => 0.012687122386\n",
      "Loss for  24936 th iteration => 0.0126865415343\n",
      "Loss for  24937 th iteration => 0.0126859889156\n",
      "Loss for  24938 th iteration => 0.0126854914705\n",
      "Loss for  24939 th iteration => 0.0126848504601\n",
      "Loss for  24940 th iteration => 0.0126843384011\n",
      "Loss for  24941 th iteration => 0.0126837070885\n",
      "Loss for  24942 th iteration => 0.0126831834299\n",
      "Loss for  24943 th iteration => 0.0126825635511\n",
      "Loss for  24944 th iteration => 0.0126820293638\n",
      "Loss for  24945 th iteration => 0.0126814198528\n",
      "Loss for  24946 th iteration => 0.0126809012221\n",
      "Loss for  24947 th iteration => 0.0126803838223\n",
      "Loss for  24948 th iteration => 0.0126797579435\n",
      "Loss for  24949 th iteration => 0.0126792398946\n",
      "Loss for  24950 th iteration => 0.0126786148983\n",
      "Loss for  24951 th iteration => 0.0126780962381\n",
      "Loss for  24952 th iteration => 0.0126774734433\n",
      "Loss for  24953 th iteration => 0.0126770502009\n",
      "Loss for  24954 th iteration => 0.0126763444908\n",
      "Loss for  24955 th iteration => 0.0126758970294\n",
      "Loss for  24956 th iteration => 0.0126752119655\n",
      "Loss for  24957 th iteration => 0.012674742383\n",
      "Loss for  24958 th iteration => 0.0126740790418\n",
      "Loss for  24959 th iteration => 0.0126735886706\n",
      "Loss for  24960 th iteration => 0.0126729457319\n",
      "Loss for  24961 th iteration => 0.0126724358751\n",
      "Loss for  24962 th iteration => 0.0126718120476\n",
      "Loss for  24963 th iteration => 0.0126712839797\n",
      "Loss for  24964 th iteration => 0.0126706780009\n",
      "Loss for  24965 th iteration => 0.0126701419194\n",
      "Loss for  24966 th iteration => 0.0126695280741\n",
      "Loss for  24967 th iteration => 0.0126689956105\n",
      "Loss for  24968 th iteration => 0.0126685021745\n",
      "Loss for  24969 th iteration => 0.012667853897\n",
      "Loss for  24970 th iteration => 0.0126673584489\n",
      "Loss for  24971 th iteration => 0.012666712396\n",
      "Loss for  24972 th iteration => 0.0126662150624\n",
      "Loss for  24973 th iteration => 0.0126655743178\n",
      "Loss for  24974 th iteration => 0.0126650772319\n",
      "Loss for  24975 th iteration => 0.0126644341655\n",
      "Loss for  24976 th iteration => 0.0126639259956\n",
      "Loss for  24977 th iteration => 0.0126632938502\n",
      "Loss for  24978 th iteration => 0.0126627849432\n",
      "Loss for  24979 th iteration => 0.0126622740626\n",
      "Loss for  24980 th iteration => 0.0126616448133\n",
      "Loss for  24981 th iteration => 0.0126611332369\n",
      "Loss for  24982 th iteration => 0.0126605049171\n",
      "Loss for  24983 th iteration => 0.0126599926833\n",
      "Loss for  24984 th iteration => 0.0126593652532\n",
      "Loss for  24985 th iteration => 0.0126588523995\n",
      "Loss for  24986 th iteration => 0.0126582258202\n",
      "Loss for  24987 th iteration => 0.0126577224243\n",
      "Loss for  24988 th iteration => 0.0126571770812\n",
      "Loss for  24989 th iteration => 0.012656591538\n",
      "Loss for  24990 th iteration => 0.0126560379737\n",
      "Loss for  24991 th iteration => 0.0126554498991\n",
      "Loss for  24992 th iteration => 0.0126548990527\n",
      "Loss for  24993 th iteration => 0.012654308614\n",
      "Loss for  24994 th iteration => 0.0126537603175\n",
      "Loss for  24995 th iteration => 0.0126531676784\n",
      "Loss for  24996 th iteration => 0.0126526217675\n",
      "Loss for  24997 th iteration => 0.0126520270883\n",
      "Loss for  24998 th iteration => 0.0126514834022\n",
      "Loss for  24999 th iteration => 0.0126508942854\n",
      "Loss for  25000 th iteration => 0.0126503442788\n",
      "Loss for  25001 th iteration => 0.012649754721\n",
      "Loss for  25002 th iteration => 0.0126492137881\n",
      "Loss for  25003 th iteration => 0.0126487178338\n",
      "Loss for  25004 th iteration => 0.0126480755436\n",
      "Loss for  25005 th iteration => 0.0126475777344\n",
      "Loss for  25006 th iteration => 0.012646944003\n",
      "Loss for  25007 th iteration => 0.0126464349309\n",
      "Loss for  25008 th iteration => 0.0126458070646\n",
      "Loss for  25009 th iteration => 0.0126452870616\n",
      "Loss for  25010 th iteration => 0.0126446699643\n",
      "Loss for  25011 th iteration => 0.012644140081\n",
      "Loss for  25012 th iteration => 0.012643532707\n",
      "Loss for  25013 th iteration => 0.0126430141379\n",
      "Loss for  25014 th iteration => 0.0126425022897\n",
      "Loss for  25015 th iteration => 0.0126418775199\n",
      "Loss for  25016 th iteration => 0.0126413650528\n",
      "Loss for  25017 th iteration => 0.0126407411322\n",
      "Loss for  25018 th iteration => 0.012640228083\n",
      "Loss for  25019 th iteration => 0.0126396049735\n",
      "Loss for  25020 th iteration => 0.012639107738\n",
      "Loss for  25021 th iteration => 0.0126385590827\n",
      "Loss for  25022 th iteration => 0.0126379728235\n",
      "Loss for  25023 th iteration => 0.0126374232146\n",
      "Loss for  25024 th iteration => 0.0126368345487\n",
      "Loss for  25025 th iteration => 0.0126362875319\n",
      "Loss for  25026 th iteration => 0.0126356966235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  25027 th iteration => 0.0126351520339\n",
      "Loss for  25028 th iteration => 0.0126345590437\n",
      "Loss for  25029 th iteration => 0.0126340167201\n",
      "Loss for  25030 th iteration => 0.0126334218055\n",
      "Loss for  25031 th iteration => 0.01263288159\n",
      "Loss for  25032 th iteration => 0.0126322931479\n",
      "Loss for  25033 th iteration => 0.0126317390425\n",
      "Loss for  25034 th iteration => 0.0126311567375\n",
      "Loss for  25035 th iteration => 0.0126306174981\n",
      "Loss for  25036 th iteration => 0.0126301226877\n",
      "Loss for  25037 th iteration => 0.0126294825074\n",
      "Loss for  25038 th iteration => 0.012628985919\n",
      "Loss for  25039 th iteration => 0.0126283542368\n",
      "Loss for  25040 th iteration => 0.0126278419772\n",
      "Loss for  25041 th iteration => 0.0126272204109\n",
      "Loss for  25042 th iteration => 0.0126266976444\n",
      "Loss for  25043 th iteration => 0.0126260864252\n",
      "Loss for  25044 th iteration => 0.0126255541911\n",
      "Loss for  25045 th iteration => 0.0126249523759\n",
      "Loss for  25046 th iteration => 0.012624537463\n",
      "Loss for  25047 th iteration => 0.0126238284332\n",
      "Loss for  25048 th iteration => 0.0126233941813\n",
      "Loss for  25049 th iteration => 0.0126227041397\n",
      "Loss for  25050 th iteration => 0.0126222517381\n",
      "Loss for  25051 th iteration => 0.0126215795068\n",
      "Loss for  25052 th iteration => 0.0126211101185\n",
      "Loss for  25053 th iteration => 0.0126204756146\n",
      "Loss for  25054 th iteration => 0.0126200581066\n",
      "Loss for  25055 th iteration => 0.0126193486682\n",
      "Loss for  25056 th iteration => 0.0126189137465\n",
      "Loss for  25057 th iteration => 0.0126182250923\n",
      "Loss for  25058 th iteration => 0.0126177703001\n",
      "Loss for  25059 th iteration => 0.0126171011411\n",
      "Loss for  25060 th iteration => 0.0126166277507\n",
      "Loss for  25061 th iteration => 0.0126159768262\n",
      "Loss for  25062 th iteration => 0.0126154860821\n",
      "Loss for  25063 th iteration => 0.0126148521591\n",
      "Loss for  25064 th iteration => 0.0126143452783\n",
      "Loss for  25065 th iteration => 0.0126137271512\n",
      "Loss for  25066 th iteration => 0.0126132053238\n",
      "Loss for  25067 th iteration => 0.0126126027313\n",
      "Loss for  25068 th iteration => 0.0126121936526\n",
      "Loss for  25069 th iteration => 0.0126115030451\n",
      "Loss for  25070 th iteration => 0.012611050733\n",
      "Loss for  25071 th iteration => 0.012610380197\n",
      "Loss for  25072 th iteration => 0.0126099114142\n",
      "Loss for  25073 th iteration => 0.0126092477518\n",
      "Loss for  25074 th iteration => 0.012608769296\n",
      "Loss for  25075 th iteration => 0.012608117408\n",
      "Loss for  25076 th iteration => 0.0126076280772\n",
      "Loss for  25077 th iteration => 0.0126069869034\n",
      "Loss for  25078 th iteration => 0.0126064877402\n",
      "Loss for  25079 th iteration => 0.0126058562429\n",
      "Loss for  25080 th iteration => 0.0126053482678\n",
      "Loss for  25081 th iteration => 0.0126047254315\n",
      "Loss for  25082 th iteration => 0.0126042108802\n",
      "Loss for  25083 th iteration => 0.0126037037842\n",
      "Loss for  25084 th iteration => 0.0126030810896\n",
      "Loss for  25085 th iteration => 0.0126025742461\n",
      "Loss for  25086 th iteration => 0.0126020525638\n",
      "Loss for  25087 th iteration => 0.0126014513058\n",
      "Loss for  25088 th iteration => 0.0126009230445\n",
      "Loss for  25089 th iteration => 0.0126003284794\n",
      "Loss for  25090 th iteration => 0.0125997962565\n",
      "Loss for  25091 th iteration => 0.0125992179554\n",
      "Loss for  25092 th iteration => 0.0125986671538\n",
      "Loss for  25093 th iteration => 0.0125980865162\n",
      "Loss for  25094 th iteration => 0.0125975382345\n",
      "Loss for  25095 th iteration => 0.0125969554218\n",
      "Loss for  25096 th iteration => 0.0125964094981\n",
      "Loss for  25097 th iteration => 0.0125958246684\n",
      "Loss for  25098 th iteration => 0.0125952809441\n",
      "Loss for  25099 th iteration => 0.0125946942519\n",
      "Loss for  25100 th iteration => 0.0125941525719\n",
      "Loss for  25101 th iteration => 0.0125935691117\n",
      "Loss for  25102 th iteration => 0.0125930361792\n",
      "Loss for  25103 th iteration => 0.0125924394156\n",
      "Loss for  25104 th iteration => 0.0125919025983\n",
      "Loss for  25105 th iteration => 0.0125914151804\n",
      "Loss for  25106 th iteration => 0.0125907743737\n",
      "Loss for  25107 th iteration => 0.0125902852203\n",
      "Loss for  25108 th iteration => 0.0125896498047\n",
      "Loss for  25109 th iteration => 0.0125891637005\n",
      "Loss for  25110 th iteration => 0.0125885226595\n",
      "Loss for  25111 th iteration => 0.0125880263417\n",
      "Loss for  25112 th iteration => 0.0125873953569\n",
      "Loss for  25113 th iteration => 0.0125868898513\n",
      "Loss for  25114 th iteration => 0.0125862679019\n",
      "Loss for  25115 th iteration => 0.0125857588417\n",
      "Loss for  25116 th iteration => 0.0125852510057\n",
      "Loss for  25117 th iteration => 0.0125846322351\n",
      "Loss for  25118 th iteration => 0.0125841238238\n",
      "Loss for  25119 th iteration => 0.0125835058545\n",
      "Loss for  25120 th iteration => 0.0125829969036\n",
      "Loss for  25121 th iteration => 0.0125823796989\n",
      "Loss for  25122 th iteration => 0.0125818734154\n",
      "Loss for  25123 th iteration => 0.0125813424078\n",
      "Loss for  25124 th iteration => 0.0125807537991\n",
      "Loss for  25125 th iteration => 0.0125802242638\n",
      "Loss for  25126 th iteration => 0.012579645118\n",
      "Loss for  25127 th iteration => 0.0125790985545\n",
      "Loss for  25128 th iteration => 0.0125785171953\n",
      "Loss for  25129 th iteration => 0.0125779730276\n",
      "Loss for  25130 th iteration => 0.0125773896133\n",
      "Loss for  25131 th iteration => 0.0125768476824\n",
      "Loss for  25132 th iteration => 0.0125762623681\n",
      "Loss for  25133 th iteration => 0.0125757225185\n",
      "Loss for  25134 th iteration => 0.0125751367649\n",
      "Loss for  25135 th iteration => 0.0125746082044\n",
      "Loss for  25136 th iteration => 0.0125740103298\n",
      "Loss for  25137 th iteration => 0.0125734738213\n",
      "Loss for  25138 th iteration => 0.0125728838236\n",
      "Loss for  25139 th iteration => 0.0125723446827\n",
      "Loss for  25140 th iteration => 0.0125718581097\n",
      "Loss for  25141 th iteration => 0.0125712198746\n",
      "Loss for  25142 th iteration => 0.0125707317222\n",
      "Loss for  25143 th iteration => 0.0125701016948\n",
      "Loss for  25144 th iteration => 0.0125696140218\n",
      "Loss for  25145 th iteration => 0.0125689776646\n",
      "Loss for  25146 th iteration => 0.0125684806445\n",
      "Loss for  25147 th iteration => 0.0125678534816\n",
      "Loss for  25148 th iteration => 0.0125673519686\n",
      "Loss for  25149 th iteration => 0.0125668448203\n",
      "Loss for  25150 th iteration => 0.0125662285486\n",
      "Loss for  25151 th iteration => 0.0125657208193\n",
      "Loss for  25152 th iteration => 0.0125651053542\n",
      "Loss for  25153 th iteration => 0.0125645970795\n",
      "Loss for  25154 th iteration => 0.012563982384\n",
      "Loss for  25155 th iteration => 0.0125634735988\n",
      "Loss for  25156 th iteration => 0.0125628596371\n",
      "Loss for  25157 th iteration => 0.012562352338\n",
      "Loss for  25158 th iteration => 0.0125618249391\n",
      "Loss for  25159 th iteration => 0.0125612484916\n",
      "Loss for  25160 th iteration => 0.0125607024244\n",
      "Loss for  25161 th iteration => 0.012560123805\n",
      "Loss for  25162 th iteration => 0.0125595800911\n",
      "Loss for  25163 th iteration => 0.0125589994568\n",
      "Loss for  25164 th iteration => 0.0125584579387\n",
      "Loss for  25165 th iteration => 0.0125578754432\n",
      "Loss for  25166 th iteration => 0.0125573359668\n",
      "Loss for  25167 th iteration => 0.0125567517605\n",
      "Loss for  25168 th iteration => 0.0125562141749\n",
      "Loss for  25169 th iteration => 0.012555632023\n",
      "Loss for  25170 th iteration => 0.0125551054297\n",
      "Loss for  25171 th iteration => 0.0125545088065\n",
      "Loss for  25172 th iteration => 0.0125539762219\n",
      "Loss for  25173 th iteration => 0.0125534930587\n",
      "Loss for  25174 th iteration => 0.0125528546017\n",
      "Loss for  25175 th iteration => 0.0125523698034\n",
      "Loss for  25176 th iteration => 0.0125517342705\n",
      "Loss for  25177 th iteration => 0.0125512573463\n",
      "Loss for  25178 th iteration => 0.0125506135457\n",
      "Loss for  25179 th iteration => 0.012550126981\n",
      "Loss for  25180 th iteration => 0.012549492667\n",
      "Loss for  25181 th iteration => 0.0125489974695\n",
      "Loss for  25182 th iteration => 0.0125483716392\n",
      "Loss for  25183 th iteration => 0.0125478701896\n",
      "Loss for  25184 th iteration => 0.0125473635851\n",
      "Loss for  25185 th iteration => 0.0125467501851\n",
      "Loss for  25186 th iteration => 0.0125462430446\n",
      "Loss for  25187 th iteration => 0.0125456304038\n",
      "Loss for  25188 th iteration => 0.0125451227618\n",
      "Loss for  25189 th iteration => 0.0125445108445\n",
      "Loss for  25190 th iteration => 0.012544002735\n",
      "Loss for  25191 th iteration => 0.012543410321\n",
      "Loss for  25192 th iteration => 0.0125429620783\n",
      "Loss for  25193 th iteration => 0.0125423004002\n",
      "Loss for  25194 th iteration => 0.0125418603955\n",
      "Loss for  25195 th iteration => 0.0125411892922\n",
      "Loss for  25196 th iteration => 0.0125407314761\n",
      "Loss for  25197 th iteration => 0.0125400778305\n",
      "Loss for  25198 th iteration => 0.0125396034171\n",
      "Loss for  25199 th iteration => 0.0125389660265\n",
      "Loss for  25200 th iteration => 0.0125384762031\n",
      "Loss for  25201 th iteration => 0.0125378538911\n",
      "Loss for  25202 th iteration => 0.0125373498191\n",
      "Loss for  25203 th iteration => 0.0125367414351\n",
      "Loss for  25204 th iteration => 0.0125362254002\n",
      "Loss for  25205 th iteration => 0.0125356474286\n",
      "Loss for  25206 th iteration => 0.012535098085\n",
      "Loss for  25207 th iteration => 0.012534527451\n",
      "Loss for  25208 th iteration => 0.0125339912764\n",
      "Loss for  25209 th iteration => 0.0125334998196\n",
      "Loss for  25210 th iteration => 0.0125328769747\n",
      "Loss for  25211 th iteration => 0.0125323784255\n",
      "Loss for  25212 th iteration => 0.0125317594283\n",
      "Loss for  25213 th iteration => 0.012531251519\n",
      "Loss for  25214 th iteration => 0.0125306417294\n",
      "Loss for  25215 th iteration => 0.0125301254602\n",
      "Loss for  25216 th iteration => 0.0125295238828\n",
      "Loss for  25217 th iteration => 0.0125290002327\n",
      "Loss for  25218 th iteration => 0.0125284058936\n",
      "Loss for  25219 th iteration => 0.0125278926321\n",
      "Loss for  25220 th iteration => 0.0125273884568\n",
      "Loss for  25221 th iteration => 0.0125267761461\n",
      "Loss for  25222 th iteration => 0.012526271504\n",
      "Loss for  25223 th iteration => 0.01252565988\n",
      "Loss for  25224 th iteration => 0.0125251642904\n",
      "Loss for  25225 th iteration => 0.0125246343244\n",
      "Loss for  25226 th iteration => 0.0125240540599\n",
      "Loss for  25227 th iteration => 0.0125235243307\n",
      "Loss for  25228 th iteration => 0.0125229459995\n",
      "Loss for  25229 th iteration => 0.0125224084292\n",
      "Loss for  25230 th iteration => 0.0125218281551\n",
      "Loss for  25231 th iteration => 0.012521292707\n",
      "Loss for  25232 th iteration => 0.0125207106416\n",
      "Loss for  25233 th iteration => 0.0125201771638\n",
      "Loss for  25234 th iteration => 0.0125195934553\n",
      "Loss for  25235 th iteration => 0.0125190617989\n",
      "Loss for  25236 th iteration => 0.0125184765925\n",
      "Loss for  25237 th iteration => 0.0125179466122\n",
      "Loss for  25238 th iteration => 0.0125173649405\n",
      "Loss for  25239 th iteration => 0.0125168364354\n",
      "Loss for  25240 th iteration => 0.0125162481871\n",
      "Loss for  25241 th iteration => 0.0125157217943\n",
      "Loss for  25242 th iteration => 0.0125152371202\n",
      "Loss for  25243 th iteration => 0.0125146068087\n",
      "Loss for  25244 th iteration => 0.0125141206503\n",
      "Loss for  25245 th iteration => 0.0125134920962\n",
      "Loss for  25246 th iteration => 0.0125130109796\n",
      "Loss for  25247 th iteration => 0.0125123777541\n",
      "Loss for  25248 th iteration => 0.0125118877741\n",
      "Loss for  25249 th iteration => 0.0125112632625\n",
      "Loss for  25250 th iteration => 0.0125107654051\n",
      "Loss for  25251 th iteration => 0.0125101486261\n",
      "Loss for  25252 th iteration => 0.0125096503785\n",
      "Loss for  25253 th iteration => 0.0125091474406\n",
      "Loss for  25254 th iteration => 0.0125085370249\n",
      "Loss for  25255 th iteration => 0.012508033594\n",
      "Loss for  25256 th iteration => 0.0125074238912\n",
      "Loss for  25257 th iteration => 0.0125069200008\n",
      "Loss for  25258 th iteration => 0.0125063109764\n",
      "Loss for  25259 th iteration => 0.0125058077952\n",
      "Loss for  25260 th iteration => 0.0125052922274\n",
      "Loss for  25261 th iteration => 0.0125047009023\n",
      "Loss for  25262 th iteration => 0.012504180549\n",
      "Loss for  25263 th iteration => 0.0125036067768\n",
      "Loss for  25264 th iteration => 0.0125030679995\n",
      "Loss for  25265 th iteration => 0.0125024922953\n",
      "Loss for  25266 th iteration => 0.0125019556285\n",
      "Loss for  25267 th iteration => 0.012501378143\n",
      "Loss for  25268 th iteration => 0.0125008434356\n",
      "Loss for  25269 th iteration => 0.0125002643161\n",
      "Loss for  25270 th iteration => 0.0124997314203\n",
      "Loss for  25271 th iteration => 0.012499150811\n",
      "Loss for  25272 th iteration => 0.0124986195822\n",
      "Loss for  25273 th iteration => 0.0124980378831\n",
      "Loss for  25274 th iteration => 0.0124975233248\n",
      "Loss for  25275 th iteration => 0.0124969244777\n",
      "Loss for  25276 th iteration => 0.0124964033247\n",
      "Loss for  25277 th iteration => 0.0124958330706\n",
      "Loss for  25278 th iteration => 0.0124953808471\n",
      "Loss for  25279 th iteration => 0.0124947279914\n",
      "Loss for  25280 th iteration => 0.0124942617786\n",
      "Loss for  25281 th iteration => 0.0124936228141\n",
      "Loss for  25282 th iteration => 0.0124931405917\n",
      "Loss for  25283 th iteration => 0.0124925318201\n",
      "Loss for  25284 th iteration => 0.0124920259312\n",
      "Loss for  25285 th iteration => 0.0124914206417\n",
      "Loss for  25286 th iteration => 0.0124909072401\n",
      "Loss for  25287 th iteration => 0.0124903093206\n",
      "Loss for  25288 th iteration => 0.012489807234\n",
      "Loss for  25289 th iteration => 0.012489302233\n",
      "Loss for  25290 th iteration => 0.0124886973157\n",
      "Loss for  25291 th iteration => 0.0124881918536\n",
      "Loss for  25292 th iteration => 0.0124875876158\n",
      "Loss for  25293 th iteration => 0.0124870817256\n",
      "Loss for  25294 th iteration => 0.0124864781333\n",
      "Loss for  25295 th iteration => 0.0124859718471\n",
      "Loss for  25296 th iteration => 0.0124853688672\n",
      "Loss for  25297 th iteration => 0.0124848697966\n",
      "Loss for  25298 th iteration => 0.0124843480992\n",
      "Loss for  25299 th iteration => 0.0124837710258\n",
      "Loss for  25300 th iteration => 0.0124832389544\n",
      "Loss for  25301 th iteration => 0.0124826601067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  25302 th iteration => 0.012482129987\n",
      "Loss for  25303 th iteration => 0.0124815495121\n",
      "Loss for  25304 th iteration => 0.0124810211966\n",
      "Loss for  25305 th iteration => 0.0124804392383\n",
      "Loss for  25306 th iteration => 0.0124799125828\n",
      "Loss for  25307 th iteration => 0.012479329282\n",
      "Loss for  25308 th iteration => 0.0124788041452\n",
      "Loss for  25309 th iteration => 0.0124782254707\n",
      "Loss for  25310 th iteration => 0.012477703705\n",
      "Loss for  25311 th iteration => 0.0124771153634\n",
      "Loss for  25312 th iteration => 0.0124765910811\n",
      "Loss for  25313 th iteration => 0.0124761114787\n",
      "Loss for  25314 th iteration => 0.0124754828713\n",
      "Loss for  25315 th iteration => 0.0124750018843\n",
      "Loss for  25316 th iteration => 0.0124743748601\n",
      "Loss for  25317 th iteration => 0.0124738925984\n",
      "Loss for  25318 th iteration => 0.0124732725739\n",
      "Loss for  25319 th iteration => 0.0124727947169\n",
      "Loss for  25320 th iteration => 0.0124721647984\n",
      "Loss for  25321 th iteration => 0.0124716792682\n",
      "Loss for  25322 th iteration => 0.0124710568799\n",
      "Loss for  25323 th iteration => 0.0124705653988\n",
      "Loss for  25324 th iteration => 0.0124700630957\n",
      "Loss for  25325 th iteration => 0.0124694587973\n",
      "Loss for  25326 th iteration => 0.0124689560217\n",
      "Loss for  25327 th iteration => 0.0124683524137\n",
      "Loss for  25328 th iteration => 0.0124678491984\n",
      "Loss for  25329 th iteration => 0.0124672466781\n",
      "Loss for  25330 th iteration => 0.0124666786146\n",
      "Loss for  25331 th iteration => 0.0124661928511\n",
      "Loss for  25332 th iteration => 0.0124655960954\n",
      "Loss for  25333 th iteration => 0.0124650974828\n",
      "Loss for  25334 th iteration => 0.0124645671439\n",
      "Loss for  25335 th iteration => 0.0124639898607\n",
      "Loss for  25336 th iteration => 0.0124634613863\n",
      "Loss for  25337 th iteration => 0.0124628825598\n",
      "Loss for  25338 th iteration => 0.0124623558047\n",
      "Loss for  25339 th iteration => 0.0124617755768\n",
      "Loss for  25340 th iteration => 0.0124612503988\n",
      "Loss for  25341 th iteration => 0.0124606689081\n",
      "Loss for  25342 th iteration => 0.0124601451683\n",
      "Loss for  25343 th iteration => 0.012459563716\n",
      "Loss for  25344 th iteration => 0.0124590422721\n",
      "Loss for  25345 th iteration => 0.012458456763\n",
      "Loss for  25346 th iteration => 0.0124579381281\n",
      "Loss for  25347 th iteration => 0.0124574603305\n",
      "Loss for  25348 th iteration => 0.0124568331372\n",
      "Loss for  25349 th iteration => 0.0124563540102\n",
      "Loss for  25350 th iteration => 0.0124557283438\n",
      "Loss for  25351 th iteration => 0.0124552479956\n",
      "Loss for  25352 th iteration => 0.0124546253766\n",
      "Loss for  25353 th iteration => 0.0124541493411\n",
      "Loss for  25354 th iteration => 0.0124535207248\n",
      "Loss for  25355 th iteration => 0.0124530373109\n",
      "Loss for  25356 th iteration => 0.0124524159318\n",
      "Loss for  25357 th iteration => 0.0124519315135\n",
      "Loss for  25358 th iteration => 0.0124514304094\n",
      "Loss for  25359 th iteration => 0.0124508281329\n",
      "Loss for  25360 th iteration => 0.0124503265757\n",
      "Loss for  25361 th iteration => 0.0124497249687\n",
      "Loss for  25362 th iteration => 0.0124492229909\n",
      "Loss for  25363 th iteration => 0.0124486220197\n",
      "Loss for  25364 th iteration => 0.0124481196531\n",
      "Loss for  25365 th iteration => 0.0124475192851\n",
      "Loss for  25366 th iteration => 0.0124470165608\n",
      "Loss for  25367 th iteration => 0.0124464336064\n",
      "Loss for  25368 th iteration => 0.0124459953074\n",
      "Loss for  25369 th iteration => 0.0124453369846\n",
      "Loss for  25370 th iteration => 0.0124448936689\n",
      "Loss for  25371 th iteration => 0.0124442457163\n",
      "Loss for  25372 th iteration => 0.0124437693864\n",
      "Loss for  25373 th iteration => 0.0124431913699\n",
      "Loss for  25374 th iteration => 0.0124426672048\n",
      "Loss for  25375 th iteration => 0.0124420876864\n",
      "Loss for  25376 th iteration => 0.0124415651983\n",
      "Loss for  25377 th iteration => 0.0124409843185\n",
      "Loss for  25378 th iteration => 0.0124404633667\n",
      "Loss for  25379 th iteration => 0.0124398836595\n",
      "Loss for  25380 th iteration => 0.0124393616319\n",
      "Loss for  25381 th iteration => 0.0124387802117\n",
      "Loss for  25382 th iteration => 0.0124382519789\n",
      "Loss for  25383 th iteration => 0.0124376766988\n",
      "Loss for  25384 th iteration => 0.0124371431001\n",
      "Loss for  25385 th iteration => 0.0124365731235\n",
      "Loss for  25386 th iteration => 0.0124360438962\n",
      "Loss for  25387 th iteration => 0.012435563833\n",
      "Loss for  25388 th iteration => 0.0124349435221\n",
      "Loss for  25389 th iteration => 0.0124344688139\n",
      "Loss for  25390 th iteration => 0.0124338421563\n",
      "Loss for  25391 th iteration => 0.012433360402\n",
      "Loss for  25392 th iteration => 0.0124327406512\n",
      "Loss for  25393 th iteration => 0.0124322580836\n",
      "Loss for  25394 th iteration => 0.0124317589263\n",
      "Loss for  25395 th iteration => 0.0124311581022\n",
      "Loss for  25396 th iteration => 0.0124306585104\n",
      "Loss for  25397 th iteration => 0.0124300583357\n",
      "Loss for  25398 th iteration => 0.0124295583413\n",
      "Loss for  25399 th iteration => 0.012428958783\n",
      "Loss for  25400 th iteration => 0.0124284584174\n",
      "Loss for  25401 th iteration => 0.0124278594431\n",
      "Loss for  25402 th iteration => 0.0124273587368\n",
      "Loss for  25403 th iteration => 0.0124267759477\n",
      "Loss for  25404 th iteration => 0.0124263426806\n",
      "Loss for  25405 th iteration => 0.0124256786767\n",
      "Loss for  25406 th iteration => 0.0124252121195\n",
      "Loss for  25407 th iteration => 0.0124246370952\n",
      "Loss for  25408 th iteration => 0.012424113127\n",
      "Loss for  25409 th iteration => 0.0124235366293\n",
      "Loss for  25410 th iteration => 0.0124230143089\n",
      "Loss for  25411 th iteration => 0.0124224364772\n",
      "Loss for  25412 th iteration => 0.0124219156649\n",
      "Loss for  25413 th iteration => 0.0124213366355\n",
      "Loss for  25414 th iteration => 0.0124208171946\n",
      "Loss for  25415 th iteration => 0.0124202372938\n",
      "Loss for  25416 th iteration => 0.0124197205707\n",
      "Loss for  25417 th iteration => 0.0124191370778\n",
      "Loss for  25418 th iteration => 0.0124186146931\n",
      "Loss for  25419 th iteration => 0.0124180367986\n",
      "Loss for  25420 th iteration => 0.0124175174171\n",
      "Loss for  25421 th iteration => 0.0124170402012\n",
      "Loss for  25422 th iteration => 0.0124164193942\n",
      "Loss for  25423 th iteration => 0.0124159410663\n",
      "Loss for  25424 th iteration => 0.0124153215654\n",
      "Loss for  25425 th iteration => 0.0124148422284\n",
      "Loss for  25426 th iteration => 0.0124142285225\n",
      "Loss for  25427 th iteration => 0.0124137537301\n",
      "Loss for  25428 th iteration => 0.0124131304425\n",
      "Loss for  25429 th iteration => 0.0124126517237\n",
      "Loss for  25430 th iteration => 0.0124121550149\n",
      "Loss for  25431 th iteration => 0.0124115551047\n",
      "Loss for  25432 th iteration => 0.0124110579529\n",
      "Loss for  25433 th iteration => 0.0124104586999\n",
      "Loss for  25434 th iteration => 0.0124099611371\n",
      "Loss for  25435 th iteration => 0.0124093625083\n",
      "Loss for  25436 th iteration => 0.0124088645657\n",
      "Loss for  25437 th iteration => 0.012408267441\n",
      "Loss for  25438 th iteration => 0.0124077020184\n",
      "Loss for  25439 th iteration => 0.0124072235933\n",
      "Loss for  25440 th iteration => 0.012406628887\n",
      "Loss for  25441 th iteration => 0.0124061377768\n",
      "Loss for  25442 th iteration => 0.0124056120529\n",
      "Loss for  25443 th iteration => 0.0124050404979\n",
      "Loss for  25444 th iteration => 0.0124045164144\n",
      "Loss for  25445 th iteration => 0.0124039435314\n",
      "Loss for  25446 th iteration => 0.0124034209493\n",
      "Loss for  25447 th iteration => 0.0124028468739\n",
      "Loss for  25448 th iteration => 0.0124023256572\n",
      "Loss for  25449 th iteration => 0.0124017505223\n",
      "Loss for  25450 th iteration => 0.0124012305378\n",
      "Loss for  25451 th iteration => 0.0124006544733\n",
      "Loss for  25452 th iteration => 0.0124001355908\n",
      "Loss for  25453 th iteration => 0.0123995613994\n",
      "Loss for  25454 th iteration => 0.0123990523473\n",
      "Loss for  25455 th iteration => 0.0123985803849\n",
      "Loss for  25456 th iteration => 0.0123979574895\n",
      "Loss for  25457 th iteration => 0.0123974842608\n",
      "Loss for  25458 th iteration => 0.0123968628263\n",
      "Loss for  25459 th iteration => 0.012396388437\n",
      "Loss for  25460 th iteration => 0.0123957683566\n",
      "Loss for  25461 th iteration => 0.0123952929103\n",
      "Loss for  25462 th iteration => 0.0123946757534\n",
      "Loss for  25463 th iteration => 0.0123942106706\n",
      "Loss for  25464 th iteration => 0.0123935812062\n",
      "Loss for  25465 th iteration => 0.0123930529148\n",
      "Loss for  25466 th iteration => 0.0123925446015\n",
      "Loss for  25467 th iteration => 0.0123919579861\n",
      "Loss for  25468 th iteration => 0.0123914648471\n",
      "Loss for  25469 th iteration => 0.0123909671976\n",
      "Loss for  25470 th iteration => 0.0123903719182\n",
      "Loss for  25471 th iteration => 0.0123898739145\n",
      "Loss for  25472 th iteration => 0.0123892792001\n",
      "Loss for  25473 th iteration => 0.0123887808722\n",
      "Loss for  25474 th iteration => 0.0123881866917\n",
      "Loss for  25475 th iteration => 0.0123876880693\n",
      "Loss for  25476 th iteration => 0.0123870943923\n",
      "Loss for  25477 th iteration => 0.012386601664\n",
      "Loss for  25478 th iteration => 0.012386088011\n",
      "Loss for  25479 th iteration => 0.012385521303\n",
      "Loss for  25480 th iteration => 0.0123849957339\n",
      "Loss for  25481 th iteration => 0.0123844276175\n",
      "Loss for  25482 th iteration => 0.0123839036294\n",
      "Loss for  25483 th iteration => 0.0123833342415\n",
      "Loss for  25484 th iteration => 0.0123828116973\n",
      "Loss for  25485 th iteration => 0.0123822411718\n",
      "Loss for  25486 th iteration => 0.0123817199374\n",
      "Loss for  25487 th iteration => 0.0123811484052\n",
      "Loss for  25488 th iteration => 0.0123806283492\n",
      "Loss for  25489 th iteration => 0.0123800559387\n",
      "Loss for  25490 th iteration => 0.0123795369326\n",
      "Loss for  25491 th iteration => 0.0123789680448\n",
      "Loss for  25492 th iteration => 0.0123784365649\n",
      "Loss for  25493 th iteration => 0.0123779182816\n",
      "Loss for  25494 th iteration => 0.0123774437607\n",
      "Loss for  25495 th iteration => 0.0123768270485\n",
      "Loss for  25496 th iteration => 0.0123763514275\n",
      "Loss for  25497 th iteration => 0.0123757377354\n",
      "Loss for  25498 th iteration => 0.012375259423\n",
      "Loss for  25499 th iteration => 0.0123746464548\n",
      "Loss for  25500 th iteration => 0.0123741614276\n",
      "Loss for  25501 th iteration => 0.0123735550373\n",
      "Loss for  25502 th iteration => 0.0123730642164\n",
      "Loss for  25503 th iteration => 0.0123724634878\n",
      "Loss for  25504 th iteration => 0.0123719677748\n",
      "Loss for  25505 th iteration => 0.0123713718108\n",
      "Loss for  25506 th iteration => 0.0123708816168\n",
      "Loss for  25507 th iteration => 0.0123703847499\n",
      "Loss for  25508 th iteration => 0.0123697922521\n",
      "Loss for  25509 th iteration => 0.0123692950837\n",
      "Loss for  25510 th iteration => 0.0123687030956\n",
      "Loss for  25511 th iteration => 0.0123682060487\n",
      "Loss for  25512 th iteration => 0.0123677105073\n",
      "Loss for  25513 th iteration => 0.0123671226894\n",
      "Loss for  25514 th iteration => 0.0123665499315\n",
      "Loss for  25515 th iteration => 0.0123661125992\n",
      "Loss for  25516 th iteration => 0.0123654663495\n",
      "Loss for  25517 th iteration => 0.0123650256713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  25518 th iteration => 0.012364387939\n",
      "Loss for  25519 th iteration => 0.0123639172476\n",
      "Loss for  25520 th iteration => 0.0123633459471\n",
      "Loss for  25521 th iteration => 0.0123628287346\n",
      "Loss for  25522 th iteration => 0.0123622562548\n",
      "Loss for  25523 th iteration => 0.0123617403931\n",
      "Loss for  25524 th iteration => 0.0123611668657\n",
      "Loss for  25525 th iteration => 0.0123606522227\n",
      "Loss for  25526 th iteration => 0.012360078863\n",
      "Loss for  25527 th iteration => 0.0123595670659\n",
      "Loss for  25528 th iteration => 0.0123589888553\n",
      "Loss for  25529 th iteration => 0.0123584719171\n",
      "Loss for  25530 th iteration => 0.0123578987863\n",
      "Loss for  25531 th iteration => 0.0123573775138\n",
      "Loss for  25532 th iteration => 0.0123568086588\n",
      "Loss for  25533 th iteration => 0.0123562869358\n",
      "Loss for  25534 th iteration => 0.0123558154841\n",
      "Loss for  25535 th iteration => 0.0123551994263\n",
      "Loss for  25536 th iteration => 0.012354727121\n",
      "Loss for  25537 th iteration => 0.0123541152638\n",
      "Loss for  25538 th iteration => 0.0123535902377\n",
      "Loss for  25539 th iteration => 0.0123530910044\n",
      "Loss for  25540 th iteration => 0.0123525020201\n",
      "Loss for  25541 th iteration => 0.0123520184065\n",
      "Loss for  25542 th iteration => 0.0123515231801\n",
      "Loss for  25543 th iteration => 0.0123509322328\n",
      "Loss for  25544 th iteration => 0.0123504366688\n",
      "Loss for  25545 th iteration => 0.0123498462677\n",
      "Loss for  25546 th iteration => 0.0123493503958\n",
      "Loss for  25547 th iteration => 0.0123487605102\n",
      "Loss for  25548 th iteration => 0.0123482643593\n",
      "Loss for  25549 th iteration => 0.0123476749595\n",
      "Loss for  25550 th iteration => 0.0123471785581\n",
      "Loss for  25551 th iteration => 0.0123465896148\n",
      "Loss for  25552 th iteration => 0.0123460967668\n",
      "Loss for  25553 th iteration => 0.0123455894179\n",
      "Loss for  25554 th iteration => 0.0123450261826\n",
      "Loss for  25555 th iteration => 0.0123445040555\n",
      "Loss for  25556 th iteration => 0.0123439395629\n",
      "Loss for  25557 th iteration => 0.0123434188639\n",
      "Loss for  25558 th iteration => 0.0123428532468\n",
      "Loss for  25559 th iteration => 0.0123423338429\n",
      "Loss for  25560 th iteration => 0.0123417674474\n",
      "Loss for  25561 th iteration => 0.0123412076103\n",
      "Loss for  25562 th iteration => 0.0123407274419\n",
      "Loss for  25563 th iteration => 0.0123401262309\n",
      "Loss for  25564 th iteration => 0.0123396389922\n",
      "Loss for  25565 th iteration => 0.0123390456142\n",
      "Loss for  25566 th iteration => 0.0123385456308\n",
      "Loss for  25567 th iteration => 0.0123379837044\n",
      "Loss for  25568 th iteration => 0.012337471721\n",
      "Loss for  25569 th iteration => 0.0123370008237\n",
      "Loss for  25570 th iteration => 0.0123363874141\n",
      "Loss for  25571 th iteration => 0.0123359155003\n",
      "Loss for  25572 th iteration => 0.0123353032972\n",
      "Loss for  25573 th iteration => 0.0123348304666\n",
      "Loss for  25574 th iteration => 0.0123342226007\n",
      "Loss for  25575 th iteration => 0.0123337538903\n",
      "Loss for  25576 th iteration => 0.0123331380934\n",
      "Loss for  25577 th iteration => 0.012332663688\n",
      "Loss for  25578 th iteration => 0.0123320534551\n",
      "Loss for  25579 th iteration => 0.012331574248\n",
      "Loss for  25580 th iteration => 0.0123309686907\n",
      "Loss for  25581 th iteration => 0.012330485586\n",
      "Loss for  25582 th iteration => 0.0123299943423\n",
      "Loss for  25583 th iteration => 0.0123293696339\n",
      "Loss for  25584 th iteration => 0.0123288673914\n",
      "Loss for  25585 th iteration => 0.0123283729075\n",
      "Loss for  25586 th iteration => 0.0123277852503\n",
      "Loss for  25587 th iteration => 0.0123272905401\n",
      "Loss for  25588 th iteration => 0.0123267033136\n",
      "Loss for  25589 th iteration => 0.0123262158433\n",
      "Loss for  25590 th iteration => 0.0123257068183\n",
      "Loss for  25591 th iteration => 0.0123251384368\n",
      "Loss for  25592 th iteration => 0.0123246324077\n",
      "Loss for  25593 th iteration => 0.0123240687116\n",
      "Loss for  25594 th iteration => 0.0123235506068\n",
      "Loss for  25595 th iteration => 0.0123229857761\n",
      "Loss for  25596 th iteration => 0.0123224689758\n",
      "Loss for  25597 th iteration => 0.0123219031401\n",
      "Loss for  25598 th iteration => 0.0123213875144\n",
      "Loss for  25599 th iteration => 0.0123208208006\n",
      "Loss for  25600 th iteration => 0.0123203062223\n",
      "Loss for  25601 th iteration => 0.0123197392521\n",
      "Loss for  25602 th iteration => 0.0123191920423\n",
      "Loss for  25603 th iteration => 0.012318695045\n",
      "Loss for  25604 th iteration => 0.0123181135831\n",
      "Loss for  25605 th iteration => 0.0123176158266\n",
      "Loss for  25606 th iteration => 0.0123170547975\n",
      "Loss for  25607 th iteration => 0.0123165411122\n",
      "Loss for  25608 th iteration => 0.0123160704509\n",
      "Loss for  25609 th iteration => 0.0123154603795\n",
      "Loss for  25610 th iteration => 0.0123149887889\n",
      "Loss for  25611 th iteration => 0.0123143808704\n",
      "Loss for  25612 th iteration => 0.0123139131314\n",
      "Loss for  25613 th iteration => 0.012313299788\n",
      "Loss for  25614 th iteration => 0.0123128262781\n",
      "Loss for  25615 th iteration => 0.0123122185747\n",
      "Loss for  25616 th iteration => 0.0123117401859\n",
      "Loss for  25617 th iteration => 0.0123111372349\n",
      "Loss for  25618 th iteration => 0.0123106548408\n",
      "Loss for  25619 th iteration => 0.0123100557734\n",
      "Loss for  25620 th iteration => 0.012309571495\n",
      "Loss for  25621 th iteration => 0.0123090799653\n",
      "Loss for  25622 th iteration => 0.0123084615633\n",
      "Loss for  25623 th iteration => 0.0123079587003\n",
      "Loss for  25624 th iteration => 0.0123074642753\n",
      "Loss for  25625 th iteration => 0.0123068801505\n",
      "Loss for  25626 th iteration => 0.0123063945074\n",
      "Loss for  25627 th iteration => 0.0123058897287\n",
      "Loss for  25628 th iteration => 0.0123053203581\n",
      "Loss for  25629 th iteration => 0.0123048148985\n",
      "Loss for  25630 th iteration => 0.0123042523987\n",
      "Loss for  25631 th iteration => 0.0123037364658\n",
      "Loss for  25632 th iteration => 0.0123031729019\n",
      "Loss for  25633 th iteration => 0.0123026582021\n",
      "Loss for  25634 th iteration => 0.012302093702\n",
      "Loss for  25635 th iteration => 0.0123015801072\n",
      "Loss for  25636 th iteration => 0.0123010147958\n",
      "Loss for  25637 th iteration => 0.0123005021808\n",
      "Loss for  25638 th iteration => 0.0122999361805\n",
      "Loss for  25639 th iteration => 0.0122994244227\n",
      "Loss for  25640 th iteration => 0.0122988593709\n",
      "Loss for  25641 th iteration => 0.0122983141962\n",
      "Loss for  25642 th iteration => 0.012297818352\n",
      "Loss for  25643 th iteration => 0.0122972568038\n",
      "Loss for  25644 th iteration => 0.0122967477382\n",
      "Loss for  25645 th iteration => 0.0122962800719\n",
      "Loss for  25646 th iteration => 0.0122956703814\n",
      "Loss for  25647 th iteration => 0.0122952018286\n",
      "Loss for  25648 th iteration => 0.0122945932121\n",
      "Loss for  25649 th iteration => 0.012294123869\n",
      "Loss for  25650 th iteration => 0.0122935189233\n",
      "Loss for  25651 th iteration => 0.0122930527959\n",
      "Loss for  25652 th iteration => 0.0122924411316\n",
      "Loss for  25653 th iteration => 0.0122919700199\n",
      "Loss for  25654 th iteration => 0.0122913632133\n",
      "Loss for  25655 th iteration => 0.0122908879901\n",
      "Loss for  25656 th iteration => 0.012290285173\n",
      "Loss for  25657 th iteration => 0.0122898084983\n",
      "Loss for  25658 th iteration => 0.0122893175223\n",
      "Loss for  25659 th iteration => 0.0122886960291\n",
      "Loss for  25660 th iteration => 0.0122882007253\n",
      "Loss for  25661 th iteration => 0.0122877066451\n",
      "Loss for  25662 th iteration => 0.0122871255295\n",
      "Loss for  25663 th iteration => 0.0122866312714\n",
      "Loss for  25664 th iteration => 0.0122860505353\n",
      "Loss for  25665 th iteration => 0.0122855629496\n",
      "Loss for  25666 th iteration => 0.0122850627271\n",
      "Loss for  25667 th iteration => 0.0122844922225\n",
      "Loss for  25668 th iteration => 0.0122839918775\n",
      "Loss for  25669 th iteration => 0.0122834307495\n",
      "Loss for  25670 th iteration => 0.0122829169805\n",
      "Loss for  25671 th iteration => 0.0122823548644\n",
      "Loss for  25672 th iteration => 0.0122818422515\n",
      "Loss for  25673 th iteration => 0.0122812792731\n",
      "Loss for  25674 th iteration => 0.0122807676904\n",
      "Loss for  25675 th iteration => 0.0122802048436\n",
      "Loss for  25676 th iteration => 0.0122796513942\n",
      "Loss for  25677 th iteration => 0.0122791757947\n",
      "Loss for  25678 th iteration => 0.0122785796363\n",
      "Loss for  25679 th iteration => 0.012278098859\n",
      "Loss for  25680 th iteration => 0.0122775095293\n",
      "Loss for  25681 th iteration => 0.0122770160384\n",
      "Loss for  25682 th iteration => 0.0122764574162\n",
      "Loss for  25683 th iteration => 0.0122759508255\n",
      "Loss for  25684 th iteration => 0.0122754858543\n",
      "Loss for  25685 th iteration => 0.0122748770119\n",
      "Loss for  25686 th iteration => 0.0122744111885\n",
      "Loss for  25687 th iteration => 0.0122738033847\n",
      "Loss for  25688 th iteration => 0.0122733368042\n",
      "Loss for  25689 th iteration => 0.0122727306584\n",
      "Loss for  25690 th iteration => 0.0122722744058\n",
      "Loss for  25691 th iteration => 0.0122716563494\n",
      "Loss for  25692 th iteration => 0.0122711953107\n",
      "Loss for  25693 th iteration => 0.0122705835899\n",
      "Loss for  25694 th iteration => 0.0122700634874\n",
      "Loss for  25695 th iteration => 0.0122695636332\n",
      "Loss for  25696 th iteration => 0.0122689887726\n",
      "Loss for  25697 th iteration => 0.012268503811\n",
      "Loss for  25698 th iteration => 0.0122680119603\n",
      "Loss for  25699 th iteration => 0.0122674319509\n",
      "Loss for  25700 th iteration => 0.0122669399052\n",
      "Loss for  25701 th iteration => 0.0122663602919\n",
      "Loss for  25702 th iteration => 0.0122658680775\n",
      "Loss for  25703 th iteration => 0.0122652888333\n",
      "Loss for  25704 th iteration => 0.0122647973506\n",
      "Loss for  25705 th iteration => 0.0122643068994\n",
      "Loss for  25706 th iteration => 0.0122637311691\n",
      "Loss for  25707 th iteration => 0.0122631698903\n",
      "Loss for  25708 th iteration => 0.0122627331426\n",
      "Loss for  25709 th iteration => 0.0122621080148\n",
      "Loss for  25710 th iteration => 0.012261683942\n",
      "Loss for  25711 th iteration => 0.0122610413562\n",
      "Loss for  25712 th iteration => 0.012260574141\n",
      "Loss for  25713 th iteration => 0.0122600071013\n",
      "Loss for  25714 th iteration => 0.012259503171\n",
      "Loss for  25715 th iteration => 0.0122589354915\n",
      "Loss for  25716 th iteration => 0.0122584323679\n",
      "Loss for  25717 th iteration => 0.0122578641666\n",
      "Loss for  25718 th iteration => 0.0122573617313\n",
      "Loss for  25719 th iteration => 0.012256798128\n",
      "Loss for  25720 th iteration => 0.0122562976698\n",
      "Loss for  25721 th iteration => 0.0122557252723\n",
      "Loss for  25722 th iteration => 0.0122552214163\n",
      "Loss for  25723 th iteration => 0.0122546523617\n",
      "Loss for  25724 th iteration => 0.0122541458653\n",
      "Loss for  25725 th iteration => 0.0122536004148\n",
      "Loss for  25726 th iteration => 0.012253146442\n",
      "Loss for  25727 th iteration => 0.0122525481215\n",
      "Loss for  25728 th iteration => 0.012252075869\n",
      "Loss for  25729 th iteration => 0.0122514770209\n",
      "Loss for  25730 th iteration => 0.01225100062\n",
      "Loss for  25731 th iteration => 0.0122504057984\n",
      "Loss for  25732 th iteration => 0.0122499260993\n",
      "Loss for  25733 th iteration => 0.0122493344587\n",
      "Loss for  25734 th iteration => 0.0122488619373\n",
      "Loss for  25735 th iteration => 0.0122483724753\n",
      "Loss for  25736 th iteration => 0.0122477934093\n",
      "Loss for  25737 th iteration => 0.0122473037487\n",
      "Loss for  25738 th iteration => 0.0122467250816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  25739 th iteration => 0.0122462352486\n",
      "Loss for  25740 th iteration => 0.0122456569533\n",
      "Loss for  25741 th iteration => 0.0122451686616\n",
      "Loss for  25742 th iteration => 0.0122445807046\n",
      "Loss for  25743 th iteration => 0.0122441486778\n",
      "Loss for  25744 th iteration => 0.0122435836355\n",
      "Loss for  25745 th iteration => 0.0122430830241\n",
      "Loss for  25746 th iteration => 0.0122425233278\n",
      "Loss for  25747 th iteration => 0.0122420150472\n",
      "Loss for  25748 th iteration => 0.0122414545409\n",
      "Loss for  25749 th iteration => 0.0122409472366\n",
      "Loss for  25750 th iteration => 0.0122403860417\n",
      "Loss for  25751 th iteration => 0.0122398795922\n",
      "Loss for  25752 th iteration => 0.0122393178271\n",
      "Loss for  25753 th iteration => 0.0122388121137\n",
      "Loss for  25754 th iteration => 0.0122382498945\n",
      "Loss for  25755 th iteration => 0.0122377448011\n",
      "Loss for  25756 th iteration => 0.0122371843301\n",
      "Loss for  25757 th iteration => 0.0122366409866\n",
      "Loss for  25758 th iteration => 0.012236157153\n",
      "Loss for  25759 th iteration => 0.0122355762426\n",
      "Loss for  25760 th iteration => 0.0122350954776\n",
      "Loss for  25761 th iteration => 0.0122346399027\n",
      "Loss for  25762 th iteration => 0.0122340338766\n",
      "Loss for  25763 th iteration => 0.0122335722063\n",
      "Loss for  25764 th iteration => 0.0122329671825\n",
      "Loss for  25765 th iteration => 0.0122325047881\n",
      "Loss for  25766 th iteration => 0.0122319006722\n",
      "Loss for  25767 th iteration => 0.0122314376455\n",
      "Loss for  25768 th iteration => 0.0122308356884\n",
      "Loss for  25769 th iteration => 0.012230382985\n",
      "Loss for  25770 th iteration => 0.0122297691627\n",
      "Loss for  25771 th iteration => 0.0122292532769\n",
      "Loss for  25772 th iteration => 0.012228761631\n",
      "Loss for  25773 th iteration => 0.0122281855753\n",
      "Loss for  25774 th iteration => 0.0122276913504\n",
      "Loss for  25775 th iteration => 0.0122271177606\n",
      "Loss for  25776 th iteration => 0.0122266371253\n",
      "Loss for  25777 th iteration => 0.0122261472846\n",
      "Loss for  25778 th iteration => 0.0122255723849\n",
      "Loss for  25779 th iteration => 0.0122250823928\n",
      "Loss for  25780 th iteration => 0.0122245078427\n",
      "Loss for  25781 th iteration => 0.0122240177245\n",
      "Loss for  25782 th iteration => 0.012223443498\n",
      "Loss for  25783 th iteration => 0.0122229532784\n",
      "Loss for  25784 th iteration => 0.0122223950275\n",
      "Loss for  25785 th iteration => 0.0122219693023\n",
      "Loss for  25786 th iteration => 0.0122213291991\n",
      "Loss for  25787 th iteration => 0.0122209298534\n",
      "Loss for  25788 th iteration => 0.0122202724166\n",
      "Loss for  25789 th iteration => 0.0122198151296\n",
      "Loss for  25790 th iteration => 0.0122192542984\n",
      "Loss for  25791 th iteration => 0.0122187510578\n",
      "Loss for  25792 th iteration => 0.0122181896824\n",
      "Loss for  25793 th iteration => 0.0122176871512\n",
      "Loss for  25794 th iteration => 0.0122171253467\n",
      "Loss for  25795 th iteration => 0.0122166234097\n",
      "Loss for  25796 th iteration => 0.0122160612887\n",
      "Loss for  25797 th iteration => 0.012215559833\n",
      "Loss for  25798 th iteration => 0.012214998378\n",
      "Loss for  25799 th iteration => 0.0122144803861\n",
      "Loss for  25800 th iteration => 0.0122139637025\n",
      "Loss for  25801 th iteration => 0.0122134143408\n",
      "Loss for  25802 th iteration => 0.0122129090718\n",
      "Loss for  25803 th iteration => 0.012212446805\n",
      "Loss for  25804 th iteration => 0.0122118459544\n",
      "Loss for  25805 th iteration => 0.0122113831289\n",
      "Loss for  25806 th iteration => 0.0122107830189\n",
      "Loss for  25807 th iteration => 0.0122103197228\n",
      "Loss for  25808 th iteration => 0.0122097234618\n",
      "Loss for  25809 th iteration => 0.0122092650429\n",
      "Loss for  25810 th iteration => 0.0122086593122\n",
      "Loss for  25811 th iteration => 0.0122081976901\n",
      "Loss for  25812 th iteration => 0.0122075950465\n",
      "Loss for  25813 th iteration => 0.012207131371\n",
      "Loss for  25814 th iteration => 0.0122066485272\n",
      "Loss for  25815 th iteration => 0.0122060262281\n",
      "Loss for  25816 th iteration => 0.0122055446451\n",
      "Loss for  25817 th iteration => 0.0122050567845\n",
      "Loss for  25818 th iteration => 0.0122044834878\n",
      "Loss for  25819 th iteration => 0.0122039955028\n",
      "Loss for  25820 th iteration => 0.0122034225271\n",
      "Loss for  25821 th iteration => 0.0122029344423\n",
      "Loss for  25822 th iteration => 0.0122023617623\n",
      "Loss for  25823 th iteration => 0.0122018744052\n",
      "Loss for  25824 th iteration => 0.0122013898673\n",
      "Loss for  25825 th iteration => 0.012200818963\n",
      "Loss for  25826 th iteration => 0.012200263215\n",
      "Loss for  25827 th iteration => 0.0121998326251\n",
      "Loss for  25828 th iteration => 0.0121992080944\n",
      "Loss for  25829 th iteration => 0.0121987972207\n",
      "Loss for  25830 th iteration => 0.012198154083\n",
      "Loss for  25831 th iteration => 0.0121976935578\n",
      "Loss for  25832 th iteration => 0.0121971327659\n",
      "Loss for  25833 th iteration => 0.0121966332096\n",
      "Loss for  25834 th iteration => 0.0121960719907\n",
      "Loss for  25835 th iteration => 0.0121955730256\n",
      "Loss for  25836 th iteration => 0.012195011492\n",
      "Loss for  25837 th iteration => 0.0121945130057\n",
      "Loss for  25838 th iteration => 0.0121939518956\n",
      "Loss for  25839 th iteration => 0.0121934644901\n",
      "Loss for  25840 th iteration => 0.0121928906877\n",
      "Loss for  25841 th iteration => 0.0121923737823\n",
      "Loss for  25842 th iteration => 0.0121918553017\n",
      "Loss for  25843 th iteration => 0.0121913112618\n",
      "Loss for  25844 th iteration => 0.0121908038107\n",
      "Loss for  25845 th iteration => 0.0121903414321\n",
      "Loss for  25846 th iteration => 0.0121897444311\n",
      "Loss for  25847 th iteration => 0.0121892816444\n",
      "Loss for  25848 th iteration => 0.0121886872796\n",
      "Loss for  25849 th iteration => 0.0121882303762\n",
      "Loss for  25850 th iteration => 0.0121876265801\n",
      "Loss for  25851 th iteration => 0.0121871668062\n",
      "Loss for  25852 th iteration => 0.0121865657665\n",
      "Loss for  25853 th iteration => 0.0121861046449\n",
      "Loss for  25854 th iteration => 0.012185623277\n",
      "Loss for  25855 th iteration => 0.0121850020376\n",
      "Loss for  25856 th iteration => 0.012184523259\n",
      "Loss for  25857 th iteration => 0.0121840372495\n",
      "Loss for  25858 th iteration => 0.0121834656618\n",
      "Loss for  25859 th iteration => 0.0121829795464\n",
      "Loss for  25860 th iteration => 0.0121824082597\n",
      "Loss for  25861 th iteration => 0.0121819220626\n",
      "Loss for  25862 th iteration => 0.0121813510522\n",
      "Loss for  25863 th iteration => 0.0121808647969\n",
      "Loss for  25864 th iteration => 0.0121803113365\n",
      "Loss for  25865 th iteration => 0.0121798872031\n",
      "Loss for  25866 th iteration => 0.0121792579456\n",
      "Loss for  25867 th iteration => 0.0121788339208\n",
      "Loss for  25868 th iteration => 0.0121782046694\n",
      "Loss for  25869 th iteration => 0.0121777551454\n",
      "Loss for  25870 th iteration => 0.0121771973481\n",
      "Loss for  25871 th iteration => 0.0121766981699\n",
      "Loss for  25872 th iteration => 0.0121761398953\n",
      "Loss for  25873 th iteration => 0.012175641358\n",
      "Loss for  25874 th iteration => 0.0121750827189\n",
      "Loss for  25875 th iteration => 0.0121745847094\n",
      "Loss for  25876 th iteration => 0.0121740258165\n",
      "Loss for  25877 th iteration => 0.0121735282242\n",
      "Loss for  25878 th iteration => 0.0121729698683\n",
      "Loss for  25879 th iteration => 0.0121724406393\n",
      "Loss for  25880 th iteration => 0.0121719517585\n",
      "Loss for  25881 th iteration => 0.0121714011952\n",
      "Loss for  25882 th iteration => 0.0121708905833\n",
      "Loss for  25883 th iteration => 0.012170342287\n",
      "Loss for  25884 th iteration => 0.0121698400109\n",
      "Loss for  25885 th iteration => 0.0121693793583\n",
      "Loss for  25886 th iteration => 0.0121687841697\n",
      "Loss for  25887 th iteration => 0.0121683230775\n",
      "Loss for  25888 th iteration => 0.0121677285081\n",
      "Loss for  25889 th iteration => 0.0121672670612\n",
      "Loss for  25890 th iteration => 0.0121666761104\n",
      "Loss for  25891 th iteration => 0.0121662244954\n",
      "Loss for  25892 th iteration => 0.0121656205394\n",
      "Loss for  25893 th iteration => 0.0121651090523\n",
      "Loss for  25894 th iteration => 0.0121646210906\n",
      "Loss for  25895 th iteration => 0.0121640517103\n",
      "Loss for  25896 th iteration => 0.0121635783283\n",
      "Loss for  25897 th iteration => 0.0121630951395\n",
      "Loss for  25898 th iteration => 0.0121625242572\n",
      "Loss for  25899 th iteration => 0.0121620409513\n",
      "Loss for  25900 th iteration => 0.0121614703805\n",
      "Loss for  25901 th iteration => 0.0121609869816\n",
      "Loss for  25902 th iteration => 0.0121604166977\n",
      "Loss for  25903 th iteration => 0.0121599332295\n",
      "Loss for  25904 th iteration => 0.012159363208\n",
      "Loss for  25905 th iteration => 0.0121588813091\n",
      "Loss for  25906 th iteration => 0.012158310542\n",
      "Loss for  25907 th iteration => 0.0121578721926\n",
      "Loss for  25908 th iteration => 0.0121573181974\n",
      "Loss for  25909 th iteration => 0.0121568185671\n",
      "Loss for  25910 th iteration => 0.0121562641141\n",
      "Loss for  25911 th iteration => 0.0121557651043\n",
      "Loss for  25912 th iteration => 0.0121552103056\n",
      "Loss for  25913 th iteration => 0.0121547118042\n",
      "Loss for  25914 th iteration => 0.0121541567694\n",
      "Loss for  25915 th iteration => 0.0121536586666\n",
      "Loss for  25916 th iteration => 0.0121531054459\n",
      "Loss for  25917 th iteration => 0.0121525645306\n",
      "Loss for  25918 th iteration => 0.0121520975593\n",
      "Loss for  25919 th iteration => 0.0121515131327\n",
      "Loss for  25920 th iteration => 0.0121510344335\n",
      "Loss for  25921 th iteration => 0.012150480951\n",
      "Loss for  25922 th iteration => 0.0121499898837\n",
      "Loss for  25923 th iteration => 0.0121495335355\n",
      "Loss for  25924 th iteration => 0.0121489373898\n",
      "Loss for  25925 th iteration => 0.01214848053\n",
      "Loss for  25926 th iteration => 0.0121478850751\n",
      "Loss for  25927 th iteration => 0.0121474277896\n",
      "Loss for  25928 th iteration => 0.0121468346238\n",
      "Loss for  25929 th iteration => 0.0121463052453\n",
      "Loss for  25930 th iteration => 0.0121458503483\n",
      "Loss for  25931 th iteration => 0.0121452695056\n",
      "Loss for  25932 th iteration => 0.0121448001654\n",
      "Loss for  25933 th iteration => 0.0121442157943\n",
      "Loss for  25934 th iteration => 0.0121437445744\n",
      "Loss for  25935 th iteration => 0.0121431619748\n",
      "Loss for  25936 th iteration => 0.0121426973907\n",
      "Loss for  25937 th iteration => 0.0121422168718\n",
      "Loss for  25938 th iteration => 0.0121416468378\n",
      "Loss for  25939 th iteration => 0.0121411662025\n",
      "Loss for  25940 th iteration => 0.0121405964783\n",
      "Loss for  25941 th iteration => 0.0121401178087\n",
      "Loss for  25942 th iteration => 0.0121395186459\n",
      "Loss for  25943 th iteration => 0.0121390267008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  25944 th iteration => 0.0121385439501\n",
      "Loss for  25945 th iteration => 0.0121379768264\n",
      "Loss for  25946 th iteration => 0.0121374976693\n",
      "Loss for  25947 th iteration => 0.0121370135306\n",
      "Loss for  25948 th iteration => 0.0121364510241\n",
      "Loss for  25949 th iteration => 0.0121359666149\n",
      "Loss for  25950 th iteration => 0.0121354198651\n",
      "Loss for  25951 th iteration => 0.0121348550465\n",
      "Loss for  25952 th iteration => 0.0121344340065\n",
      "Loss for  25953 th iteration => 0.0121338071561\n",
      "Loss for  25954 th iteration => 0.0121333822627\n",
      "Loss for  25955 th iteration => 0.0121327616291\n",
      "Loss for  25956 th iteration => 0.0121323004527\n",
      "Loss for  25957 th iteration => 0.0121317428649\n",
      "Loss for  25958 th iteration => 0.0121312510541\n",
      "Loss for  25959 th iteration => 0.0121306935036\n",
      "Loss for  25960 th iteration => 0.0121302018171\n",
      "Loss for  25961 th iteration => 0.0121296446755\n",
      "Loss for  25962 th iteration => 0.0121291631872\n",
      "Loss for  25963 th iteration => 0.012128594548\n",
      "Loss for  25964 th iteration => 0.0121280822124\n",
      "Loss for  25965 th iteration => 0.0121275871026\n",
      "Loss for  25966 th iteration => 0.0121271281771\n",
      "Loss for  25967 th iteration => 0.0121265383719\n",
      "Loss for  25968 th iteration => 0.0121260791343\n",
      "Loss for  25969 th iteration => 0.0121254898185\n",
      "Loss for  25970 th iteration => 0.0121250303503\n",
      "Loss for  25971 th iteration => 0.012124441442\n",
      "Loss for  25972 th iteration => 0.0121239818231\n",
      "Loss for  25973 th iteration => 0.0121233967994\n",
      "Loss for  25974 th iteration => 0.0121229465635\n",
      "Loss for  25975 th iteration => 0.0121223480007\n",
      "Loss for  25976 th iteration => 0.0121218399924\n",
      "Loss for  25977 th iteration => 0.0121213739409\n",
      "Loss for  25978 th iteration => 0.012120894967\n",
      "Loss for  25979 th iteration => 0.0121203269759\n",
      "Loss for  25980 th iteration => 0.0121198479022\n",
      "Loss for  25981 th iteration => 0.0121192802031\n",
      "Loss for  25982 th iteration => 0.0121188010532\n",
      "Loss for  25983 th iteration => 0.0121182336218\n",
      "Loss for  25984 th iteration => 0.0121177544187\n",
      "Loss for  25985 th iteration => 0.0121171872315\n",
      "Loss for  25986 th iteration => 0.0121167079978\n",
      "Loss for  25987 th iteration => 0.0121161410314\n",
      "Loss for  25988 th iteration => 0.0121156648714\n",
      "Loss for  25989 th iteration => 0.0121150936294\n",
      "Loss for  25990 th iteration => 0.0121146605965\n",
      "Loss for  25991 th iteration => 0.0121141108304\n",
      "Loss for  25992 th iteration => 0.0121136142174\n",
      "Loss for  25993 th iteration => 0.0121130641422\n",
      "Loss for  25994 th iteration => 0.0121125679995\n",
      "Loss for  25995 th iteration => 0.0121120177229\n",
      "Loss for  25996 th iteration => 0.0121115219425\n",
      "Loss for  25997 th iteration => 0.0121109739308\n",
      "Loss for  25998 th iteration => 0.0121104283775\n",
      "Loss for  25999 th iteration => 0.0121099792369\n",
      "Loss for  26000 th iteration => 0.01210938418\n",
      "Loss for  26001 th iteration => 0.0121089153751\n",
      "Loss for  26002 th iteration => 0.0121083589317\n",
      "Loss for  26003 th iteration => 0.0121078718438\n",
      "Loss for  26004 th iteration => 0.012107310535\n",
      "Loss for  26005 th iteration => 0.0121068253815\n",
      "Loss for  26006 th iteration => 0.012106371499\n",
      "Loss for  26007 th iteration => 0.0121057801455\n",
      "Loss for  26008 th iteration => 0.0121053270374\n",
      "Loss for  26009 th iteration => 0.0121047211366\n",
      "Loss for  26010 th iteration => 0.0121042192624\n",
      "Loss for  26011 th iteration => 0.0121037592536\n",
      "Loss for  26012 th iteration => 0.0121031744513\n",
      "Loss for  26013 th iteration => 0.0121027143778\n",
      "Loss for  26014 th iteration => 0.0121021332727\n",
      "Loss for  26015 th iteration => 0.0121016772263\n",
      "Loss for  26016 th iteration => 0.0121010865226\n",
      "Loss for  26017 th iteration => 0.0121006335236\n",
      "Loss for  26018 th iteration => 0.0121001584326\n",
      "Loss for  26019 th iteration => 0.012099590037\n",
      "Loss for  26020 th iteration => 0.0120991164497\n",
      "Loss for  26021 th iteration => 0.0120985055664\n",
      "Loss for  26022 th iteration => 0.0120980305395\n",
      "Loss for  26023 th iteration => 0.0120975532837\n",
      "Loss for  26024 th iteration => 0.0120969875349\n",
      "Loss for  26025 th iteration => 0.012096510252\n",
      "Loss for  26026 th iteration => 0.0120959447197\n",
      "Loss for  26027 th iteration => 0.0120954674317\n",
      "Loss for  26028 th iteration => 0.0120949020933\n",
      "Loss for  26029 th iteration => 0.0120944248217\n",
      "Loss for  26030 th iteration => 0.0120938730575\n",
      "Loss for  26031 th iteration => 0.012093464721\n",
      "Loss for  26032 th iteration => 0.0120928275803\n",
      "Loss for  26033 th iteration => 0.0120923914652\n",
      "Loss for  26034 th iteration => 0.0120918425426\n",
      "Loss for  26035 th iteration => 0.0120913488045\n",
      "Loss for  26036 th iteration => 0.0120907996809\n",
      "Loss for  26037 th iteration => 0.0120903063041\n",
      "Loss for  26038 th iteration => 0.0120897570846\n",
      "Loss for  26039 th iteration => 0.0120892639638\n",
      "Loss for  26040 th iteration => 0.0120887166307\n",
      "Loss for  26041 th iteration => 0.0120881767292\n",
      "Loss for  26042 th iteration => 0.0120877237915\n",
      "Loss for  26043 th iteration => 0.0120871356272\n",
      "Loss for  26044 th iteration => 0.0120866666718\n",
      "Loss for  26045 th iteration => 0.0120861138203\n",
      "Loss for  26046 th iteration => 0.0120856268313\n",
      "Loss for  26047 th iteration => 0.0120850690657\n",
      "Loss for  26048 th iteration => 0.0120845845487\n",
      "Loss for  26049 th iteration => 0.0120841310614\n",
      "Loss for  26050 th iteration => 0.0120835430407\n",
      "Loss for  26051 th iteration => 0.012083090907\n",
      "Loss for  26052 th iteration => 0.0120824886494\n",
      "Loss for  26053 th iteration => 0.0120819877387\n",
      "Loss for  26054 th iteration => 0.0120815283166\n",
      "Loss for  26055 th iteration => 0.0120809474331\n",
      "Loss for  26056 th iteration => 0.0120804922757\n",
      "Loss for  26057 th iteration => 0.0120799042741\n",
      "Loss for  26058 th iteration => 0.012079448045\n",
      "Loss for  26059 th iteration => 0.0120788610123\n",
      "Loss for  26060 th iteration => 0.0120784075432\n",
      "Loss for  26061 th iteration => 0.0120779346408\n",
      "Loss for  26062 th iteration => 0.0120773682885\n",
      "Loss for  26063 th iteration => 0.0120768075229\n",
      "Loss for  26064 th iteration => 0.0120763784458\n",
      "Loss for  26065 th iteration => 0.0120758139259\n",
      "Loss for  26066 th iteration => 0.0120753389848\n",
      "Loss for  26067 th iteration => 0.0120747746641\n",
      "Loss for  26068 th iteration => 0.0120742997333\n",
      "Loss for  26069 th iteration => 0.0120737355899\n",
      "Loss for  26070 th iteration => 0.0120732606905\n",
      "Loss for  26071 th iteration => 0.0120727111328\n",
      "Loss for  26072 th iteration => 0.0120723061723\n",
      "Loss for  26073 th iteration => 0.0120716649031\n",
      "Loss for  26074 th iteration => 0.0120712329467\n",
      "Loss for  26075 th iteration => 0.0120706867006\n",
      "Loss for  26076 th iteration => 0.0120701938199\n",
      "Loss for  26077 th iteration => 0.012069647433\n",
      "Loss for  26078 th iteration => 0.0120691548525\n",
      "Loss for  26079 th iteration => 0.0120686084283\n",
      "Loss for  26080 th iteration => 0.0120681160445\n",
      "Loss for  26081 th iteration => 0.0120675710772\n",
      "Loss for  26082 th iteration => 0.0120670286296\n",
      "Loss for  26083 th iteration => 0.0120665860887\n",
      "Loss for  26084 th iteration => 0.0120659904138\n",
      "Loss for  26085 th iteration => 0.0120655275682\n",
      "Loss for  26086 th iteration => 0.012064974292\n",
      "Loss for  26087 th iteration => 0.0120644892858\n",
      "Loss for  26088 th iteration => 0.0120639382011\n",
      "Loss for  26089 th iteration => 0.012063460423\n",
      "Loss for  26090 th iteration => 0.0120630096097\n",
      "Loss for  26091 th iteration => 0.0120624227837\n",
      "Loss for  26092 th iteration => 0.0120618864475\n",
      "Loss for  26093 th iteration => 0.012061453466\n",
      "Loss for  26094 th iteration => 0.0120608724313\n",
      "Loss for  26095 th iteration => 0.0120604156318\n",
      "Loss for  26096 th iteration => 0.0120598348805\n",
      "Loss for  26097 th iteration => 0.012059378048\n",
      "Loss for  26098 th iteration => 0.0120587975031\n",
      "Loss for  26099 th iteration => 0.0120583407123\n",
      "Loss for  26100 th iteration => 0.0120577631547\n",
      "Loss for  26101 th iteration => 0.0120573157566\n",
      "Loss for  26102 th iteration => 0.0120567236501\n",
      "Loss for  26103 th iteration => 0.012056275848\n",
      "Loss for  26104 th iteration => 0.0120557077383\n",
      "Loss for  26105 th iteration => 0.0120552886834\n",
      "Loss for  26106 th iteration => 0.0120547244732\n",
      "Loss for  26107 th iteration => 0.0120542527003\n",
      "Loss for  26108 th iteration => 0.0120536887211\n",
      "Loss for  26109 th iteration => 0.012053216927\n",
      "Loss for  26110 th iteration => 0.0120526531563\n",
      "Loss for  26111 th iteration => 0.0120521813625\n",
      "Loss for  26112 th iteration => 0.0120516177782\n",
      "Loss for  26113 th iteration => 0.0120511466208\n",
      "Loss for  26114 th iteration => 0.0120505661201\n",
      "Loss for  26115 th iteration => 0.0120500704552\n",
      "Loss for  26116 th iteration => 0.0120496021773\n",
      "Loss for  26117 th iteration => 0.0120491189633\n",
      "Loss for  26118 th iteration => 0.0120485696354\n",
      "Loss for  26119 th iteration => 0.0120480899951\n",
      "Loss for  26120 th iteration => 0.0120475494114\n",
      "Loss for  26121 th iteration => 0.01204699478\n",
      "Loss for  26122 th iteration => 0.0120465779372\n",
      "Loss for  26123 th iteration => 0.0120459603639\n",
      "Loss for  26124 th iteration => 0.0120455423785\n",
      "Loss for  26125 th iteration => 0.0120449294517\n",
      "Loss for  26126 th iteration => 0.0120444751297\n",
      "Loss for  26127 th iteration => 0.0120439254624\n",
      "Loss for  26128 th iteration => 0.0120434403941\n",
      "Loss for  26129 th iteration => 0.0120428910785\n",
      "Loss for  26130 th iteration => 0.0120424058169\n",
      "Loss for  26131 th iteration => 0.0120418590119\n",
      "Loss for  26132 th iteration => 0.0120413383708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  26133 th iteration => 0.0120408637019\n",
      "Loss for  26134 th iteration => 0.0120403209025\n",
      "Loss for  26135 th iteration => 0.0120398370082\n",
      "Loss for  26136 th iteration => 0.0120393831841\n",
      "Loss for  26137 th iteration => 0.0120388029769\n",
      "Loss for  26138 th iteration => 0.01203834899\n",
      "Loss for  26139 th iteration => 0.0120377691187\n",
      "Loss for  26140 th iteration => 0.0120373150464\n",
      "Loss for  26141 th iteration => 0.0120367354331\n",
      "Loss for  26142 th iteration => 0.0120362839617\n",
      "Loss for  26143 th iteration => 0.0120357001625\n",
      "Loss for  26144 th iteration => 0.0120352000827\n",
      "Loss for  26145 th iteration => 0.0120347360098\n",
      "Loss for  26146 th iteration => 0.0120341642501\n",
      "Loss for  26147 th iteration => 0.0120337000672\n",
      "Loss for  26148 th iteration => 0.0120331283204\n",
      "Loss for  26149 th iteration => 0.0120326688073\n",
      "Loss for  26150 th iteration => 0.0120321996738\n",
      "Loss for  26151 th iteration => 0.0120316367404\n",
      "Loss for  26152 th iteration => 0.0120311697241\n",
      "Loss for  26153 th iteration => 0.0120305739852\n",
      "Loss for  26154 th iteration => 0.0120300943607\n",
      "Loss for  26155 th iteration => 0.0120296233193\n",
      "Loss for  26156 th iteration => 0.0120290627615\n",
      "Loss for  26157 th iteration => 0.0120285917817\n",
      "Loss for  26158 th iteration => 0.0120280313468\n",
      "Loss for  26159 th iteration => 0.0120275604483\n",
      "Loss for  26160 th iteration => 0.0120270128018\n",
      "Loss for  26161 th iteration => 0.0120266096335\n",
      "Loss for  26162 th iteration => 0.0120259808335\n",
      "Loss for  26163 th iteration => 0.0120255487381\n",
      "Loss for  26164 th iteration => 0.012025004967\n",
      "Loss for  26165 th iteration => 0.0120245172124\n",
      "Loss for  26166 th iteration => 0.0120239734908\n",
      "Loss for  26167 th iteration => 0.0120234858444\n",
      "Loss for  26168 th iteration => 0.01202294379\n",
      "Loss for  26169 th iteration => 0.0120224056326\n",
      "Loss for  26170 th iteration => 0.0120219665341\n",
      "Loss for  26171 th iteration => 0.0120213744121\n",
      "Loss for  26172 th iteration => 0.0120209159317\n",
      "Loss for  26173 th iteration => 0.0120203656799\n",
      "Loss for  26174 th iteration => 0.0120198850837\n",
      "Loss for  26175 th iteration => 0.0120193367016\n",
      "Loss for  26176 th iteration => 0.0120188611486\n",
      "Loss for  26177 th iteration => 0.0120183030649\n",
      "Loss for  26178 th iteration => 0.0120178011335\n",
      "Loss for  26179 th iteration => 0.0120173174275\n",
      "Loss for  26180 th iteration => 0.0120168627105\n",
      "Loss for  26181 th iteration => 0.0120162871449\n",
      "Loss for  26182 th iteration => 0.0120158324319\n",
      "Loss for  26183 th iteration => 0.0120152570337\n",
      "Loss for  26184 th iteration => 0.0120148023984\n",
      "Loss for  26185 th iteration => 0.0120142270937\n",
      "Loss for  26186 th iteration => 0.0120137726079\n",
      "Loss for  26187 th iteration => 0.0120131973352\n",
      "Loss for  26188 th iteration => 0.0120127581884\n",
      "Loss for  26189 th iteration => 0.0120121679721\n",
      "Loss for  26190 th iteration => 0.0120116671525\n",
      "Loss for  26191 th iteration => 0.0120112106821\n",
      "Loss for  26192 th iteration => 0.012010743707\n",
      "Loss for  26193 th iteration => 0.0120101821913\n",
      "Loss for  26194 th iteration => 0.0120097152044\n",
      "Loss for  26195 th iteration => 0.0120091538856\n",
      "Loss for  26196 th iteration => 0.0120086869079\n",
      "Loss for  26197 th iteration => 0.0120081257645\n",
      "Loss for  26198 th iteration => 0.0120076595526\n",
      "Loss for  26199 th iteration => 0.0120070740966\n",
      "Loss for  26200 th iteration => 0.0120065893053\n",
      "Loss for  26201 th iteration => 0.0120061205535\n",
      "Loss for  26202 th iteration => 0.0120055616461\n",
      "Loss for  26203 th iteration => 0.0120050930854\n",
      "Loss for  26204 th iteration => 0.0120046215947\n",
      "Loss for  26205 th iteration => 0.0120040713703\n",
      "Loss for  26206 th iteration => 0.0120035257698\n",
      "Loss for  26207 th iteration => 0.0120031194976\n",
      "Loss for  26208 th iteration => 0.012002503412\n",
      "Loss for  26209 th iteration => 0.0120020685206\n",
      "Loss for  26210 th iteration => 0.0120015252257\n",
      "Loss for  26211 th iteration => 0.0120010408873\n",
      "Loss for  26212 th iteration => 0.0120004977551\n",
      "Loss for  26213 th iteration => 0.0120000134109\n",
      "Loss for  26214 th iteration => 0.0119994729072\n",
      "Loss for  26215 th iteration => 0.0119989400368\n",
      "Loss for  26216 th iteration => 0.011998496039\n",
      "Loss for  26217 th iteration => 0.0119979135683\n",
      "Loss for  26218 th iteration => 0.0119974532092\n",
      "Loss for  26219 th iteration => 0.011996905894\n",
      "Loss for  26220 th iteration => 0.011996429433\n",
      "Loss for  26221 th iteration => 0.0119958758483\n",
      "Loss for  26222 th iteration => 0.0119953994204\n",
      "Loss for  26223 th iteration => 0.0119948472077\n",
      "Loss for  26224 th iteration => 0.0119943457684\n",
      "Loss for  26225 th iteration => 0.0119938609654\n",
      "Loss for  26226 th iteration => 0.0119934063687\n",
      "Loss for  26227 th iteration => 0.0119928345836\n",
      "Loss for  26228 th iteration => 0.0119923801337\n",
      "Loss for  26229 th iteration => 0.0119918083719\n",
      "Loss for  26230 th iteration => 0.0119913541391\n",
      "Loss for  26231 th iteration => 0.0119907823299\n",
      "Loss for  26232 th iteration => 0.0119903296263\n",
      "Loss for  26233 th iteration => 0.0119897570353\n",
      "Loss for  26234 th iteration => 0.0119892613272\n",
      "Loss for  26235 th iteration => 0.011988808163\n",
      "Loss for  26236 th iteration => 0.0119883440483\n",
      "Loss for  26237 th iteration => 0.0119877833976\n",
      "Loss for  26238 th iteration => 0.0119873192674\n",
      "Loss for  26239 th iteration => 0.0119867588164\n",
      "Loss for  26240 th iteration => 0.0119862946918\n",
      "Loss for  26241 th iteration => 0.0119857344188\n",
      "Loss for  26242 th iteration => 0.0119852708916\n",
      "Loss for  26243 th iteration => 0.0119846849603\n",
      "Loss for  26244 th iteration => 0.0119842035305\n",
      "Loss for  26245 th iteration => 0.011983737628\n",
      "Loss for  26246 th iteration => 0.0119831795925\n",
      "Loss for  26247 th iteration => 0.0119827137946\n",
      "Loss for  26248 th iteration => 0.011982155836\n",
      "Loss for  26249 th iteration => 0.0119816901613\n",
      "Loss for  26250 th iteration => 0.0119811436836\n",
      "Loss for  26251 th iteration => 0.0119807452797\n",
      "Loss for  26252 th iteration => 0.0119801229426\n",
      "Loss for  26253 th iteration => 0.0119796929762\n",
      "Loss for  26254 th iteration => 0.0119791526901\n",
      "Loss for  26255 th iteration => 0.011978669055\n",
      "Loss for  26256 th iteration => 0.0119781296024\n",
      "Loss for  26257 th iteration => 0.0119775944271\n",
      "Loss for  26258 th iteration => 0.0119771627074\n",
      "Loss for  26259 th iteration => 0.0119765700983\n",
      "Loss for  26260 th iteration => 0.0119761404211\n",
      "Loss for  26261 th iteration => 0.0119755497098\n",
      "Loss for  26262 th iteration => 0.0119750943763\n",
      "Loss for  26263 th iteration => 0.0119745483694\n",
      "Loss for  26264 th iteration => 0.0119740711258\n",
      "Loss for  26265 th iteration => 0.0119735272734\n",
      "Loss for  26266 th iteration => 0.0119730284603\n",
      "Loss for  26267 th iteration => 0.0119725348992\n",
      "Loss for  26268 th iteration => 0.011972002083\n",
      "Loss for  26269 th iteration => 0.0119715237678\n",
      "Loss for  26270 th iteration => 0.0119710705475\n",
      "Loss for  26271 th iteration => 0.0119705011003\n",
      "Loss for  26272 th iteration => 0.0119700479954\n",
      "Loss for  26273 th iteration => 0.0119694786022\n",
      "Loss for  26274 th iteration => 0.0119690256833\n",
      "Loss for  26275 th iteration => 0.0119684562729\n",
      "Loss for  26276 th iteration => 0.0119680074428\n",
      "Loss for  26277 th iteration => 0.0119674302538\n",
      "Loss for  26278 th iteration => 0.011966936827\n",
      "Loss for  26279 th iteration => 0.0119664776117\n",
      "Loss for  26280 th iteration => 0.0119659118844\n",
      "Loss for  26281 th iteration => 0.0119654595379\n",
      "Loss for  26282 th iteration => 0.0119649990827\n",
      "Loss for  26283 th iteration => 0.0119644386709\n",
      "Loss for  26284 th iteration => 0.0119639782269\n",
      "Loss for  26285 th iteration => 0.0119634213884\n",
      "Loss for  26286 th iteration => 0.0119628757712\n",
      "Loss for  26287 th iteration => 0.0119624507889\n",
      "Loss for  26288 th iteration => 0.0119618926526\n",
      "Loss for  26289 th iteration => 0.01196143047\n",
      "Loss for  26290 th iteration => 0.0119608724245\n",
      "Loss for  26291 th iteration => 0.0119604103507\n",
      "Loss for  26292 th iteration => 0.0119598523768\n",
      "Loss for  26293 th iteration => 0.0119593907884\n",
      "Loss for  26294 th iteration => 0.0119588320437\n",
      "Loss for  26295 th iteration => 0.0119584111491\n",
      "Loss for  26296 th iteration => 0.0119578748737\n",
      "Loss for  26297 th iteration => 0.0119573907566\n",
      "Loss for  26298 th iteration => 0.0119568554457\n",
      "Loss for  26299 th iteration => 0.0119563114937\n",
      "Loss for  26300 th iteration => 0.0119559001681\n",
      "Loss for  26301 th iteration => 0.0119552905951\n",
      "Loss for  26302 th iteration => 0.0119548815295\n",
      "Loss for  26303 th iteration => 0.011954273848\n",
      "Loss for  26304 th iteration => 0.0119538284099\n",
      "Loss for  26305 th iteration => 0.0119532865504\n",
      "Loss for  26306 th iteration => 0.0119528086847\n",
      "Loss for  26307 th iteration => 0.0119522686692\n",
      "Loss for  26308 th iteration => 0.0119517505167\n",
      "Loss for  26309 th iteration => 0.011951294711\n",
      "Loss for  26310 th iteration => 0.0119507302046\n",
      "Loss for  26311 th iteration => 0.0119502801712\n",
      "Loss for  26312 th iteration => 0.0119498362604\n",
      "Loss for  26313 th iteration => 0.0119492666106\n",
      "Loss for  26314 th iteration => 0.011948817105\n",
      "Loss for  26315 th iteration => 0.0119482476423\n",
      "Loss for  26316 th iteration => 0.0119478002378\n",
      "Loss for  26317 th iteration => 0.0119472113459\n",
      "Loss for  26318 th iteration => 0.0119467260793\n",
      "Loss for  26319 th iteration => 0.0119462715258\n",
      "Loss for  26320 th iteration => 0.0119457075152\n",
      "Loss for  26321 th iteration => 0.0119452532641\n",
      "Loss for  26322 th iteration => 0.0119446896295\n",
      "Loss for  26323 th iteration => 0.0119442462944\n",
      "Loss for  26324 th iteration => 0.011943668515\n",
      "Loss for  26325 th iteration => 0.0119432261572\n",
      "Loss for  26326 th iteration => 0.0119426488029\n",
      "Loss for  26327 th iteration => 0.0119421547246\n",
      "Loss for  26328 th iteration => 0.0119416965377\n",
      "Loss for  26329 th iteration => 0.0119412375302\n",
      "Loss for  26330 th iteration => 0.011940679636\n",
      "Loss for  26331 th iteration => 0.0119402206977\n",
      "Loss for  26332 th iteration => 0.0119396629146\n",
      "Loss for  26333 th iteration => 0.0119392040644\n",
      "Loss for  26334 th iteration => 0.0119386485163\n",
      "Loss for  26335 th iteration => 0.0119381162577\n",
      "Loss for  26336 th iteration => 0.0119376829785\n",
      "Loss for  26337 th iteration => 0.011937127262\n",
      "Loss for  26338 th iteration => 0.0119366668703\n",
      "Loss for  26339 th iteration => 0.0119361269576\n",
      "Loss for  26340 th iteration => 0.0119357306046\n",
      "Loss for  26341 th iteration => 0.0119351125616\n",
      "Loss for  26342 th iteration => 0.0119347187066\n",
      "Loss for  26343 th iteration => 0.0119341004725\n",
      "Loss for  26344 th iteration => 0.0119336731677\n",
      "Loss for  26345 th iteration => 0.0119331374261\n",
      "Loss for  26346 th iteration => 0.0119326568516\n",
      "Loss for  26347 th iteration => 0.0119321227578\n",
      "Loss for  26348 th iteration => 0.011931587904\n",
      "Loss for  26349 th iteration => 0.0119311651324\n",
      "Loss for  26350 th iteration => 0.0119305710401\n",
      "Loss for  26351 th iteration => 0.0119301244972\n",
      "Loss for  26352 th iteration => 0.0119295831033\n",
      "Loss for  26353 th iteration => 0.0119291086898\n",
      "Loss for  26354 th iteration => 0.0119285680824\n",
      "Loss for  26355 th iteration => 0.0119280930371\n",
      "Loss for  26356 th iteration => 0.0119275557322\n",
      "Loss for  26357 th iteration => 0.0119270436622\n",
      "Loss for  26358 th iteration => 0.0119265801112\n",
      "Loss for  26359 th iteration => 0.0119260294851\n",
      "Loss for  26360 th iteration => 0.0119255725099\n",
      "Loss for  26361 th iteration => 0.0119251319968\n",
      "Loss for  26362 th iteration => 0.0119245655036\n",
      "Loss for  26363 th iteration => 0.0119241177478\n",
      "Loss for  26364 th iteration => 0.0119235353691\n",
      "Loss for  26365 th iteration => 0.0119231238683\n",
      "Loss for  26366 th iteration => 0.0119225238621\n",
      "Loss for  26367 th iteration => 0.0119220350994\n",
      "Loss for  26368 th iteration => 0.0119215819802\n",
      "Loss for  26369 th iteration => 0.0119210228789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  26370 th iteration => 0.0119205778694\n",
      "Loss for  26371 th iteration => 0.0119200056806\n",
      "Loss for  26372 th iteration => 0.011919561697\n",
      "Loss for  26373 th iteration => 0.0119189886671\n",
      "Loss for  26374 th iteration => 0.0119184977173\n",
      "Loss for  26375 th iteration => 0.0119180249834\n",
      "Loss for  26376 th iteration => 0.0119174802069\n",
      "Loss for  26377 th iteration => 0.0119170186765\n",
      "Loss for  26378 th iteration => 0.0119165613251\n",
      "Loss for  26379 th iteration => 0.0119160058786\n",
      "Loss for  26380 th iteration => 0.0119155486396\n",
      "Loss for  26381 th iteration => 0.0119149957477\n",
      "Loss for  26382 th iteration => 0.0119144658813\n",
      "Loss for  26383 th iteration => 0.0119140334743\n",
      "Loss for  26384 th iteration => 0.0119134800244\n",
      "Loss for  26385 th iteration => 0.0119130213083\n",
      "Loss for  26386 th iteration => 0.0119124829152\n",
      "Loss for  26387 th iteration => 0.0119120884117\n",
      "Loss for  26388 th iteration => 0.0119114728472\n",
      "Loss for  26389 th iteration => 0.0119110299435\n",
      "Loss for  26390 th iteration => 0.0119105129169\n",
      "Loss for  26391 th iteration => 0.0119100390497\n",
      "Loss for  26392 th iteration => 0.0119095058368\n",
      "Loss for  26393 th iteration => 0.0119090266295\n",
      "Loss for  26394 th iteration => 0.0119084961916\n",
      "Loss for  26395 th iteration => 0.0119079606197\n",
      "Loss for  26396 th iteration => 0.0119075425654\n",
      "Loss for  26397 th iteration => 0.0119069491806\n",
      "Loss for  26398 th iteration => 0.0119065039953\n",
      "Loss for  26399 th iteration => 0.0119059654244\n",
      "Loss for  26400 th iteration => 0.0119054920809\n",
      "Loss for  26401 th iteration => 0.0119049543674\n",
      "Loss for  26402 th iteration => 0.0119044803204\n",
      "Loss for  26403 th iteration => 0.0119039476817\n",
      "Loss for  26404 th iteration => 0.0119034326954\n",
      "Loss for  26405 th iteration => 0.0119029707014\n",
      "Loss for  26406 th iteration => 0.0119024390431\n",
      "Loss for  26407 th iteration => 0.0119019696351\n",
      "Loss for  26408 th iteration => 0.011901523901\n",
      "Loss for  26409 th iteration => 0.0119009584901\n",
      "Loss for  26410 th iteration => 0.0119005151544\n",
      "Loss for  26411 th iteration => 0.0118999322609\n",
      "Loss for  26412 th iteration => 0.0118994486369\n",
      "Loss for  26413 th iteration => 0.0118989982197\n",
      "Loss for  26414 th iteration => 0.0118984378898\n",
      "Loss for  26415 th iteration => 0.0118979878983\n",
      "Loss for  26416 th iteration => 0.0118974285971\n",
      "Loss for  26417 th iteration => 0.0118969899883\n",
      "Loss for  26418 th iteration => 0.0118964150718\n",
      "Loss for  26419 th iteration => 0.0118959781464\n",
      "Loss for  26420 th iteration => 0.0118954055394\n",
      "Loss for  26421 th iteration => 0.0118949125593\n",
      "Loss for  26422 th iteration => 0.0118944577942\n",
      "Loss for  26423 th iteration => 0.0118940025591\n",
      "Loss for  26424 th iteration => 0.0118934487182\n",
      "Loss for  26425 th iteration => 0.0118929935929\n",
      "Loss for  26426 th iteration => 0.0118924406835\n",
      "Loss for  26427 th iteration => 0.0118919093728\n",
      "Loss for  26428 th iteration => 0.0118914839972\n",
      "Loss for  26429 th iteration => 0.0118909321534\n",
      "Loss for  26430 th iteration => 0.0118904755483\n",
      "Loss for  26431 th iteration => 0.011889923698\n",
      "Loss for  26432 th iteration => 0.0118894672919\n",
      "Loss for  26433 th iteration => 0.011888927223\n",
      "Loss for  26434 th iteration => 0.0118885406873\n",
      "Loss for  26435 th iteration => 0.0118879164926\n",
      "Loss for  26436 th iteration => 0.0118874968236\n",
      "Loss for  26437 th iteration => 0.0118869671956\n",
      "Loss for  26438 th iteration => 0.011886488116\n",
      "Loss for  26439 th iteration => 0.0118859608064\n",
      "Loss for  26440 th iteration => 0.0118854216226\n",
      "Loss for  26441 th iteration => 0.0118850157056\n",
      "Loss for  26442 th iteration => 0.0118844135085\n",
      "Loss for  26443 th iteration => 0.0118839747465\n",
      "Loss for  26444 th iteration => 0.0118834399583\n",
      "Loss for  26445 th iteration => 0.0118829665421\n",
      "Loss for  26446 th iteration => 0.011882432658\n",
      "Loss for  26447 th iteration => 0.0118819584908\n",
      "Loss for  26448 th iteration => 0.0118814296764\n",
      "Loss for  26449 th iteration => 0.0118809097408\n",
      "Loss for  26450 th iteration => 0.0118804544211\n",
      "Loss for  26451 th iteration => 0.0118799196528\n",
      "Loss for  26452 th iteration => 0.0118794572074\n",
      "Loss for  26453 th iteration => 0.0118790143226\n",
      "Loss for  26454 th iteration => 0.0118784497772\n",
      "Loss for  26455 th iteration => 0.0118780096131\n",
      "Loss for  26456 th iteration => 0.0118774219553\n",
      "Loss for  26457 th iteration => 0.0118769454842\n",
      "Loss for  26458 th iteration => 0.0118764980131\n",
      "Loss for  26459 th iteration => 0.0118759384494\n",
      "Loss for  26460 th iteration => 0.0118754914351\n",
      "Loss for  26461 th iteration => 0.0118749315789\n",
      "Loss for  26462 th iteration => 0.0118744854464\n",
      "Loss for  26463 th iteration => 0.0118739208917\n",
      "Loss for  26464 th iteration => 0.011873434978\n",
      "Loss for  26465 th iteration => 0.0118729799088\n",
      "Loss for  26466 th iteration => 0.011872425025\n",
      "Loss for  26467 th iteration => 0.0118719797432\n",
      "Loss for  26468 th iteration => 0.0118715275267\n",
      "Loss for  26469 th iteration => 0.011870974367\n",
      "Loss for  26470 th iteration => 0.0118705222472\n",
      "Loss for  26471 th iteration => 0.011869972625\n",
      "Loss for  26472 th iteration => 0.0118694326011\n",
      "Loss for  26473 th iteration => 0.0118690181693\n",
      "Loss for  26474 th iteration => 0.0118684670333\n",
      "Loss for  26475 th iteration => 0.0118680134054\n",
      "Loss for  26476 th iteration => 0.0118674622752\n",
      "Loss for  26477 th iteration => 0.0118670088334\n",
      "Loss for  26478 th iteration => 0.0118664595943\n",
      "Loss for  26479 th iteration => 0.0118659408388\n",
      "Loss for  26480 th iteration => 0.0118655102019\n",
      "Loss for  26481 th iteration => 0.0118650419819\n",
      "Loss for  26482 th iteration => 0.0118645073022\n",
      "Loss for  26483 th iteration => 0.0118639735697\n",
      "Loss for  26484 th iteration => 0.0118635770846\n",
      "Loss for  26485 th iteration => 0.0118629725189\n",
      "Loss for  26486 th iteration => 0.01186254706\n",
      "Loss for  26487 th iteration => 0.0118620163951\n",
      "Loss for  26488 th iteration => 0.0118615424324\n",
      "Loss for  26489 th iteration => 0.0118610124343\n",
      "Loss for  26490 th iteration => 0.0118605379569\n",
      "Loss for  26491 th iteration => 0.011860011105\n",
      "Loss for  26492 th iteration => 0.0118594845584\n",
      "Loss for  26493 th iteration => 0.0118590614457\n",
      "Loss for  26494 th iteration => 0.0118584815985\n",
      "Loss for  26495 th iteration => 0.0118580351523\n",
      "Loss for  26496 th iteration => 0.0118575008435\n",
      "Loss for  26497 th iteration => 0.0118570311771\n",
      "Loss for  26498 th iteration => 0.011856499598\n",
      "Loss for  26499 th iteration => 0.0118559944658\n",
      "Loss for  26500 th iteration => 0.0118555361297\n",
      "Loss for  26501 th iteration => 0.0118549913375\n",
      "Loss for  26502 th iteration => 0.0118545389866\n",
      "Loss for  26503 th iteration => 0.0118541050684\n",
      "Loss for  26504 th iteration => 0.0118535438882\n",
      "Loss for  26505 th iteration => 0.0118531035833\n",
      "Loss for  26506 th iteration => 0.0118525245154\n",
      "Loss for  26507 th iteration => 0.0118520457293\n",
      "Loss for  26508 th iteration => 0.0118515990708\n",
      "Loss for  26509 th iteration => 0.0118510427932\n",
      "Loss for  26510 th iteration => 0.0118505966133\n",
      "Loss for  26511 th iteration => 0.0118500400206\n",
      "Loss for  26512 th iteration => 0.0118495943807\n",
      "Loss for  26513 th iteration => 0.0118490395555\n",
      "Loss for  26514 th iteration => 0.0118486090011\n",
      "Loss for  26515 th iteration => 0.0118480384603\n",
      "Loss for  26516 th iteration => 0.0118475487262\n",
      "Loss for  26517 th iteration => 0.0118470906375\n",
      "Loss for  26518 th iteration => 0.0118465427501\n",
      "Loss for  26519 th iteration => 0.0118460905609\n",
      "Loss for  26520 th iteration => 0.01184563996\n",
      "Loss for  26521 th iteration => 0.0118450927068\n",
      "Loss for  26522 th iteration => 0.0118445597066\n",
      "Loss for  26523 th iteration => 0.0118441420904\n",
      "Loss for  26524 th iteration => 0.0118435934851\n",
      "Loss for  26525 th iteration => 0.0118431414583\n",
      "Loss for  26526 th iteration => 0.0118425928282\n",
      "Loss for  26527 th iteration => 0.0118421410161\n",
      "Loss for  26528 th iteration => 0.0118415949726\n",
      "Loss for  26529 th iteration => 0.0118410814705\n",
      "Loss for  26530 th iteration => 0.01184064783\n",
      "Loss for  26531 th iteration => 0.0118401793967\n",
      "Loss for  26532 th iteration => 0.0118396497657\n",
      "Loss for  26533 th iteration => 0.0118391185759\n",
      "Loss for  26534 th iteration => 0.0118387342435\n",
      "Loss for  26535 th iteration => 0.0118381183276\n",
      "Loss for  26536 th iteration => 0.0118376910664\n",
      "Loss for  26537 th iteration => 0.0118371620675\n",
      "Loss for  26538 th iteration => 0.0118366904919\n",
      "Loss for  26539 th iteration => 0.0118361623434\n",
      "Loss for  26540 th iteration => 0.0118356900689\n",
      "Loss for  26541 th iteration => 0.0118351669234\n",
      "Loss for  26542 th iteration => 0.011834641331\n",
      "Loss for  26543 th iteration => 0.0118341974014\n",
      "Loss for  26544 th iteration => 0.0118336644823\n",
      "Loss for  26545 th iteration => 0.0118331973241\n",
      "Loss for  26546 th iteration => 0.0118326656204\n",
      "Loss for  26547 th iteration => 0.0118321973987\n",
      "Loss for  26548 th iteration => 0.0118316711576\n",
      "Loss for  26549 th iteration => 0.0118311646597\n",
      "Loss for  26550 th iteration => 0.0118307172515\n",
      "Loss for  26551 th iteration => 0.0118302826302\n",
      "Loss for  26552 th iteration => 0.0118297227686\n",
      "Loss for  26553 th iteration => 0.0118292835268\n",
      "Loss for  26554 th iteration => 0.0118287235169\n",
      "Loss for  26555 th iteration => 0.0118282881418\n",
      "Loss for  26556 th iteration => 0.0118277115331\n",
      "Loss for  26557 th iteration => 0.0118272314644\n",
      "Loss for  26558 th iteration => 0.0118267880613\n",
      "Loss for  26559 th iteration => 0.0118262326016\n",
      "Loss for  26560 th iteration => 0.0118257897992\n",
      "Loss for  26561 th iteration => 0.0118252350672\n",
      "Loss for  26562 th iteration => 0.011824806595\n",
      "Loss for  26563 th iteration => 0.0118242365053\n",
      "Loss for  26564 th iteration => 0.0118237498807\n",
      "Loss for  26565 th iteration => 0.0118232945025\n",
      "Loss for  26566 th iteration => 0.0118227477043\n",
      "Loss for  26567 th iteration => 0.0118223000786\n",
      "Loss for  26568 th iteration => 0.0118218501757\n",
      "Loss for  26569 th iteration => 0.0118213048034\n",
      "Loss for  26570 th iteration => 0.0118207727803\n",
      "Loss for  26571 th iteration => 0.0118203582106\n",
      "Loss for  26572 th iteration => 0.0118198127764\n",
      "Loss for  26573 th iteration => 0.0118193615115\n",
      "Loss for  26574 th iteration => 0.0118188160277\n",
      "Loss for  26575 th iteration => 0.0118183650003\n",
      "Loss for  26576 th iteration => 0.0118178214227\n",
      "Loss for  26577 th iteration => 0.0118173076293\n",
      "Loss for  26578 th iteration => 0.0118168781949\n",
      "Loss for  26579 th iteration => 0.0118164140461\n",
      "Loss for  26580 th iteration => 0.0118158835416\n",
      "Loss for  26581 th iteration => 0.0118153544131\n",
      "Loss for  26582 th iteration => 0.0118149601857\n",
      "Loss for  26583 th iteration => 0.0118143622801\n",
      "Loss for  26584 th iteration => 0.0118139392274\n",
      "Loss for  26585 th iteration => 0.0118134134841\n",
      "Loss for  26586 th iteration => 0.0118129425573\n",
      "Loss for  26587 th iteration => 0.0118124176413\n",
      "Loss for  26588 th iteration => 0.0118119460377\n",
      "Loss for  26589 th iteration => 0.011811426421\n",
      "Loss for  26590 th iteration => 0.0118108989553\n",
      "Loss for  26591 th iteration => 0.011810459204\n",
      "Loss for  26592 th iteration => 0.0118099294776\n",
      "Loss for  26593 th iteration => 0.011809463028\n",
      "Loss for  26594 th iteration => 0.0118089344938\n",
      "Loss for  26595 th iteration => 0.0118084670032\n",
      "Loss for  26596 th iteration => 0.0118079441282\n",
      "Loss for  26597 th iteration => 0.0118074359481\n",
      "Loss for  26598 th iteration => 0.011806980924\n",
      "Loss for  26599 th iteration => 0.0118064570827\n",
      "Loss for  26600 th iteration => 0.0118059945858\n",
      "Loss for  26601 th iteration => 0.0118055592587\n",
      "Loss for  26602 th iteration => 0.0118049802813\n",
      "Loss for  26603 th iteration => 0.0118045081149\n",
      "Loss for  26604 th iteration => 0.0118040678567\n",
      "Loss for  26605 th iteration => 0.0118035129818\n",
      "Loss for  26606 th iteration => 0.0118030733003\n",
      "Loss for  26607 th iteration => 0.0118025180101\n",
      "Loss for  26608 th iteration => 0.0118020789639\n",
      "Loss for  26609 th iteration => 0.011801523634\n",
      "Loss for  26610 th iteration => 0.0118010280883\n",
      "Loss for  26611 th iteration => 0.0118005891907\n",
      "Loss for  26612 th iteration => 0.0118000456304\n",
      "Loss for  26613 th iteration => 0.0117996026675\n",
      "Loss for  26614 th iteration => 0.0117990474144\n",
      "Loss for  26615 th iteration => 0.0117986081983\n",
      "Loss for  26616 th iteration => 0.011798070549\n",
      "Loss for  26617 th iteration => 0.0117976669436\n",
      "Loss for  26618 th iteration => 0.0117971233317\n",
      "Loss for  26619 th iteration => 0.0117966739251\n",
      "Loss for  26620 th iteration => 0.0117961302873\n",
      "Loss for  26621 th iteration => 0.0117956810944\n",
      "Loss for  26622 th iteration => 0.0117951374147\n",
      "Loss for  26623 th iteration => 0.0117946898308\n",
      "Loss for  26624 th iteration => 0.0117941242514\n",
      "Loss for  26625 th iteration => 0.0117936537664\n",
      "Loss for  26626 th iteration => 0.0117932033488\n",
      "Loss for  26627 th iteration => 0.0117926613215\n",
      "Loss for  26628 th iteration => 0.0117922111981\n",
      "Loss for  26629 th iteration => 0.0117916811329\n",
      "Loss for  26630 th iteration => 0.0117912440601\n",
      "Loss for  26631 th iteration => 0.0117907375565\n",
      "Loss for  26632 th iteration => 0.011790271934\n",
      "Loss for  26633 th iteration => 0.0117897523282\n",
      "Loss for  26634 th iteration => 0.0117892213343\n",
      "Loss for  26635 th iteration => 0.0117888251814\n",
      "Loss for  26636 th iteration => 0.0117882274457\n",
      "Loss for  26637 th iteration => 0.0117877979036\n",
      "Loss for  26638 th iteration => 0.011787273229\n",
      "Loss for  26639 th iteration => 0.0117868054547\n",
      "Loss for  26640 th iteration => 0.0117862819087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  26641 th iteration => 0.0117858131559\n",
      "Loss for  26642 th iteration => 0.0117852952626\n",
      "Loss for  26643 th iteration => 0.0117847776362\n",
      "Loss for  26644 th iteration => 0.0117843326459\n",
      "Loss for  26645 th iteration => 0.0117838050627\n",
      "Loss for  26646 th iteration => 0.011783340689\n",
      "Loss for  26647 th iteration => 0.0117828153555\n",
      "Loss for  26648 th iteration => 0.0117823190699\n",
      "Loss for  26649 th iteration => 0.0117818768698\n",
      "Loss for  26650 th iteration => 0.0117814411895\n",
      "Loss for  26651 th iteration => 0.0117808854544\n",
      "Loss for  26652 th iteration => 0.0117804502255\n",
      "Loss for  26653 th iteration => 0.0117798941998\n",
      "Loss for  26654 th iteration => 0.0117794601897\n",
      "Loss for  26655 th iteration => 0.0117788907055\n",
      "Loss for  26656 th iteration => 0.0117784141193\n",
      "Loss for  26657 th iteration => 0.0117779751022\n",
      "Loss for  26658 th iteration => 0.0117774232476\n",
      "Loss for  26659 th iteration => 0.0117769849582\n",
      "Loss for  26660 th iteration => 0.0117764330753\n",
      "Loss for  26661 th iteration => 0.0117760080718\n",
      "Loss for  26662 th iteration => 0.0117754412362\n",
      "Loss for  26663 th iteration => 0.0117749594147\n",
      "Loss for  26664 th iteration => 0.0117745088077\n",
      "Loss for  26665 th iteration => 0.011773965003\n",
      "Loss for  26666 th iteration => 0.0117735235803\n",
      "Loss for  26667 th iteration => 0.0117730764303\n",
      "Loss for  26668 th iteration => 0.0117725359384\n",
      "Loss for  26669 th iteration => 0.011772005869\n",
      "Loss for  26670 th iteration => 0.0117715965341\n",
      "Loss for  26671 th iteration => 0.0117710562619\n",
      "Loss for  26672 th iteration => 0.0117706078731\n",
      "Loss for  26673 th iteration => 0.0117700675048\n",
      "Loss for  26674 th iteration => 0.0117696193963\n",
      "Loss for  26675 th iteration => 0.0117690819049\n",
      "Loss for  26676 th iteration => 0.0117685674659\n",
      "Loss for  26677 th iteration => 0.0117681437023\n",
      "Loss for  26678 th iteration => 0.0117676876215\n",
      "Loss for  26679 th iteration => 0.0117671579518\n",
      "Loss for  26680 th iteration => 0.0117666323153\n",
      "Loss for  26681 th iteration => 0.0117662465309\n",
      "Loss for  26682 th iteration => 0.0117656428302\n",
      "Loss for  26683 th iteration => 0.0117652281809\n",
      "Loss for  26684 th iteration => 0.0117647090706\n",
      "Loss for  26685 th iteration => 0.0117642394622\n",
      "Loss for  26686 th iteration => 0.0117637243931\n",
      "Loss for  26687 th iteration => 0.0117631940705\n",
      "Loss for  26688 th iteration => 0.0117627979825\n",
      "Loss for  26689 th iteration => 0.0117622081196\n",
      "Loss for  26690 th iteration => 0.0117617759706\n",
      "Loss for  26691 th iteration => 0.011761253321\n",
      "Loss for  26692 th iteration => 0.0117607877411\n",
      "Loss for  26693 th iteration => 0.0117602700845\n",
      "Loss for  26694 th iteration => 0.0117597569124\n",
      "Loss for  26695 th iteration => 0.0117593133201\n",
      "Loss for  26696 th iteration => 0.0117587869052\n",
      "Loss for  26697 th iteration => 0.0117583254307\n",
      "Loss for  26698 th iteration => 0.0117578014474\n",
      "Loss for  26699 th iteration => 0.0117573198054\n",
      "Loss for  26700 th iteration => 0.0117568598465\n",
      "Loss for  26701 th iteration => 0.0117564256045\n",
      "Loss for  26702 th iteration => 0.0117558725107\n",
      "Loss for  26703 th iteration => 0.0117554388569\n",
      "Loss for  26704 th iteration => 0.0117548853344\n",
      "Loss for  26705 th iteration => 0.0117544528663\n",
      "Loss for  26706 th iteration => 0.0117538865095\n",
      "Loss for  26707 th iteration => 0.0117534113647\n",
      "Loss for  26708 th iteration => 0.0117529742756\n",
      "Loss for  26709 th iteration => 0.0117524245677\n",
      "Loss for  26710 th iteration => 0.0117519883319\n",
      "Loss for  26711 th iteration => 0.0117514380829\n",
      "Loss for  26712 th iteration => 0.0117510164453\n",
      "Loss for  26713 th iteration => 0.0117504515126\n",
      "Loss for  26714 th iteration => 0.0117499701083\n",
      "Loss for  26715 th iteration => 0.0117495336107\n",
      "Loss for  26716 th iteration => 0.0117490882563\n",
      "Loss for  26717 th iteration => 0.0117485483421\n",
      "Loss for  26718 th iteration => 0.0117481032082\n",
      "Loss for  26719 th iteration => 0.011747566199\n",
      "Loss for  26720 th iteration => 0.0117470362474\n",
      "Loss for  26721 th iteration => 0.01174662947\n",
      "Loss for  26722 th iteration => 0.0117460912262\n",
      "Loss for  26723 th iteration => 0.0117456449105\n",
      "Loss for  26724 th iteration => 0.0117451065498\n",
      "Loss for  26725 th iteration => 0.0117446610734\n",
      "Loss for  26726 th iteration => 0.0117441029442\n",
      "Loss for  26727 th iteration => 0.0117436351894\n",
      "Loss for  26728 th iteration => 0.0117431891312\n",
      "Loss for  26729 th iteration => 0.0117427376954\n",
      "Loss for  26730 th iteration => 0.0117422093104\n",
      "Loss for  26731 th iteration => 0.0117416845135\n",
      "Loss for  26732 th iteration => 0.011741251173\n",
      "Loss for  26733 th iteration => 0.0117407476478\n",
      "Loss for  26734 th iteration => 0.0117402868061\n",
      "Loss for  26735 th iteration => 0.0117397717086\n",
      "Loss for  26736 th iteration => 0.0117392442508\n",
      "Loss for  26737 th iteration => 0.0117388526267\n",
      "Loss for  26738 th iteration => 0.0117382591095\n",
      "Loss for  26739 th iteration => 0.0117378330713\n",
      "Loss for  26740 th iteration => 0.0117373127843\n",
      "Loss for  26741 th iteration => 0.01173684875\n",
      "Loss for  26742 th iteration => 0.0117363310027\n",
      "Loss for  26743 th iteration => 0.0117358202255\n",
      "Loss for  26744 th iteration => 0.0117353993171\n",
      "Loss for  26745 th iteration => 0.0117348366208\n",
      "Loss for  26746 th iteration => 0.0117343962434\n",
      "Loss for  26747 th iteration => 0.0117338736241\n",
      "Loss for  26748 th iteration => 0.0117334124095\n",
      "Loss for  26749 th iteration => 0.011732895383\n",
      "Loss for  26750 th iteration => 0.0117323954554\n",
      "Loss for  26751 th iteration => 0.011731954507\n",
      "Loss for  26752 th iteration => 0.0117315301186\n",
      "Loss for  26753 th iteration => 0.0117309767036\n",
      "Loss for  26754 th iteration => 0.0117305481542\n",
      "Loss for  26755 th iteration => 0.01172997627\n",
      "Loss for  26756 th iteration => 0.0117295085742\n",
      "Loss for  26757 th iteration => 0.0117290750495\n",
      "Loss for  26758 th iteration => 0.0117285256862\n",
      "Loss for  26759 th iteration => 0.0117280929199\n",
      "Loss for  26760 th iteration => 0.0117275429563\n",
      "Loss for  26761 th iteration => 0.0117271125882\n",
      "Loss for  26762 th iteration => 0.0117265574416\n",
      "Loss for  26763 th iteration => 0.0117260805106\n",
      "Loss for  26764 th iteration => 0.0117256445205\n",
      "Loss for  26765 th iteration => 0.0117250941741\n",
      "Loss for  26766 th iteration => 0.0117246626044\n",
      "Loss for  26767 th iteration => 0.0117241077702\n",
      "Loss for  26768 th iteration => 0.0117236812262\n",
      "Loss for  26769 th iteration => 0.011723142768\n",
      "Loss for  26770 th iteration => 0.0117227464499\n",
      "Loss for  26771 th iteration => 0.0117222101942\n",
      "Loss for  26772 th iteration => 0.011721765768\n",
      "Loss for  26773 th iteration => 0.0117212294026\n",
      "Loss for  26774 th iteration => 0.0117207876473\n",
      "Loss for  26775 th iteration => 0.0117202210938\n",
      "Loss for  26776 th iteration => 0.0117197638537\n",
      "Loss for  26777 th iteration => 0.0117193183722\n",
      "Loss for  26778 th iteration => 0.0117187834793\n",
      "Loss for  26779 th iteration => 0.0117183383502\n",
      "Loss for  26780 th iteration => 0.0117178158879\n",
      "Loss for  26781 th iteration => 0.0117173815017\n",
      "Loss for  26782 th iteration => 0.011716880835\n",
      "Loss for  26783 th iteration => 0.0117164261044\n",
      "Loss for  26784 th iteration => 0.011715903481\n",
      "Loss for  26785 th iteration => 0.011715382457\n",
      "Loss for  26786 th iteration => 0.0117149984418\n",
      "Loss for  26787 th iteration => 0.0117144035922\n",
      "Loss for  26788 th iteration => 0.01171399069\n",
      "Loss for  26789 th iteration => 0.0117134760839\n",
      "Loss for  26790 th iteration => 0.011713010289\n",
      "Loss for  26791 th iteration => 0.0117125015504\n",
      "Loss for  26792 th iteration => 0.0117119755357\n",
      "Loss for  26793 th iteration => 0.0117115475627\n",
      "Loss for  26794 th iteration => 0.0117110287938\n",
      "Loss for  26795 th iteration => 0.0117105674973\n",
      "Loss for  26796 th iteration => 0.0117100509936\n",
      "Loss for  26797 th iteration => 0.0117095470032\n",
      "Loss for  26798 th iteration => 0.0117091204689\n",
      "Loss for  26799 th iteration => 0.0117085674353\n",
      "Loss for  26800 th iteration => 0.0117081256108\n",
      "Loss for  26801 th iteration => 0.0117076049155\n",
      "Loss for  26802 th iteration => 0.0117071460303\n",
      "Loss for  26803 th iteration => 0.0117066317143\n",
      "Loss for  26804 th iteration => 0.0117061502529\n",
      "Loss for  26805 th iteration => 0.0117056926496\n",
      "Loss for  26806 th iteration => 0.0117052640873\n",
      "Loss for  26807 th iteration => 0.0117047136482\n",
      "Loss for  26808 th iteration => 0.011704286078\n",
      "Loss for  26809 th iteration => 0.0117037211571\n",
      "Loss for  26810 th iteration => 0.0117032519083\n",
      "Loss for  26811 th iteration => 0.0117028200665\n",
      "Loss for  26812 th iteration => 0.0117022732812\n",
      "Loss for  26813 th iteration => 0.0117018423379\n",
      "Loss for  26814 th iteration => 0.011701294811\n",
      "Loss for  26815 th iteration => 0.0117008672382\n",
      "Loss for  26816 th iteration => 0.0117003163197\n",
      "Loss for  26817 th iteration => 0.011699840708\n",
      "Loss for  26818 th iteration => 0.0116994061464\n",
      "Loss for  26819 th iteration => 0.0116988583697\n",
      "Loss for  26820 th iteration => 0.0116984289898\n",
      "Loss for  26821 th iteration => 0.0116978975938\n",
      "Loss for  26822 th iteration => 0.0116975026713\n",
      "Loss for  26823 th iteration => 0.0116969688292\n",
      "Loss for  26824 th iteration => 0.0116965261042\n",
      "Loss for  26825 th iteration => 0.0116959921411\n",
      "Loss for  26826 th iteration => 0.0116955497173\n",
      "Loss for  26827 th iteration => 0.0116950179519\n",
      "Loss for  26828 th iteration => 0.0116945006676\n",
      "Loss for  26829 th iteration => 0.0116940889781\n",
      "Loss for  26830 th iteration => 0.0116935564612\n",
      "Loss for  26831 th iteration => 0.0116931130658\n",
      "Loss for  26832 th iteration => 0.011692580353\n",
      "Loss for  26833 th iteration => 0.0116921395826\n",
      "Loss for  26834 th iteration => 0.01169159759\n",
      "Loss for  26835 th iteration => 0.0116912042441\n",
      "Loss for  26836 th iteration => 0.0116906828901\n",
      "Loss for  26837 th iteration => 0.0116901663938\n",
      "Loss for  26838 th iteration => 0.0116897800303\n",
      "Loss for  26839 th iteration => 0.011689189963\n",
      "Loss for  26840 th iteration => 0.0116887778728\n",
      "Loss for  26841 th iteration => 0.0116882651588\n",
      "Loss for  26842 th iteration => 0.0116878015501\n",
      "Loss for  26843 th iteration => 0.0116872930014\n",
      "Loss for  26844 th iteration => 0.0116867695233\n",
      "Loss for  26845 th iteration => 0.0116863788363\n",
      "Loss for  26846 th iteration => 0.0116857960712\n",
      "Loss for  26847 th iteration => 0.0116853689912\n",
      "Loss for  26848 th iteration => 0.0116848537323\n",
      "Loss for  26849 th iteration => 0.0116843931497\n",
      "Loss for  26850 th iteration => 0.0116838847509\n",
      "Loss for  26851 th iteration => 0.0116833749526\n",
      "Loss for  26852 th iteration => 0.0116829372589\n",
      "Loss for  26853 th iteration => 0.0116824189146\n",
      "Loss for  26854 th iteration => 0.0116819617519\n",
      "Loss for  26855 th iteration => 0.01168144855\n",
      "Loss for  26856 th iteration => 0.0116809539848\n",
      "Loss for  26857 th iteration => 0.0116805159908\n",
      "Loss for  26858 th iteration => 0.0116800958925\n",
      "Loss for  26859 th iteration => 0.0116795461832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  26860 th iteration => 0.0116791226819\n",
      "Loss for  26861 th iteration => 0.011678554634\n",
      "Loss for  26862 th iteration => 0.0116780905631\n",
      "Loss for  26863 th iteration => 0.0116776620439\n",
      "Loss for  26864 th iteration => 0.0116771160272\n",
      "Loss for  26865 th iteration => 0.0116766883845\n",
      "Loss for  26866 th iteration => 0.0116761416472\n",
      "Loss for  26867 th iteration => 0.011675717944\n",
      "Loss for  26868 th iteration => 0.0116751641589\n",
      "Loss for  26869 th iteration => 0.0116746901699\n",
      "Loss for  26870 th iteration => 0.0116742628946\n",
      "Loss for  26871 th iteration => 0.0116737119746\n",
      "Loss for  26872 th iteration => 0.0116732897744\n",
      "Loss for  26873 th iteration => 0.0116727369294\n",
      "Loss for  26874 th iteration => 0.0116722619792\n",
      "Loss for  26875 th iteration => 0.0116718278717\n",
      "Loss for  26876 th iteration => 0.0116713867329\n",
      "Loss for  26877 th iteration => 0.0116708552656\n",
      "Loss for  26878 th iteration => 0.0116704147401\n",
      "Loss for  26879 th iteration => 0.0116698493668\n",
      "Loss for  26880 th iteration => 0.0116694019749\n",
      "Loss for  26881 th iteration => 0.011668959853\n",
      "Loss for  26882 th iteration => 0.0116684297798\n",
      "Loss for  26883 th iteration => 0.0116679880338\n",
      "Loss for  26884 th iteration => 0.0116674593925\n",
      "Loss for  26885 th iteration => 0.011666952125\n",
      "Loss for  26886 th iteration => 0.0116665379042\n",
      "Loss for  26887 th iteration => 0.0116660920082\n",
      "Loss for  26888 th iteration => 0.0116655680764\n",
      "Loss for  26889 th iteration => 0.0116650518015\n",
      "Loss for  26890 th iteration => 0.0116646767655\n",
      "Loss for  26891 th iteration => 0.0116640729673\n",
      "Loss for  26892 th iteration => 0.0116636696688\n",
      "Loss for  26893 th iteration => 0.0116631615963\n",
      "Loss for  26894 th iteration => 0.0116626377926\n",
      "Loss for  26895 th iteration => 0.0116622577312\n",
      "Loss for  26896 th iteration => 0.0116616647722\n",
      "Loss for  26897 th iteration => 0.0116612468903\n",
      "Loss for  26898 th iteration => 0.0116607347787\n",
      "Loss for  26899 th iteration => 0.0116602749661\n",
      "Loss for  26900 th iteration => 0.0116597668164\n",
      "Loss for  26901 th iteration => 0.0116592539073\n",
      "Loss for  26902 th iteration => 0.0116588518047\n",
      "Loss for  26903 th iteration => 0.0116582845484\n",
      "Loss for  26904 th iteration => 0.0116578533941\n",
      "Loss for  26905 th iteration => 0.0116573404419\n",
      "Loss for  26906 th iteration => 0.0116568444004\n",
      "Loss for  26907 th iteration => 0.0116564165522\n",
      "Loss for  26908 th iteration => 0.0116558732256\n",
      "Loss for  26909 th iteration => 0.0116554380266\n",
      "Loss for  26910 th iteration => 0.0116550257873\n",
      "Loss for  26911 th iteration => 0.0116544788511\n",
      "Loss for  26912 th iteration => 0.0116539707593\n",
      "Loss for  26913 th iteration => 0.011653571961\n",
      "Loss for  26914 th iteration => 0.0116530274886\n",
      "Loss for  26915 th iteration => 0.0116526021795\n",
      "Loss for  26916 th iteration => 0.0116520570161\n",
      "Loss for  26917 th iteration => 0.0116516342439\n",
      "Loss for  26918 th iteration => 0.0116510749483\n",
      "Loss for  26919 th iteration => 0.0116506080474\n",
      "Loss for  26920 th iteration => 0.0116501800001\n",
      "Loss for  26921 th iteration => 0.011649637943\n",
      "Loss for  26922 th iteration => 0.0116492109825\n",
      "Loss for  26923 th iteration => 0.0116486698159\n",
      "Loss for  26924 th iteration => 0.0116481854778\n",
      "Loss for  26925 th iteration => 0.0116477592195\n",
      "Loss for  26926 th iteration => 0.0116472261109\n",
      "Loss for  26927 th iteration => 0.0116468020916\n",
      "Loss for  26928 th iteration => 0.0116463655443\n",
      "Loss for  26929 th iteration => 0.0116457902243\n",
      "Loss for  26930 th iteration => 0.0116453545887\n",
      "Loss for  26931 th iteration => 0.0116449164157\n",
      "Loss for  26932 th iteration => 0.011644386272\n",
      "Loss for  26933 th iteration => 0.0116439484251\n",
      "Loss for  26934 th iteration => 0.0116434193488\n",
      "Loss for  26935 th iteration => 0.0116429047599\n",
      "Loss for  26936 th iteration => 0.0116425002704\n",
      "Loss for  26937 th iteration => 0.0116419714928\n",
      "Loss for  26938 th iteration => 0.0116415327458\n",
      "Loss for  26939 th iteration => 0.0116410037471\n",
      "Loss for  26940 th iteration => 0.0116405669898\n",
      "Loss for  26941 th iteration => 0.0116400198632\n",
      "Loss for  26942 th iteration => 0.0116395577525\n",
      "Loss for  26943 th iteration => 0.0116391182322\n",
      "Loss for  26944 th iteration => 0.011638604181\n",
      "Loss for  26945 th iteration => 0.0116382275536\n",
      "Loss for  26946 th iteration => 0.0116376319536\n",
      "Loss for  26947 th iteration => 0.0116372272942\n",
      "Loss for  26948 th iteration => 0.0116367212287\n",
      "Loss for  26949 th iteration => 0.0116362592963\n",
      "Loss for  26950 th iteration => 0.0116357589386\n",
      "Loss for  26951 th iteration => 0.0116352319995\n",
      "Loss for  26952 th iteration => 0.0116348151254\n",
      "Loss for  26953 th iteration => 0.0116343055323\n",
      "Loss for  26954 th iteration => 0.0116338474575\n",
      "Loss for  26955 th iteration => 0.01163334112\n",
      "Loss for  26956 th iteration => 0.0116328319514\n",
      "Loss for  26957 th iteration => 0.011632430109\n",
      "Loss for  26958 th iteration => 0.0116318662224\n",
      "Loss for  26959 th iteration => 0.0116314365062\n",
      "Loss for  26960 th iteration => 0.0116309260912\n",
      "Loss for  26961 th iteration => 0.011630432169\n",
      "Loss for  26962 th iteration => 0.0116300066048\n",
      "Loss for  26963 th iteration => 0.0116294652251\n",
      "Loss for  26964 th iteration => 0.0116290319935\n",
      "Loss for  26965 th iteration => 0.0116286216508\n",
      "Loss for  26966 th iteration => 0.0116280392295\n",
      "Loss for  26967 th iteration => 0.0116276058347\n",
      "Loss for  26968 th iteration => 0.0116271815336\n",
      "Loss for  26969 th iteration => 0.0116266394784\n",
      "Loss for  26970 th iteration => 0.011626216017\n",
      "Loss for  26971 th iteration => 0.0116256732763\n",
      "Loss for  26972 th iteration => 0.0116252535721\n",
      "Loss for  26973 th iteration => 0.0116246941492\n",
      "Loss for  26974 th iteration => 0.0116242306929\n",
      "Loss for  26975 th iteration => 0.0116238044914\n",
      "Loss for  26976 th iteration => 0.0116232648564\n",
      "Loss for  26977 th iteration => 0.0116228404212\n",
      "Loss for  26978 th iteration => 0.0116222961432\n",
      "Loss for  26979 th iteration => 0.01162182716\n",
      "Loss for  26980 th iteration => 0.0116213988155\n",
      "Loss for  26981 th iteration => 0.0116208572445\n",
      "Loss for  26982 th iteration => 0.0116204347927\n",
      "Loss for  26983 th iteration => 0.011619888346\n",
      "Loss for  26984 th iteration => 0.0116194195565\n",
      "Loss for  26985 th iteration => 0.0116189873021\n",
      "Loss for  26986 th iteration => 0.0116185518561\n",
      "Loss for  26987 th iteration => 0.0116180232571\n",
      "Loss for  26988 th iteration => 0.0116175881705\n",
      "Loss for  26989 th iteration => 0.0116170637884\n",
      "Loss for  26990 th iteration => 0.0116165456222\n",
      "Loss for  26991 th iteration => 0.0116161464769\n",
      "Loss for  26992 th iteration => 0.0116156191491\n",
      "Loss for  26993 th iteration => 0.0116151832509\n",
      "Loss for  26994 th iteration => 0.0116146585006\n",
      "Loss for  26995 th iteration => 0.0116141536974\n",
      "Loss for  26996 th iteration => 0.0116137449222\n",
      "Loss for  26997 th iteration => 0.011613303469\n",
      "Loss for  26998 th iteration => 0.0116127847004\n",
      "Loss for  26999 th iteration => 0.0116122708595\n",
      "Loss for  27000 th iteration => 0.0116118452667\n",
      "Loss for  27001 th iteration => 0.0116113537509\n",
      "Loss for  27002 th iteration => 0.0116109028587\n",
      "Loss for  27003 th iteration => 0.0116103925956\n",
      "Loss for  27004 th iteration => 0.0116098799721\n",
      "Loss for  27005 th iteration => 0.0116095021449\n",
      "Loss for  27006 th iteration => 0.0116089199692\n",
      "Loss for  27007 th iteration => 0.011608512796\n",
      "Loss for  27008 th iteration => 0.0116080091779\n",
      "Loss for  27009 th iteration => 0.0116074956795\n",
      "Loss for  27010 th iteration => 0.0116071075928\n",
      "Loss for  27011 th iteration => 0.0116065333542\n",
      "Loss for  27012 th iteration => 0.0116061121538\n",
      "Loss for  27013 th iteration => 0.0116056038044\n",
      "Loss for  27014 th iteration => 0.0116051490959\n",
      "Loss for  27015 th iteration => 0.0116046476908\n",
      "Loss for  27016 th iteration => 0.0116041434998\n",
      "Loss for  27017 th iteration => 0.0116037123234\n",
      "Loss for  27018 th iteration => 0.0116032014064\n",
      "Loss for  27019 th iteration => 0.0116027495943\n",
      "Loss for  27020 th iteration => 0.0116022448245\n",
      "Loss for  27021 th iteration => 0.0116017538143\n",
      "Loss for  27022 th iteration => 0.0116013133242\n",
      "Loss for  27023 th iteration => 0.011600806237\n",
      "Loss for  27024 th iteration => 0.0116003593494\n",
      "Loss for  27025 th iteration => 0.0115999444352\n",
      "Loss for  27026 th iteration => 0.0115993778813\n",
      "Loss for  27027 th iteration => 0.0115989228855\n",
      "Loss for  27028 th iteration => 0.0115985006551\n",
      "Loss for  27029 th iteration => 0.0115979611402\n",
      "Loss for  27030 th iteration => 0.0115975412507\n",
      "Loss for  27031 th iteration => 0.0115969901858\n",
      "Loss for  27032 th iteration => 0.0115965252345\n",
      "Loss for  27033 th iteration => 0.0115961006705\n",
      "Loss for  27034 th iteration => 0.0115955638514\n",
      "Loss for  27035 th iteration => 0.0115951406154\n",
      "Loss for  27036 th iteration => 0.0115946026198\n",
      "Loss for  27037 th iteration => 0.0115941352556\n",
      "Loss for  27038 th iteration => 0.0115937063086\n",
      "Loss for  27039 th iteration => 0.0115931695177\n",
      "Loss for  27040 th iteration => 0.0115927475914\n",
      "Loss for  27041 th iteration => 0.0115923166934\n",
      "Loss for  27042 th iteration => 0.011591745933\n",
      "Loss for  27043 th iteration => 0.0115913132003\n",
      "Loss for  27044 th iteration => 0.0115908790339\n",
      "Loss for  27045 th iteration => 0.0115903536228\n",
      "Loss for  27046 th iteration => 0.0115899198451\n",
      "Loss for  27047 th iteration => 0.0115893990743\n",
      "Loss for  27048 th iteration => 0.0115888850152\n",
      "Loss for  27049 th iteration => 0.0115884848954\n",
      "Loss for  27050 th iteration => 0.0115879606722\n",
      "Loss for  27051 th iteration => 0.0115875261605\n",
      "Loss for  27052 th iteration => 0.0115870055861\n",
      "Loss for  27053 th iteration => 0.0115865036858\n",
      "Loss for  27054 th iteration => 0.0115860933869\n",
      "Loss for  27055 th iteration => 0.0115856544172\n",
      "Loss for  27056 th iteration => 0.0115851387944\n",
      "Loss for  27057 th iteration => 0.0115846281702\n",
      "Loss for  27058 th iteration => 0.0115842043895\n",
      "Loss for  27059 th iteration => 0.0115837128863\n",
      "Loss for  27060 th iteration => 0.0115832652986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  27061 th iteration => 0.0115827578265\n",
      "Loss for  27062 th iteration => 0.0115822483599\n",
      "Loss for  27063 th iteration => 0.0115818269234\n",
      "Loss for  27064 th iteration => 0.0115813330851\n",
      "Loss for  27065 th iteration => 0.0115808870699\n",
      "Loss for  27066 th iteration => 0.0115803881228\n",
      "Loss for  27067 th iteration => 0.011579873992\n",
      "Loss for  27068 th iteration => 0.0115794562111\n",
      "Loss for  27069 th iteration => 0.0115789491126\n",
      "Loss for  27070 th iteration => 0.011578497441\n",
      "Loss for  27071 th iteration => 0.0115779936374\n",
      "Loss for  27072 th iteration => 0.0115774959175\n",
      "Loss for  27073 th iteration => 0.0115770868743\n",
      "Loss for  27074 th iteration => 0.0115765386839\n",
      "Loss for  27075 th iteration => 0.011576108689\n",
      "Loss for  27076 th iteration => 0.0115756018665\n",
      "Loss for  27077 th iteration => 0.011575117026\n",
      "Loss for  27078 th iteration => 0.0115746872178\n",
      "Loss for  27079 th iteration => 0.0115741594495\n",
      "Loss for  27080 th iteration => 0.011573720764\n",
      "Loss for  27081 th iteration => 0.0115732173604\n",
      "Loss for  27082 th iteration => 0.0115727492253\n",
      "Loss for  27083 th iteration => 0.011572298754\n",
      "Loss for  27084 th iteration => 0.0115718788206\n",
      "Loss for  27085 th iteration => 0.0115713412973\n",
      "Loss for  27086 th iteration => 0.0115709224066\n",
      "Loss for  27087 th iteration => 0.0115703861791\n",
      "Loss for  27088 th iteration => 0.0115698975225\n",
      "Loss for  27089 th iteration => 0.01156948953\n",
      "Loss for  27090 th iteration => 0.0115689547199\n",
      "Loss for  27091 th iteration => 0.0115685336551\n",
      "Loss for  27092 th iteration => 0.0115679987251\n",
      "Loss for  27093 th iteration => 0.0115675208796\n",
      "Loss for  27094 th iteration => 0.0115671016099\n",
      "Loss for  27095 th iteration => 0.0115665739419\n",
      "Loss for  27096 th iteration => 0.0115661554342\n",
      "Loss for  27097 th iteration => 0.0115656130823\n",
      "Loss for  27098 th iteration => 0.0115651490396\n",
      "Loss for  27099 th iteration => 0.0115647224027\n",
      "Loss for  27100 th iteration => 0.0115642900021\n",
      "Loss for  27101 th iteration => 0.0115637671005\n",
      "Loss for  27102 th iteration => 0.0115633350914\n",
      "Loss for  27103 th iteration => 0.0115628161196\n",
      "Loss for  27104 th iteration => 0.0115623010777\n",
      "Loss for  27105 th iteration => 0.0115619065487\n",
      "Loss for  27106 th iteration => 0.0115613848214\n",
      "Loss for  27107 th iteration => 0.0115609520888\n",
      "Loss for  27108 th iteration => 0.0115604333795\n",
      "Loss for  27109 th iteration => 0.0115599303398\n",
      "Loss for  27110 th iteration => 0.0115595242406\n",
      "Loss for  27111 th iteration => 0.0115590035475\n",
      "Loss for  27112 th iteration => 0.0115585702265\n",
      "Loss for  27113 th iteration => 0.0115580617278\n",
      "Loss for  27114 th iteration => 0.0115576387741\n",
      "Loss for  27115 th iteration => 0.0115571503905\n",
      "Loss for  27116 th iteration => 0.01155670858\n",
      "Loss for  27117 th iteration => 0.0115561999221\n",
      "Loss for  27118 th iteration => 0.011555692618\n",
      "Loss for  27119 th iteration => 0.0115552718418\n",
      "Loss for  27120 th iteration => 0.0115547809143\n",
      "Loss for  27121 th iteration => 0.0115543360919\n",
      "Loss for  27122 th iteration => 0.0115538418182\n",
      "Loss for  27123 th iteration => 0.0115533236847\n",
      "Loss for  27124 th iteration => 0.0115529116025\n",
      "Loss for  27125 th iteration => 0.01155240852\n",
      "Loss for  27126 th iteration => 0.0115519570887\n",
      "Loss for  27127 th iteration => 0.0115514584384\n",
      "Loss for  27128 th iteration => 0.0115509548586\n",
      "Loss for  27129 th iteration => 0.0115505330952\n",
      "Loss for  27130 th iteration => 0.0115500276744\n",
      "Loss for  27131 th iteration => 0.0115495789062\n",
      "Loss for  27132 th iteration => 0.0115490775362\n",
      "Loss for  27133 th iteration => 0.011548586208\n",
      "Loss for  27134 th iteration => 0.0115481554087\n",
      "Loss for  27135 th iteration => 0.0115476482266\n",
      "Loss for  27136 th iteration => 0.0115472015456\n",
      "Loss for  27137 th iteration => 0.0115466990342\n",
      "Loss for  27138 th iteration => 0.0115462177612\n",
      "Loss for  27139 th iteration => 0.0115457870118\n",
      "Loss for  27140 th iteration => 0.0115453780416\n",
      "Loss for  27141 th iteration => 0.0115448387981\n",
      "Loss for  27142 th iteration => 0.0115444282433\n",
      "Loss for  27143 th iteration => 0.0115438699968\n",
      "Loss for  27144 th iteration => 0.0115434156883\n",
      "Loss for  27145 th iteration => 0.0115429993306\n",
      "Loss for  27146 th iteration => 0.0115424628594\n",
      "Loss for  27147 th iteration => 0.0115420487269\n",
      "Loss for  27148 th iteration => 0.0115415038489\n",
      "Loss for  27149 th iteration => 0.0115410402979\n",
      "Loss for  27150 th iteration => 0.011540621985\n",
      "Loss for  27151 th iteration => 0.0115400878254\n",
      "Loss for  27152 th iteration => 0.0115396716639\n",
      "Loss for  27153 th iteration => 0.0115391378516\n",
      "Loss for  27154 th iteration => 0.0115386730461\n",
      "Loss for  27155 th iteration => 0.011538251275\n",
      "Loss for  27156 th iteration => 0.0115377159829\n",
      "Loss for  27157 th iteration => 0.011537301639\n",
      "Loss for  27158 th iteration => 0.0115367803834\n",
      "Loss for  27159 th iteration => 0.0115363973082\n",
      "Loss for  27160 th iteration => 0.0115358784498\n",
      "Loss for  27161 th iteration => 0.0115354469127\n",
      "Loss for  27162 th iteration => 0.0115349286185\n",
      "Loss for  27163 th iteration => 0.011534420228\n",
      "Loss for  27164 th iteration => 0.0115340251423\n",
      "Loss for  27165 th iteration => 0.0115335073642\n",
      "Loss for  27166 th iteration => 0.0115330751919\n",
      "Loss for  27167 th iteration => 0.0115325577676\n",
      "Loss for  27168 th iteration => 0.0115320600816\n",
      "Loss for  27169 th iteration => 0.0115316541067\n",
      "Loss for  27170 th iteration => 0.0115311372735\n",
      "Loss for  27171 th iteration => 0.0115307045965\n",
      "Loss for  27172 th iteration => 0.0115301998742\n",
      "Loss for  27173 th iteration => 0.0115297780073\n",
      "Loss for  27174 th iteration => 0.0115292932226\n",
      "Loss for  27175 th iteration => 0.0115288524887\n",
      "Loss for  27176 th iteration => 0.0115283454682\n",
      "Loss for  27177 th iteration => 0.0115278420023\n",
      "Loss for  27178 th iteration => 0.0115274220772\n",
      "Loss for  27179 th iteration => 0.0115269344279\n",
      "Loss for  27180 th iteration => 0.0115264905768\n",
      "Loss for  27181 th iteration => 0.0115259982415\n",
      "Loss for  27182 th iteration => 0.011525483184\n",
      "Loss for  27183 th iteration => 0.0115250727408\n",
      "Loss for  27184 th iteration => 0.011524572856\n",
      "Loss for  27185 th iteration => 0.0115241226727\n",
      "Loss for  27186 th iteration => 0.0115236266766\n",
      "Loss for  27187 th iteration => 0.0115231251554\n",
      "Loss for  27188 th iteration => 0.0115227053293\n",
      "Loss for  27189 th iteration => 0.0115222033455\n",
      "Loss for  27190 th iteration => 0.0115217555841\n",
      "Loss for  27191 th iteration => 0.011521257543\n",
      "Loss for  27192 th iteration => 0.0115207673119\n",
      "Loss for  27193 th iteration => 0.0115203387336\n",
      "Loss for  27194 th iteration => 0.0115198352128\n",
      "Loss for  27195 th iteration => 0.0115193893125\n",
      "Loss for  27196 th iteration => 0.0115188907655\n",
      "Loss for  27197 th iteration => 0.0115184096813\n",
      "Loss for  27198 th iteration => 0.0115179797859\n",
      "Loss for  27199 th iteration => 0.0115175761975\n",
      "Loss for  27200 th iteration => 0.0115170383393\n",
      "Loss for  27201 th iteration => 0.0115165467719\n",
      "Loss for  27202 th iteration => 0.0115161561187\n",
      "Loss for  27203 th iteration => 0.0115156205123\n",
      "Loss for  27204 th iteration => 0.0115152087776\n",
      "Loss for  27205 th iteration => 0.0115146721495\n",
      "Loss for  27206 th iteration => 0.0115141915866\n",
      "Loss for  27207 th iteration => 0.0115137895222\n",
      "Loss for  27208 th iteration => 0.0115132562299\n",
      "Loss for  27209 th iteration => 0.0115128427046\n",
      "Loss for  27210 th iteration => 0.0115123082086\n",
      "Loss for  27211 th iteration => 0.0115118983965\n",
      "Loss for  27212 th iteration => 0.0115113619837\n",
      "Loss for  27213 th iteration => 0.0115108978167\n",
      "Loss for  27214 th iteration => 0.011510484879\n",
      "Loss for  27215 th iteration => 0.0115099456987\n",
      "Loss for  27216 th iteration => 0.0115094858034\n",
      "Loss for  27217 th iteration => 0.0115090668534\n",
      "Loss for  27218 th iteration => 0.0115086360984\n",
      "Loss for  27219 th iteration => 0.0115081204802\n",
      "Loss for  27220 th iteration => 0.0115076907704\n",
      "Loss for  27221 th iteration => 0.0115071361928\n",
      "Loss for  27222 th iteration => 0.0115067064862\n",
      "Loss for  27223 th iteration => 0.0115062751014\n",
      "Loss for  27224 th iteration => 0.0115057605028\n",
      "Loss for  27225 th iteration => 0.0115053303913\n",
      "Loss for  27226 th iteration => 0.0115047866687\n",
      "Loss for  27227 th iteration => 0.0115043471073\n",
      "Loss for  27228 th iteration => 0.011503915222\n",
      "Loss for  27229 th iteration => 0.0115034015117\n",
      "Loss for  27230 th iteration => 0.0115029721564\n",
      "Loss for  27231 th iteration => 0.0115024451035\n",
      "Loss for  27232 th iteration => 0.0115020712446\n",
      "Loss for  27233 th iteration => 0.0115015617784\n",
      "Loss for  27234 th iteration => 0.011501131386\n",
      "Loss for  27235 th iteration => 0.0115006248142\n",
      "Loss for  27236 th iteration => 0.0115001067829\n",
      "Loss for  27237 th iteration => 0.0114997171767\n",
      "Loss for  27238 th iteration => 0.0114992138229\n",
      "Loss for  27239 th iteration => 0.0114987158797\n",
      "Loss for  27240 th iteration => 0.0114983409234\n",
      "Loss for  27241 th iteration => 0.0114977690602\n",
      "Loss for  27242 th iteration => 0.0114973703735\n",
      "Loss for  27243 th iteration => 0.0114968772853\n",
      "Loss for  27244 th iteration => 0.0114963707205\n",
      "Loss for  27245 th iteration => 0.0114959944099\n",
      "Loss for  27246 th iteration => 0.0114954270267\n",
      "Loss for  27247 th iteration => 0.0114950140796\n",
      "Loss for  27248 th iteration => 0.0114945183668\n",
      "Loss for  27249 th iteration => 0.0114940237654\n",
      "Loss for  27250 th iteration => 0.0114936278333\n",
      "Loss for  27251 th iteration => 0.0114930800512\n",
      "Loss for  27252 th iteration => 0.0114926585965\n",
      "Loss for  27253 th iteration => 0.0114921618312\n",
      "Loss for  27254 th iteration => 0.0114916762474\n",
      "Loss for  27255 th iteration => 0.0114912486707\n",
      "Loss for  27256 th iteration => 0.0114907464395\n",
      "Loss for  27257 th iteration => 0.0114903038769\n",
      "Loss for  27258 th iteration => 0.0114898074012\n",
      "Loss for  27259 th iteration => 0.0114893298439\n",
      "Loss for  27260 th iteration => 0.011488900201\n",
      "Loss for  27261 th iteration => 0.0114884990897\n",
      "Loss for  27262 th iteration => 0.0114879651961\n",
      "Loss for  27263 th iteration => 0.0114874754925\n",
      "Loss for  27264 th iteration => 0.0114870860943\n",
      "Loss for  27265 th iteration => 0.0114865530508\n",
      "Loss for  27266 th iteration => 0.0114861434643\n",
      "Loss for  27267 th iteration => 0.0114856111877\n",
      "Loss for  27268 th iteration => 0.0114851314522\n",
      "Loss for  27269 th iteration => 0.01148473128\n",
      "Loss for  27270 th iteration => 0.0114842003569\n",
      "Loss for  27271 th iteration => 0.0114837891659\n",
      "Loss for  27272 th iteration => 0.0114832593513\n",
      "Loss for  27273 th iteration => 0.0114827873517\n",
      "Loss for  27274 th iteration => 0.0114823777768\n",
      "Loss for  27275 th iteration => 0.0114818531539\n",
      "Loss for  27276 th iteration => 0.0114814454521\n",
      "Loss for  27277 th iteration => 0.0114809085802\n",
      "Loss for  27278 th iteration => 0.0114804478677\n",
      "Loss for  27279 th iteration => 0.0114800317225\n",
      "Loss for  27280 th iteration => 0.0114796025839\n",
      "Loss for  27281 th iteration => 0.0114790899923\n",
      "Loss for  27282 th iteration => 0.0114786654374\n",
      "Loss for  27283 th iteration => 0.0114781094192\n",
      "Loss for  27284 th iteration => 0.0114776829523\n",
      "Loss for  27285 th iteration => 0.0114772532457\n",
      "Loss for  27286 th iteration => 0.0114767416082\n",
      "Loss for  27287 th iteration => 0.0114763171928\n",
      "Loss for  27288 th iteration => 0.0114757710708\n",
      "Loss for  27289 th iteration => 0.0114753351608\n",
      "Loss for  27290 th iteration => 0.011474905012\n",
      "Loss for  27291 th iteration => 0.0114743954602\n",
      "Loss for  27292 th iteration => 0.0114739047735\n",
      "Loss for  27293 th iteration => 0.0114735011347\n",
      "Loss for  27294 th iteration => 0.0114730720639\n",
      "Loss for  27295 th iteration => 0.0114725629072\n",
      "Loss for  27296 th iteration => 0.0114720658058\n",
      "Loss for  27297 th iteration => 0.011471647498\n",
      "Loss for  27298 th iteration => 0.0114711638635\n",
      "Loss for  27299 th iteration => 0.0114707294133\n",
      "Loss for  27300 th iteration => 0.0114702280716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  27301 th iteration => 0.0114697234311\n",
      "Loss for  27302 th iteration => 0.0114693265473\n",
      "Loss for  27303 th iteration => 0.0114688338062\n",
      "Loss for  27304 th iteration => 0.0114683853864\n",
      "Loss for  27305 th iteration => 0.0114679002732\n",
      "Loss for  27306 th iteration => 0.0114673896791\n",
      "Loss for  27307 th iteration => 0.0114669813594\n",
      "Loss for  27308 th iteration => 0.0114664866478\n",
      "Loss for  27309 th iteration => 0.0114660405172\n",
      "Loss for  27310 th iteration => 0.0114655537408\n",
      "Loss for  27311 th iteration => 0.0114650538618\n",
      "Loss for  27312 th iteration => 0.0114646369763\n",
      "Loss for  27313 th iteration => 0.0114641408438\n",
      "Loss for  27314 th iteration => 0.0114636964539\n",
      "Loss for  27315 th iteration => 0.0114632095203\n",
      "Loss for  27316 th iteration => 0.0114627182588\n",
      "Loss for  27317 th iteration => 0.0114622933998\n",
      "Loss for  27318 th iteration => 0.0114617978895\n",
      "Loss for  27319 th iteration => 0.0114613210817\n",
      "Loss for  27320 th iteration => 0.0114608974591\n",
      "Loss for  27321 th iteration => 0.0114604981271\n",
      "Loss for  27322 th iteration => 0.0114599638922\n",
      "Loss for  27323 th iteration => 0.0114594746226\n",
      "Loss for  27324 th iteration => 0.0114590919182\n",
      "Loss for  27325 th iteration => 0.0114585593393\n",
      "Loss for  27326 th iteration => 0.0114581538031\n",
      "Loss for  27327 th iteration => 0.0114576215135\n",
      "Loss for  27328 th iteration => 0.0114571413552\n",
      "Loss for  27329 th iteration => 0.0114567483945\n",
      "Loss for  27330 th iteration => 0.0114562177859\n",
      "Loss for  27331 th iteration => 0.0114558107888\n",
      "Loss for  27332 th iteration => 0.0114552812714\n",
      "Loss for  27333 th iteration => 0.0114548080409\n",
      "Loss for  27334 th iteration => 0.0114544061651\n",
      "Loss for  27335 th iteration => 0.01145387809\n",
      "Loss for  27336 th iteration => 0.0114534778355\n",
      "Loss for  27337 th iteration => 0.0114529383495\n",
      "Loss for  27338 th iteration => 0.0114524793072\n",
      "Loss for  27339 th iteration => 0.0114520713276\n",
      "Loss for  27340 th iteration => 0.0114516428813\n",
      "Loss for  27341 th iteration => 0.0114511340546\n",
      "Loss for  27342 th iteration => 0.0114507110898\n",
      "Loss for  27343 th iteration => 0.0114501516332\n",
      "Loss for  27344 th iteration => 0.0114497336921\n",
      "Loss for  27345 th iteration => 0.0114493047355\n",
      "Loss for  27346 th iteration => 0.0114487969611\n",
      "Loss for  27347 th iteration => 0.0114482947044\n",
      "Loss for  27348 th iteration => 0.0114479037833\n",
      "Loss for  27349 th iteration => 0.0114473970207\n",
      "Loss for  27350 th iteration => 0.0114469676951\n",
      "Loss for  27351 th iteration => 0.0114464626759\n",
      "Loss for  27352 th iteration => 0.0114459671045\n",
      "Loss for  27353 th iteration => 0.0114455697939\n",
      "Loss for  27354 th iteration => 0.0114451492133\n",
      "Loss for  27355 th iteration => 0.0114446383478\n",
      "Loss for  27356 th iteration => 0.0114441196874\n",
      "Loss for  27357 th iteration => 0.0114437487422\n",
      "Loss for  27358 th iteration => 0.0114432427582\n",
      "Loss for  27359 th iteration => 0.011442818441\n",
      "Loss for  27360 th iteration => 0.0114423154487\n",
      "Loss for  27361 th iteration => 0.011441802473\n",
      "Loss for  27362 th iteration => 0.0114414186424\n",
      "Loss for  27363 th iteration => 0.0114409176022\n",
      "Loss for  27364 th iteration => 0.011440426592\n",
      "Loss for  27365 th iteration => 0.011440055695\n",
      "Loss for  27366 th iteration => 0.0114394859306\n",
      "Loss for  27367 th iteration => 0.0114390939092\n",
      "Loss for  27368 th iteration => 0.0114386066519\n",
      "Loss for  27369 th iteration => 0.0114381024494\n",
      "Loss for  27370 th iteration => 0.0114376968461\n",
      "Loss for  27371 th iteration => 0.0114372025772\n",
      "Loss for  27372 th iteration => 0.0114367606585\n",
      "Loss for  27373 th iteration => 0.0114362720318\n",
      "Loss for  27374 th iteration => 0.0114357781306\n",
      "Loss for  27375 th iteration => 0.0114353640782\n",
      "Loss for  27376 th iteration => 0.0114348684945\n",
      "Loss for  27377 th iteration => 0.0114344282082\n",
      "Loss for  27378 th iteration => 0.0114339396974\n",
      "Loss for  27379 th iteration => 0.011433454028\n",
      "Loss for  27380 th iteration => 0.0114330321113\n",
      "Loss for  27381 th iteration => 0.011432535727\n",
      "Loss for  27382 th iteration => 0.0114320965602\n",
      "Loss for  27383 th iteration => 0.0114316095802\n",
      "Loss for  27384 th iteration => 0.011431130168\n",
      "Loss for  27385 th iteration => 0.0114307009474\n",
      "Loss for  27386 th iteration => 0.011430225175\n",
      "Loss for  27387 th iteration => 0.0114298467499\n",
      "Loss for  27388 th iteration => 0.0114293153974\n",
      "Loss for  27389 th iteration => 0.0114289132967\n",
      "Loss for  27390 th iteration => 0.0114283822712\n",
      "Loss for  27391 th iteration => 0.0114279034475\n",
      "Loss for  27392 th iteration => 0.0114275148775\n",
      "Loss for  27393 th iteration => 0.0114269854668\n",
      "Loss for  27394 th iteration => 0.0114265819296\n",
      "Loss for  27395 th iteration => 0.0114260537354\n",
      "Loss for  27396 th iteration => 0.0114255816746\n",
      "Loss for  27397 th iteration => 0.0114251842892\n",
      "Loss for  27398 th iteration => 0.0114246564099\n",
      "Loss for  27399 th iteration => 0.0114242518377\n",
      "Loss for  27400 th iteration => 0.011423727258\n",
      "Loss for  27401 th iteration => 0.0114232598949\n",
      "Loss for  27402 th iteration => 0.0114228549621\n",
      "Loss for  27403 th iteration => 0.0114223343535\n",
      "Loss for  27404 th iteration => 0.0114219337912\n",
      "Loss for  27405 th iteration => 0.0114214190335\n",
      "Loss for  27406 th iteration => 0.011421042941\n",
      "Loss for  27407 th iteration => 0.0114205389588\n",
      "Loss for  27408 th iteration => 0.011420111335\n",
      "Loss for  27409 th iteration => 0.0114195657791\n",
      "Loss for  27410 th iteration => 0.0114191458848\n",
      "Loss for  27411 th iteration => 0.0114187170215\n",
      "Loss for  27412 th iteration => 0.0114182138561\n",
      "Loss for  27413 th iteration => 0.0114177873897\n",
      "Loss for  27414 th iteration => 0.0114172499499\n",
      "Loss for  27415 th iteration => 0.0114168213633\n",
      "Loss for  27416 th iteration => 0.0114163921812\n",
      "Loss for  27417 th iteration => 0.0114158897121\n",
      "Loss for  27418 th iteration => 0.0114154654736\n",
      "Loss for  27419 th iteration => 0.0114149342771\n",
      "Loss for  27420 th iteration => 0.0114144977971\n",
      "Loss for  27421 th iteration => 0.011414068409\n",
      "Loss for  27422 th iteration => 0.0114135784639\n",
      "Loss for  27423 th iteration => 0.0114131632266\n",
      "Loss for  27424 th iteration => 0.0114126848065\n",
      "Loss for  27425 th iteration => 0.011412257381\n",
      "Loss for  27426 th iteration => 0.0114117568836\n",
      "Loss for  27427 th iteration => 0.0114112556515\n",
      "Loss for  27428 th iteration => 0.0114108651883\n",
      "Loss for  27429 th iteration => 0.0114103700287\n",
      "Loss for  27430 th iteration => 0.0114099403477\n",
      "Loss for  27431 th iteration => 0.0114094480203\n",
      "Loss for  27432 th iteration => 0.0114089497823\n",
      "Loss for  27433 th iteration => 0.0114085558791\n",
      "Loss for  27434 th iteration => 0.011408068236\n",
      "Loss for  27435 th iteration => 0.0114075725881\n",
      "Loss for  27436 th iteration => 0.0114071981709\n",
      "Loss for  27437 th iteration => 0.0114066421626\n",
      "Loss for  27438 th iteration => 0.0114062347148\n",
      "Loss for  27439 th iteration => 0.0114057467347\n",
      "Loss for  27440 th iteration => 0.0114052587207\n",
      "Loss for  27441 th iteration => 0.0114048453473\n",
      "Loss for  27442 th iteration => 0.0114043514801\n",
      "Loss for  27443 th iteration => 0.0114039142969\n",
      "Loss for  27444 th iteration => 0.0114034272822\n",
      "Loss for  27445 th iteration => 0.0114029463022\n",
      "Loss for  27446 th iteration => 0.0114025254097\n",
      "Loss for  27447 th iteration => 0.0114020310336\n",
      "Loss for  27448 th iteration => 0.0114015946766\n",
      "Loss for  27449 th iteration => 0.0114011099949\n",
      "Loss for  27450 th iteration => 0.0114006341383\n",
      "Loss for  27451 th iteration => 0.0114002062706\n",
      "Loss for  27452 th iteration => 0.0113997330142\n",
      "Loss for  27453 th iteration => 0.0113993567538\n",
      "Loss for  27454 th iteration => 0.0113988272369\n",
      "Loss for  27455 th iteration => 0.0113984282045\n",
      "Loss for  27456 th iteration => 0.0113979005266\n",
      "Loss for  27457 th iteration => 0.0113974228953\n",
      "Loss for  27458 th iteration => 0.011397037146\n",
      "Loss for  27459 th iteration => 0.0113965093749\n",
      "Loss for  27460 th iteration => 0.0113961090942\n",
      "Loss for  27461 th iteration => 0.0113955846292\n",
      "Loss for  27462 th iteration => 0.0113951127481\n",
      "Loss for  27463 th iteration => 0.0113947188025\n",
      "Loss for  27464 th iteration => 0.0113941923791\n",
      "Loss for  27465 th iteration => 0.0113937912398\n",
      "Loss for  27466 th iteration => 0.0113932707498\n",
      "Loss for  27467 th iteration => 0.0113928026105\n",
      "Loss for  27468 th iteration => 0.0113924017013\n",
      "Loss for  27469 th iteration => 0.0113918835815\n",
      "Loss for  27470 th iteration => 0.0113914312431\n",
      "Loss for  27471 th iteration => 0.011391024748\n",
      "Loss for  27472 th iteration => 0.0113905977376\n",
      "Loss for  27473 th iteration => 0.0113900971723\n",
      "Loss for  27474 th iteration => 0.0113896746071\n",
      "Loss for  27475 th iteration => 0.0113891269221\n",
      "Loss for  27476 th iteration => 0.0113887113356\n",
      "Loss for  27477 th iteration => 0.0113882839514\n",
      "Loss for  27478 th iteration => 0.0113877841353\n",
      "Loss for  27479 th iteration => 0.011387363273\n",
      "Loss for  27480 th iteration => 0.0113868227394\n",
      "Loss for  27481 th iteration => 0.0113863988739\n",
      "Loss for  27482 th iteration => 0.0113859712312\n",
      "Loss for  27483 th iteration => 0.011385473463\n",
      "Loss for  27484 th iteration => 0.0113849858482\n",
      "Loss for  27485 th iteration => 0.011384585522\n",
      "Loss for  27486 th iteration => 0.011384087352\n",
      "Loss for  27487 th iteration => 0.0113836630793\n",
      "Loss for  27488 th iteration => 0.0113831489995\n",
      "Loss for  27489 th iteration => 0.0113827816657\n",
      "Loss for  27490 th iteration => 0.011382279405\n",
      "Loss for  27491 th iteration => 0.0113817942569\n",
      "Loss for  27492 th iteration => 0.0113814327808\n",
      "Loss for  27493 th iteration => 0.0113808539497\n",
      "Loss for  27494 th iteration => 0.0113804760526\n",
      "Loss for  27495 th iteration => 0.0113799807177\n",
      "Loss for  27496 th iteration => 0.0113794873389\n",
      "Loss for  27497 th iteration => 0.0113790955094\n",
      "Loss for  27498 th iteration => 0.0113786089662\n",
      "Loss for  27499 th iteration => 0.0113781686144\n",
      "Loss for  27500 th iteration => 0.0113776902161\n",
      "Loss for  27501 th iteration => 0.0113771882781\n",
      "Loss for  27502 th iteration => 0.0113767859274\n",
      "Loss for  27503 th iteration => 0.0113762986005\n",
      "Loss for  27504 th iteration => 0.0113758134059\n",
      "Loss for  27505 th iteration => 0.0113754296212\n",
      "Loss for  27506 th iteration => 0.0113748868164\n",
      "Loss for  27507 th iteration => 0.0113744771523\n",
      "Loss for  27508 th iteration => 0.0113739915866\n",
      "Loss for  27509 th iteration => 0.011373510292\n",
      "Loss for  27510 th iteration => 0.0113730952132\n",
      "Loss for  27511 th iteration => 0.0113726039939\n",
      "Loss for  27512 th iteration => 0.011372169122\n",
      "Loss for  27513 th iteration => 0.0113716865075\n",
      "Loss for  27514 th iteration => 0.0113712094571\n",
      "Loss for  27515 th iteration => 0.0113707876611\n",
      "Loss for  27516 th iteration => 0.0113703132116\n",
      "Loss for  27517 th iteration => 0.0113699302758\n",
      "Loss for  27518 th iteration => 0.011369418115\n",
      "Loss for  27519 th iteration => 0.011369026258\n",
      "Loss for  27520 th iteration => 0.011368500773\n",
      "Loss for  27521 th iteration => 0.0113680193948\n",
      "Loss for  27522 th iteration => 0.0113676421633\n",
      "Loss for  27523 th iteration => 0.0113671157178\n",
      "Loss for  27524 th iteration => 0.0113667187558\n",
      "Loss for  27525 th iteration => 0.0113661964354\n",
      "Loss for  27526 th iteration => 0.011365720924\n",
      "Loss for  27527 th iteration => 0.0113653354237\n",
      "Loss for  27528 th iteration => 0.0113648103526\n",
      "Loss for  27529 th iteration => 0.0113644129561\n",
      "Loss for  27530 th iteration => 0.0113638860335\n",
      "Loss for  27531 th iteration => 0.0113634310775\n",
      "Loss for  27532 th iteration => 0.0113630357446\n",
      "Loss for  27533 th iteration => 0.011362503551\n",
      "Loss for  27534 th iteration => 0.011362053198\n",
      "Loss for  27535 th iteration => 0.0113616427824\n",
      "Loss for  27536 th iteration => 0.0113611233333\n",
      "Loss for  27537 th iteration => 0.0113607252163\n",
      "Loss for  27538 th iteration => 0.0113603062178\n",
      "Loss for  27539 th iteration => 0.0113597526047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  27540 th iteration => 0.0113593463532\n",
      "Loss for  27541 th iteration => 0.0113589209604\n",
      "Loss for  27542 th iteration => 0.0113584242986\n",
      "Loss for  27543 th iteration => 0.0113579247415\n",
      "Loss for  27544 th iteration => 0.0113575416321\n",
      "Loss for  27545 th iteration => 0.0113570455012\n",
      "Loss for  27546 th iteration => 0.0113566227153\n",
      "Loss for  27547 th iteration => 0.0113560887113\n",
      "Loss for  27548 th iteration => 0.0113556675485\n",
      "Loss for  27549 th iteration => 0.0113552412597\n",
      "Loss for  27550 th iteration => 0.0113547581505\n",
      "Loss for  27551 th iteration => 0.0113544106683\n",
      "Loss for  27552 th iteration => 0.0113538066667\n",
      "Loss for  27553 th iteration => 0.0113534578799\n",
      "Loss for  27554 th iteration => 0.0113529530489\n",
      "Loss for  27555 th iteration => 0.0113524462897\n",
      "Loss for  27556 th iteration => 0.0113520799545\n",
      "Loss for  27557 th iteration => 0.0113515803808\n",
      "Loss for  27558 th iteration => 0.0113511645692\n",
      "Loss for  27559 th iteration => 0.0113506682694\n",
      "Loss for  27560 th iteration => 0.0113501621103\n",
      "Loss for  27561 th iteration => 0.0113497872587\n",
      "Loss for  27562 th iteration => 0.0113492910456\n",
      "Loss for  27563 th iteration => 0.0113488095591\n",
      "Loss for  27564 th iteration => 0.01134840119\n",
      "Loss for  27565 th iteration => 0.0113479215602\n",
      "Loss for  27566 th iteration => 0.0113474967788\n",
      "Loss for  27567 th iteration => 0.0113470197343\n",
      "Loss for  27568 th iteration => 0.0113465193932\n",
      "Loss for  27569 th iteration => 0.0113461213382\n",
      "Loss for  27570 th iteration => 0.0113456356148\n",
      "Loss for  27571 th iteration => 0.0113451996011\n",
      "Loss for  27572 th iteration => 0.0113447233009\n",
      "Loss for  27573 th iteration => 0.0113442302565\n",
      "Loss for  27574 th iteration => 0.0113438246334\n",
      "Loss for  27575 th iteration => 0.0113433403938\n",
      "Loss for  27576 th iteration => 0.0113428609249\n",
      "Loss for  27577 th iteration => 0.0113424499342\n",
      "Loss for  27578 th iteration => 0.0113419608161\n",
      "Loss for  27579 th iteration => 0.0113415286834\n",
      "Loss for  27580 th iteration => 0.0113410475511\n",
      "Loss for  27581 th iteration => 0.0113405720948\n",
      "Loss for  27582 th iteration => 0.0113401544588\n",
      "Loss for  27583 th iteration => 0.0113396667935\n",
      "Loss for  27584 th iteration => 0.0113392029559\n",
      "Loss for  27585 th iteration => 0.0113387845206\n",
      "Loss for  27586 th iteration => 0.0113383968736\n",
      "Loss for  27587 th iteration => 0.0113378732963\n",
      "Loss for  27588 th iteration => 0.0113373906864\n",
      "Loss for  27589 th iteration => 0.0113370201216\n",
      "Loss for  27590 th iteration => 0.0113364940769\n",
      "Loss for  27591 th iteration => 0.0113361015996\n",
      "Loss for  27592 th iteration => 0.0113355815225\n",
      "Loss for  27593 th iteration => 0.0113351039018\n",
      "Loss for  27594 th iteration => 0.0113347255992\n",
      "Loss for  27595 th iteration => 0.0113342022482\n",
      "Loss for  27596 th iteration => 0.0113337372934\n",
      "Loss for  27597 th iteration => 0.0113333500709\n",
      "Loss for  27598 th iteration => 0.0113328277177\n",
      "Loss for  27599 th iteration => 0.0113324323287\n",
      "Loss for  27600 th iteration => 0.0113319137447\n",
      "Loss for  27601 th iteration => 0.0113314504308\n",
      "Loss for  27602 th iteration => 0.01133105753\n",
      "Loss for  27603 th iteration => 0.0113305392521\n",
      "Loss for  27604 th iteration => 0.0113300927734\n",
      "Loss for  27605 th iteration => 0.0113296956768\n",
      "Loss for  27606 th iteration => 0.0113292710781\n",
      "Loss for  27607 th iteration => 0.0113287777945\n",
      "Loss for  27608 th iteration => 0.0113283575409\n",
      "Loss for  27609 th iteration => 0.0113278117082\n",
      "Loss for  27610 th iteration => 0.011327406481\n",
      "Loss for  27611 th iteration => 0.0113269816519\n",
      "Loss for  27612 th iteration => 0.011326488967\n",
      "Loss for  27613 th iteration => 0.0113260716324\n",
      "Loss for  27614 th iteration => 0.0113255308121\n",
      "Loss for  27615 th iteration => 0.0113251182168\n",
      "Loss for  27616 th iteration => 0.0113246947789\n",
      "Loss for  27617 th iteration => 0.0113241674962\n",
      "Loss for  27618 th iteration => 0.0113237478047\n",
      "Loss for  27619 th iteration => 0.0113233221466\n",
      "Loss for  27620 th iteration => 0.0113228308747\n",
      "Loss for  27621 th iteration => 0.0113224120884\n",
      "Loss for  27622 th iteration => 0.011321892648\n",
      "Loss for  27623 th iteration => 0.0113215441496\n",
      "Loss for  27624 th iteration => 0.0113210414487\n",
      "Loss for  27625 th iteration => 0.0113205390686\n",
      "Loss for  27626 th iteration => 0.0113201740311\n",
      "Loss for  27627 th iteration => 0.011319676276\n",
      "Loss for  27628 th iteration => 0.0113192638097\n",
      "Loss for  27629 th iteration => 0.0113187698704\n",
      "Loss for  27630 th iteration => 0.0113182663225\n",
      "Loss for  27631 th iteration => 0.0113178942864\n",
      "Loss for  27632 th iteration => 0.0113174003873\n",
      "Loss for  27633 th iteration => 0.0113169131314\n",
      "Loss for  27634 th iteration => 0.0113165268409\n",
      "Loss for  27635 th iteration => 0.0113160443553\n",
      "Loss for  27636 th iteration => 0.0113156097761\n",
      "Loss for  27637 th iteration => 0.0113151355181\n",
      "Loss for  27638 th iteration => 0.011314637871\n",
      "Loss for  27639 th iteration => 0.0113142417946\n",
      "Loss for  27640 th iteration => 0.0113137595824\n",
      "Loss for  27641 th iteration => 0.0113132769868\n",
      "Loss for  27642 th iteration => 0.0113129024572\n",
      "Loss for  27643 th iteration => 0.0113123603384\n",
      "Loss for  27644 th iteration => 0.0113119575459\n",
      "Loss for  27645 th iteration => 0.0113114784985\n",
      "Loss for  27646 th iteration => 0.0113109981544\n",
      "Loss for  27647 th iteration => 0.011310590302\n",
      "Loss for  27648 th iteration => 0.0113101046989\n",
      "Loss for  27649 th iteration => 0.0113096376945\n",
      "Loss for  27650 th iteration => 0.0113092379203\n",
      "Loss for  27651 th iteration => 0.0113087209521\n",
      "Loss for  27652 th iteration => 0.0113083072771\n",
      "Loss for  27653 th iteration => 0.0113078370559\n",
      "Loss for  27654 th iteration => 0.0113074608118\n",
      "Loss for  27655 th iteration => 0.0113069472134\n",
      "Loss for  27656 th iteration => 0.0113065673024\n",
      "Loss for  27657 th iteration => 0.011306022048\n",
      "Loss for  27658 th iteration => 0.0113055880653\n",
      "Loss for  27659 th iteration => 0.0113051971953\n",
      "Loss for  27660 th iteration => 0.0113046761038\n",
      "Loss for  27661 th iteration => 0.0113042029784\n",
      "Loss for  27662 th iteration => 0.0113038285197\n",
      "Loss for  27663 th iteration => 0.0113033069151\n",
      "Loss for  27664 th iteration => 0.0113029153586\n",
      "Loss for  27665 th iteration => 0.0113023986058\n",
      "Loss for  27666 th iteration => 0.011301928237\n",
      "Loss for  27667 th iteration => 0.0113015474146\n",
      "Loss for  27668 th iteration => 0.0113010279068\n",
      "Loss for  27669 th iteration => 0.0113005688477\n",
      "Loss for  27670 th iteration => 0.0113001799305\n",
      "Loss for  27671 th iteration => 0.0112996617832\n",
      "Loss for  27672 th iteration => 0.01129927529\n",
      "Loss for  27673 th iteration => 0.011298747662\n",
      "Loss for  27674 th iteration => 0.0112982982487\n",
      "Loss for  27675 th iteration => 0.0112979072619\n",
      "Loss for  27676 th iteration => 0.0112974853756\n",
      "Loss for  27677 th iteration => 0.0112969418404\n",
      "Loss for  27678 th iteration => 0.0112965432312\n",
      "Loss for  27679 th iteration => 0.0112961197172\n",
      "Loss for  27680 th iteration => 0.0112956305735\n",
      "Loss for  27681 th iteration => 0.0112952122272\n",
      "Loss for  27682 th iteration => 0.0112946726149\n",
      "Loss for  27683 th iteration => 0.0112942671003\n",
      "Loss for  27684 th iteration => 0.011293843532\n",
      "Loss for  27685 th iteration => 0.0112933548061\n",
      "Loss for  27686 th iteration => 0.0112929409403\n",
      "Loss for  27687 th iteration => 0.0112924036026\n",
      "Loss for  27688 th iteration => 0.0112919918875\n",
      "Loss for  27689 th iteration => 0.0112915728729\n",
      "Loss for  27690 th iteration => 0.0112910545232\n",
      "Loss for  27691 th iteration => 0.0112907159943\n",
      "Loss for  27692 th iteration => 0.0112902099704\n",
      "Loss for  27693 th iteration => 0.0112897337493\n",
      "Loss for  27694 th iteration => 0.011289389099\n",
      "Loss for  27695 th iteration => 0.0112887930974\n",
      "Loss for  27696 th iteration => 0.0112884474848\n",
      "Loss for  27697 th iteration => 0.0112879508749\n",
      "Loss for  27698 th iteration => 0.0112874466258\n",
      "Loss for  27699 th iteration => 0.0112870854023\n",
      "Loss for  27700 th iteration => 0.0112865889676\n",
      "Loss for  27701 th iteration => 0.0112861001529\n",
      "Loss for  27702 th iteration => 0.0112857237905\n",
      "Loss for  27703 th iteration => 0.0112852332823\n",
      "Loss for  27704 th iteration => 0.0112848191871\n",
      "Loss for  27705 th iteration => 0.0112843317976\n",
      "Loss for  27706 th iteration => 0.0112838393914\n",
      "Loss for  27707 th iteration => 0.0112834595464\n",
      "Loss for  27708 th iteration => 0.011282982501\n",
      "Loss for  27709 th iteration => 0.0112824911842\n",
      "Loss for  27710 th iteration => 0.0112820991088\n",
      "Loss for  27711 th iteration => 0.0112816162566\n",
      "Loss for  27712 th iteration => 0.0112811873956\n",
      "Loss for  27713 th iteration => 0.0112807127488\n",
      "Loss for  27714 th iteration => 0.0112802264001\n",
      "Loss for  27715 th iteration => 0.0112798274247\n",
      "Loss for  27716 th iteration => 0.0112793458736\n",
      "Loss for  27717 th iteration => 0.0112788728658\n",
      "Loss for  27718 th iteration => 0.0112784893531\n",
      "Loss for  27719 th iteration => 0.0112779615718\n",
      "Loss for  27720 th iteration => 0.0112775565359\n",
      "Loss for  27721 th iteration => 0.0112770797276\n",
      "Loss for  27722 th iteration => 0.0112766070081\n",
      "Loss for  27723 th iteration => 0.0112761972986\n",
      "Loss for  27724 th iteration => 0.0112757149015\n",
      "Loss for  27725 th iteration => 0.0112752524921\n",
      "Loss for  27726 th iteration => 0.0112748404482\n",
      "Loss for  27727 th iteration => 0.0112744587804\n",
      "Loss for  27728 th iteration => 0.0112739419885\n",
      "Loss for  27729 th iteration => 0.0112734588367\n",
      "Loss for  27730 th iteration => 0.011273097307\n",
      "Loss for  27731 th iteration => 0.0112725772635\n",
      "Loss for  27732 th iteration => 0.0112721067436\n",
      "Loss for  27733 th iteration => 0.0112717362995\n",
      "Loss for  27734 th iteration => 0.0112712170139\n",
      "Loss for  27735 th iteration => 0.0112708282398\n",
      "Loss for  27736 th iteration => 0.0112703129318\n",
      "Loss for  27737 th iteration => 0.0112698441132\n",
      "Loss for  27738 th iteration => 0.0112694679518\n",
      "Loss for  27739 th iteration => 0.0112689501427\n",
      "Loss for  27740 th iteration => 0.0112684919807\n",
      "Loss for  27741 th iteration => 0.0112681081165\n",
      "Loss for  27742 th iteration => 0.0112675915292\n",
      "Loss for  27743 th iteration => 0.0112672008052\n",
      "Loss for  27744 th iteration => 0.0112666888979\n",
      "Loss for  27745 th iteration => 0.0112662293607\n",
      "Loss for  27746 th iteration => 0.0112658494363\n",
      "Loss for  27747 th iteration => 0.0112654404679\n",
      "Loss for  27748 th iteration => 0.0112648931979\n",
      "Loss for  27749 th iteration => 0.0112645019175\n",
      "Loss for  27750 th iteration => 0.0112640803108\n",
      "Loss for  27751 th iteration => 0.0112635942859\n",
      "Loss for  27752 th iteration => 0.0112631795958\n",
      "Loss for  27753 th iteration => 0.0112626367047\n",
      "Loss for  27754 th iteration => 0.0112622383285\n",
      "Loss for  27755 th iteration => 0.0112618177937\n",
      "Loss for  27756 th iteration => 0.0112612880073\n",
      "Loss for  27757 th iteration => 0.0112608827034\n",
      "Loss for  27758 th iteration => 0.0112604603395\n",
      "Loss for  27759 th iteration => 0.011259975646\n",
      "Loss for  27760 th iteration => 0.0112595600704\n",
      "Loss for  27761 th iteration => 0.0112590317131\n",
      "Loss for  27762 th iteration => 0.0112586205699\n",
      "Loss for  27763 th iteration => 0.0112582001589\n",
      "Loss for  27764 th iteration => 0.0112576831435\n",
      "Loss for  27765 th iteration => 0.0112572658227\n",
      "Loss for  27766 th iteration => 0.0112568430904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  27767 th iteration => 0.0112564451006\n",
      "Loss for  27768 th iteration => 0.0112559496674\n",
      "Loss for  27769 th iteration => 0.0112554369769\n",
      "Loss for  27770 th iteration => 0.0112550906776\n",
      "Loss for  27771 th iteration => 0.0112545955479\n",
      "Loss for  27772 th iteration => 0.0112540975566\n",
      "Loss for  27773 th iteration => 0.011253736716\n",
      "Loss for  27774 th iteration => 0.0112532423754\n",
      "Loss for  27775 th iteration => 0.0112527719325\n",
      "Loss for  27776 th iteration => 0.0112524162459\n",
      "Loss for  27777 th iteration => 0.0112518487346\n",
      "Loss for  27778 th iteration => 0.0112514833931\n",
      "Loss for  27779 th iteration => 0.011250998513\n",
      "Loss for  27780 th iteration => 0.0112505097898\n",
      "Loss for  27781 th iteration => 0.011250131854\n",
      "Loss for  27782 th iteration => 0.0112496573006\n",
      "Loss for  27783 th iteration => 0.0112491688396\n",
      "Loss for  27784 th iteration => 0.0112487791961\n",
      "Loss for  27785 th iteration => 0.01124829896\n",
      "Loss for  27786 th iteration => 0.011247872682\n",
      "Loss for  27787 th iteration => 0.0112474013257\n",
      "Loss for  27788 th iteration => 0.0112469167006\n",
      "Loss for  27789 th iteration => 0.0112465204876\n",
      "Loss for  27790 th iteration => 0.0112460427303\n",
      "Loss for  27791 th iteration => 0.0112455695943\n",
      "Loss for  27792 th iteration => 0.0112451922549\n",
      "Loss for  27793 th iteration => 0.0112446645329\n",
      "Loss for  27794 th iteration => 0.0112442625696\n",
      "Loss for  27795 th iteration => 0.0112437902764\n",
      "Loss for  27796 th iteration => 0.0112433175554\n",
      "Loss for  27797 th iteration => 0.0112429111038\n",
      "Loss for  27798 th iteration => 0.0112424336827\n",
      "Loss for  27799 th iteration => 0.0112419706295\n",
      "Loss for  27800 th iteration => 0.0112415599015\n",
      "Loss for  27801 th iteration => 0.0112410920606\n",
      "Loss for  27802 th iteration => 0.0112407213985\n",
      "Loss for  27803 th iteration => 0.0112402238968\n",
      "Loss for  27804 th iteration => 0.011239823979\n",
      "Loss for  27805 th iteration => 0.0112393160161\n",
      "Loss for  27806 th iteration => 0.0112388530256\n",
      "Loss for  27807 th iteration => 0.0112384896395\n",
      "Loss for  27808 th iteration => 0.011237973461\n",
      "Loss for  27809 th iteration => 0.0112375088781\n",
      "Loss for  27810 th iteration => 0.0112371368154\n",
      "Loss for  27811 th iteration => 0.0112366218741\n",
      "Loss for  27812 th iteration => 0.0112362342064\n",
      "Loss for  27813 th iteration => 0.0112357229508\n",
      "Loss for  27814 th iteration => 0.0112352595346\n",
      "Loss for  27815 th iteration => 0.0112348820927\n",
      "Loss for  27816 th iteration => 0.0112343684711\n",
      "Loss for  27817 th iteration => 0.0112339153559\n",
      "Loss for  27818 th iteration => 0.0112335304261\n",
      "Loss for  27819 th iteration => 0.011233018312\n",
      "Loss for  27820 th iteration => 0.0112326364233\n",
      "Loss for  27821 th iteration => 0.0112321148255\n",
      "Loss for  27822 th iteration => 0.0112316700731\n",
      "Loss for  27823 th iteration => 0.0112312761148\n",
      "Loss for  27824 th iteration => 0.0112307619287\n",
      "Loss for  27825 th iteration => 0.0112303216305\n",
      "Loss for  27826 th iteration => 0.0112299297333\n",
      "Loss for  27827 th iteration => 0.0112295099617\n",
      "Loss for  27828 th iteration => 0.0112290275183\n",
      "Loss for  27829 th iteration => 0.0112286159071\n",
      "Loss for  27830 th iteration => 0.0112280774848\n",
      "Loss for  27831 th iteration => 0.0112276796873\n",
      "Loss for  27832 th iteration => 0.0112272632063\n",
      "Loss for  27833 th iteration => 0.0112267361936\n",
      "Loss for  27834 th iteration => 0.011226332183\n",
      "Loss for  27835 th iteration => 0.0112259119225\n",
      "Loss for  27836 th iteration => 0.0112254305345\n",
      "Loss for  27837 th iteration => 0.0112250204817\n",
      "Loss for  27838 th iteration => 0.011224495024\n",
      "Loss for  27839 th iteration => 0.0112241729324\n",
      "Loss for  27840 th iteration => 0.0112236737728\n",
      "Loss for  27841 th iteration => 0.0112231625526\n",
      "Loss for  27842 th iteration => 0.0112228262724\n",
      "Loss for  27843 th iteration => 0.0112223275603\n",
      "Loss for  27844 th iteration => 0.0112218301042\n",
      "Loss for  27845 th iteration => 0.0112214800629\n",
      "Loss for  27846 th iteration => 0.0112209837511\n",
      "Loss for  27847 th iteration => 0.0112205172063\n",
      "Loss for  27848 th iteration => 0.0112201656957\n",
      "Loss for  27849 th iteration => 0.0112195929712\n",
      "Loss for  27850 th iteration => 0.0112192396262\n",
      "Loss for  27851 th iteration => 0.011218751482\n",
      "Loss for  27852 th iteration => 0.0112182610309\n",
      "Loss for  27853 th iteration => 0.0112178944023\n",
      "Loss for  27854 th iteration => 0.0112174067966\n",
      "Loss for  27855 th iteration => 0.0112169292296\n",
      "Loss for  27856 th iteration => 0.011216596628\n",
      "Loss for  27857 th iteration => 0.011216031009\n",
      "Loss for  27858 th iteration => 0.0112156492653\n",
      "Loss for  27859 th iteration => 0.0112151827632\n",
      "Loss for  27860 th iteration => 0.0112146913705\n",
      "Loss for  27861 th iteration => 0.0112143046166\n",
      "Loss for  27862 th iteration => 0.0112138317057\n",
      "Loss for  27863 th iteration => 0.0112133517684\n",
      "Loss for  27864 th iteration => 0.0112129602283\n",
      "Loss for  27865 th iteration => 0.0112124814759\n",
      "Loss for  27866 th iteration => 0.0112120155453\n",
      "Loss for  27867 th iteration => 0.011211637666\n",
      "Loss for  27868 th iteration => 0.0112111122767\n",
      "Loss for  27869 th iteration => 0.0112107153291\n",
      "Loss for  27870 th iteration => 0.0112102428599\n",
      "Loss for  27871 th iteration => 0.0112097728999\n",
      "Loss for  27872 th iteration => 0.0112093716647\n",
      "Loss for  27873 th iteration => 0.0112088945809\n",
      "Loss for  27874 th iteration => 0.0112084335791\n",
      "Loss for  27875 th iteration => 0.011208029877\n",
      "Loss for  27876 th iteration => 0.0112076553609\n",
      "Loss for  27877 th iteration => 0.011207103837\n",
      "Loss for  27878 th iteration => 0.0112067019342\n",
      "Loss for  27879 th iteration => 0.0112063019121\n",
      "Loss for  27880 th iteration => 0.0112057970698\n",
      "Loss for  27881 th iteration => 0.0112053337195\n",
      "Loss for  27882 th iteration => 0.0112049752962\n",
      "Loss for  27883 th iteration => 0.0112044612989\n",
      "Loss for  27884 th iteration => 0.0112039971232\n",
      "Loss for  27885 th iteration => 0.0112036304195\n",
      "Loss for  27886 th iteration => 0.0112031180613\n",
      "Loss for  27887 th iteration => 0.0112027330993\n",
      "Loss for  27888 th iteration => 0.0112022243881\n",
      "Loss for  27889 th iteration => 0.0112017604092\n",
      "Loss for  27890 th iteration => 0.0112013889214\n",
      "Loss for  27891 th iteration => 0.0112008780789\n",
      "Loss for  27892 th iteration => 0.0112004237976\n",
      "Loss for  27893 th iteration => 0.0112000451836\n",
      "Loss for  27894 th iteration => 0.0111995350561\n",
      "Loss for  27895 th iteration => 0.0111991485911\n",
      "Loss for  27896 th iteration => 0.0111986441475\n",
      "Loss for  27897 th iteration => 0.0111981871312\n",
      "Loss for  27898 th iteration => 0.0111978055337\n",
      "Loss for  27899 th iteration => 0.0111972996244\n",
      "Loss for  27900 th iteration => 0.0111968505577\n",
      "Loss for  27901 th iteration => 0.0111964719691\n",
      "Loss for  27902 th iteration => 0.0111960657011\n",
      "Loss for  27903 th iteration => 0.0111955294911\n",
      "Loss for  27904 th iteration => 0.0111951395832\n",
      "Loss for  27905 th iteration => 0.0111947230926\n",
      "Loss for  27906 th iteration => 0.0111942427976\n",
      "Loss for  27907 th iteration => 0.0111938334965\n",
      "Loss for  27908 th iteration => 0.0111932988999\n",
      "Loss for  27909 th iteration => 0.0111929030881\n",
      "Loss for  27910 th iteration => 0.0111924889299\n",
      "Loss for  27911 th iteration => 0.0111919657231\n",
      "Loss for  27912 th iteration => 0.0111915637025\n",
      "Loss for  27913 th iteration => 0.0111911467138\n",
      "Loss for  27914 th iteration => 0.0111906674785\n",
      "Loss for  27915 th iteration => 0.011190259706\n",
      "Loss for  27916 th iteration => 0.0111897353933\n",
      "Loss for  27917 th iteration => 0.0111893286291\n",
      "Loss for  27918 th iteration => 0.01118891693\n",
      "Loss for  27919 th iteration => 0.0111884023808\n",
      "Loss for  27920 th iteration => 0.0111879901008\n",
      "Loss for  27921 th iteration => 0.0111875748792\n",
      "Loss for  27922 th iteration => 0.0111870776307\n",
      "Loss for  27923 th iteration => 0.0111867317321\n",
      "Loss for  27924 th iteration => 0.0111862388891\n",
      "Loss for  27925 th iteration => 0.011185772435\n",
      "Loss for  27926 th iteration => 0.0111854281674\n",
      "Loss for  27927 th iteration => 0.0111848529685\n",
      "Loss for  27928 th iteration => 0.0111845051813\n",
      "Loss for  27929 th iteration => 0.0111840213915\n",
      "Loss for  27930 th iteration => 0.0111835284908\n",
      "Loss for  27931 th iteration => 0.0111831682879\n",
      "Loss for  27932 th iteration => 0.0111826851836\n",
      "Loss for  27933 th iteration => 0.0111822040671\n",
      "Loss for  27934 th iteration => 0.0111818318242\n",
      "Loss for  27935 th iteration => 0.0111813499155\n",
      "Loss for  27936 th iteration => 0.0111808848795\n",
      "Loss for  27937 th iteration => 0.0111805286476\n",
      "Loss for  27938 th iteration => 0.0111799805024\n",
      "Loss for  27939 th iteration => 0.0111796100423\n",
      "Loss for  27940 th iteration => 0.0111791465401\n",
      "Loss for  27941 th iteration => 0.011178659908\n",
      "Loss for  27942 th iteration => 0.0111782737804\n",
      "Loss for  27943 th iteration => 0.0111778041513\n",
      "Loss for  27944 th iteration => 0.0111773285829\n",
      "Loss for  27945 th iteration => 0.0111769377767\n",
      "Loss for  27946 th iteration => 0.011176462577\n",
      "Loss for  27947 th iteration => 0.0111759996605\n",
      "Loss for  27948 th iteration => 0.0111756228889\n",
      "Loss for  27949 th iteration => 0.0111751029162\n",
      "Loss for  27950 th iteration => 0.0111747068649\n",
      "Loss for  27951 th iteration => 0.0111742383\n",
      "Loss for  27952 th iteration => 0.0111737718203\n",
      "Loss for  27953 th iteration => 0.011173371579\n",
      "Loss for  27954 th iteration => 0.0111728986446\n",
      "Loss for  27955 th iteration => 0.0111724407818\n",
      "Loss for  27956 th iteration => 0.0111720365525\n",
      "Loss for  27957 th iteration => 0.0111715597553\n",
      "Loss for  27958 th iteration => 0.0111711104154\n",
      "Loss for  27959 th iteration => 0.0111707127302\n",
      "Loss for  27960 th iteration => 0.0111703150161\n",
      "Loss for  27961 th iteration => 0.0111698133388\n",
      "Loss for  27962 th iteration => 0.0111693532935\n",
      "Loss for  27963 th iteration => 0.0111689966639\n",
      "Loss for  27964 th iteration => 0.0111684889129\n",
      "Loss for  27965 th iteration => 0.0111680247655\n",
      "Loss for  27966 th iteration => 0.011167660298\n",
      "Loss for  27967 th iteration => 0.0111671514849\n",
      "Loss for  27968 th iteration => 0.0111667691579\n",
      "Loss for  27969 th iteration => 0.0111662500542\n",
      "Loss for  27970 th iteration => 0.0111658175447\n",
      "Loss for  27971 th iteration => 0.0111654329767\n",
      "Loss for  27972 th iteration => 0.0111649290664\n",
      "Loss for  27973 th iteration => 0.0111644729785\n",
      "Loss for  27974 th iteration => 0.011164097729\n",
      "Loss for  27975 th iteration => 0.0111635922271\n",
      "Loss for  27976 th iteration => 0.0111631444224\n",
      "Loss for  27977 th iteration => 0.011162762909\n",
      "Loss for  27978 th iteration => 0.0111622576207\n",
      "Loss for  27979 th iteration => 0.0111618729117\n",
      "Loss for  27980 th iteration => 0.0111613697747\n",
      "Loss for  27981 th iteration => 0.0111609289\n",
      "Loss for  27982 th iteration => 0.0111605489929\n",
      "Loss for  27983 th iteration => 0.0111601426886\n",
      "Loss for  27984 th iteration => 0.0111596035126\n",
      "Loss for  27985 th iteration => 0.0111592171313\n",
      "Loss for  27986 th iteration => 0.0111588060577\n",
      "Loss for  27987 th iteration => 0.011158278123\n",
      "Loss for  27988 th iteration => 0.0111578855905\n",
      "Loss for  27989 th iteration => 0.0111574718591\n",
      "Loss for  27990 th iteration => 0.0111569946054\n",
      "Loss for  27991 th iteration => 0.011156590005\n",
      "Loss for  27992 th iteration => 0.0111560607914\n",
      "Loss for  27993 th iteration => 0.0111556635963\n",
      "Loss for  27994 th iteration => 0.0111552551469\n",
      "Loss for  27995 th iteration => 0.0111547355661\n",
      "Loss for  27996 th iteration => 0.0111543329052\n",
      "Loss for  27997 th iteration => 0.0111539210072\n",
      "Loss for  27998 th iteration => 0.011153410362\n",
      "Loss for  27999 th iteration => 0.0111530025313\n",
      "Loss for  28000 th iteration => 0.0111525881395\n",
      "Loss for  28001 th iteration => 0.0111521222701\n",
      "Loss for  28002 th iteration => 0.0111517249017\n",
      "Loss for  28003 th iteration => 0.0111512633534\n",
      "Loss for  28004 th iteration => 0.0111508703025\n",
      "Loss for  28005 th iteration => 0.0111503845891\n",
      "Loss for  28006 th iteration => 0.0111498833257\n",
      "Loss for  28007 th iteration => 0.0111495413662\n",
      "Loss for  28008 th iteration => 0.0111490564611\n",
      "Loss for  28009 th iteration => 0.0111485662959\n",
      "Loss for  28010 th iteration => 0.0111482128508\n",
      "Loss for  28011 th iteration => 0.0111477287886\n",
      "Loss for  28012 th iteration => 0.0111472493359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  28013 th iteration => 0.0111468847531\n",
      "Loss for  28014 th iteration => 0.0111464029748\n",
      "Loss for  28015 th iteration => 0.0111459412358\n",
      "Loss for  28016 th iteration => 0.0111455891688\n",
      "Loss for  28017 th iteration => 0.0111450382974\n",
      "Loss for  28018 th iteration => 0.0111446743428\n",
      "Loss for  28019 th iteration => 0.0111442017717\n",
      "Loss for  28020 th iteration => 0.0111437219579\n",
      "Loss for  28021 th iteration => 0.0111433507058\n",
      "Loss for  28022 th iteration => 0.0111428862872\n",
      "Loss for  28023 th iteration => 0.0111424061877\n",
      "Loss for  28024 th iteration => 0.0111420228239\n",
      "Loss for  28025 th iteration => 0.0111415529068\n",
      "Loss for  28026 th iteration => 0.0111410838428\n",
      "Loss for  28027 th iteration => 0.0111407233338\n",
      "Loss for  28028 th iteration => 0.0111401940383\n",
      "Loss for  28029 th iteration => 0.011139805462\n",
      "Loss for  28030 th iteration => 0.0111393422183\n",
      "Loss for  28031 th iteration => 0.0111388710352\n",
      "Loss for  28032 th iteration => 0.0111384782916\n",
      "Loss for  28033 th iteration => 0.0111380107395\n",
      "Loss for  28034 th iteration => 0.011137548089\n",
      "Loss for  28035 th iteration => 0.0111371513783\n",
      "Loss for  28036 th iteration => 0.0111366800201\n",
      "Loss for  28037 th iteration => 0.0111362252055\n",
      "Loss for  28038 th iteration => 0.0111358247228\n",
      "Loss for  28039 th iteration => 0.0111353500464\n",
      "Loss for  28040 th iteration => 0.0111349035689\n",
      "Loss for  28041 th iteration => 0.0111345098937\n",
      "Loss for  28042 th iteration => 0.011134113724\n",
      "Loss for  28043 th iteration => 0.0111336154147\n",
      "Loss for  28044 th iteration => 0.0111331565533\n",
      "Loss for  28045 th iteration => 0.0111328036594\n",
      "Loss for  28046 th iteration => 0.0111322994329\n",
      "Loss for  28047 th iteration => 0.0111318361194\n",
      "Loss for  28048 th iteration => 0.0111314758269\n",
      "Loss for  28049 th iteration => 0.0111309699264\n",
      "Loss for  28050 th iteration => 0.0111305912914\n",
      "Loss for  28051 th iteration => 0.0111300723174\n",
      "Loss for  28052 th iteration => 0.0111296443807\n",
      "Loss for  28053 th iteration => 0.0111292627047\n",
      "Loss for  28054 th iteration => 0.0111287629775\n",
      "Loss for  28055 th iteration => 0.0111283059858\n",
      "Loss for  28056 th iteration => 0.0111279359704\n",
      "Loss for  28057 th iteration => 0.0111274349024\n",
      "Loss for  28058 th iteration => 0.011126985548\n",
      "Loss for  28059 th iteration => 0.0111266096565\n",
      "Loss for  28060 th iteration => 0.0111261075149\n",
      "Loss for  28061 th iteration => 0.0111256650929\n",
      "Loss for  28062 th iteration => 0.0111252837585\n",
      "Loss for  28063 th iteration => 0.0111247834307\n",
      "Loss for  28064 th iteration => 0.0111244095352\n",
      "Loss for  28065 th iteration => 0.0111240084512\n",
      "Loss for  28066 th iteration => 0.011123466155\n",
      "Loss for  28067 th iteration => 0.0111230860743\n",
      "Loss for  28068 th iteration => 0.0111226806098\n",
      "Loss for  28069 th iteration => 0.0111221488827\n",
      "Loss for  28070 th iteration => 0.0111217629304\n",
      "Loss for  28071 th iteration => 0.0111213534985\n",
      "Loss for  28072 th iteration => 0.0111208316212\n",
      "Loss for  28073 th iteration => 0.0111204401023\n",
      "Loss for  28074 th iteration => 0.0111200283944\n",
      "Loss for  28075 th iteration => 0.0111195549245\n",
      "Loss for  28076 th iteration => 0.0111191531596\n",
      "Loss for  28077 th iteration => 0.0111186279017\n",
      "Loss for  28078 th iteration => 0.0111182326205\n",
      "Loss for  28079 th iteration => 0.0111178277476\n",
      "Loss for  28080 th iteration => 0.0111173108365\n",
      "Loss for  28081 th iteration => 0.0111169106301\n",
      "Loss for  28082 th iteration => 0.0111165030268\n",
      "Loss for  28083 th iteration => 0.0111159995996\n",
      "Loss for  28084 th iteration => 0.0111156745459\n",
      "Loss for  28085 th iteration => 0.0111151830513\n",
      "Loss for  28086 th iteration => 0.0111146899343\n",
      "Loss for  28087 th iteration => 0.0111143540407\n",
      "Loss for  28088 th iteration => 0.0111138643145\n",
      "Loss for  28089 th iteration => 0.0111134032228\n",
      "Loss for  28090 th iteration => 0.0111130678843\n",
      "Loss for  28091 th iteration => 0.0111124910739\n",
      "Loss for  28092 th iteration => 0.0111121562939\n",
      "Loss for  28093 th iteration => 0.0111116764689\n",
      "Loss for  28094 th iteration => 0.0111111820391\n",
      "Loss for  28095 th iteration => 0.0111108366808\n",
      "Loss for  28096 th iteration => 0.0111103578614\n",
      "Loss for  28097 th iteration => 0.0111098730896\n",
      "Loss for  28098 th iteration => 0.0111095174733\n",
      "Loss for  28099 th iteration => 0.0111090396908\n",
      "Loss for  28100 th iteration => 0.0111085642313\n",
      "Loss for  28101 th iteration => 0.0111081986681\n",
      "Loss for  28102 th iteration => 0.0111077219532\n",
      "Loss for  28103 th iteration => 0.0111072554698\n",
      "Loss for  28104 th iteration => 0.011106883258\n",
      "Loss for  28105 th iteration => 0.0111064152453\n",
      "Loss for  28106 th iteration => 0.0111059448508\n",
      "Loss for  28107 th iteration => 0.0111055967877\n",
      "Loss for  28108 th iteration => 0.0111050586277\n",
      "Loss for  28109 th iteration => 0.0111046794854\n",
      "Loss for  28110 th iteration => 0.0111042182365\n",
      "Loss for  28111 th iteration => 0.0111037437353\n",
      "Loss for  28112 th iteration => 0.0111033604551\n",
      "Loss for  28113 th iteration => 0.0111028949852\n",
      "Loss for  28114 th iteration => 0.0111024288996\n",
      "Loss for  28115 th iteration => 0.0111020416796\n",
      "Loss for  28116 th iteration => 0.011101572486\n",
      "Loss for  28117 th iteration => 0.0111011147153\n",
      "Loss for  28118 th iteration => 0.011100743928\n",
      "Loss for  28119 th iteration => 0.011100230813\n",
      "Loss for  28120 th iteration => 0.0110998395024\n",
      "Loss for  28121 th iteration => 0.0110993796915\n",
      "Loss for  28122 th iteration => 0.0110989162531\n",
      "Loss for  28123 th iteration => 0.0110985214375\n",
      "Loss for  28124 th iteration => 0.0110980656343\n",
      "Loss for  28125 th iteration => 0.0110977037479\n",
      "Loss for  28126 th iteration => 0.0110972095881\n",
      "Loss for  28127 th iteration => 0.0110967527891\n",
      "Loss for  28128 th iteration => 0.011096390775\n",
      "Loss for  28129 th iteration => 0.0110958980192\n",
      "Loss for  28130 th iteration => 0.0110954429154\n",
      "Loss for  28131 th iteration => 0.0110950893113\n",
      "Loss for  28132 th iteration => 0.0110945862138\n",
      "Loss for  28133 th iteration => 0.0110942102611\n",
      "Loss for  28134 th iteration => 0.0110936900354\n",
      "Loss for  28135 th iteration => 0.0110932690983\n",
      "Loss for  28136 th iteration => 0.0110928902951\n",
      "Loss for  28137 th iteration => 0.011092393027\n",
      "Loss for  28138 th iteration => 0.011091935103\n",
      "Loss for  28139 th iteration => 0.011091572018\n",
      "Loss for  28140 th iteration => 0.0110910734743\n",
      "Loss for  28141 th iteration => 0.0110906230263\n",
      "Loss for  28142 th iteration => 0.0110902541568\n",
      "Loss for  28143 th iteration => 0.0110897546018\n",
      "Loss for  28144 th iteration => 0.011089310933\n",
      "Loss for  28145 th iteration => 0.0110889367072\n",
      "Loss for  28146 th iteration => 0.0110884379533\n",
      "Loss for  28147 th iteration => 0.0110880587281\n",
      "Loss for  28148 th iteration => 0.0110875580798\n",
      "Loss for  28149 th iteration => 0.0110871226972\n",
      "Loss for  28150 th iteration => 0.0110867520924\n",
      "Loss for  28151 th iteration => 0.0110863513577\n",
      "Loss for  28152 th iteration => 0.0110858137634\n",
      "Loss for  28153 th iteration => 0.0110854373588\n",
      "Loss for  28154 th iteration => 0.0110850327621\n",
      "Loss for  28155 th iteration => 0.011084504841\n",
      "Loss for  28156 th iteration => 0.011084122938\n",
      "Loss for  28157 th iteration => 0.0110837148761\n",
      "Loss for  28158 th iteration => 0.0110831959369\n",
      "Loss for  28159 th iteration => 0.0110828088288\n",
      "Loss for  28160 th iteration => 0.0110823992345\n",
      "Loss for  28161 th iteration => 0.011081929467\n",
      "Loss for  28162 th iteration => 0.0110815297989\n",
      "Loss for  28163 th iteration => 0.0110810061942\n",
      "Loss for  28164 th iteration => 0.0110806158756\n",
      "Loss for  28165 th iteration => 0.0110802135621\n",
      "Loss for  28166 th iteration => 0.0110796975048\n",
      "Loss for  28167 th iteration => 0.0110793025936\n",
      "Loss for  28168 th iteration => 0.0110788979979\n",
      "Loss for  28169 th iteration => 0.0110783937054\n",
      "Loss for  28170 th iteration => 0.0110780770873\n",
      "Loss for  28171 th iteration => 0.0110775871246\n",
      "Loss for  28172 th iteration => 0.0110770920606\n",
      "Loss for  28173 th iteration => 0.0110767655076\n",
      "Loss for  28174 th iteration => 0.0110762766807\n",
      "Loss for  28175 th iteration => 0.0110757905119\n",
      "Loss for  28176 th iteration => 0.0110754543241\n",
      "Loss for  28177 th iteration => 0.0110749678156\n",
      "Loss for  28178 th iteration => 0.0110745110735\n",
      "Loss for  28179 th iteration => 0.0110741760209\n",
      "Loss for  28180 th iteration => 0.0110736053032\n",
      "Loss for  28181 th iteration => 0.0110732720458\n",
      "Loss for  28182 th iteration => 0.0110727953366\n",
      "Loss for  28183 th iteration => 0.0110723044361\n",
      "Loss for  28184 th iteration => 0.0110719617179\n",
      "Loss for  28185 th iteration => 0.0110714861982\n",
      "Loss for  28186 th iteration => 0.0110710036735\n",
      "Loss for  28187 th iteration => 0.0110706517814\n",
      "Loss for  28188 th iteration => 0.0110701774795\n",
      "Loss for  28189 th iteration => 0.0110697030206\n",
      "Loss for  28190 th iteration => 0.0110693422336\n",
      "Loss for  28191 th iteration => 0.0110688691774\n",
      "Loss for  28192 th iteration => 0.0110684024826\n",
      "Loss for  28193 th iteration => 0.0110680353465\n",
      "Loss for  28194 th iteration => 0.011067571705\n",
      "Loss for  28195 th iteration => 0.0110671004034\n",
      "Loss for  28196 th iteration => 0.0110667600663\n",
      "Loss for  28197 th iteration => 0.0110662220797\n",
      "Loss for  28198 th iteration => 0.0110657967958\n",
      "Loss for  28199 th iteration => 0.0110654414895\n",
      "Loss for  28200 th iteration => 0.0110649154507\n",
      "Loss for  28201 th iteration => 0.0110645363998\n",
      "Loss for  28202 th iteration => 0.0110640769124\n",
      "Loss for  28203 th iteration => 0.0110636093451\n",
      "Loss for  28204 th iteration => 0.0110632265526\n",
      "Loss for  28205 th iteration => 0.0110627638368\n",
      "Loss for  28206 th iteration => 0.0110623033059\n",
      "Loss for  28207 th iteration => 0.0110619169587\n",
      "Loss for  28208 th iteration => 0.011061451481\n",
      "Loss for  28209 th iteration => 0.0110609973386\n",
      "Loss for  28210 th iteration => 0.0110606076189\n",
      "Loss for  28211 th iteration => 0.0110601398326\n",
      "Loss for  28212 th iteration => 0.011059691485\n",
      "Loss for  28213 th iteration => 0.0110593143417\n",
      "Loss for  28214 th iteration => 0.0110589198919\n",
      "Loss for  28215 th iteration => 0.0110583935026\n",
      "Loss for  28216 th iteration => 0.0110580071547\n",
      "Loss for  28217 th iteration => 0.0110576143442\n",
      "Loss for  28218 th iteration => 0.0110570956255\n",
      "Loss for  28219 th iteration => 0.0110567003671\n",
      "Loss for  28220 th iteration => 0.0110563103468\n",
      "Loss for  28221 th iteration => 0.0110558219899\n",
      "Loss for  28222 th iteration => 0.0110553702855\n",
      "Loss for  28223 th iteration => 0.0110550179989\n",
      "Loss for  28224 th iteration => 0.0110545231662\n",
      "Loss for  28225 th iteration => 0.0110540670799\n",
      "Loss for  28226 th iteration => 0.0110537084435\n",
      "Loss for  28227 th iteration => 0.011053212258\n",
      "Loss for  28228 th iteration => 0.0110527638468\n",
      "Loss for  28229 th iteration => 0.0110523993016\n",
      "Loss for  28230 th iteration => 0.0110519043278\n",
      "Loss for  28231 th iteration => 0.0110515270991\n",
      "Loss for  28232 th iteration => 0.011051021633\n",
      "Loss for  28233 th iteration => 0.0110505970002\n",
      "Loss for  28234 th iteration => 0.0110502172598\n",
      "Loss for  28235 th iteration => 0.0110497277984\n",
      "Loss for  28236 th iteration => 0.0110492797915\n",
      "Loss for  28237 th iteration => 0.0110489091614\n",
      "Loss for  28238 th iteration => 0.0110484191939\n",
      "Loss for  28239 th iteration => 0.011047976641\n",
      "Loss for  28240 th iteration => 0.0110476014629\n",
      "Loss for  28241 th iteration => 0.0110471112313\n",
      "Loss for  28242 th iteration => 0.0110466734957\n",
      "Loss for  28243 th iteration => 0.0110463028391\n",
      "Loss for  28244 th iteration => 0.011045909362\n",
      "Loss for  28245 th iteration => 0.0110453850412\n",
      "Loss for  28246 th iteration => 0.0110450063539\n",
      "Loss for  28247 th iteration => 0.0110446013095\n",
      "Loss for  28248 th iteration => 0.0110441328203\n",
      "Loss for  28249 th iteration => 0.0110437382236\n",
      "Loss for  28250 th iteration => 0.0110432102573\n",
      "Loss for  28251 th iteration => 0.0110428279388\n",
      "Loss for  28252 th iteration => 0.0110424303727\n",
      "Loss for  28253 th iteration => 0.0110419105199\n",
      "Loss for  28254 th iteration => 0.0110415233646\n",
      "Loss for  28255 th iteration => 0.011041123199\n",
      "Loss for  28256 th iteration => 0.0110406108149\n",
      "Loss for  28257 th iteration => 0.0110402190967\n",
      "Loss for  28258 th iteration => 0.0110398166914\n",
      "Loss for  28259 th iteration => 0.0110393111488\n",
      "Loss for  28260 th iteration => 0.0110389151343\n",
      "Loss for  28261 th iteration => 0.0110385108392\n",
      "Loss for  28262 th iteration => 0.0110380115278\n",
      "Loss for  28263 th iteration => 0.0110376114766\n",
      "Loss for  28264 th iteration => 0.0110372056321\n",
      "Loss for  28265 th iteration => 0.0110367194043\n",
      "Loss for  28266 th iteration => 0.0110363902435\n",
      "Loss for  28267 th iteration => 0.0110359063771\n",
      "Loss for  28268 th iteration => 0.0110354495029\n",
      "Loss for  28269 th iteration => 0.0110351232654\n",
      "Loss for  28270 th iteration => 0.0110345484894\n",
      "Loss for  28271 th iteration => 0.0110341524664\n",
      "Loss for  28272 th iteration => 0.0110338198744\n",
      "Loss for  28273 th iteration => 0.0110332556561\n",
      "Loss for  28274 th iteration => 0.0110329218985\n",
      "Loss for  28275 th iteration => 0.0110324497903\n",
      "Loss for  28276 th iteration => 0.0110319632344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  28277 th iteration => 0.0110316211764\n",
      "Loss for  28278 th iteration => 0.0110311504386\n",
      "Loss for  28279 th iteration => 0.011030670937\n",
      "Loss for  28280 th iteration => 0.0110303208314\n",
      "Loss for  28281 th iteration => 0.0110298514894\n",
      "Loss for  28282 th iteration => 0.0110293787685\n",
      "Loss for  28283 th iteration => 0.0110290208609\n",
      "Loss for  28284 th iteration => 0.0110285529397\n",
      "Loss for  28285 th iteration => 0.0110280867333\n",
      "Loss for  28286 th iteration => 0.0110277233887\n",
      "Loss for  28287 th iteration => 0.0110272650142\n",
      "Loss for  28288 th iteration => 0.0110267930856\n",
      "Loss for  28289 th iteration => 0.0110264220788\n",
      "Loss for  28290 th iteration => 0.0110259599482\n",
      "Loss for  28291 th iteration => 0.0110254970308\n",
      "Loss for  28292 th iteration => 0.0110251521068\n",
      "Loss for  28293 th iteration => 0.0110246245985\n",
      "Loss for  28294 th iteration => 0.0110242016403\n",
      "Loss for  28295 th iteration => 0.01102384463\n",
      "Loss for  28296 th iteration => 0.0110233268126\n",
      "Loss for  28297 th iteration => 0.0110229484477\n",
      "Loss for  28298 th iteration => 0.0110224930678\n",
      "Loss for  28299 th iteration => 0.0110220298051\n",
      "Loss for  28300 th iteration => 0.011021648086\n",
      "Loss for  28301 th iteration => 0.0110211904374\n",
      "Loss for  28302 th iteration => 0.0110207328737\n",
      "Loss for  28303 th iteration => 0.0110203479763\n",
      "Loss for  28304 th iteration => 0.0110198884956\n",
      "Loss for  28305 th iteration => 0.0110194360234\n",
      "Loss for  28306 th iteration => 0.0110190481192\n",
      "Loss for  28307 th iteration => 0.011018598066\n",
      "Loss for  28308 th iteration => 0.011018238652\n",
      "Loss for  28309 th iteration => 0.0110177543494\n",
      "Loss for  28310 th iteration => 0.0110173035626\n",
      "Loss for  28311 th iteration => 0.0110169441397\n",
      "Loss for  28312 th iteration => 0.0110164614763\n",
      "Loss for  28313 th iteration => 0.011016096995\n",
      "Loss for  28314 th iteration => 0.01101557498\n",
      "Loss for  28315 th iteration => 0.0110151674885\n",
      "Loss for  28316 th iteration => 0.0110147941356\n",
      "Loss for  28317 th iteration => 0.0110142800076\n",
      "Loss for  28318 th iteration => 0.0110138686405\n",
      "Loss for  28319 th iteration => 0.0110134926503\n",
      "Loss for  28320 th iteration => 0.0110130043561\n",
      "Loss for  28321 th iteration => 0.0110125501662\n",
      "Loss for  28322 th iteration => 0.0110121929326\n",
      "Loss for  28323 th iteration => 0.011011703951\n",
      "Loss for  28324 th iteration => 0.0110112557036\n",
      "Loss for  28325 th iteration => 0.0110108936142\n",
      "Loss for  28326 th iteration => 0.0110104041908\n",
      "Loss for  28327 th iteration => 0.0110099612396\n",
      "Loss for  28328 th iteration => 0.0110095946914\n",
      "Loss for  28329 th iteration => 0.0110091050653\n",
      "Loss for  28330 th iteration => 0.0110086667814\n",
      "Loss for  28331 th iteration => 0.0110082961606\n",
      "Loss for  28332 th iteration => 0.0110078068813\n",
      "Loss for  28333 th iteration => 0.0110074334268\n",
      "Loss for  28334 th iteration => 0.0110069492304\n",
      "Loss for  28335 th iteration => 0.0110066168616\n",
      "Loss for  28336 th iteration => 0.0110061401843\n",
      "Loss for  28337 th iteration => 0.0110056624768\n",
      "Loss for  28338 th iteration => 0.011005323347\n",
      "Loss for  28339 th iteration => 0.0110048568308\n",
      "Loss for  28340 th iteration => 0.0110044647623\n",
      "Loss for  28341 th iteration => 0.0110039361051\n",
      "Loss for  28342 th iteration => 0.0110035609818\n",
      "Loss for  28343 th iteration => 0.0110031661496\n",
      "Loss for  28344 th iteration => 0.0110026452501\n",
      "Loss for  28345 th iteration => 0.0110022654366\n",
      "Loss for  28346 th iteration => 0.0110018682029\n",
      "Loss for  28347 th iteration => 0.0110013544297\n",
      "Loss for  28348 th iteration => 0.0110009701943\n",
      "Loss for  28349 th iteration => 0.0110005709115\n",
      "Loss for  28350 th iteration => 0.0110000636501\n",
      "Loss for  28351 th iteration => 0.0109996752541\n",
      "Loss for  28352 th iteration => 0.0109992742648\n",
      "Loss for  28353 th iteration => 0.0109987729176\n",
      "Loss for  28354 th iteration => 0.0109983806151\n",
      "Loss for  28355 th iteration => 0.0109979787693\n",
      "Loss for  28356 th iteration => 0.0109975154378\n",
      "Loss for  28357 th iteration => 0.0109970525555\n",
      "Loss for  28358 th iteration => 0.0109966842017\n",
      "Loss for  28359 th iteration => 0.0109962284676\n",
      "Loss for  28360 th iteration => 0.0109958444605\n",
      "Loss for  28361 th iteration => 0.0109953945877\n",
      "Loss for  28362 th iteration => 0.0109949401312\n",
      "Loss for  28363 th iteration => 0.0109946181928\n",
      "Loss for  28364 th iteration => 0.0109940432991\n",
      "Loss for  28365 th iteration => 0.0109936519998\n",
      "Loss for  28366 th iteration => 0.0109933248526\n",
      "Loss for  28367 th iteration => 0.0109927593839\n",
      "Loss for  28368 th iteration => 0.0109924326303\n",
      "Loss for  28369 th iteration => 0.0109919624955\n",
      "Loss for  28370 th iteration => 0.0109914755301\n",
      "Loss for  28371 th iteration => 0.0109911414556\n",
      "Loss for  28372 th iteration => 0.0109906728568\n",
      "Loss for  28373 th iteration => 0.010990191817\n",
      "Loss for  28374 th iteration => 0.0109898506452\n",
      "Loss for  28375 th iteration => 0.0109893836054\n",
      "Loss for  28376 th iteration => 0.0109889082487\n",
      "Loss for  28377 th iteration => 0.0109885601971\n",
      "Loss for  28378 th iteration => 0.0109880947388\n",
      "Loss for  28379 th iteration => 0.0109876248291\n",
      "Loss for  28380 th iteration => 0.0109872701091\n",
      "Loss for  28381 th iteration => 0.0109868062543\n",
      "Loss for  28382 th iteration => 0.010986341562\n",
      "Loss for  28383 th iteration => 0.0109859817597\n",
      "Loss for  28384 th iteration => 0.0109855281809\n",
      "Loss for  28385 th iteration => 0.0109850574168\n",
      "Loss for  28386 th iteration => 0.0109846899816\n",
      "Loss for  28387 th iteration => 0.0109842332298\n",
      "Loss for  28388 th iteration => 0.0109837694293\n",
      "Loss for  28389 th iteration => 0.0109833984517\n",
      "Loss for  28390 th iteration => 0.0109829389861\n",
      "Loss for  28391 th iteration => 0.0109824826011\n",
      "Loss for  28392 th iteration => 0.010982135763\n",
      "Loss for  28393 th iteration => 0.0109816170611\n",
      "Loss for  28394 th iteration => 0.0109811957552\n",
      "Loss for  28395 th iteration => 0.0109808396259\n",
      "Loss for  28396 th iteration => 0.0109803283213\n",
      "Loss for  28397 th iteration => 0.0109799086248\n",
      "Loss for  28398 th iteration => 0.0109795444176\n",
      "Loss for  28399 th iteration => 0.0109790402933\n",
      "Loss for  28400 th iteration => 0.0109786603426\n",
      "Loss for  28401 th iteration => 0.010978209087\n",
      "Loss for  28402 th iteration => 0.0109777527726\n",
      "Loss for  28403 th iteration => 0.0109773700076\n",
      "Loss for  28404 th iteration => 0.0109769250105\n",
      "Loss for  28405 th iteration => 0.0109765672882\n",
      "Loss for  28406 th iteration => 0.0109760856958\n",
      "Loss for  28407 th iteration => 0.0109756400591\n",
      "Loss for  28408 th iteration => 0.0109752823745\n",
      "Loss for  28409 th iteration => 0.0109748016611\n",
      "Loss for  28410 th iteration => 0.0109743554092\n",
      "Loss for  28411 th iteration => 0.0109739977727\n",
      "Loss for  28412 th iteration => 0.0109735179039\n",
      "Loss for  28413 th iteration => 0.0109730716075\n",
      "Loss for  28414 th iteration => 0.0109727240439\n",
      "Loss for  28415 th iteration => 0.0109722364885\n",
      "Loss for  28416 th iteration => 0.010971786561\n",
      "Loss for  28417 th iteration => 0.010971433668\n",
      "Loss for  28418 th iteration => 0.0109709462368\n",
      "Loss for  28419 th iteration => 0.0109705763277\n",
      "Loss for  28420 th iteration => 0.0109700678882\n",
      "Loss for  28421 th iteration => 0.0109696574864\n",
      "Loss for  28422 th iteration => 0.0109692852259\n",
      "Loss for  28423 th iteration => 0.0109687826158\n",
      "Loss for  28424 th iteration => 0.0109683690063\n",
      "Loss for  28425 th iteration => 0.010967994811\n",
      "Loss for  28426 th iteration => 0.0109674974106\n",
      "Loss for  28427 th iteration => 0.0109670807962\n",
      "Loss for  28428 th iteration => 0.0109667050713\n",
      "Loss for  28429 th iteration => 0.0109662122777\n",
      "Loss for  28430 th iteration => 0.0109657928562\n",
      "Loss for  28431 th iteration => 0.0109654159954\n",
      "Loss for  28432 th iteration => 0.0109649272223\n",
      "Loss for  28433 th iteration => 0.0109645051861\n",
      "Loss for  28434 th iteration => 0.0109641316114\n",
      "Loss for  28435 th iteration => 0.0109637589184\n",
      "Loss for  28436 th iteration => 0.0109632179741\n",
      "Loss for  28437 th iteration => 0.0109628501494\n",
      "Loss for  28438 th iteration => 0.0109624732516\n",
      "Loss for  28439 th iteration => 0.0109619400077\n",
      "Loss for  28440 th iteration => 0.0109615689045\n",
      "Loss for  28441 th iteration => 0.0109611887368\n",
      "Loss for  28442 th iteration => 0.010960669791\n",
      "Loss for  28443 th iteration => 0.0109602941874\n",
      "Loss for  28444 th iteration => 0.010959900462\n",
      "Loss for  28445 th iteration => 0.0109593885455\n",
      "Loss for  28446 th iteration => 0.0109590085589\n",
      "Loss for  28447 th iteration => 0.0109586128353\n",
      "Loss for  28448 th iteration => 0.0109581073404\n",
      "Loss for  28449 th iteration => 0.0109577232292\n",
      "Loss for  28450 th iteration => 0.0109573268968\n",
      "Loss for  28451 th iteration => 0.0109568648047\n",
      "Loss for  28452 th iteration => 0.0109563985206\n",
      "Loss for  28453 th iteration => 0.0109560415982\n",
      "Loss for  28454 th iteration => 0.01095557883\n",
      "Loss for  28455 th iteration => 0.0109551175788\n",
      "Loss for  28456 th iteration => 0.0109547566322\n",
      "Loss for  28457 th iteration => 0.0109542936935\n",
      "Loss for  28458 th iteration => 0.0109539063527\n",
      "Loss for  28459 th iteration => 0.010953401984\n",
      "Loss for  28460 th iteration => 0.0109530094554\n",
      "Loss for  28461 th iteration => 0.010952621445\n",
      "Loss for  28462 th iteration => 0.0109521211736\n",
      "Loss for  28463 th iteration => 0.0109517346243\n",
      "Loss for  28464 th iteration => 0.0109514170992\n",
      "Loss for  28465 th iteration => 0.0109508456245\n",
      "Loss for  28466 th iteration => 0.0109504559346\n",
      "Loss for  28467 th iteration => 0.0109501344853\n",
      "Loss for  28468 th iteration => 0.0109495704607\n",
      "Loss for  28469 th iteration => 0.0109492477158\n",
      "Loss for  28470 th iteration => 0.0109487818172\n",
      "Loss for  28471 th iteration => 0.010948295669\n",
      "Loss for  28472 th iteration => 0.0109479667065\n",
      "Loss for  28473 th iteration => 0.0109475025244\n",
      "Loss for  28474 th iteration => 0.0109470210367\n",
      "Loss for  28475 th iteration => 0.0109466860477\n",
      "Loss for  28476 th iteration => 0.0109462236022\n",
      "Loss for  28477 th iteration => 0.010945746567\n",
      "Loss for  28478 th iteration => 0.0109454057377\n",
      "Loss for  28479 th iteration => 0.0109449450484\n",
      "Loss for  28480 th iteration => 0.0109444722634\n",
      "Loss for  28481 th iteration => 0.0109441257746\n",
      "Loss for  28482 th iteration => 0.0109436668609\n",
      "Loss for  28483 th iteration => 0.0109431981292\n",
      "Loss for  28484 th iteration => 0.0109428461568\n",
      "Loss for  28485 th iteration => 0.0109423890375\n",
      "Loss for  28486 th iteration => 0.0109419241675\n",
      "Loss for  28487 th iteration => 0.0109415687235\n",
      "Loss for  28488 th iteration => 0.0109411214015\n",
      "Loss for  28489 th iteration => 0.0109406513284\n",
      "Loss for  28490 th iteration => 0.0109402870491\n",
      "Loss for  28491 th iteration => 0.0109398371664\n",
      "Loss for  28492 th iteration => 0.0109393732026\n",
      "Loss for  28493 th iteration => 0.0109390056206\n",
      "Loss for  28494 th iteration => 0.0109385536163\n",
      "Loss for  28495 th iteration => 0.0109380951515\n",
      "Loss for  28496 th iteration => 0.0109377244387\n",
      "Loss for  28497 th iteration => 0.0109372707396\n",
      "Loss for  28498 th iteration => 0.0109368171799\n",
      "Loss for  28499 th iteration => 0.010936443504\n",
      "Loss for  28500 th iteration => 0.010935988525\n",
      "Loss for  28501 th iteration => 0.0109355392926\n",
      "Loss for  28502 th iteration => 0.0109351628171\n",
      "Loss for  28503 th iteration => 0.0109347069616\n",
      "Loss for  28504 th iteration => 0.0109342614946\n",
      "Loss for  28505 th iteration => 0.0109338823786\n",
      "Loss for  28506 th iteration => 0.0109334260389\n",
      "Loss for  28507 th iteration => 0.0109329837903\n",
      "Loss for  28508 th iteration => 0.0109326033827\n",
      "Loss for  28509 th iteration => 0.0109322510866\n",
      "Loss for  28510 th iteration => 0.0109317122729\n",
      "Loss for  28511 th iteration => 0.0109313291214\n",
      "Loss for  28512 th iteration => 0.0109309731126\n",
      "Loss for  28513 th iteration => 0.0109304409995\n",
      "Loss for  28514 th iteration => 0.0109300550652\n",
      "Loss for  28515 th iteration => 0.010929695565\n",
      "Loss for  28516 th iteration => 0.010929169971\n",
      "Loss for  28517 th iteration => 0.0109287812165\n",
      "Loss for  28518 th iteration => 0.01092842087\n",
      "Loss for  28519 th iteration => 0.0109279087454\n",
      "Loss for  28520 th iteration => 0.0109275065365\n",
      "Loss for  28521 th iteration => 0.0109271392481\n",
      "Loss for  28522 th iteration => 0.0109266332095\n",
      "Loss for  28523 th iteration => 0.0109262277061\n",
      "Loss for  28524 th iteration => 0.010925858312\n",
      "Loss for  28525 th iteration => 0.010925357737\n",
      "Loss for  28526 th iteration => 0.0109249491428\n",
      "Loss for  28527 th iteration => 0.0109245783306\n",
      "Loss for  28528 th iteration => 0.0109240991053\n",
      "Loss for  28529 th iteration => 0.0109236538079\n",
      "Loss for  28530 th iteration => 0.0109232993258\n",
      "Loss for  28531 th iteration => 0.0109228202897\n",
      "Loss for  28532 th iteration => 0.0109223786576\n",
      "Loss for  28533 th iteration => 0.0109220206993\n",
      "Loss for  28534 th iteration => 0.0109215420718\n",
      "Loss for  28535 th iteration => 0.0109211035292\n",
      "Loss for  28536 th iteration => 0.010920742448\n",
      "Loss for  28537 th iteration => 0.0109202644423\n",
      "Loss for  28538 th iteration => 0.0109198284295\n",
      "Loss for  28539 th iteration => 0.0109194691558\n",
      "Loss for  28540 th iteration => 0.010919098028\n",
      "Loss for  28541 th iteration => 0.010918559968\n",
      "Loss for  28542 th iteration => 0.0109181974231\n",
      "Loss for  28543 th iteration => 0.0109178234154\n",
      "Loss for  28544 th iteration => 0.0109172917362\n",
      "Loss for  28545 th iteration => 0.0109169259169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  28546 th iteration => 0.0109165492194\n",
      "Loss for  28547 th iteration => 0.0109160237355\n",
      "Loss for  28548 th iteration => 0.0109156546388\n",
      "Loss for  28549 th iteration => 0.0109152773529\n",
      "Loss for  28550 th iteration => 0.0109147633119\n",
      "Loss for  28551 th iteration => 0.010914389815\n",
      "Loss for  28552 th iteration => 0.0109140001762\n",
      "Loss for  28553 th iteration => 0.0109134921017\n",
      "Loss for  28554 th iteration => 0.0109131146679\n",
      "Loss for  28555 th iteration => 0.0109127236238\n",
      "Loss for  28556 th iteration => 0.010912220941\n",
      "Loss for  28557 th iteration => 0.0109118398149\n",
      "Loss for  28558 th iteration => 0.0109114476862\n",
      "Loss for  28559 th iteration => 0.0109109498355\n",
      "Loss for  28560 th iteration => 0.0109105652554\n",
      "Loss for  28561 th iteration => 0.0109101723539\n",
      "Loss for  28562 th iteration => 0.0109096787911\n",
      "Loss for  28563 th iteration => 0.0109092909886\n",
      "Loss for  28564 th iteration => 0.0109088976572\n",
      "Loss for  28565 th iteration => 0.0109084412964\n",
      "Loss for  28566 th iteration => 0.0109079834907\n",
      "Loss for  28567 th iteration => 0.0109076235389\n",
      "Loss for  28568 th iteration => 0.0109071674645\n",
      "Loss for  28569 th iteration => 0.0109067126909\n",
      "Loss for  28570 th iteration => 0.0109063497521\n",
      "Loss for  28571 th iteration => 0.0109058722496\n",
      "Loss for  28572 th iteration => 0.0109055513638\n",
      "Loss for  28573 th iteration => 0.0109050806086\n",
      "Loss for  28574 th iteration => 0.0109046315853\n",
      "Loss for  28575 th iteration => 0.0109042542094\n",
      "Loss for  28576 th iteration => 0.0109038119384\n",
      "Loss for  28577 th iteration => 0.0109033632679\n",
      "Loss for  28578 th iteration => 0.0109029864028\n",
      "Loss for  28579 th iteration => 0.0109025435622\n",
      "Loss for  28580 th iteration => 0.0109020952519\n",
      "Loss for  28581 th iteration => 0.0109017188715\n",
      "Loss for  28582 th iteration => 0.0109012754797\n",
      "Loss for  28583 th iteration => 0.0109008275367\n",
      "Loss for  28584 th iteration => 0.0109004516155\n",
      "Loss for  28585 th iteration => 0.0109000076907\n",
      "Loss for  28586 th iteration => 0.010899560122\n",
      "Loss for  28587 th iteration => 0.0108991846351\n",
      "Loss for  28588 th iteration => 0.010898740195\n",
      "Loss for  28589 th iteration => 0.0108982930072\n",
      "Loss for  28590 th iteration => 0.0108979179305\n",
      "Loss for  28591 th iteration => 0.0108974729921\n",
      "Loss for  28592 th iteration => 0.0108970261918\n",
      "Loss for  28593 th iteration => 0.0108966515018\n",
      "Loss for  28594 th iteration => 0.0108962060819\n",
      "Loss for  28595 th iteration => 0.0108957596755\n",
      "Loss for  28596 th iteration => 0.0108953853494\n",
      "Loss for  28597 th iteration => 0.0108949394641\n",
      "Loss for  28598 th iteration => 0.0108944934578\n",
      "Loss for  28599 th iteration => 0.0108941194732\n",
      "Loss for  28600 th iteration => 0.0108936732643\n",
      "Loss for  28601 th iteration => 0.0108932278695\n",
      "Loss for  28602 th iteration => 0.010892863058\n",
      "Loss for  28603 th iteration => 0.0108924171738\n",
      "Loss for  28604 th iteration => 0.010891961552\n",
      "Loss for  28605 th iteration => 0.0108915923943\n",
      "Loss for  28606 th iteration => 0.0108911448437\n",
      "Loss for  28607 th iteration => 0.0108906940781\n",
      "Loss for  28608 th iteration => 0.0108903219747\n",
      "Loss for  28609 th iteration => 0.0108898731686\n",
      "Loss for  28610 th iteration => 0.0108894266871\n",
      "Loss for  28611 th iteration => 0.0108890517998\n",
      "Loss for  28612 th iteration => 0.0108886021375\n",
      "Loss for  28613 th iteration => 0.0108881593838\n",
      "Loss for  28614 th iteration => 0.0108877818704\n",
      "Loss for  28615 th iteration => 0.0108873317402\n",
      "Loss for  28616 th iteration => 0.0108868921729\n",
      "Loss for  28617 th iteration => 0.010886512187\n",
      "Loss for  28618 th iteration => 0.0108860619664\n",
      "Loss for  28619 th iteration => 0.0108856250587\n",
      "Loss for  28620 th iteration => 0.0108852427504\n",
      "Loss for  28621 th iteration => 0.0108847928063\n",
      "Loss for  28622 th iteration => 0.0108843580459\n",
      "Loss for  28623 th iteration => 0.010883973747\n",
      "Loss for  28624 th iteration => 0.0108836261788\n",
      "Loss for  28625 th iteration => 0.0108830971493\n",
      "Loss for  28626 th iteration => 0.0108827100543\n",
      "Loss for  28627 th iteration => 0.0108822729267\n",
      "Loss for  28628 th iteration => 0.0108819220847\n",
      "Loss for  28629 th iteration => 0.0108814470159\n",
      "Loss for  28630 th iteration => 0.0108810100942\n",
      "Loss for  28631 th iteration => 0.010880669009\n",
      "Loss for  28632 th iteration => 0.010880185274\n",
      "Loss for  28633 th iteration => 0.0108797451774\n",
      "Loss for  28634 th iteration => 0.0108793999241\n",
      "Loss for  28635 th iteration => 0.0108789170983\n",
      "Loss for  28636 th iteration => 0.0108785552431\n",
      "Loss for  28637 th iteration => 0.0108780552134\n",
      "Loss for  28638 th iteration => 0.0108776492802\n",
      "Loss for  28639 th iteration => 0.0108772862628\n",
      "Loss for  28640 th iteration => 0.0108767902868\n",
      "Loss for  28641 th iteration => 0.0108763817258\n",
      "Loss for  28642 th iteration => 0.0108760179239\n",
      "Loss for  28643 th iteration => 0.0108755254391\n",
      "Loss for  28644 th iteration => 0.0108751144351\n",
      "Loss for  28645 th iteration => 0.0108747502159\n",
      "Loss for  28646 th iteration => 0.010874260675\n",
      "Loss for  28647 th iteration => 0.0108738474081\n",
      "Loss for  28648 th iteration => 0.0108734831287\n",
      "Loss for  28649 th iteration => 0.0108729959992\n",
      "Loss for  28650 th iteration => 0.0108725806449\n",
      "Loss for  28651 th iteration => 0.0108722166525\n",
      "Loss for  28652 th iteration => 0.0108717425686\n",
      "Loss for  28653 th iteration => 0.0108714222789\n",
      "Loss for  28654 th iteration => 0.0108709557151\n",
      "Loss for  28655 th iteration => 0.0108704839609\n",
      "Loss for  28656 th iteration => 0.0108701594015\n",
      "Loss for  28657 th iteration => 0.0108696951171\n",
      "Loss for  28658 th iteration => 0.0108692255424\n",
      "Loss for  28659 th iteration => 0.0108688980978\n",
      "Loss for  28660 th iteration => 0.0108684426806\n",
      "Loss for  28661 th iteration => 0.0108679681691\n",
      "Loss for  28662 th iteration => 0.0108676329534\n",
      "Loss for  28663 th iteration => 0.0108671770426\n",
      "Loss for  28664 th iteration => 0.0108667072092\n",
      "Loss for  28665 th iteration => 0.0108663681332\n",
      "Loss for  28666 th iteration => 0.0108659119888\n",
      "Loss for  28667 th iteration => 0.0108654462907\n",
      "Loss for  28668 th iteration => 0.0108651036358\n",
      "Loss for  28669 th iteration => 0.0108646475105\n",
      "Loss for  28670 th iteration => 0.0108641854197\n",
      "Loss for  28671 th iteration => 0.0108638394595\n",
      "Loss for  28672 th iteration => 0.0108633835994\n",
      "Loss for  28673 th iteration => 0.0108629246019\n",
      "Loss for  28674 th iteration => 0.0108625756028\n",
      "Loss for  28675 th iteration => 0.0108621202474\n",
      "Loss for  28676 th iteration => 0.0108616638431\n",
      "Loss for  28677 th iteration => 0.0108613120644\n",
      "Loss for  28678 th iteration => 0.0108608574467\n",
      "Loss for  28679 th iteration => 0.0108604031491\n",
      "Loss for  28680 th iteration => 0.0108600488429\n",
      "Loss for  28681 th iteration => 0.0108596033307\n",
      "Loss for  28682 th iteration => 0.0108592248696\n",
      "Loss for  28683 th iteration => 0.0108587903917\n",
      "Loss for  28684 th iteration => 0.0108583452757\n",
      "Loss for  28685 th iteration => 0.0108579671816\n",
      "Loss for  28686 th iteration => 0.01085753223\n",
      "Loss for  28687 th iteration => 0.0108570875163\n",
      "Loss for  28688 th iteration => 0.0108567097672\n",
      "Loss for  28689 th iteration => 0.0108562743574\n",
      "Loss for  28690 th iteration => 0.0108558300519\n",
      "Loss for  28691 th iteration => 0.0108554526267\n",
      "Loss for  28692 th iteration => 0.0108550167739\n",
      "Loss for  28693 th iteration => 0.0108545728823\n",
      "Loss for  28694 th iteration => 0.0108541957602\n",
      "Loss for  28695 th iteration => 0.0108537596473\n",
      "Loss for  28696 th iteration => 0.0108532963183\n",
      "Loss for  28697 th iteration => 0.0108529600149\n",
      "Loss for  28698 th iteration => 0.0108525031681\n",
      "Loss for  28699 th iteration => 0.0108520412252\n",
      "Loss for  28700 th iteration => 0.0108517017078\n",
      "Loss for  28701 th iteration => 0.0108512470186\n",
      "Loss for  28702 th iteration => 0.0108507863424\n",
      "Loss for  28703 th iteration => 0.0108504437162\n",
      "Loss for  28704 th iteration => 0.0108499911975\n",
      "Loss for  28705 th iteration => 0.0108495316717\n",
      "Loss for  28706 th iteration => 0.0108491860391\n",
      "Loss for  28707 th iteration => 0.0108487357035\n",
      "Loss for  28708 th iteration => 0.010848277215\n",
      "Loss for  28709 th iteration => 0.0108479286756\n",
      "Loss for  28710 th iteration => 0.0108474805354\n",
      "Loss for  28711 th iteration => 0.0108470229741\n",
      "Loss for  28712 th iteration => 0.0108466716246\n",
      "Loss for  28713 th iteration => 0.010846225692\n",
      "Loss for  28714 th iteration => 0.0108457689506\n",
      "Loss for  28715 th iteration => 0.0108454158469\n",
      "Loss for  28716 th iteration => 0.0108449805413\n",
      "Loss for  28717 th iteration => 0.0108445180851\n",
      "Loss for  28718 th iteration => 0.0108441559099\n",
      "Loss for  28719 th iteration => 0.0108437194014\n",
      "Loss for  28720 th iteration => 0.0108432611481\n",
      "Loss for  28721 th iteration => 0.0108428962146\n",
      "Loss for  28722 th iteration => 0.0108424588977\n",
      "Loss for  28723 th iteration => 0.0108420042977\n",
      "Loss for  28724 th iteration => 0.0108416367617\n",
      "Loss for  28725 th iteration => 0.01084119902\n",
      "Loss for  28726 th iteration => 0.0108407475384\n",
      "Loss for  28727 th iteration => 0.0108403775519\n",
      "Loss for  28728 th iteration => 0.0108399397583\n",
      "Loss for  28729 th iteration => 0.0108394908747\n",
      "Loss for  28730 th iteration => 0.0108391185857\n",
      "Loss for  28731 th iteration => 0.0108386811029\n",
      "Loss for  28732 th iteration => 0.0108382343111\n",
      "Loss for  28733 th iteration => 0.0108378598639\n",
      "Loss for  28734 th iteration => 0.0108374230444\n",
      "Loss for  28735 th iteration => 0.0108369778518\n",
      "Loss for  28736 th iteration => 0.0108366013872\n",
      "Loss for  28737 th iteration => 0.0108361703804\n",
      "Loss for  28738 th iteration => 0.0108358239209\n",
      "Loss for  28739 th iteration => 0.0108353487144\n",
      "Loss for  28740 th iteration => 0.0108349174667\n",
      "Loss for  28741 th iteration => 0.0108345712157\n",
      "Loss for  28742 th iteration => 0.0108340963153\n",
      "Loss for  28743 th iteration => 0.0108336648393\n",
      "Loss for  28744 th iteration => 0.0108333188022\n",
      "Loss for  28745 th iteration => 0.010832844228\n",
      "Loss for  28746 th iteration => 0.0108324036832\n",
      "Loss for  28747 th iteration => 0.0108320783476\n",
      "Loss for  28748 th iteration => 0.0108315653258\n",
      "Loss for  28749 th iteration => 0.0108311792285\n",
      "Loss for  28750 th iteration => 0.0108308271059\n",
      "Loss for  28751 th iteration => 0.0108303249449\n",
      "Loss for  28752 th iteration => 0.0108299265846\n",
      "Loss for  28753 th iteration => 0.0108295688803\n",
      "Loss for  28754 th iteration => 0.0108290708896\n",
      "Loss for  28755 th iteration => 0.0108286698603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  28756 th iteration => 0.0108283112916\n",
      "Loss for  28757 th iteration => 0.0108278169103\n",
      "Loss for  28758 th iteration => 0.0108274133964\n",
      "Loss for  28759 th iteration => 0.0108270543297\n",
      "Loss for  28760 th iteration => 0.0108265630119\n",
      "Loss for  28761 th iteration => 0.0108261571928\n",
      "Loss for  28762 th iteration => 0.0108257979843\n",
      "Loss for  28763 th iteration => 0.0108253091989\n",
      "Loss for  28764 th iteration => 0.0108249012496\n",
      "Loss for  28765 th iteration => 0.0108245422457\n",
      "Loss for  28766 th iteration => 0.010824055476\n",
      "Loss for  28767 th iteration => 0.0108236455668\n",
      "Loss for  28768 th iteration => 0.0108232871044\n",
      "Loss for  28769 th iteration => 0.0108228018477\n",
      "Loss for  28770 th iteration => 0.0108223907349\n",
      "Loss for  28771 th iteration => 0.0108219685069\n",
      "Loss for  28772 th iteration => 0.0108216199819\n",
      "Loss for  28773 th iteration => 0.0108212465452\n",
      "Loss for  28774 th iteration => 0.0108207211812\n",
      "Loss for  28775 th iteration => 0.010820369383\n",
      "Loss for  28776 th iteration => 0.0108199960884\n",
      "Loss for  28777 th iteration => 0.0108194741024\n",
      "Loss for  28778 th iteration => 0.0108191190322\n",
      "Loss for  28779 th iteration => 0.0108187459833\n",
      "Loss for  28780 th iteration => 0.0108182272712\n",
      "Loss for  28781 th iteration => 0.0108178689305\n",
      "Loss for  28782 th iteration => 0.0108174967938\n",
      "Loss for  28783 th iteration => 0.0108169876855\n",
      "Loss for  28784 th iteration => 0.0108166238091\n",
      "Loss for  28785 th iteration => 0.0108162429957\n",
      "Loss for  28786 th iteration => 0.0108157377889\n",
      "Loss for  28787 th iteration => 0.0108153708424\n",
      "Loss for  28788 th iteration => 0.0108149897755\n",
      "Loss for  28789 th iteration => 0.0108144879593\n",
      "Loss for  28790 th iteration => 0.0108141181605\n",
      "Loss for  28791 th iteration => 0.0108137371249\n",
      "Loss for  28792 th iteration => 0.010813238202\n",
      "Loss for  28793 th iteration => 0.0108128657629\n",
      "Loss for  28794 th iteration => 0.0108124850355\n",
      "Loss for  28795 th iteration => 0.010811988522\n",
      "Loss for  28796 th iteration => 0.0108116136491\n",
      "Loss for  28797 th iteration => 0.0108112334994\n",
      "Loss for  28798 th iteration => 0.0108107389245\n",
      "Loss for  28799 th iteration => 0.0108103618186\n",
      "Loss for  28800 th iteration => 0.0108099825088\n",
      "Loss for  28801 th iteration => 0.0108094894142\n",
      "Loss for  28802 th iteration => 0.010809110271\n",
      "Loss for  28803 th iteration => 0.010808732056\n",
      "Loss for  28804 th iteration => 0.0108082418103\n",
      "Loss for  28805 th iteration => 0.0108079452062\n",
      "Loss for  28806 th iteration => 0.0108074858825\n",
      "Loss for  28807 th iteration => 0.0108069966576\n",
      "Loss for  28808 th iteration => 0.0108066970847\n",
      "Loss for  28809 th iteration => 0.0108062400313\n",
      "Loss for  28810 th iteration => 0.010805751718\n",
      "Loss for  28811 th iteration => 0.0108054499329\n",
      "Loss for  28812 th iteration => 0.0108049313811\n",
      "Loss for  28813 th iteration => 0.0108045705798\n",
      "Loss for  28814 th iteration => 0.0108041311263\n",
      "Loss for  28815 th iteration => 0.0108037575242\n",
      "Loss for  28816 th iteration => 0.0108033246907\n",
      "Loss for  28817 th iteration => 0.0108028857138\n",
      "Loss for  28818 th iteration => 0.0108025121895\n",
      "Loss for  28819 th iteration => 0.0108020792238\n",
      "Loss for  28820 th iteration => 0.0108016197052\n",
      "Loss for  28821 th iteration => 0.0108012891774\n",
      "Loss for  28822 th iteration => 0.0108008349825\n",
      "Loss for  28823 th iteration => 0.0108003753242\n",
      "Loss for  28824 th iteration => 0.0108000428721\n",
      "Loss for  28825 th iteration => 0.0107995910516\n",
      "Loss for  28826 th iteration => 0.0107991311751\n",
      "Loss for  28827 th iteration => 0.0107987968667\n",
      "Loss for  28828 th iteration => 0.0107983474301\n",
      "Loss for  28829 th iteration => 0.010797887259\n",
      "Loss for  28830 th iteration => 0.0107975511604\n",
      "Loss for  28831 th iteration => 0.0107971041173\n",
      "Loss for  28832 th iteration => 0.0107966435771\n",
      "Loss for  28833 th iteration => 0.0107963057526\n",
      "Loss for  28834 th iteration => 0.0107958611123\n",
      "Loss for  28835 th iteration => 0.0107954001306\n",
      "Loss for  28836 th iteration => 0.0107950606427\n",
      "Loss for  28837 th iteration => 0.0107946184141\n",
      "Loss for  28838 th iteration => 0.0107941569206\n",
      "Loss for  28839 th iteration => 0.0107938158299\n",
      "Loss for  28840 th iteration => 0.0107933760222\n",
      "Loss for  28841 th iteration => 0.0107929143774\n",
      "Loss for  28842 th iteration => 0.0107925173872\n",
      "Loss for  28843 th iteration => 0.0107921495255\n",
      "Loss for  28844 th iteration => 0.0107917097026\n",
      "Loss for  28845 th iteration => 0.0107912747834\n",
      "Loss for  28846 th iteration => 0.0107909155761\n",
      "Loss for  28847 th iteration => 0.0107904761835\n",
      "Loss for  28848 th iteration => 0.0107900295829\n",
      "Loss for  28849 th iteration => 0.0107896678452\n",
      "Loss for  28850 th iteration => 0.0107892282309\n",
      "Loss for  28851 th iteration => 0.0107887844738\n",
      "Loss for  28852 th iteration => 0.0107884203542\n",
      "Loss for  28853 th iteration => 0.0107879808825\n",
      "Loss for  28854 th iteration => 0.0107875394606\n",
      "Loss for  28855 th iteration => 0.0107871731038\n",
      "Loss for  28856 th iteration => 0.0107867341288\n",
      "Loss for  28857 th iteration => 0.0107862945474\n",
      "Loss for  28858 th iteration => 0.0107859260948\n",
      "Loss for  28859 th iteration => 0.0107854879607\n",
      "Loss for  28860 th iteration => 0.0107850497385\n",
      "Loss for  28861 th iteration => 0.0107846793277\n",
      "Loss for  28862 th iteration => 0.0107842423692\n",
      "Loss for  28863 th iteration => 0.0107838050381\n",
      "Loss for  28864 th iteration => 0.0107834328032\n",
      "Loss for  28865 th iteration => 0.0107829973459\n",
      "Loss for  28866 th iteration => 0.0107825604501\n",
      "Loss for  28867 th iteration => 0.010782186522\n",
      "Loss for  28868 th iteration => 0.010781758006\n",
      "Loss for  28869 th iteration => 0.0107814162484\n",
      "Loss for  28870 th iteration => 0.0107809459605\n",
      "Loss for  28871 th iteration => 0.0107805173759\n",
      "Loss for  28872 th iteration => 0.0107801758957\n",
      "Loss for  28873 th iteration => 0.0107797056718\n",
      "Loss for  28874 th iteration => 0.0107792770263\n",
      "Loss for  28875 th iteration => 0.0107789361853\n",
      "Loss for  28876 th iteration => 0.0107784369365\n",
      "Loss for  28877 th iteration => 0.0107780562428\n",
      "Loss for  28878 th iteration => 0.0107777082764\n",
      "Loss for  28879 th iteration => 0.0107771990281\n",
      "Loss for  28880 th iteration => 0.0107768153586\n",
      "Loss for  28881 th iteration => 0.0107763860585\n",
      "Loss for  28882 th iteration => 0.0107760526597\n",
      "Loss for  28883 th iteration => 0.0107755769345\n",
      "Loss for  28884 th iteration => 0.0107751438603\n",
      "Loss for  28885 th iteration => 0.0107748075451\n",
      "Loss for  28886 th iteration => 0.0107743326031\n",
      "Loss for  28887 th iteration => 0.0107739016908\n",
      "Loss for  28888 th iteration => 0.0107735627876\n",
      "Loss for  28889 th iteration => 0.0107730888276\n",
      "Loss for  28890 th iteration => 0.0107726595562\n",
      "Loss for  28891 th iteration => 0.0107723183842\n",
      "Loss for  28892 th iteration => 0.0107718456\n",
      "Loss for  28893 th iteration => 0.0107714174626\n",
      "Loss for  28894 th iteration => 0.0107710743323\n",
      "Loss for  28895 th iteration => 0.0107706029124\n",
      "Loss for  28896 th iteration => 0.0107701754156\n",
      "Loss for  28897 th iteration => 0.0107698306293\n",
      "Loss for  28898 th iteration => 0.0107693607572\n",
      "Loss for  28899 th iteration => 0.0107689334212\n",
      "Loss for  28900 th iteration => 0.0107685872725\n",
      "Loss for  28901 th iteration => 0.010768119127\n",
      "Loss for  28902 th iteration => 0.0107676914848\n",
      "Loss for  28903 th iteration => 0.0107673461639\n",
      "Loss for  28904 th iteration => 0.0107669906475\n",
      "Loss for  28905 th iteration => 0.0107664559457\n",
      "Loss for  28906 th iteration => 0.0107661075846\n",
      "Loss for  28907 th iteration => 0.0107657441762\n",
      "Loss for  28908 th iteration => 0.0107652281433\n",
      "Loss for  28909 th iteration => 0.0107648711835\n",
      "Loss for  28910 th iteration => 0.0107644108238\n",
      "Loss for  28911 th iteration => 0.0107640886437\n",
      "Loss for  28912 th iteration => 0.0107635802545\n",
      "Loss for  28913 th iteration => 0.0107632295208\n",
      "Loss for  28914 th iteration => 0.0107628530675\n",
      "Loss for  28915 th iteration => 0.0107623521722\n",
      "Loss for  28916 th iteration => 0.0107619935826\n",
      "Loss for  28917 th iteration => 0.0107616123064\n",
      "Loss for  28918 th iteration => 0.0107611140609\n",
      "Loss for  28919 th iteration => 0.0107607529216\n",
      "Loss for  28920 th iteration => 0.0107603720962\n",
      "Loss for  28921 th iteration => 0.0107598760276\n",
      "Loss for  28922 th iteration => 0.0107595125403\n",
      "Loss for  28923 th iteration => 0.0107591324293\n",
      "Loss for  28924 th iteration => 0.010758638077\n",
      "Loss for  28925 th iteration => 0.0107582724383\n",
      "Loss for  28926 th iteration => 0.0107578932979\n",
      "Loss for  28927 th iteration => 0.010757400214\n",
      "Loss for  28928 th iteration => 0.0107570326152\n",
      "Loss for  28929 th iteration => 0.0107566546948\n",
      "Loss for  28930 th iteration => 0.0107561624432\n",
      "Loss for  28931 th iteration => 0.0107557930706\n",
      "Loss for  28932 th iteration => 0.0107554166128\n",
      "Loss for  28933 th iteration => 0.0107549247692\n",
      "Loss for  28934 th iteration => 0.0107545538042\n",
      "Loss for  28935 th iteration => 0.0107541790449\n",
      "Loss for  28936 th iteration => 0.0107536881322\n",
      "Loss for  28937 th iteration => 0.0107534030625\n",
      "Loss for  28938 th iteration => 0.0107528753924\n",
      "Loss for  28939 th iteration => 0.0107525254411\n",
      "Loss for  28940 th iteration => 0.0107520909239\n",
      "Loss for  28941 th iteration => 0.0107517188132\n",
      "Loss for  28942 th iteration => 0.0107512913448\n",
      "Loss for  28943 th iteration => 0.0107508255944\n",
      "Loss for  28944 th iteration => 0.0107505178974\n",
      "Loss for  28945 th iteration => 0.010750058912\n",
      "Loss for  28946 th iteration => 0.0107495919641\n",
      "Loss for  28947 th iteration => 0.0107492832085\n",
      "Loss for  28948 th iteration => 0.0107488267752\n",
      "Loss for  28949 th iteration => 0.0107483585793\n",
      "Loss for  28950 th iteration => 0.0107480488077\n",
      "Loss for  28951 th iteration => 0.0107475949337\n",
      "Loss for  28952 th iteration => 0.010747125441\n",
      "Loss for  28953 th iteration => 0.0107468146945\n",
      "Loss for  28954 th iteration => 0.0107463633871\n",
      "Loss for  28955 th iteration => 0.0107458925498\n",
      "Loss for  28956 th iteration => 0.0107455808685\n",
      "Loss for  28957 th iteration => 0.0107451321346\n",
      "Loss for  28958 th iteration => 0.0107446599064\n",
      "Loss for  28959 th iteration => 0.0107443473291\n",
      "Loss for  28960 th iteration => 0.0107439011757\n",
      "Loss for  28961 th iteration => 0.0107434281524\n",
      "Loss for  28962 th iteration => 0.0107430496097\n",
      "Loss for  28963 th iteration => 0.0107426817778\n",
      "Loss for  28964 th iteration => 0.0107422500046\n",
      "Loss for  28965 th iteration => 0.0107418007536\n",
      "Loss for  28966 th iteration => 0.0107414675502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  28967 th iteration => 0.0107410199526\n",
      "Loss for  28968 th iteration => 0.0107405685596\n",
      "Loss for  28969 th iteration => 0.0107402351365\n",
      "Loss for  28970 th iteration => 0.0107397901864\n",
      "Loss for  28971 th iteration => 0.0107393366263\n",
      "Loss for  28972 th iteration => 0.0107390030031\n",
      "Loss for  28973 th iteration => 0.0107385607059\n",
      "Loss for  28974 th iteration => 0.0107381049539\n",
      "Loss for  28975 th iteration => 0.0107377711496\n",
      "Loss for  28976 th iteration => 0.0107373315107\n",
      "Loss for  28977 th iteration => 0.0107368735429\n",
      "Loss for  28978 th iteration => 0.0107365395757\n",
      "Loss for  28979 th iteration => 0.0107361026003\n",
      "Loss for  28980 th iteration => 0.010735643715\n",
      "Loss for  28981 th iteration => 0.0107352534382\n",
      "Loss for  28982 th iteration => 0.0107348889888\n",
      "Loss for  28983 th iteration => 0.0107344541944\n",
      "Loss for  28984 th iteration => 0.0107340172891\n",
      "Loss for  28985 th iteration => 0.0107336673431\n",
      "Loss for  28986 th iteration => 0.010733235\n",
      "Loss for  28987 th iteration => 0.0107327879197\n",
      "Loss for  28988 th iteration => 0.0107324324115\n",
      "Loss for  28989 th iteration => 0.0107320005375\n",
      "Loss for  28990 th iteration => 0.0107315553379\n",
      "Loss for  28991 th iteration => 0.0107311977175\n",
      "Loss for  28992 th iteration => 0.0107307666541\n",
      "Loss for  28993 th iteration => 0.0107303228578\n",
      "Loss for  28994 th iteration => 0.0107299632617\n",
      "Loss for  28995 th iteration => 0.010729533341\n",
      "Loss for  28996 th iteration => 0.0107290904836\n",
      "Loss for  28997 th iteration => 0.0107287290449\n",
      "Loss for  28998 th iteration => 0.0107283005897\n",
      "Loss for  28999 th iteration => 0.0107278582192\n",
      "Loss for  29000 th iteration => 0.0107274950676\n",
      "Loss for  29001 th iteration => 0.0107270683918\n",
      "Loss for  29002 th iteration => 0.0107266266651\n",
      "Loss for  29003 th iteration => 0.010726221725\n",
      "Loss for  29004 th iteration => 0.0107258548038\n",
      "Loss for  29005 th iteration => 0.010725417467\n",
      "Loss for  29006 th iteration => 0.0107249896405\n",
      "Loss for  29007 th iteration => 0.0107246213591\n",
      "Loss for  29008 th iteration => 0.0107241949683\n",
      "Loss for  29009 th iteration => 0.010723858071\n",
      "Loss for  29010 th iteration => 0.010723393545\n",
      "Loss for  29011 th iteration => 0.0107229672672\n",
      "Loss for  29012 th iteration => 0.010722630718\n",
      "Loss for  29013 th iteration => 0.0107221660029\n",
      "Loss for  29014 th iteration => 0.0107217398409\n",
      "Loss for  29015 th iteration => 0.0107214036413\n",
      "Loss for  29016 th iteration => 0.0107209387328\n",
      "Loss for  29017 th iteration => 0.0107205126893\n",
      "Loss for  29018 th iteration => 0.0107201768408\n",
      "Loss for  29019 th iteration => 0.0107197117346\n",
      "Loss for  29020 th iteration => 0.0107192858122\n",
      "Loss for  29021 th iteration => 0.0107189517454\n",
      "Loss for  29022 th iteration => 0.01071845702\n",
      "Loss for  29023 th iteration => 0.0107180790324\n",
      "Loss for  29024 th iteration => 0.0107176518317\n",
      "Loss for  29025 th iteration => 0.0107173240087\n",
      "Loss for  29026 th iteration => 0.0107168536615\n",
      "Loss for  29027 th iteration => 0.0107164223265\n",
      "Loss for  29028 th iteration => 0.0107160922418\n",
      "Loss for  29029 th iteration => 0.0107156230863\n",
      "Loss for  29030 th iteration => 0.0107151928609\n",
      "Loss for  29031 th iteration => 0.0107148608212\n",
      "Loss for  29032 th iteration => 0.0107143930438\n",
      "Loss for  29033 th iteration => 0.0107139634407\n",
      "Loss for  29034 th iteration => 0.0107136297445\n",
      "Loss for  29035 th iteration => 0.0107131635266\n",
      "Loss for  29036 th iteration => 0.0107127340715\n",
      "Loss for  29037 th iteration => 0.0107123990092\n",
      "Loss for  29038 th iteration => 0.0107119345275\n",
      "Loss for  29039 th iteration => 0.0107115047588\n",
      "Loss for  29040 th iteration => 0.0107111686127\n",
      "Loss for  29041 th iteration => 0.0107107060392\n",
      "Loss for  29042 th iteration => 0.0107102755081\n",
      "Loss for  29043 th iteration => 0.0107099397471\n",
      "Loss for  29044 th iteration => 0.0107094625995\n",
      "Loss for  29045 th iteration => 0.0107090613036\n",
      "Loss for  29046 th iteration => 0.0107086450334\n",
      "Loss for  29047 th iteration => 0.0107083091238\n",
      "Loss for  29048 th iteration => 0.0107079441708\n",
      "Loss for  29049 th iteration => 0.010707422065\n",
      "Loss for  29050 th iteration => 0.010707082894\n",
      "Loss for  29051 th iteration => 0.0107067211681\n",
      "Loss for  29052 th iteration => 0.0107061993605\n",
      "Loss for  29053 th iteration => 0.0107058569338\n",
      "Loss for  29054 th iteration => 0.010705498451\n",
      "Loss for  29055 th iteration => 0.01070497692\n",
      "Loss for  29056 th iteration => 0.0107046312433\n",
      "Loss for  29057 th iteration => 0.0107042760191\n",
      "Loss for  29058 th iteration => 0.0107037547437\n",
      "Loss for  29059 th iteration => 0.0107034069418\n",
      "Loss for  29060 th iteration => 0.0107030464243\n",
      "Loss for  29061 th iteration => 0.0107025387947\n",
      "Loss for  29062 th iteration => 0.0107021927313\n",
      "Loss for  29063 th iteration => 0.0107017309693\n",
      "Loss for  29064 th iteration => 0.0107014053351\n",
      "Loss for  29065 th iteration => 0.0107009133818\n",
      "Loss for  29066 th iteration => 0.0107005580461\n",
      "Loss for  29067 th iteration => 0.0107001793576\n",
      "Loss for  29068 th iteration => 0.010699688303\n",
      "Loss for  29069 th iteration => 0.0106993311518\n",
      "Loss for  29070 th iteration => 0.0106989538955\n",
      "Loss for  29071 th iteration => 0.0106984633181\n",
      "Loss for  29072 th iteration => 0.0106981045316\n",
      "Loss for  29073 th iteration => 0.010697728942\n",
      "Loss for  29074 th iteration => 0.0106972384314\n",
      "Loss for  29075 th iteration => 0.0106968781851\n",
      "Loss for  29076 th iteration => 0.0106965044904\n",
      "Loss for  29077 th iteration => 0.0106960136473\n",
      "Loss for  29078 th iteration => 0.010695652112\n",
      "Loss for  29079 th iteration => 0.0106952805341\n",
      "Loss for  29080 th iteration => 0.01069478897\n",
      "Loss for  29081 th iteration => 0.0106944264184\n",
      "Loss for  29082 th iteration => 0.0106939816337\n",
      "Loss for  29083 th iteration => 0.0106936402454\n",
      "Loss for  29084 th iteration => 0.0106931712982\n",
      "Loss for  29085 th iteration => 0.0106928819128\n",
      "Loss for  29086 th iteration => 0.0106924212175\n",
      "Loss for  29087 th iteration => 0.0106919497037\n",
      "Loss for  29088 th iteration => 0.0106916603961\n",
      "Loss for  29089 th iteration => 0.0106912024678\n",
      "Loss for  29090 th iteration => 0.0106907283732\n",
      "Loss for  29091 th iteration => 0.0106904391529\n",
      "Loss for  29092 th iteration => 0.0106899839961\n",
      "Loss for  29093 th iteration => 0.010689507307\n",
      "Loss for  29094 th iteration => 0.0106892181831\n",
      "Loss for  29095 th iteration => 0.010688765802\n",
      "Loss for  29096 th iteration => 0.0106882865057\n",
      "Loss for  29097 th iteration => 0.0106879233929\n",
      "Loss for  29098 th iteration => 0.0106876220507\n",
      "Loss for  29099 th iteration => 0.0106870678663\n",
      "Loss for  29100 th iteration => 0.0106867037172\n",
      "Loss for  29101 th iteration => 0.0106863382033\n",
      "Loss for  29102 th iteration => 0.0106859144015\n",
      "Loss for  29103 th iteration => 0.0106854550057\n",
      "Loss for  29104 th iteration => 0.0106851473932\n",
      "Loss for  29105 th iteration => 0.0106846973498\n",
      "Loss for  29106 th iteration => 0.0106842347103\n",
      "Loss for  29107 th iteration => 0.0106839277765\n",
      "Loss for  29108 th iteration => 0.010683480569\n",
      "Loss for  29109 th iteration => 0.0106830146897\n",
      "Loss for  29110 th iteration => 0.0106827084277\n",
      "Loss for  29111 th iteration => 0.0106822640591\n",
      "Loss for  29112 th iteration => 0.010681795598\n",
      "Loss for  29113 th iteration => 0.0106814228298\n",
      "Loss for  29114 th iteration => 0.0106810599408\n",
      "Loss for  29115 th iteration => 0.0106806322494\n",
      "Loss for  29116 th iteration => 0.0106801849632\n",
      "Loss for  29117 th iteration => 0.0106798617513\n",
      "Loss for  29118 th iteration => 0.010679416591\n",
      "Loss for  29119 th iteration => 0.0106789654803\n",
      "Loss for  29120 th iteration => 0.010678643465\n",
      "Loss for  29121 th iteration => 0.0106782011975\n",
      "Loss for  29122 th iteration => 0.0106777462815\n",
      "Loss for  29123 th iteration => 0.0106774254419\n",
      "Loss for  29124 th iteration => 0.010676986069\n",
      "Loss for  29125 th iteration => 0.010676527605\n",
      "Loss for  29126 th iteration => 0.0106761480161\n",
      "Loss for  29127 th iteration => 0.0106758307051\n",
      "Loss for  29128 th iteration => 0.0106753115436\n",
      "Loss for  29129 th iteration => 0.0106749184\n",
      "Loss for  29130 th iteration => 0.0106745818217\n",
      "Loss for  29131 th iteration => 0.0106741419463\n",
      "Loss for  29132 th iteration => 0.0106736997718\n",
      "Loss for  29133 th iteration => 0.0106733648441\n",
      "Loss for  29134 th iteration => 0.0106729279128\n",
      "Loss for  29135 th iteration => 0.0106724814357\n",
      "Loss for  29136 th iteration => 0.0106721481255\n",
      "Loss for  29137 th iteration => 0.010671714139\n",
      "Loss for  29138 th iteration => 0.0106712640485\n",
      "Loss for  29139 th iteration => 0.0106708774334\n",
      "Loss for  29140 th iteration => 0.010670554273\n",
      "Loss for  29141 th iteration => 0.0106700495047\n",
      "Loss for  29142 th iteration => 0.0106696555727\n",
      "Loss for  29143 th iteration => 0.0106693105651\n",
      "Loss for  29144 th iteration => 0.0106688817299\n",
      "Loss for  29145 th iteration => 0.0106684395597\n",
      "Loss for  29146 th iteration => 0.010668089928\n",
      "Loss for  29147 th iteration => 0.0106676624428\n",
      "Loss for  29148 th iteration => 0.0106672209302\n",
      "Loss for  29149 th iteration => 0.0106668695263\n",
      "Loss for  29150 th iteration => 0.0106664437045\n",
      "Loss for  29151 th iteration => 0.0106660024104\n",
      "Loss for  29152 th iteration => 0.0106656493605\n",
      "Loss for  29153 th iteration => 0.010665225507\n",
      "Loss for  29154 th iteration => 0.0106647844333\n",
      "Loss for  29155 th iteration => 0.0106643854314\n",
      "Loss for  29156 th iteration => 0.0106640528191\n",
      "Loss for  29157 th iteration => 0.0106635679684\n",
      "Loss for  29158 th iteration => 0.0106631656845\n",
      "Loss for  29159 th iteration => 0.0106628078819\n",
      "Loss for  29160 th iteration => 0.0106623766288\n",
      "Loss for  29161 th iteration => 0.0106619474707\n",
      "Loss for  29162 th iteration => 0.0106615884794\n",
      "Loss for  29163 th iteration => 0.0106611603241\n",
      "Loss for  29164 th iteration => 0.0106607293844\n",
      "Loss for  29165 th iteration => 0.0106603693162\n",
      "Loss for  29166 th iteration => 0.01065994772\n",
      "Loss for  29167 th iteration => 0.0106596155285\n",
      "Loss for  29168 th iteration => 0.0106591183856\n",
      "Loss for  29169 th iteration => 0.0106587722876\n",
      "Loss for  29170 th iteration => 0.010658404575\n",
      "Loss for  29171 th iteration => 0.0106579051139\n",
      "Loss for  29172 th iteration => 0.0106575408334\n",
      "Loss for  29173 th iteration => 0.0106571180993\n",
      "Loss for  29174 th iteration => 0.0106567841903\n",
      "Loss for  29175 th iteration => 0.0106563277931\n",
      "Loss for  29176 th iteration => 0.0106559053735\n",
      "Loss for  29177 th iteration => 0.0106555719018\n",
      "Loss for  29178 th iteration => 0.0106551150239\n",
      "Loss for  29179 th iteration => 0.010654692916\n",
      "Loss for  29180 th iteration => 0.0106543604021\n",
      "Loss for  29181 th iteration => 0.0106538718234\n",
      "Loss for  29182 th iteration => 0.0106535122418\n",
      "Loss for  29183 th iteration => 0.0106531507924\n",
      "Loss for  29184 th iteration => 0.0106526597748\n",
      "Loss for  29185 th iteration => 0.0106522891759\n",
      "Loss for  29186 th iteration => 0.0106518660573\n",
      "Loss for  29187 th iteration => 0.0106515320238\n",
      "Loss for  29188 th iteration => 0.0106510772662\n",
      "Loss for  29189 th iteration => 0.0106506545077\n",
      "Loss for  29190 th iteration => 0.0106503209334\n",
      "Loss for  29191 th iteration => 0.0106498656281\n",
      "Loss for  29192 th iteration => 0.0106494432257\n",
      "Loss for  29193 th iteration => 0.0106491110899\n",
      "Loss for  29194 th iteration => 0.010648629173\n",
      "Loss for  29195 th iteration => 0.0106482537079\n",
      "Loss for  29196 th iteration => 0.0106479127447\n",
      "Loss for  29197 th iteration => 0.0106474273913\n",
      "Loss for  29198 th iteration => 0.0106470428917\n",
      "Loss for  29199 th iteration => 0.0106466206169\n",
      "Loss for  29200 th iteration => 0.010646288444\n",
      "Loss for  29201 th iteration => 0.010645828666\n",
      "Loss for  29202 th iteration => 0.010645406073\n",
      "Loss for  29203 th iteration => 0.0106450728162\n",
      "Loss for  29204 th iteration => 0.0106446149434\n",
      "Loss for  29205 th iteration => 0.0106441915891\n",
      "Loss for  29206 th iteration => 0.0106438575193\n",
      "Loss for  29207 th iteration => 0.010643401717\n",
      "Loss for  29208 th iteration => 0.0106429771704\n",
      "Loss for  29209 th iteration => 0.0106426437039\n",
      "Loss for  29210 th iteration => 0.0106421724986\n",
      "Loss for  29211 th iteration => 0.0106417781277\n",
      "Loss for  29212 th iteration => 0.0106414312096\n",
      "Loss for  29213 th iteration => 0.0106409585924\n",
      "Loss for  29214 th iteration => 0.0106405653565\n",
      "Loss for  29215 th iteration => 0.0106401521139\n",
      "Loss for  29216 th iteration => 0.0106398100342\n",
      "Loss for  29217 th iteration => 0.0106393538527\n",
      "Loss for  29218 th iteration => 0.0106389378364\n",
      "Loss for  29219 th iteration => 0.0106385993701\n",
      "Loss for  29220 th iteration => 0.0106382518644\n",
      "Loss for  29221 th iteration => 0.0106377297617\n",
      "Loss for  29222 th iteration => 0.0106373899304\n",
      "Loss for  29223 th iteration => 0.0106370331452\n",
      "Loss for  29224 th iteration => 0.010636531211\n",
      "Loss for  29225 th iteration => 0.0106361837359\n",
      "Loss for  29226 th iteration => 0.0106357274537\n",
      "Loss for  29227 th iteration => 0.010635422761\n",
      "Loss for  29228 th iteration => 0.0106349181775\n",
      "Loss for  29229 th iteration => 0.0106345742521\n",
      "Loss for  29230 th iteration => 0.0106342173898\n",
      "Loss for  29231 th iteration => 0.0106337108407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  29232 th iteration => 0.01063336391\n",
      "Loss for  29233 th iteration => 0.0106330068662\n",
      "Loss for  29234 th iteration => 0.0106325094852\n",
      "Loss for  29235 th iteration => 0.0106321635491\n",
      "Loss for  29236 th iteration => 0.0106317112227\n",
      "Loss for  29237 th iteration => 0.010631387248\n",
      "Loss for  29238 th iteration => 0.0106309537377\n",
      "Loss for  29239 th iteration => 0.0106305010037\n",
      "Loss for  29240 th iteration => 0.0106301774171\n",
      "Loss for  29241 th iteration => 0.0106296938783\n",
      "Loss for  29242 th iteration => 0.0106293397505\n",
      "Loss for  29243 th iteration => 0.0106289683904\n",
      "Loss for  29244 th iteration => 0.0106284838679\n",
      "Loss for  29245 th iteration => 0.0106281287204\n",
      "Loss for  29246 th iteration => 0.010627759841\n",
      "Loss for  29247 th iteration => 0.0106272739684\n",
      "Loss for  29248 th iteration => 0.0106269179583\n",
      "Loss for  29249 th iteration => 0.010626551763\n",
      "Loss for  29250 th iteration => 0.0106260641839\n",
      "Loss for  29251 th iteration => 0.0106257081898\n",
      "Loss for  29252 th iteration => 0.0106252662562\n",
      "Loss for  29253 th iteration => 0.0106249327219\n",
      "Loss for  29254 th iteration => 0.0106244592213\n",
      "Loss for  29255 th iteration => 0.0106240972638\n",
      "Loss for  29256 th iteration => 0.0106237258842\n",
      "Loss for  29257 th iteration => 0.0106232522746\n",
      "Loss for  29258 th iteration => 0.0106229755659\n",
      "Loss for  29259 th iteration => 0.0106225230608\n",
      "Loss for  29260 th iteration => 0.0106220451194\n",
      "Loss for  29261 th iteration => 0.0106217699708\n",
      "Loss for  29262 th iteration => 0.0106213204933\n",
      "Loss for  29263 th iteration => 0.0106208401457\n",
      "Loss for  29264 th iteration => 0.0106204855079\n",
      "Loss for  29265 th iteration => 0.0106201248846\n",
      "Loss for  29266 th iteration => 0.010619707347\n",
      "Loss for  29267 th iteration => 0.0106192453425\n",
      "Loss for  29268 th iteration => 0.0106189554125\n",
      "Loss for  29269 th iteration => 0.0106185055936\n",
      "Loss for  29270 th iteration => 0.0106180387725\n",
      "Loss for  29271 th iteration => 0.0106177508365\n",
      "Loss for  29272 th iteration => 0.0106173040911\n",
      "Loss for  29273 th iteration => 0.0106168329214\n",
      "Loss for  29274 th iteration => 0.0106164724325\n",
      "Loss for  29275 th iteration => 0.0106161765627\n",
      "Loss for  29276 th iteration => 0.0106156294621\n",
      "Loss for  29277 th iteration => 0.0106152685729\n",
      "Loss for  29278 th iteration => 0.0106149095035\n",
      "Loss for  29279 th iteration => 0.0106144916872\n",
      "Loss for  29280 th iteration => 0.0106140348618\n",
      "Loss for  29281 th iteration => 0.0106137346157\n",
      "Loss for  29282 th iteration => 0.0106132912371\n",
      "Loss for  29283 th iteration => 0.0106128291976\n",
      "Loss for  29284 th iteration => 0.0106125312983\n",
      "Loss for  29285 th iteration => 0.0106120910337\n",
      "Loss for  29286 th iteration => 0.0106116256401\n",
      "Loss for  29287 th iteration => 0.010611259005\n",
      "Loss for  29288 th iteration => 0.0106109017269\n",
      "Loss for  29289 th iteration => 0.0106104810753\n",
      "Loss for  29290 th iteration => 0.0106100329282\n",
      "Loss for  29291 th iteration => 0.0106097220241\n",
      "Loss for  29292 th iteration => 0.0106092816643\n",
      "Loss for  29293 th iteration => 0.0106088278952\n",
      "Loss for  29294 th iteration => 0.0106085197016\n",
      "Loss for  29295 th iteration => 0.0106080824958\n",
      "Loss for  29296 th iteration => 0.0106076243251\n",
      "Loss for  29297 th iteration => 0.0106072524873\n",
      "Loss for  29298 th iteration => 0.0106068968137\n",
      "Loss for  29299 th iteration => 0.0106064739322\n",
      "Loss for  29300 th iteration => 0.0106060331155\n",
      "Loss for  29301 th iteration => 0.0106057127331\n",
      "Loss for  29302 th iteration => 0.0106052755462\n",
      "Loss for  29303 th iteration => 0.010604828735\n",
      "Loss for  29304 th iteration => 0.0106045113943\n",
      "Loss for  29305 th iteration => 0.0106040773989\n",
      "Loss for  29306 th iteration => 0.0106036255512\n",
      "Loss for  29307 th iteration => 0.0106032495746\n",
      "Loss for  29308 th iteration => 0.0106029393976\n",
      "Loss for  29309 th iteration => 0.0106024253308\n",
      "Loss for  29310 th iteration => 0.0106020353999\n",
      "Loss for  29311 th iteration => 0.0106017066942\n",
      "Loss for  29312 th iteration => 0.0106012729463\n",
      "Loss for  29313 th iteration => 0.0106008316944\n",
      "Loss for  29314 th iteration => 0.0106005063278\n",
      "Loss for  29315 th iteration => 0.0106000758054\n",
      "Loss for  29316 th iteration => 0.0105996294273\n",
      "Loss for  29317 th iteration => 0.0105992489586\n",
      "Loss for  29318 th iteration => 0.0105989350898\n",
      "Loss for  29319 th iteration => 0.0105984302179\n",
      "Loss for  29320 th iteration => 0.0105980399378\n",
      "Loss for  29321 th iteration => 0.0105977038872\n",
      "Loss for  29322 th iteration => 0.0105972737116\n",
      "Loss for  29323 th iteration => 0.0105968369249\n",
      "Loss for  29324 th iteration => 0.0105965044837\n",
      "Loss for  29325 th iteration => 0.0105960775656\n",
      "Loss for  29326 th iteration => 0.0105956359085\n",
      "Loss for  29327 th iteration => 0.0105952509559\n",
      "Loss for  29328 th iteration => 0.0105949344072\n",
      "Loss for  29329 th iteration => 0.0105944380859\n",
      "Loss for  29330 th iteration => 0.0105940467852\n",
      "Loss for  29331 th iteration => 0.0105937079303\n",
      "Loss for  29332 th iteration => 0.0105932860061\n",
      "Loss for  29333 th iteration => 0.0105928477234\n",
      "Loss for  29334 th iteration => 0.0105925039847\n",
      "Loss for  29335 th iteration => 0.0105920843986\n",
      "Loss for  29336 th iteration => 0.0105916460787\n",
      "Loss for  29337 th iteration => 0.01059125298\n",
      "Loss for  29338 th iteration => 0.0105909312838\n",
      "Loss for  29339 th iteration => 0.0105904459246\n",
      "Loss for  29340 th iteration => 0.0105900484827\n",
      "Loss for  29341 th iteration => 0.0105897003046\n",
      "Loss for  29342 th iteration => 0.0105892742497\n",
      "Loss for  29343 th iteration => 0.010588846353\n",
      "Loss for  29344 th iteration => 0.0105884971095\n",
      "Loss for  29345 th iteration => 0.0105880744737\n",
      "Loss for  29346 th iteration => 0.0105876443518\n",
      "Loss for  29347 th iteration => 0.0105872941497\n",
      "Loss for  29348 th iteration => 0.0105868751857\n",
      "Loss for  29349 th iteration => 0.0105864444361\n",
      "Loss for  29350 th iteration => 0.0105860481441\n",
      "Loss for  29351 th iteration => 0.0105856951763\n",
      "Loss for  29352 th iteration => 0.0105852682861\n",
      "Loss for  29353 th iteration => 0.0105848463658\n",
      "Loss for  29354 th iteration => 0.0105844927388\n",
      "Loss for  29355 th iteration => 0.0105840761817\n",
      "Loss for  29356 th iteration => 0.0105837461171\n",
      "Loss for  29357 th iteration => 0.0105832957233\n",
      "Loss for  29358 th iteration => 0.0105828796447\n",
      "Loss for  29359 th iteration => 0.0105825524306\n",
      "Loss for  29360 th iteration => 0.0105820601796\n",
      "Loss for  29361 th iteration => 0.0105817030038\n",
      "Loss for  29362 th iteration => 0.0105812856214\n",
      "Loss for  29363 th iteration => 0.0105809552201\n",
      "Loss for  29364 th iteration => 0.0105805065717\n",
      "Loss for  29365 th iteration => 0.0105800897095\n",
      "Loss for  29366 th iteration => 0.010579759837\n",
      "Loss for  29367 th iteration => 0.0105793104097\n",
      "Loss for  29368 th iteration => 0.0105788940591\n",
      "Loss for  29369 th iteration => 0.0105785668899\n",
      "Loss for  29370 th iteration => 0.0105780801794\n",
      "Loss for  29371 th iteration => 0.0105777188283\n",
      "Loss for  29372 th iteration => 0.0105773012206\n",
      "Loss for  29373 th iteration => 0.0105769710472\n",
      "Loss for  29374 th iteration => 0.0105765232504\n",
      "Loss for  29375 th iteration => 0.010576106194\n",
      "Loss for  29376 th iteration => 0.0105757765633\n",
      "Loss for  29377 th iteration => 0.0105753279429\n",
      "Loss for  29378 th iteration => 0.0105749114283\n",
      "Loss for  29379 th iteration => 0.0105745846706\n",
      "Loss for  29380 th iteration => 0.0105741028304\n",
      "Loss for  29381 th iteration => 0.010573737501\n",
      "Loss for  29382 th iteration => 0.010573319772\n",
      "Loss for  29383 th iteration => 0.0105729898734\n",
      "Loss for  29384 th iteration => 0.0105725427784\n",
      "Loss for  29385 th iteration => 0.0105721256288\n",
      "Loss for  29386 th iteration => 0.0105717962858\n",
      "Loss for  29387 th iteration => 0.0105713483266\n",
      "Loss for  29388 th iteration => 0.0105709317461\n",
      "Loss for  29389 th iteration => 0.010570605737\n",
      "Loss for  29390 th iteration => 0.0105701281508\n",
      "Loss for  29391 th iteration => 0.0105697590252\n",
      "Loss for  29392 th iteration => 0.0105693451707\n",
      "Loss for  29393 th iteration => 0.0105690202517\n",
      "Loss for  29394 th iteration => 0.0105685686337\n",
      "Loss for  29395 th iteration => 0.0105681473571\n",
      "Loss for  29396 th iteration => 0.0105678218948\n",
      "Loss for  29397 th iteration => 0.0105673725183\n",
      "Loss for  29398 th iteration => 0.0105669496117\n",
      "Loss for  29399 th iteration => 0.0105666254817\n",
      "Loss for  29400 th iteration => 0.0105661554112\n",
      "Loss for  29401 th iteration => 0.0105657717587\n",
      "Loss for  29402 th iteration => 0.0105654301078\n",
      "Loss for  29403 th iteration => 0.0105649582249\n",
      "Loss for  29404 th iteration => 0.0105645761122\n",
      "Loss for  29405 th iteration => 0.0105641634036\n",
      "Loss for  29406 th iteration => 0.0105638308128\n",
      "Loss for  29407 th iteration => 0.0105633816677\n",
      "Loss for  29408 th iteration => 0.0105629658096\n",
      "Loss for  29409 th iteration => 0.0105626346761\n",
      "Loss for  29410 th iteration => 0.0105621715462\n",
      "Loss for  29411 th iteration => 0.0105617835936\n",
      "Loss for  29412 th iteration => 0.0105614410665\n",
      "Loss for  29413 th iteration => 0.0105609746847\n",
      "Loss for  29414 th iteration => 0.0105605891202\n",
      "Loss for  29415 th iteration => 0.0105601800057\n",
      "Loss for  29416 th iteration => 0.0105598481855\n",
      "Loss for  29417 th iteration => 0.0105595053965\n",
      "Loss for  29418 th iteration => 0.0105589886004\n",
      "Loss for  29419 th iteration => 0.0105586552772\n",
      "Loss for  29420 th iteration => 0.0105583032566\n",
      "Loss for  29421 th iteration => 0.0105578091328\n",
      "Loss for  29422 th iteration => 0.0105574664375\n",
      "Loss for  29423 th iteration => 0.0105570130957\n",
      "Loss for  29424 th iteration => 0.0105567203208\n",
      "Loss for  29425 th iteration => 0.0105562159258\n",
      "Loss for  29426 th iteration => 0.010555878619\n",
      "Loss for  29427 th iteration => 0.0105555336416\n",
      "Loss for  29428 th iteration => 0.0105550252982\n",
      "Loss for  29429 th iteration => 0.0105546870508\n",
      "Loss for  29430 th iteration => 0.0105543355087\n",
      "Loss for  29431 th iteration => 0.0105538430973\n",
      "Loss for  29432 th iteration => 0.0105534998537\n",
      "Loss for  29433 th iteration => 0.0105530499587\n",
      "Loss for  29434 th iteration => 0.0105527510641\n",
      "Loss for  29435 th iteration => 0.0105522544374\n",
      "Loss for  29436 th iteration => 0.0105519117012\n",
      "Loss for  29437 th iteration => 0.0105515595692\n",
      "Loss for  29438 th iteration => 0.0105510700242\n",
      "Loss for  29439 th iteration => 0.0105507289568\n",
      "Loss for  29440 th iteration => 0.0105502828912\n",
      "Loss for  29441 th iteration => 0.0105499649368\n",
      "Loss for  29442 th iteration => 0.0105494874957\n",
      "Loss for  29443 th iteration => 0.010549140475\n",
      "Loss for  29444 th iteration => 0.0105487743599\n",
      "Loss for  29445 th iteration => 0.0105482947201\n",
      "Loss for  29446 th iteration => 0.0105479471886\n",
      "Loss for  29447 th iteration => 0.0105475842323\n",
      "Loss for  29448 th iteration => 0.0105471020652\n",
      "Loss for  29449 th iteration => 0.010546754164\n",
      "Loss for  29450 th iteration => 0.0105463945486\n",
      "Loss for  29451 th iteration => 0.0105459095346\n",
      "Loss for  29452 th iteration => 0.0105455632273\n",
      "Loss for  29453 th iteration => 0.0105451226306\n",
      "Loss for  29454 th iteration => 0.0105448001222\n",
      "Loss for  29455 th iteration => 0.0105443275243\n",
      "Loss for  29456 th iteration => 0.0105439748127\n",
      "Loss for  29457 th iteration => 0.0105436116138\n",
      "Loss for  29458 th iteration => 0.0105431351731\n",
      "Loss for  29459 th iteration => 0.0105427826532\n",
      "Loss for  29460 th iteration => 0.0105424235286\n",
      "Loss for  29461 th iteration => 0.0105419429602\n",
      "Loss for  29462 th iteration => 0.0105415989578\n",
      "Loss for  29463 th iteration => 0.0105412431281\n",
      "Loss for  29464 th iteration => 0.0105408347181\n",
      "Loss for  29465 th iteration => 0.0105403686537\n",
      "Loss for  29466 th iteration => 0.0105400975209\n",
      "Loss for  29467 th iteration => 0.0105396510666\n",
      "Loss for  29468 th iteration => 0.0105391787174\n",
      "Loss for  29469 th iteration => 0.0105388302349\n",
      "Loss for  29470 th iteration => 0.0105385478281\n",
      "Loss for  29471 th iteration => 0.0105379917722\n",
      "Loss for  29472 th iteration => 0.0105376436969\n",
      "Loss for  29473 th iteration => 0.0105372888536\n",
      "Loss for  29474 th iteration => 0.0105368803576\n",
      "Loss for  29475 th iteration => 0.0105364178471\n",
      "Loss for  29476 th iteration => 0.0105361398296\n",
      "Loss for  29477 th iteration => 0.0105356976769\n",
      "Loss for  29478 th iteration => 0.010535229338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  29479 th iteration => 0.0105348767022\n",
      "Loss for  29480 th iteration => 0.0105345234441\n",
      "Loss for  29481 th iteration => 0.0105341111882\n",
      "Loss for  29482 th iteration => 0.0105336581877\n",
      "Loss for  29483 th iteration => 0.0105333708193\n",
      "Loss for  29484 th iteration => 0.0105329292439\n",
      "Loss for  29485 th iteration => 0.0105324689512\n",
      "Loss for  29486 th iteration => 0.0105321856863\n",
      "Loss for  29487 th iteration => 0.0105317475209\n",
      "Loss for  29488 th iteration => 0.0105312822862\n",
      "Loss for  29489 th iteration => 0.0105309266191\n",
      "Loss for  29490 th iteration => 0.010530574131\n",
      "Loss for  29491 th iteration => 0.0105301623225\n",
      "Loss for  29492 th iteration => 0.010529710959\n",
      "Loss for  29493 th iteration => 0.0105294188438\n",
      "Loss for  29494 th iteration => 0.0105289813296\n",
      "Loss for  29495 th iteration => 0.010528522845\n",
      "Loss for  29496 th iteration => 0.0105281638314\n",
      "Loss for  29497 th iteration => 0.0105278710235\n",
      "Loss for  29498 th iteration => 0.0105273379836\n",
      "Loss for  29499 th iteration => 0.010526979349\n",
      "Loss for  29500 th iteration => 0.0105266274585\n",
      "Loss for  29501 th iteration => 0.0105262168513\n",
      "Loss for  29502 th iteration => 0.0105257662312\n",
      "Loss for  29503 th iteration => 0.0105254696537\n",
      "Loss for  29504 th iteration => 0.0105250367995\n",
      "Loss for  29505 th iteration => 0.0105245803997\n",
      "Loss for  29506 th iteration => 0.010524217945\n",
      "Loss for  29507 th iteration => 0.0105238673766\n",
      "Loss for  29508 th iteration => 0.0105234538521\n",
      "Loss for  29509 th iteration => 0.0105230108931\n",
      "Loss for  29510 th iteration => 0.0105227065667\n",
      "Loss for  29511 th iteration => 0.010522274517\n",
      "Loss for  29512 th iteration => 0.0105218239977\n",
      "Loss for  29513 th iteration => 0.010521458715\n",
      "Loss for  29514 th iteration => 0.010521160245\n",
      "Loss for  29515 th iteration => 0.0105206405403\n",
      "Loss for  29516 th iteration => 0.0105202564556\n",
      "Loss for  29517 th iteration => 0.0105199450554\n",
      "Loss for  29518 th iteration => 0.010519514007\n",
      "Loss for  29519 th iteration => 0.0105190695366\n",
      "Loss for  29520 th iteration => 0.0105187631268\n",
      "Loss for  29521 th iteration => 0.0105183355905\n",
      "Loss for  29522 th iteration => 0.0105178857186\n",
      "Loss for  29523 th iteration => 0.0105175177489\n",
      "Loss for  29524 th iteration => 0.0105171689688\n",
      "Loss for  29525 th iteration => 0.0105167548449\n",
      "Loss for  29526 th iteration => 0.0105163169749\n",
      "Loss for  29527 th iteration => 0.0105160037323\n",
      "Loss for  29528 th iteration => 0.0105155771337\n",
      "Loss for  29529 th iteration => 0.0105151324141\n",
      "Loss for  29530 th iteration => 0.0105147613041\n",
      "Loss for  29531 th iteration => 0.0105144136041\n",
      "Loss for  29532 th iteration => 0.0105139973373\n",
      "Loss for  29533 th iteration => 0.0105135654301\n",
      "Loss for  29534 th iteration => 0.0105132459087\n",
      "Loss for  29535 th iteration => 0.0105128203248\n",
      "Loss for  29536 th iteration => 0.0105123803207\n",
      "Loss for  29537 th iteration => 0.0105120073441\n",
      "Loss for  29538 th iteration => 0.0105117011953\n",
      "Loss for  29539 th iteration => 0.0105111991613\n",
      "Loss for  29540 th iteration => 0.0105108148338\n",
      "Loss for  29541 th iteration => 0.0105104896316\n",
      "Loss for  29542 th iteration => 0.0105100652559\n",
      "Loss for  29543 th iteration => 0.0105096295942\n",
      "Loss for  29544 th iteration => 0.0105093099351\n",
      "Loss for  29545 th iteration => 0.0105088891383\n",
      "Loss for  29546 th iteration => 0.0105084482476\n",
      "Loss for  29547 th iteration => 0.0105080730911\n",
      "Loss for  29548 th iteration => 0.0105077266668\n",
      "Loss for  29549 th iteration => 0.0105073114872\n",
      "Loss for  29550 th iteration => 0.0105068809601\n",
      "Loss for  29551 th iteration => 0.0105065557336\n",
      "Loss for  29552 th iteration => 0.0105061360594\n",
      "Loss for  29553 th iteration => 0.010505699315\n",
      "Loss for  29554 th iteration => 0.0105053215686\n",
      "Loss for  29555 th iteration => 0.0105049760262\n",
      "Loss for  29556 th iteration => 0.0105045593361\n",
      "Loss for  29557 th iteration => 0.0105041333926\n",
      "Loss for  29558 th iteration => 0.0105038037351\n",
      "Loss for  29559 th iteration => 0.0105033924182\n",
      "Loss for  29560 th iteration => 0.010502956925\n",
      "Loss for  29561 th iteration => 0.0105025680761\n",
      "Loss for  29562 th iteration => 0.0105022627925\n",
      "Loss for  29563 th iteration => 0.0105017764507\n",
      "Loss for  29564 th iteration => 0.0105013839799\n",
      "Loss for  29565 th iteration => 0.0105010454628\n",
      "Loss for  29566 th iteration => 0.0105006296377\n",
      "Loss for  29567 th iteration => 0.0105002010274\n",
      "Loss for  29568 th iteration => 0.01049986174\n",
      "Loss for  29569 th iteration => 0.0104994500574\n",
      "Loss for  29570 th iteration => 0.0104990200771\n",
      "Loss for  29571 th iteration => 0.010498630171\n",
      "Loss for  29572 th iteration => 0.0104982883666\n",
      "Loss for  29573 th iteration => 0.0104978695202\n",
      "Loss for  29574 th iteration => 0.0104974474481\n",
      "Loss for  29575 th iteration => 0.0104971051562\n",
      "Loss for  29576 th iteration => 0.0104966911481\n",
      "Loss for  29577 th iteration => 0.0104962653186\n",
      "Loss for  29578 th iteration => 0.0104958782371\n",
      "Loss for  29579 th iteration => 0.0104955580478\n",
      "Loss for  29580 th iteration => 0.0104950865014\n",
      "Loss for  29581 th iteration => 0.0104946942566\n",
      "Loss for  29582 th iteration => 0.0104943498259\n",
      "Loss for  29583 th iteration => 0.0104939413231\n",
      "Loss for  29584 th iteration => 0.0104936158129\n",
      "Loss for  29585 th iteration => 0.0104931315339\n",
      "Loss for  29586 th iteration => 0.0104928065222\n",
      "Loss for  29587 th iteration => 0.0104924430829\n",
      "Loss for  29588 th iteration => 0.0104919525576\n",
      "Loss for  29589 th iteration => 0.0104916058704\n",
      "Loss for  29590 th iteration => 0.010491197271\n",
      "Loss for  29591 th iteration => 0.0104908710214\n",
      "Loss for  29592 th iteration => 0.0104904290471\n",
      "Loss for  29593 th iteration => 0.0104900211726\n",
      "Loss for  29594 th iteration => 0.0104896981725\n",
      "Loss for  29595 th iteration => 0.0104892123725\n",
      "Loss for  29596 th iteration => 0.0104888632522\n",
      "Loss for  29597 th iteration => 0.0104884544214\n",
      "Loss for  29598 th iteration => 0.0104881281149\n",
      "Loss for  29599 th iteration => 0.0104876870091\n",
      "Loss for  29600 th iteration => 0.0104872789278\n",
      "Loss for  29601 th iteration => 0.0104869548815\n",
      "Loss for  29602 th iteration => 0.0104864734346\n",
      "Loss for  29603 th iteration => 0.0104861220656\n",
      "Loss for  29604 th iteration => 0.0104858029066\n",
      "Loss for  29605 th iteration => 0.0104852981146\n",
      "Loss for  29606 th iteration => 0.0104849461595\n",
      "Loss for  29607 th iteration => 0.0104845379957\n",
      "Loss for  29608 th iteration => 0.010484213415\n",
      "Loss for  29609 th iteration => 0.0104837363499\n",
      "Loss for  29610 th iteration => 0.010483398296\n",
      "Loss for  29611 th iteration => 0.0104830429466\n",
      "Loss for  29612 th iteration => 0.0104825608378\n",
      "Loss for  29613 th iteration => 0.0104822068945\n",
      "Loss for  29614 th iteration => 0.0104817987789\n",
      "Loss for  29615 th iteration => 0.0104814731917\n",
      "Loss for  29616 th iteration => 0.0104810003571\n",
      "Loss for  29617 th iteration => 0.0104806575421\n",
      "Loss for  29618 th iteration => 0.010480303397\n",
      "Loss for  29619 th iteration => 0.0104798248237\n",
      "Loss for  29620 th iteration => 0.0104794689874\n",
      "Loss for  29621 th iteration => 0.0104790609727\n",
      "Loss for  29622 th iteration => 0.0104787347446\n",
      "Loss for  29623 th iteration => 0.0104782947591\n",
      "Loss for  29624 th iteration => 0.0104778875399\n",
      "Loss for  29625 th iteration => 0.0104775653526\n",
      "Loss for  29626 th iteration => 0.010477090356\n",
      "Loss for  29627 th iteration => 0.0104767324234\n",
      "Loss for  29628 th iteration => 0.0104763243497\n",
      "Loss for  29629 th iteration => 0.0104759981488\n",
      "Loss for  29630 th iteration => 0.0104755587769\n",
      "Loss for  29631 th iteration => 0.0104751515184\n",
      "Loss for  29632 th iteration => 0.0104748288837\n",
      "Loss for  29633 th iteration => 0.0104743571571\n",
      "Loss for  29634 th iteration => 0.0104739972174\n",
      "Loss for  29635 th iteration => 0.0104735891321\n",
      "Loss for  29636 th iteration => 0.01047326298\n",
      "Loss for  29637 th iteration => 0.0104728241531\n",
      "Loss for  29638 th iteration => 0.0104724169017\n",
      "Loss for  29639 th iteration => 0.0104720939786\n",
      "Loss for  29640 th iteration => 0.0104716252329\n",
      "Loss for  29641 th iteration => 0.0104712633702\n",
      "Loss for  29642 th iteration => 0.0104708567716\n",
      "Loss for  29643 th iteration => 0.0104705373517\n",
      "Loss for  29644 th iteration => 0.0104700959203\n",
      "Loss for  29645 th iteration => 0.0104696798302\n",
      "Loss for  29646 th iteration => 0.0104693622231\n",
      "Loss for  29647 th iteration => 0.0104688982257\n",
      "Loss for  29648 th iteration => 0.0104685257758\n",
      "Loss for  29649 th iteration => 0.0104681890231\n",
      "Loss for  29650 th iteration => 0.0104677219893\n",
      "Loss for  29651 th iteration => 0.0104673511306\n",
      "Loss for  29652 th iteration => 0.01046694212\n",
      "Loss for  29653 th iteration => 0.0104666182077\n",
      "Loss for  29654 th iteration => 0.0104661786996\n",
      "Loss for  29655 th iteration => 0.0104657653714\n",
      "Loss for  29656 th iteration => 0.0104654453469\n",
      "Loss for  29657 th iteration => 0.0104649841595\n",
      "Loss for  29658 th iteration => 0.0104646089329\n",
      "Loss for  29659 th iteration => 0.0104642047127\n",
      "Loss for  29660 th iteration => 0.0104638772081\n",
      "Loss for  29661 th iteration => 0.0104634375947\n",
      "Loss for  29662 th iteration => 0.010463028098\n",
      "Loss for  29663 th iteration => 0.0104627044562\n",
      "Loss for  29664 th iteration => 0.0104622468573\n",
      "Loss for  29665 th iteration => 0.0104618694492\n",
      "Loss for  29666 th iteration => 0.0104615340632\n",
      "Loss for  29667 th iteration => 0.0104610774572\n",
      "Loss for  29668 th iteration => 0.0104608103558\n",
      "Loss for  29669 th iteration => 0.0104602970412\n",
      "Loss for  29670 th iteration => 0.0104599712903\n",
      "Loss for  29671 th iteration => 0.0104596276447\n",
      "Loss for  29672 th iteration => 0.0104591422174\n",
      "Loss for  29673 th iteration => 0.0104588053502\n",
      "Loss for  29674 th iteration => 0.0104583533793\n",
      "Loss for  29675 th iteration => 0.0104580798178\n",
      "Loss for  29676 th iteration => 0.0104575728232\n",
      "Loss for  29677 th iteration => 0.0104572442295\n",
      "Loss for  29678 th iteration => 0.0104569005492\n",
      "Loss for  29679 th iteration => 0.0104564163481\n",
      "Loss for  29680 th iteration => 0.0104560782166\n",
      "Loss for  29681 th iteration => 0.0104556303502\n",
      "Loss for  29682 th iteration => 0.0104553510089\n",
      "Loss for  29683 th iteration => 0.0104548498284\n",
      "Loss for  29684 th iteration => 0.0104545185437\n",
      "Loss for  29685 th iteration => 0.0104541748859\n",
      "Loss for  29686 th iteration => 0.0104536918412\n",
      "Loss for  29687 th iteration => 0.0104533526502\n",
      "Loss for  29688 th iteration => 0.0104529083886\n",
      "Loss for  29689 th iteration => 0.0104526239075\n",
      "Loss for  29690 th iteration => 0.0104521280652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  29691 th iteration => 0.0104517942327\n",
      "Loss for  29692 th iteration => 0.0104514506509\n",
      "Loss for  29693 th iteration => 0.0104509686967\n",
      "Loss for  29694 th iteration => 0.0104506286393\n",
      "Loss for  29695 th iteration => 0.0104501875122\n",
      "Loss for  29696 th iteration => 0.0104498984936\n",
      "Loss for  29697 th iteration => 0.0104494075412\n",
      "Loss for  29698 th iteration => 0.0104490712963\n",
      "Loss for  29699 th iteration => 0.0104487278404\n",
      "Loss for  29700 th iteration => 0.0104482469147\n",
      "Loss for  29701 th iteration => 0.010447906173\n",
      "Loss for  29702 th iteration => 0.0104474677379\n",
      "Loss for  29703 th iteration => 0.0104471747479\n",
      "Loss for  29704 th iteration => 0.0104466882636\n",
      "Loss for  29705 th iteration => 0.0104463497342\n",
      "Loss for  29706 th iteration => 0.0104460064508\n",
      "Loss for  29707 th iteration => 0.010445526495\n",
      "Loss for  29708 th iteration => 0.0104451852408\n",
      "Loss for  29709 th iteration => 0.010444749081\n",
      "Loss for  29710 th iteration => 0.0104444526526\n",
      "Loss for  29711 th iteration => 0.0104439702393\n",
      "Loss for  29712 th iteration => 0.010443629546\n",
      "Loss for  29713 th iteration => 0.0104432864789\n",
      "Loss for  29714 th iteration => 0.0104428081619\n",
      "Loss for  29715 th iteration => 0.0104424720709\n",
      "Loss for  29716 th iteration => 0.0104420340694\n",
      "Loss for  29717 th iteration => 0.010441725916\n",
      "Loss for  29718 th iteration => 0.0104412559798\n",
      "Loss for  29719 th iteration => 0.0104409131959\n",
      "Loss for  29720 th iteration => 0.0104405602739\n",
      "Loss for  29721 th iteration => 0.0104400860628\n",
      "Loss for  29722 th iteration => 0.0104397437344\n",
      "Loss for  29723 th iteration => 0.0104393122696\n",
      "Loss for  29724 th iteration => 0.0104389990397\n",
      "Loss for  29725 th iteration => 0.0104385788255\n",
      "Loss for  29726 th iteration => 0.0104381422829\n",
      "Loss for  29727 th iteration => 0.0104378334063\n",
      "Loss for  29728 th iteration => 0.0104373644813\n",
      "Loss for  29729 th iteration => 0.010437019016\n",
      "Loss for  29730 th iteration => 0.0104366692517\n",
      "Loss for  29731 th iteration => 0.010436194918\n",
      "Loss for  29732 th iteration => 0.0104358539245\n",
      "Loss for  29733 th iteration => 0.0104354212087\n",
      "Loss for  29734 th iteration => 0.0104351089969\n",
      "Loss for  29735 th iteration => 0.0104346434651\n",
      "Loss for  29736 th iteration => 0.0104342957602\n",
      "Loss for  29737 th iteration => 0.0104339458948\n",
      "Loss for  29738 th iteration => 0.0104334741365\n",
      "Loss for  29739 th iteration => 0.0104331359579\n",
      "Loss for  29740 th iteration => 0.0104327882092\n",
      "Loss for  29741 th iteration => 0.0104323904193\n",
      "Loss for  29742 th iteration => 0.010431928421\n",
      "Loss for  29743 th iteration => 0.0104316673877\n",
      "Loss for  29744 th iteration => 0.0104312315491\n",
      "Loss for  29745 th iteration => 0.0104307629923\n",
      "Loss for  29746 th iteration => 0.0104304226636\n",
      "Loss for  29747 th iteration => 0.010430075597\n",
      "Loss for  29748 th iteration => 0.0104296769664\n",
      "Loss for  29749 th iteration => 0.0104292182007\n",
      "Loss for  29750 th iteration => 0.0104289530822\n",
      "Loss for  29751 th iteration => 0.0104285187548\n",
      "Loss for  29752 th iteration => 0.0104280530796\n",
      "Loss for  29753 th iteration => 0.010427710776\n",
      "Loss for  29754 th iteration => 0.0104273643293\n",
      "Loss for  29755 th iteration => 0.0104269650583\n",
      "Loss for  29756 th iteration => 0.0104265090858\n",
      "Loss for  29757 th iteration => 0.0104262402621\n",
      "Loss for  29758 th iteration => 0.0104258075007\n",
      "Loss for  29759 th iteration => 0.0104253444059\n",
      "Loss for  29760 th iteration => 0.0104250002922\n",
      "Loss for  29761 th iteration => 0.0104246544066\n",
      "Loss for  29762 th iteration => 0.0104242546841\n",
      "Loss for  29763 th iteration => 0.0104238010911\n",
      "Loss for  29764 th iteration => 0.0104235289194\n",
      "Loss for  29765 th iteration => 0.0104230977762\n",
      "Loss for  29766 th iteration => 0.010422636978\n",
      "Loss for  29767 th iteration => 0.0104222912091\n",
      "Loss for  29768 th iteration => 0.0104219458293\n",
      "Loss for  29769 th iteration => 0.0104215458336\n",
      "Loss for  29770 th iteration => 0.0104210942306\n",
      "Loss for  29771 th iteration => 0.0104208190468\n",
      "Loss for  29772 th iteration => 0.0104203895711\n",
      "Loss for  29773 th iteration => 0.0104199308027\n",
      "Loss for  29774 th iteration => 0.0104195835243\n",
      "Loss for  29775 th iteration => 0.010419238598\n",
      "Loss for  29776 th iteration => 0.0104188384969\n",
      "Loss for  29777 th iteration => 0.0104183885173\n",
      "Loss for  29778 th iteration => 0.0104181106373\n",
      "Loss for  29779 th iteration => 0.010417682876\n",
      "Loss for  29780 th iteration => 0.0104172258859\n",
      "Loss for  29781 th iteration => 0.010416877235\n",
      "Loss for  29782 th iteration => 0.0104165327129\n",
      "Loss for  29783 th iteration => 0.0104161326648\n",
      "Loss for  29784 th iteration => 0.0104156839634\n",
      "Loss for  29785 th iteration => 0.0104154036842\n",
      "Loss for  29786 th iteration => 0.0104149776819\n",
      "Loss for  29787 th iteration => 0.0104145222333\n",
      "Loss for  29788 th iteration => 0.0104141723387\n",
      "Loss for  29789 th iteration => 0.0104138281742\n",
      "Loss for  29790 th iteration => 0.0104134283287\n",
      "Loss for  29791 th iteration => 0.0104129806247\n",
      "Loss for  29792 th iteration => 0.0104126277253\n",
      "Loss for  29793 th iteration => 0.0104123444417\n",
      "Loss for  29794 th iteration => 0.0104118199564\n",
      "Loss for  29795 th iteration => 0.0104114687132\n",
      "Loss for  29796 th iteration => 0.0104111247949\n",
      "Loss for  29797 th iteration => 0.0104107256877\n",
      "Loss for  29798 th iteration => 0.0104102790752\n",
      "Loss for  29799 th iteration => 0.0104099242845\n",
      "Loss for  29800 th iteration => 0.0104095818279\n",
      "Loss for  29801 th iteration => 0.0104091777917\n",
      "Loss for  29802 th iteration => 0.0104087413752\n",
      "Loss for  29803 th iteration => 0.010408448351\n",
      "Loss for  29804 th iteration => 0.0104080245169\n",
      "Loss for  29805 th iteration => 0.0104075785232\n",
      "Loss for  29806 th iteration => 0.0104072229962\n",
      "Loss for  29807 th iteration => 0.0104068807202\n",
      "Loss for  29808 th iteration => 0.0104064774602\n",
      "Loss for  29809 th iteration => 0.0104060407274\n",
      "Loss for  29810 th iteration => 0.0104057466864\n",
      "Loss for  29811 th iteration => 0.0104053248068\n",
      "Loss for  29812 th iteration => 0.0104048792627\n",
      "Loss for  29813 th iteration => 0.0104045230905\n",
      "Loss for  29814 th iteration => 0.0104041809613\n",
      "Loss for  29815 th iteration => 0.0104037785856\n",
      "Loss for  29816 th iteration => 0.0104033413045\n",
      "Loss for  29817 th iteration => 0.0104030464438\n",
      "Loss for  29818 th iteration => 0.0104026265509\n",
      "Loss for  29819 th iteration => 0.0104021812974\n",
      "Loss for  29820 th iteration => 0.0104018245653\n",
      "Loss for  29821 th iteration => 0.010401482551\n",
      "Loss for  29822 th iteration => 0.0104010811616\n",
      "Loss for  29823 th iteration => 0.0104006431139\n",
      "Loss for  29824 th iteration => 0.0104003476188\n",
      "Loss for  29825 th iteration => 0.0103999297431\n",
      "Loss for  29826 th iteration => 0.0103994846303\n",
      "Loss for  29827 th iteration => 0.0103991274188\n",
      "Loss for  29828 th iteration => 0.0103987854893\n",
      "Loss for  29829 th iteration => 0.0103983851823\n",
      "Loss for  29830 th iteration => 0.0103979470227\n",
      "Loss for  29831 th iteration => 0.0103975866234\n",
      "Loss for  29832 th iteration => 0.0103972460215\n",
      "Loss for  29833 th iteration => 0.0103968412136\n",
      "Loss for  29834 th iteration => 0.0103964120902\n",
      "Loss for  29835 th iteration => 0.0103961085053\n",
      "Loss for  29836 th iteration => 0.0103956908435\n",
      "Loss for  29837 th iteration => 0.0103952522266\n",
      "Loss for  29838 th iteration => 0.0103948916761\n",
      "Loss for  29839 th iteration => 0.0103945510441\n",
      "Loss for  29840 th iteration => 0.0103941476913\n",
      "Loss for  29841 th iteration => 0.0103937167727\n",
      "Loss for  29842 th iteration => 0.0103934134414\n",
      "Loss for  29843 th iteration => 0.0103929979266\n",
      "Loss for  29844 th iteration => 0.0103925587461\n",
      "Loss for  29845 th iteration => 0.0103921981005\n",
      "Loss for  29846 th iteration => 0.0103918574166\n",
      "Loss for  29847 th iteration => 0.0103914555884\n",
      "Loss for  29848 th iteration => 0.0103910229657\n",
      "Loss for  29849 th iteration => 0.0103906598323\n",
      "Loss for  29850 th iteration => 0.0103903661809\n",
      "Loss for  29851 th iteration => 0.0103898673774\n",
      "Loss for  29852 th iteration => 0.0103894902458\n",
      "Loss for  29853 th iteration => 0.0103891801878\n",
      "Loss for  29854 th iteration => 0.0103887651998\n",
      "Loss for  29855 th iteration => 0.010388331454\n",
      "Loss for  29856 th iteration => 0.0103879677489\n",
      "Loss for  29857 th iteration => 0.0103876284148\n",
      "Loss for  29858 th iteration => 0.0103872242459\n",
      "Loss for  29859 th iteration => 0.0103867978928\n",
      "Loss for  29860 th iteration => 0.0103864888372\n",
      "Loss for  29861 th iteration => 0.010386076108\n",
      "Loss for  29862 th iteration => 0.0103856412029\n",
      "Loss for  29863 th iteration => 0.0103852777272\n",
      "Loss for  29864 th iteration => 0.0103849382244\n",
      "Loss for  29865 th iteration => 0.0103845359555\n",
      "Loss for  29866 th iteration => 0.0103841072718\n",
      "Loss for  29867 th iteration => 0.0103837413896\n",
      "Loss for  29868 th iteration => 0.0103834455094\n",
      "Loss for  29869 th iteration => 0.0103829534692\n",
      "Loss for  29870 th iteration => 0.0103825760718\n",
      "Loss for  29871 th iteration => 0.0103822613893\n",
      "Loss for  29872 th iteration => 0.0103818493612\n",
      "Loss for  29873 th iteration => 0.0103814190103\n",
      "Loss for  29874 th iteration => 0.0103810529477\n",
      "Loss for  29875 th iteration => 0.0103807146238\n",
      "Loss for  29876 th iteration => 0.0103803105743\n",
      "Loss for  29877 th iteration => 0.0103798867545\n",
      "Loss for  29878 th iteration => 0.0103795737126\n",
      "Loss for  29879 th iteration => 0.0103791640412\n",
      "Loss for  29880 th iteration => 0.0103787320274\n",
      "Loss for  29881 th iteration => 0.0103783568873\n",
      "Loss for  29882 th iteration => 0.010378037857\n",
      "Loss for  29883 th iteration => 0.010377626125\n",
      "Loss for  29884 th iteration => 0.0103771997313\n",
      "Loss for  29885 th iteration => 0.0103768317225\n",
      "Loss for  29886 th iteration => 0.0103764945054\n",
      "Loss for  29887 th iteration => 0.0103760886916\n",
      "Loss for  29888 th iteration => 0.0103756693335\n",
      "Loss for  29889 th iteration => 0.010375352457\n",
      "Loss for  29890 th iteration => 0.0103749431602\n",
      "Loss for  29891 th iteration => 0.010374514688\n",
      "Loss for  29892 th iteration => 0.0103741474079\n",
      "Loss for  29893 th iteration => 0.0103738098408\n",
      "Loss for  29894 th iteration => 0.0103734065088\n",
      "Loss for  29895 th iteration => 0.0103729837389\n",
      "Loss for  29896 th iteration => 0.0103726145136\n",
      "Loss for  29897 th iteration => 0.0103723148949\n",
      "Loss for  29898 th iteration => 0.0103718327229\n",
      "Loss for  29899 th iteration => 0.0103714551263\n",
      "Loss for  29900 th iteration => 0.0103711343149\n",
      "Loss for  29901 th iteration => 0.0103707259934\n",
      "Loss for  29902 th iteration => 0.0103703007171\n",
      "Loss for  29903 th iteration => 0.0103699315765\n",
      "Loss for  29904 th iteration => 0.0103695949238\n",
      "Loss for  29905 th iteration => 0.0103691906785\n",
      "Loss for  29906 th iteration => 0.010368770887\n",
      "Loss for  29907 th iteration => 0.0103684005547\n",
      "Loss for  29908 th iteration => 0.0103680985716\n",
      "Loss for  29909 th iteration => 0.010367620191\n",
      "Loss for  29910 th iteration => 0.0103672437477\n",
      "Loss for  29911 th iteration => 0.0103669203844\n",
      "Loss for  29912 th iteration => 0.0103665197218\n",
      "Loss for  29913 th iteration => 0.0103660944176\n",
      "Loss for  29914 th iteration => 0.0103657138893\n",
      "Loss for  29915 th iteration => 0.0103653855353\n",
      "Loss for  29916 th iteration => 0.0103649789662\n",
      "Loss for  29917 th iteration => 0.0103645598106\n",
      "Loss for  29918 th iteration => 0.0103642312603\n",
      "Loss for  29919 th iteration => 0.0103638302458\n",
      "Loss for  29920 th iteration => 0.0103634077239\n",
      "Loss for  29921 th iteration => 0.0103630273856\n",
      "Loss for  29922 th iteration => 0.0103626970816\n",
      "Loss for  29923 th iteration => 0.010362290997\n",
      "Loss for  29924 th iteration => 0.0103618735673\n",
      "Loss for  29925 th iteration => 0.0103615433088\n",
      "Loss for  29926 th iteration => 0.0103611433661\n",
      "Loss for  29927 th iteration => 0.0103607225538\n",
      "Loss for  29928 th iteration => 0.0103603414959\n",
      "Loss for  29929 th iteration => 0.0103600098049\n",
      "Loss for  29930 th iteration => 0.0103596055772\n",
      "Loss for  29931 th iteration => 0.0103591879554\n",
      "Loss for  29932 th iteration => 0.010358856538\n",
      "Loss for  29933 th iteration => 0.0103584590029\n",
      "Loss for  29934 th iteration => 0.0103580388931\n",
      "Loss for  29935 th iteration => 0.0103576562606\n",
      "Loss for  29936 th iteration => 0.0103573237136\n",
      "Loss for  29937 th iteration => 0.0103569274787\n",
      "Loss for  29938 th iteration => 0.010356611034\n",
      "Loss for  29939 th iteration => 0.0103561299094\n",
      "Loss for  29940 th iteration => 0.0103557961345\n",
      "Loss for  29941 th iteration => 0.0103553994003\n",
      "Loss for  29942 th iteration => 0.010355080698\n",
      "Loss for  29943 th iteration => 0.0103546485473\n",
      "Loss for  29944 th iteration => 0.0103542528641\n",
      "Loss for  29945 th iteration => 0.0103539381692\n",
      "Loss for  29946 th iteration => 0.0103534564543\n",
      "Loss for  29947 th iteration => 0.0103531217339\n",
      "Loss for  29948 th iteration => 0.0103527255565\n",
      "Loss for  29949 th iteration => 0.0103524074797\n",
      "Loss for  29950 th iteration => 0.0103519320176\n",
      "Loss for  29951 th iteration => 0.0103516231624\n",
      "Loss for  29952 th iteration => 0.0103512667574\n",
      "Loss for  29953 th iteration => 0.0103507840658\n",
      "Loss for  29954 th iteration => 0.0103504486896\n",
      "Loss for  29955 th iteration => 0.0103500532581\n",
      "Loss for  29956 th iteration => 0.0103497368553\n",
      "Loss for  29957 th iteration => 0.0103492595784\n",
      "Loss for  29958 th iteration => 0.0103489230568\n",
      "Loss for  29959 th iteration => 0.010348527168\n",
      "Loss for  29960 th iteration => 0.0103482082366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  29961 th iteration => 0.01034777693\n",
      "Loss for  29962 th iteration => 0.0103473821192\n",
      "Loss for  29963 th iteration => 0.0103470677237\n",
      "Loss for  29964 th iteration => 0.0103465893958\n",
      "Loss for  29965 th iteration => 0.0103462520632\n",
      "Loss for  29966 th iteration => 0.0103458567976\n",
      "Loss for  29967 th iteration => 0.0103455391683\n",
      "Loss for  29968 th iteration => 0.0103450660223\n",
      "Loss for  29969 th iteration => 0.0103447278036\n",
      "Loss for  29970 th iteration => 0.0103444255661\n",
      "Loss for  29971 th iteration => 0.0103439204664\n",
      "Loss for  29972 th iteration => 0.010343582215\n",
      "Loss for  29973 th iteration => 0.0103431876895\n",
      "Loss for  29974 th iteration => 0.0103428725707\n",
      "Loss for  29975 th iteration => 0.0103423978562\n",
      "Loss for  29976 th iteration => 0.0103420585302\n",
      "Loss for  29977 th iteration => 0.0103416635824\n",
      "Loss for  29978 th iteration => 0.0103413453449\n",
      "Loss for  29979 th iteration => 0.0103408758059\n",
      "Loss for  29980 th iteration => 0.0103405592037\n",
      "Loss for  29981 th iteration => 0.0103402071881\n",
      "Loss for  29982 th iteration => 0.0103397307126\n",
      "Loss for  29983 th iteration => 0.010339390961\n",
      "Loss for  29984 th iteration => 0.0103389968804\n",
      "Loss for  29985 th iteration => 0.010338680732\n",
      "Loss for  29986 th iteration => 0.0103382092275\n",
      "Loss for  29987 th iteration => 0.0103378684582\n",
      "Loss for  29988 th iteration => 0.010337473985\n",
      "Loss for  29989 th iteration => 0.0103371552289\n",
      "Loss for  29990 th iteration => 0.0103367246799\n",
      "Loss for  29991 th iteration => 0.0103363313265\n",
      "Loss for  29992 th iteration => 0.0103360175821\n",
      "Loss for  29993 th iteration => 0.0103355443151\n",
      "Loss for  29994 th iteration => 0.0103352029438\n",
      "Loss for  29995 th iteration => 0.0103348091962\n",
      "Loss for  29996 th iteration => 0.010334492446\n",
      "Loss for  29997 th iteration => 0.0103340239556\n",
      "Loss for  29998 th iteration => 0.0103336816221\n",
      "Loss for  29999 th iteration => 0.0103332875112\n",
      "Loss for  30000 th iteration => 0.0103329687521\n",
      "Loss for  30001 th iteration => 0.0103325387299\n",
      "Loss for  30002 th iteration => 0.010332145758\n",
      "Loss for  30003 th iteration => 0.0103318315862\n",
      "Loss for  30004 th iteration => 0.0103313610198\n",
      "Loss for  30005 th iteration => 0.0103310181754\n",
      "Loss for  30006 th iteration => 0.0103306248368\n",
      "Loss for  30007 th iteration => 0.0103303077587\n",
      "Loss for  30008 th iteration => 0.0103298417913\n",
      "Loss for  30009 th iteration => 0.0103294980355\n",
      "Loss for  30010 th iteration => 0.0103291043601\n",
      "Loss for  30011 th iteration => 0.0103287856334\n",
      "Loss for  30012 th iteration => 0.0103283560298\n",
      "Loss for  30013 th iteration => 0.0103279635107\n",
      "Loss for  30014 th iteration => 0.0103276491701\n",
      "Loss for  30015 th iteration => 0.0103271808411\n",
      "Loss for  30016 th iteration => 0.0103268366575\n",
      "Loss for  30017 th iteration => 0.0103264437964\n",
      "Loss for  30018 th iteration => 0.0103261266406\n",
      "Loss for  30019 th iteration => 0.0103256627484\n",
      "Loss for  30020 th iteration => 0.0103253176999\n",
      "Loss for  30021 th iteration => 0.0103249245258\n",
      "Loss for  30022 th iteration => 0.0103246058641\n",
      "Loss for  30023 th iteration => 0.0103241765812\n",
      "Loss for  30024 th iteration => 0.0103237845789\n",
      "Loss for  30025 th iteration => 0.010323470306\n",
      "Loss for  30026 th iteration => 0.0103230037916\n",
      "Loss for  30027 th iteration => 0.0103226583913\n",
      "Loss for  30028 th iteration => 0.0103222660694\n",
      "Loss for  30029 th iteration => 0.0103219490648\n",
      "Loss for  30030 th iteration => 0.0103214868391\n",
      "Loss for  30031 th iteration => 0.0103211406163\n",
      "Loss for  30032 th iteration => 0.0103207480028\n",
      "Loss for  30033 th iteration => 0.0103204294359\n",
      "Loss for  30034 th iteration => 0.0103200003849\n",
      "Loss for  30035 th iteration => 0.0103196089569\n",
      "Loss for  30036 th iteration => 0.010319294968\n",
      "Loss for  30037 th iteration => 0.0103188298827\n",
      "Loss for  30038 th iteration => 0.0103184833778\n",
      "Loss for  30039 th iteration => 0.0103180916503\n",
      "Loss for  30040 th iteration => 0.0103177750061\n",
      "Loss for  30041 th iteration => 0.0103173140741\n",
      "Loss for  30042 th iteration => 0.0103169667855\n",
      "Loss for  30043 th iteration => 0.0103165747857\n",
      "Loss for  30044 th iteration => 0.0103162563408\n",
      "Loss for  30045 th iteration => 0.0103158279522\n",
      "Loss for  30046 th iteration => 0.0103154233125\n",
      "Loss for  30047 th iteration => 0.0103151381363\n",
      "Loss for  30048 th iteration => 0.0103146590885\n",
      "Loss for  30049 th iteration => 0.0103143114142\n",
      "Loss for  30050 th iteration => 0.0103139204542\n",
      "Loss for  30051 th iteration => 0.0103136047831\n",
      "Loss for  30052 th iteration => 0.0103131444325\n",
      "Loss for  30053 th iteration => 0.0103127960068\n",
      "Loss for  30054 th iteration => 0.0103124047895\n",
      "Loss for  30055 th iteration => 0.0103120868596\n",
      "Loss for  30056 th iteration => 0.0103116592282\n",
      "Loss for  30057 th iteration => 0.0103112535472\n",
      "Loss for  30058 th iteration => 0.0103109696083\n",
      "Loss for  30059 th iteration => 0.0103104914676\n",
      "Loss for  30060 th iteration => 0.0103101427102\n",
      "Loss for  30061 th iteration => 0.0103097525556\n",
      "Loss for  30062 th iteration => 0.01030943801\n",
      "Loss for  30063 th iteration => 0.0103089779666\n",
      "Loss for  30064 th iteration => 0.0103086284873\n",
      "Loss for  30065 th iteration => 0.0103082380888\n",
      "Loss for  30066 th iteration => 0.0103079210933\n",
      "Loss for  30067 th iteration => 0.0103074648371\n",
      "Loss for  30068 th iteration => 0.0103071147521\n",
      "Loss for  30069 th iteration => 0.0103068047532\n",
      "Loss for  30070 th iteration => 0.0103063269892\n",
      "Loss for  30071 th iteration => 0.010305977886\n",
      "Loss for  30072 th iteration => 0.0103055766785\n",
      "Loss for  30073 th iteration => 0.0103052875534\n",
      "Loss for  30074 th iteration => 0.0103048146108\n",
      "Loss for  30075 th iteration => 0.0103044640121\n",
      "Loss for  30076 th iteration => 0.0103040746034\n",
      "Loss for  30077 th iteration => 0.0103037593885\n",
      "Loss for  30078 th iteration => 0.0103033026456\n",
      "Loss for  30079 th iteration => 0.010302951391\n",
      "Loss for  30080 th iteration => 0.0103025617721\n",
      "Loss for  30081 th iteration => 0.0103022449701\n",
      "Loss for  30082 th iteration => 0.0103018173646\n",
      "Loss for  30083 th iteration => 0.0103014136954\n",
      "Loss for  30084 th iteration => 0.0103011277554\n",
      "Loss for  30085 th iteration => 0.0103006543998\n",
      "Loss for  30086 th iteration => 0.010300303017\n",
      "Loss for  30087 th iteration => 0.0102999050767\n",
      "Loss for  30088 th iteration => 0.0102996124253\n",
      "Loss for  30089 th iteration => 0.0102991435623\n",
      "Loss for  30090 th iteration => 0.0102987913252\n",
      "Loss for  30091 th iteration => 0.0102984027743\n",
      "Loss for  30092 th iteration => 0.0102980872823\n",
      "Loss for  30093 th iteration => 0.01029763314\n",
      "Loss for  30094 th iteration => 0.0102972803062\n",
      "Loss for  30095 th iteration => 0.0102968915747\n",
      "Loss for  30096 th iteration => 0.0102965754189\n",
      "Loss for  30097 th iteration => 0.0102961472378\n",
      "Loss for  30098 th iteration => 0.0102957459498\n",
      "Loss for  30099 th iteration => 0.0102954576999\n",
      "Loss for  30100 th iteration => 0.0102949876006\n",
      "Loss for  30101 th iteration => 0.0102946346874\n",
      "Loss for  30102 th iteration => 0.0102942471446\n",
      "Loss for  30103 th iteration => 0.0102939335265\n",
      "Loss for  30104 th iteration => 0.0102934783469\n",
      "Loss for  30105 th iteration => 0.0102931248535\n",
      "Loss for  30106 th iteration => 0.0102927371358\n",
      "Loss for  30107 th iteration => 0.0102924215971\n",
      "Loss for  30108 th iteration => 0.0102919948048\n",
      "Loss for  30109 th iteration => 0.0102915899476\n",
      "Loss for  30110 th iteration => 0.0102913063916\n",
      "Loss for  30111 th iteration => 0.0102908348541\n",
      "Loss for  30112 th iteration => 0.0102904833942\n",
      "Loss for  30113 th iteration => 0.0102900838774\n",
      "Loss for  30114 th iteration => 0.0102897941353\n",
      "Loss for  30115 th iteration => 0.0102893267337\n",
      "Loss for  30116 th iteration => 0.0102889724794\n",
      "Loss for  30117 th iteration => 0.0102885780886\n",
      "Loss for  30118 th iteration => 0.0102882824605\n",
      "Loss for  30119 th iteration => 0.010287818995\n",
      "Loss for  30120 th iteration => 0.0102874640127\n",
      "Loss for  30121 th iteration => 0.0102870774315\n",
      "Loss for  30122 th iteration => 0.0102867629588\n",
      "Loss for  30123 th iteration => 0.0102863357174\n",
      "Loss for  30124 th iteration => 0.0102859316242\n",
      "Loss for  30125 th iteration => 0.0102855694825\n",
      "Loss for  30126 th iteration => 0.0102852547161\n",
      "Loss for  30127 th iteration => 0.0102848260419\n",
      "Loss for  30128 th iteration => 0.0102844269368\n",
      "Loss for  30129 th iteration => 0.0102841375763\n",
      "Loss for  30130 th iteration => 0.0102836716107\n",
      "Loss for  30131 th iteration => 0.0102833168229\n",
      "Loss for  30132 th iteration => 0.0102829226003\n",
      "Loss for  30133 th iteration => 0.0102826276841\n",
      "Loss for  30134 th iteration => 0.0102821654311\n",
      "Loss for  30135 th iteration => 0.0102818093538\n",
      "Loss for  30136 th iteration => 0.010281423901\n",
      "Loss for  30137 th iteration => 0.0102811101766\n",
      "Loss for  30138 th iteration => 0.0102806830126\n",
      "Loss for  30139 th iteration => 0.0102802792491\n",
      "Loss for  30140 th iteration => 0.0102799175733\n",
      "Loss for  30141 th iteration => 0.0102796035643\n",
      "Loss for  30142 th iteration => 0.0102791750248\n",
      "Loss for  30143 th iteration => 0.0102787754772\n",
      "Loss for  30144 th iteration => 0.0102784876586\n",
      "Loss for  30145 th iteration => 0.0102780223417\n",
      "Loss for  30146 th iteration => 0.0102776674904\n",
      "Loss for  30147 th iteration => 0.0102772726114\n",
      "Loss for  30148 th iteration => 0.0102769795255\n",
      "Loss for  30149 th iteration => 0.0102765177255\n",
      "Loss for  30150 th iteration => 0.0102761607043\n",
      "Loss for  30151 th iteration => 0.0102757764455\n",
      "Loss for  30152 th iteration => 0.010275463496\n",
      "Loss for  30153 th iteration => 0.0102750366134\n",
      "Loss for  30154 th iteration => 0.0102746326809\n",
      "Loss for  30155 th iteration => 0.0102742717357\n",
      "Loss for  30156 th iteration => 0.0102739585097\n",
      "Loss for  30157 th iteration => 0.0102735303029\n",
      "Loss for  30158 th iteration => 0.0102731295613\n",
      "Loss for  30159 th iteration => 0.0102728443005\n",
      "Loss for  30160 th iteration => 0.0102723789418\n",
      "Loss for  30161 th iteration => 0.0102720244431\n",
      "Loss for  30162 th iteration => 0.0102716281822\n",
      "Loss for  30163 th iteration => 0.0102713379055\n",
      "Loss for  30164 th iteration => 0.0102708758925\n",
      "Loss for  30165 th iteration => 0.0102705190681\n",
      "Loss for  30166 th iteration => 0.0102701271035\n",
      "Loss for  30167 th iteration => 0.0102698320683\n",
      "Loss for  30168 th iteration => 0.0102693732297\n",
      "Loss for  30169 th iteration => 0.0102690148413\n",
      "Loss for  30170 th iteration => 0.0102686318845\n",
      "Loss for  30171 th iteration => 0.0102683198369\n",
      "Loss for  30172 th iteration => 0.0102678919479\n",
      "Loss for  30173 th iteration => 0.0102674898933\n",
      "Loss for  30174 th iteration => 0.0102671292228\n",
      "Loss for  30175 th iteration => 0.0102668169123\n",
      "Loss for  30176 th iteration => 0.0102663877847\n",
      "Loss for  30177 th iteration => 0.0102659890258\n",
      "Loss for  30178 th iteration => 0.0102657030877\n",
      "Loss for  30179 th iteration => 0.0102652398291\n",
      "Loss for  30180 th iteration => 0.010264884068\n",
      "Loss for  30181 th iteration => 0.0102644894627\n",
      "Loss for  30182 th iteration => 0.0102641989533\n",
      "Loss for  30183 th iteration => 0.0102637387404\n",
      "Loss for  30184 th iteration => 0.0102633808319\n",
      "Loss for  30185 th iteration => 0.0102629902071\n",
      "Loss for  30186 th iteration => 0.0102626953678\n",
      "Loss for  30187 th iteration => 0.01026223804\n",
      "Loss for  30188 th iteration => 0.0102618785618\n",
      "Loss for  30189 th iteration => 0.0102614970411\n",
      "Loss for  30190 th iteration => 0.0102611859515\n",
      "Loss for  30191 th iteration => 0.010260757435\n",
      "Loss for  30192 th iteration => 0.0102603563068\n",
      "Loss for  30193 th iteration => 0.0102599964221\n",
      "Loss for  30194 th iteration => 0.0102596850811\n",
      "Loss for  30195 th iteration => 0.0102592554002\n",
      "Loss for  30196 th iteration => 0.0102588571887\n",
      "Loss for  30197 th iteration => 0.010258572523\n",
      "Loss for  30198 th iteration => 0.0102581100453\n",
      "Loss for  30199 th iteration => 0.0102577538081\n",
      "Loss for  30200 th iteration => 0.0102573594728\n",
      "Loss for  30201 th iteration => 0.0102570706101\n",
      "Loss for  30202 th iteration => 0.0102566109248\n",
      "Loss for  30203 th iteration => 0.010256252693\n",
      "Loss for  30204 th iteration => 0.0102558620704\n",
      "Loss for  30205 th iteration => 0.0102555695113\n",
      "Loss for  30206 th iteration => 0.0102551333482\n",
      "Loss for  30207 th iteration => 0.0102547298746\n",
      "Loss for  30208 th iteration => 0.010254371888\n",
      "Loss for  30209 th iteration => 0.0102540617935\n",
      "Loss for  30210 th iteration => 0.0102536329467\n",
      "Loss for  30211 th iteration => 0.0102532320263\n",
      "Loss for  30212 th iteration => 0.0102528733066\n",
      "Loss for  30213 th iteration => 0.0102525629706\n",
      "Loss for  30214 th iteration => 0.0102521330251\n",
      "Loss for  30215 th iteration => 0.0102517345486\n",
      "Loss for  30216 th iteration => 0.0102513751563\n",
      "Loss for  30217 th iteration => 0.0102510645865\n",
      "Loss for  30218 th iteration => 0.0102506335815\n",
      "Loss for  30219 th iteration => 0.0102502387688\n",
      "Loss for  30220 th iteration => 0.010249952293\n",
      "Loss for  30221 th iteration => 0.0102494924519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  30222 th iteration => 0.0102491345755\n",
      "Loss for  30223 th iteration => 0.0102487432332\n",
      "Loss for  30224 th iteration => 0.0102484533229\n",
      "Loss for  30225 th iteration => 0.0102480168008\n",
      "Loss for  30226 th iteration => 0.010247613484\n",
      "Loss for  30227 th iteration => 0.0102472481201\n",
      "Loss for  30228 th iteration => 0.0102469558776\n",
      "Loss for  30229 th iteration => 0.0102465185736\n",
      "Loss for  30230 th iteration => 0.0102461168994\n",
      "Loss for  30231 th iteration => 0.0102457598639\n",
      "Loss for  30232 th iteration => 0.0102454505931\n",
      "Loss for  30233 th iteration => 0.0102450207463\n",
      "Loss for  30234 th iteration => 0.0102446213866\n",
      "Loss for  30235 th iteration => 0.0102442637448\n",
      "Loss for  30236 th iteration => 0.0102439542476\n",
      "Loss for  30237 th iteration => 0.010243523394\n",
      "Loss for  30238 th iteration => 0.010243126395\n",
      "Loss for  30239 th iteration => 0.010242844894\n",
      "Loss for  30240 th iteration => 0.0102423833346\n",
      "Loss for  30241 th iteration => 0.0102420264764\n",
      "Loss for  30242 th iteration => 0.0102416327647\n",
      "Loss for  30243 th iteration => 0.01024134786\n",
      "Loss for  30244 th iteration => 0.0102408885566\n",
      "Loss for  30245 th iteration => 0.010240530028\n",
      "Loss for  30246 th iteration => 0.0102401394605\n",
      "Loss for  30247 th iteration => 0.0102398523501\n",
      "Loss for  30248 th iteration => 0.0102394141571\n",
      "Loss for  30249 th iteration => 0.0102390118264\n",
      "Loss for  30250 th iteration => 0.0102386465855\n",
      "Loss for  30251 th iteration => 0.0102383574701\n",
      "Loss for  30252 th iteration => 0.0102379184803\n",
      "Loss for  30253 th iteration => 0.0102375172844\n",
      "Loss for  30254 th iteration => 0.0102371618967\n",
      "Loss for  30255 th iteration => 0.0102368538559\n",
      "Loss for  30256 th iteration => 0.0102364231995\n",
      "Loss for  30257 th iteration => 0.0102360241259\n",
      "Loss for  30258 th iteration => 0.0102356682323\n",
      "Loss for  30259 th iteration => 0.0102353599763\n",
      "Loss for  30260 th iteration => 0.0102349283898\n",
      "Loss for  30261 th iteration => 0.0102345313434\n",
      "Loss for  30262 th iteration => 0.0102341749964\n",
      "Loss for  30263 th iteration => 0.0102338665318\n",
      "Loss for  30264 th iteration => 0.0102334340494\n",
      "Loss for  30265 th iteration => 0.0102330400197\n",
      "Loss for  30266 th iteration => 0.0102327584432\n",
      "Loss for  30267 th iteration => 0.0102323197227\n",
      "Loss for  30268 th iteration => 0.0102319173484\n",
      "Loss for  30269 th iteration => 0.0102315490747\n",
      "Loss for  30270 th iteration => 0.0102312656265\n",
      "Loss for  30271 th iteration => 0.0102308261139\n",
      "Loss for  30272 th iteration => 0.010230424338\n",
      "Loss for  30273 th iteration => 0.0102300584613\n",
      "Loss for  30274 th iteration => 0.0102297732886\n",
      "Loss for  30275 th iteration => 0.0102293329693\n",
      "Loss for  30276 th iteration => 0.0102289317587\n",
      "Loss for  30277 th iteration => 0.0102285681825\n",
      "Loss for  30278 th iteration => 0.0102282814273\n",
      "Loss for  30279 th iteration => 0.0102278402873\n",
      "Loss for  30280 th iteration => 0.0102274403533\n",
      "Loss for  30281 th iteration => 0.0102270868165\n",
      "Loss for  30282 th iteration => 0.0102267798675\n",
      "Loss for  30283 th iteration => 0.0102263479965\n",
      "Loss for  30284 th iteration => 0.0102259499415\n",
      "Loss for  30285 th iteration => 0.0102255960266\n",
      "Loss for  30286 th iteration => 0.0102252888774\n",
      "Loss for  30287 th iteration => 0.0102248561718\n",
      "Loss for  30288 th iteration => 0.0102244599086\n",
      "Loss for  30289 th iteration => 0.0102241056636\n",
      "Loss for  30290 th iteration => 0.0102237983203\n",
      "Loss for  30291 th iteration => 0.0102233648116\n",
      "Loss for  30292 th iteration => 0.0102229714343\n",
      "Loss for  30293 th iteration => 0.0102226936563\n",
      "Loss for  30294 th iteration => 0.010222252704\n",
      "Loss for  30295 th iteration => 0.0102218510031\n",
      "Loss for  30296 th iteration => 0.0102214831232\n",
      "Loss for  30297 th iteration => 0.0102212038261\n",
      "Loss for  30298 th iteration => 0.0102207620673\n",
      "Loss for  30299 th iteration => 0.0102203608787\n",
      "Loss for  30300 th iteration => 0.0102199951507\n",
      "Loss for  30301 th iteration => 0.0102197144683\n",
      "Loss for  30302 th iteration => 0.0102192718902\n",
      "Loss for  30303 th iteration => 0.0102188711839\n",
      "Loss for  30304 th iteration => 0.0102185075198\n",
      "Loss for  30305 th iteration => 0.010218225581\n",
      "Loss for  30306 th iteration => 0.0102177821714\n",
      "Loss for  30307 th iteration => 0.0102173820277\n",
      "Loss for  30308 th iteration => 0.0102170311107\n",
      "Loss for  30309 th iteration => 0.0102167253278\n",
      "Loss for  30310 th iteration => 0.0102162928398\n",
      "Loss for  30311 th iteration => 0.0102158943799\n",
      "Loss for  30312 th iteration => 0.0102155431852\n",
      "Loss for  30313 th iteration => 0.010215237214\n",
      "Loss for  30314 th iteration => 0.01021480397\n",
      "Loss for  30315 th iteration => 0.0102144071133\n",
      "Loss for  30316 th iteration => 0.0102140559562\n",
      "Loss for  30317 th iteration => 0.0102137734036\n",
      "Loss for  30318 th iteration => 0.0102132917808\n",
      "Loss for  30319 th iteration => 0.0102129206604\n",
      "Loss for  30320 th iteration => 0.010212648913\n",
      "Loss for  30321 th iteration => 0.0102122057535\n",
      "Loss for  30322 th iteration => 0.0102118041033\n",
      "Loss for  30323 th iteration => 0.0102114350304\n",
      "Loss for  30324 th iteration => 0.0102111620275\n",
      "Loss for  30325 th iteration => 0.0102107180587\n",
      "Loss for  30326 th iteration => 0.0102103168541\n",
      "Loss for  30327 th iteration => 0.0102099497443\n",
      "Loss for  30328 th iteration => 0.0102096756093\n",
      "Loss for  30329 th iteration => 0.0102092308197\n",
      "Loss for  30330 th iteration => 0.010208830033\n",
      "Loss for  30331 th iteration => 0.0102084648048\n",
      "Loss for  30332 th iteration => 0.0102081896567\n",
      "Loss for  30333 th iteration => 0.0102077440353\n",
      "Loss for  30334 th iteration => 0.0102073436396\n",
      "Loss for  30335 th iteration => 0.0102069802142\n",
      "Loss for  30336 th iteration => 0.0102067041677\n",
      "Loss for  30337 th iteration => 0.0102062577043\n",
      "Loss for  30338 th iteration => 0.0102058576736\n",
      "Loss for  30339 th iteration => 0.0102054959751\n",
      "Loss for  30340 th iteration => 0.0102052191406\n",
      "Loss for  30341 th iteration => 0.0102047718255\n",
      "Loss for  30342 th iteration => 0.0102043726286\n",
      "Loss for  30343 th iteration => 0.0102040246573\n",
      "Loss for  30344 th iteration => 0.0102037206716\n",
      "Loss for  30345 th iteration => 0.0102032863283\n",
      "Loss for  30346 th iteration => 0.0102028885397\n",
      "Loss for  30347 th iteration => 0.0102025411311\n",
      "Loss for  30348 th iteration => 0.0102022606168\n",
      "Loss for  30349 th iteration => 0.0102017766089\n",
      "Loss for  30350 th iteration => 0.0102014048927\n",
      "Loss for  30351 th iteration => 0.0102010578561\n",
      "Loss for  30352 th iteration => 0.0102007758699\n",
      "Loss for  30353 th iteration => 0.0102002922273\n",
      "Loss for  30354 th iteration => 0.010199921829\n",
      "Loss for  30355 th iteration => 0.0101996568785\n",
      "Loss for  30356 th iteration => 0.0101992098352\n",
      "Loss for  30357 th iteration => 0.0101988082686\n",
      "Loss for  30358 th iteration => 0.0101984396365\n",
      "Loss for  30359 th iteration => 0.0101981738137\n",
      "Loss for  30360 th iteration => 0.0101977259519\n",
      "Loss for  30361 th iteration => 0.0101973247361\n",
      "Loss for  30362 th iteration => 0.0101969577955\n",
      "Loss for  30363 th iteration => 0.0101966912091\n",
      "Loss for  30364 th iteration => 0.0101962425194\n",
      "Loss for  30365 th iteration => 0.0101958416297\n",
      "Loss for  30366 th iteration => 0.0101954763083\n",
      "Loss for  30367 th iteration => 0.0101952090631\n",
      "Loss for  30368 th iteration => 0.0101947595364\n",
      "Loss for  30369 th iteration => 0.0101943589491\n",
      "Loss for  30370 th iteration => 0.0101939951772\n",
      "Loss for  30371 th iteration => 0.0101937273741\n",
      "Loss for  30372 th iteration => 0.0101932770018\n",
      "Loss for  30373 th iteration => 0.0101928766938\n",
      "Loss for  30374 th iteration => 0.0101925144041\n",
      "Loss for  30375 th iteration => 0.0101922461403\n",
      "Loss for  30376 th iteration => 0.0101917949146\n",
      "Loss for  30377 th iteration => 0.0101913948637\n",
      "Loss for  30378 th iteration => 0.0101910342384\n",
      "Loss for  30379 th iteration => 0.0101907936501\n",
      "Loss for  30380 th iteration => 0.0101902869493\n",
      "Loss for  30381 th iteration => 0.010189913672\n",
      "Loss for  30382 th iteration => 0.0101895703804\n",
      "Loss for  30383 th iteration => 0.0101892940815\n",
      "Loss for  30384 th iteration => 0.0101888058451\n",
      "Loss for  30385 th iteration => 0.0101884336233\n",
      "Loss for  30386 th iteration => 0.0101880907136\n",
      "Loss for  30387 th iteration => 0.0101878131667\n",
      "Loss for  30388 th iteration => 0.0101873251658\n",
      "Loss for  30389 th iteration => 0.0101869539634\n",
      "Loss for  30390 th iteration => 0.0101866114394\n",
      "Loss for  30391 th iteration => 0.0101863327294\n",
      "Loss for  30392 th iteration => 0.0101858449111\n",
      "Loss for  30393 th iteration => 0.0101854746931\n",
      "Loss for  30394 th iteration => 0.0101851325588\n",
      "Loss for  30395 th iteration => 0.0101848527673\n",
      "Loss for  30396 th iteration => 0.0101843650809\n",
      "Loss for  30397 th iteration => 0.0101839958132\n",
      "Loss for  30398 th iteration => 0.0101836540724\n",
      "Loss for  30399 th iteration => 0.0101833732784\n",
      "Loss for  30400 th iteration => 0.0101828856747\n",
      "Loss for  30401 th iteration => 0.0101825178942\n",
      "Loss for  30402 th iteration => 0.0101822610691\n",
      "Loss for  30403 th iteration => 0.0101818089277\n",
      "Loss for  30404 th iteration => 0.0101814066886\n",
      "Loss for  30405 th iteration => 0.0101810403005\n",
      "Loss for  30406 th iteration => 0.0101807831128\n",
      "Loss for  30407 th iteration => 0.0101803301379\n",
      "Loss for  30408 th iteration => 0.010179928126\n",
      "Loss for  30409 th iteration => 0.0101795630685\n",
      "Loss for  30410 th iteration => 0.0101793056073\n",
      "Loss for  30411 th iteration => 0.0101788517919\n",
      "Loss for  30412 th iteration => 0.0101784499868\n",
      "Loss for  30413 th iteration => 0.0101780861998\n",
      "Loss for  30414 th iteration => 0.0101778285511\n",
      "Loss for  30415 th iteration => 0.0101773738888\n",
      "Loss for  30416 th iteration => 0.0101769722707\n",
      "Loss for  30417 th iteration => 0.0101766096962\n",
      "Loss for  30418 th iteration => 0.0101763519429\n",
      "Loss for  30419 th iteration => 0.0101758964276\n",
      "Loss for  30420 th iteration => 0.0101754949774\n",
      "Loss for  30421 th iteration => 0.0101751338482\n",
      "Loss for  30422 th iteration => 0.0101749066311\n",
      "Loss for  30423 th iteration => 0.01017439045\n",
      "Loss for  30424 th iteration => 0.0101740181204\n",
      "Loss for  30425 th iteration => 0.0101736580913\n",
      "Loss for  30426 th iteration => 0.0101734308654\n",
      "Loss for  30427 th iteration => 0.0101729139079\n",
      "Loss for  30428 th iteration => 0.0101725416851\n",
      "Loss for  30429 th iteration => 0.0101721827032\n",
      "Loss for  30430 th iteration => 0.0101719555627\n",
      "Loss for  30431 th iteration => 0.0101714377873\n",
      "Loss for  30432 th iteration => 0.0101710658933\n",
      "Loss for  30433 th iteration => 0.0101707284903\n",
      "Loss for  30434 th iteration => 0.0101704570081\n",
      "Loss for  30435 th iteration => 0.0101699620587\n",
      "Loss for  30436 th iteration => 0.0101695910471\n",
      "Loss for  30437 th iteration => 0.0101692540445\n",
      "Loss for  30438 th iteration => 0.0101689816817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  30439 th iteration => 0.0101684867523\n",
      "Loss for  30440 th iteration => 0.010168116592\n",
      "Loss for  30441 th iteration => 0.0101677799934\n",
      "Loss for  30442 th iteration => 0.0101675068216\n",
      "Loss for  30443 th iteration => 0.0101670118678\n",
      "Loss for  30444 th iteration => 0.0101666425289\n",
      "Loss for  30445 th iteration => 0.0101663063375\n",
      "Loss for  30446 th iteration => 0.0101660324258\n",
      "Loss for  30447 th iteration => 0.0101655374049\n",
      "Loss for  30448 th iteration => 0.0101651688584\n",
      "Loss for  30449 th iteration => 0.0101648330776\n",
      "Loss for  30450 th iteration => 0.0101645584925\n",
      "Loss for  30451 th iteration => 0.0101640633634\n",
      "Loss for  30452 th iteration => 0.0101636955811\n",
      "Loss for  30453 th iteration => 0.0101633602141\n",
      "Loss for  30454 th iteration => 0.01016308502\n",
      "Loss for  30455 th iteration => 0.010162589743\n",
      "Loss for  30456 th iteration => 0.0101622226977\n",
      "Loss for  30457 th iteration => 0.0101618877477\n",
      "Loss for  30458 th iteration => 0.0101616120066\n",
      "Loss for  30459 th iteration => 0.0101611165434\n",
      "Loss for  30460 th iteration => 0.010160750463\n",
      "Loss for  30461 th iteration => 0.0101605067981\n",
      "Loss for  30462 th iteration => 0.0101600483965\n",
      "Loss for  30463 th iteration => 0.0101596437605\n",
      "Loss for  30464 th iteration => 0.0101592787021\n",
      "Loss for  30465 th iteration => 0.0101590351644\n",
      "Loss for  30466 th iteration => 0.0101585759274\n",
      "Loss for  30467 th iteration => 0.0101581713981\n",
      "Loss for  30468 th iteration => 0.0101578073121\n",
      "Loss for  30469 th iteration => 0.0101575639717\n",
      "Loss for  30470 th iteration => 0.0101571038949\n",
      "Loss for  30471 th iteration => 0.0101566994558\n",
      "Loss for  30472 th iteration => 0.0101563362942\n",
      "Loss for  30473 th iteration => 0.0101560932189\n",
      "Loss for  30474 th iteration => 0.0101556322982\n",
      "Loss for  30475 th iteration => 0.0101552279334\n",
      "Loss for  30476 th iteration => 0.01015486565\n",
      "Loss for  30477 th iteration => 0.0101546229047\n",
      "Loss for  30478 th iteration => 0.0101541611364\n",
      "Loss for  30479 th iteration => 0.0101537568307\n",
      "Loss for  30480 th iteration => 0.0101533956199\n",
      "Loss for  30481 th iteration => 0.0101531884072\n",
      "Loss for  30482 th iteration => 0.0101526569253\n",
      "Loss for  30483 th iteration => 0.0101522861604\n",
      "Loss for  30484 th iteration => 0.010151925631\n",
      "Loss for  30485 th iteration => 0.0101517191249\n",
      "Loss for  30486 th iteration => 0.0101511865677\n",
      "Loss for  30487 th iteration => 0.0101508159086\n",
      "Loss for  30488 th iteration => 0.0101504560222\n",
      "Loss for  30489 th iteration => 0.0101502502898\n",
      "Loss for  30490 th iteration => 0.0101497166284\n",
      "Loss for  30491 th iteration => 0.0101493460752\n",
      "Loss for  30492 th iteration => 0.0101489867947\n",
      "Loss for  30493 th iteration => 0.0101487819005\n",
      "Loss for  30494 th iteration => 0.0101482471072\n",
      "Loss for  30495 th iteration => 0.0101478766598\n",
      "Loss for  30496 th iteration => 0.0101475179493\n",
      "Loss for  30497 th iteration => 0.0101473139557\n",
      "Loss for  30498 th iteration => 0.010146778004\n",
      "Loss for  30499 th iteration => 0.0101464076623\n",
      "Loss for  30500 th iteration => 0.0101460494873\n",
      "Loss for  30501 th iteration => 0.0101458464539\n",
      "Loss for  30502 th iteration => 0.0101453093185\n",
      "Loss for  30503 th iteration => 0.0101449390824\n",
      "Loss for  30504 th iteration => 0.0101445814095\n",
      "Loss for  30505 th iteration => 0.010144379394\n",
      "Loss for  30506 th iteration => 0.0101438410505\n",
      "Loss for  30507 th iteration => 0.0101434709388\n",
      "Loss for  30508 th iteration => 0.0101431430986\n",
      "Loss for  30509 th iteration => 0.010142880759\n",
      "Loss for  30510 th iteration => 0.0101423731721\n",
      "Loss for  30511 th iteration => 0.010142003729\n",
      "Loss for  30512 th iteration => 0.0101416763122\n",
      "Loss for  30513 th iteration => 0.010141413528\n",
      "Loss for  30514 th iteration => 0.0101409057119\n",
      "Loss for  30515 th iteration => 0.0101405369126\n",
      "Loss for  30516 th iteration => 0.0101402099222\n",
      "Loss for  30517 th iteration => 0.0101399467494\n",
      "Loss for  30518 th iteration => 0.0101394386698\n",
      "Loss for  30519 th iteration => 0.0101390704903\n",
      "Loss for  30520 th iteration => 0.0101387439291\n",
      "Loss for  30521 th iteration => 0.0101384804215\n",
      "Loss for  30522 th iteration => 0.0101379720453\n",
      "Loss for  30523 th iteration => 0.0101376044627\n",
      "Loss for  30524 th iteration => 0.0101372783332\n",
      "Loss for  30525 th iteration => 0.010137014543\n",
      "Loss for  30526 th iteration => 0.0101365058384\n",
      "Loss for  30527 th iteration => 0.01013613883\n",
      "Loss for  30528 th iteration => 0.0101358131351\n",
      "Loss for  30529 th iteration => 0.0101355491124\n",
      "Loss for  30530 th iteration => 0.0101350400488\n",
      "Loss for  30531 th iteration => 0.0101346735929\n",
      "Loss for  30532 th iteration => 0.010134348335\n",
      "Loss for  30533 th iteration => 0.0101340841284\n",
      "Loss for  30534 th iteration => 0.0101335746762\n",
      "Loss for  30535 th iteration => 0.0101332087518\n",
      "Loss for  30536 th iteration => 0.0101328839334\n",
      "Loss for  30537 th iteration => 0.0101326195896\n",
      "Loss for  30538 th iteration => 0.0101321097204\n",
      "Loss for  30539 th iteration => 0.010131744307\n",
      "Loss for  30540 th iteration => 0.0101314199305\n",
      "Loss for  30541 th iteration => 0.0101311554948\n",
      "Loss for  30542 th iteration => 0.0101306451811\n",
      "Loss for  30543 th iteration => 0.010130280259\n",
      "Loss for  30544 th iteration => 0.0101299563269\n",
      "Loss for  30545 th iteration => 0.0101296918426\n",
      "Loss for  30546 th iteration => 0.0101291810582\n",
      "Loss for  30547 th iteration => 0.0101288166082\n",
      "Loss for  30548 th iteration => 0.0101284931227\n",
      "Loss for  30549 th iteration => 0.0101282286319\n",
      "Loss for  30550 th iteration => 0.0101277173514\n",
      "Loss for  30551 th iteration => 0.0101273533549\n",
      "Loss for  30552 th iteration => 0.0101270303182\n",
      "Loss for  30553 th iteration => 0.0101267658615\n",
      "Loss for  30554 th iteration => 0.0101262540604\n",
      "Loss for  30555 th iteration => 0.0101258904994\n",
      "Loss for  30556 th iteration => 0.0101255679139\n",
      "Loss for  30557 th iteration => 0.0101253035303\n",
      "Loss for  30558 th iteration => 0.0101247911851\n",
      "Loss for  30559 th iteration => 0.0101244280421\n",
      "Loss for  30560 th iteration => 0.0101241059098\n",
      "Loss for  30561 th iteration => 0.0101238416371\n",
      "Loss for  30562 th iteration => 0.0101233287252\n",
      "Loss for  30563 th iteration => 0.0101229659833\n",
      "Loss for  30564 th iteration => 0.0101226443064\n",
      "Loss for  30565 th iteration => 0.0101223801808\n",
      "Loss for  30566 th iteration => 0.0101218666805\n",
      "Loss for  30567 th iteration => 0.0101215043233\n",
      "Loss for  30568 th iteration => 0.0101211831037\n",
      "Loss for  30569 th iteration => 0.0101209191605\n",
      "Loss for  30570 th iteration => 0.0101204050508\n",
      "Loss for  30571 th iteration => 0.0101200431685\n",
      "Loss for  30572 th iteration => 0.0101198259226\n",
      "Loss for  30573 th iteration => 0.0101193551585\n",
      "Loss for  30574 th iteration => 0.010118943832\n",
      "Loss for  30575 th iteration => 0.0101185823655\n",
      "Loss for  30576 th iteration => 0.010118366031\n",
      "Loss for  30577 th iteration => 0.0101178944405\n",
      "Loss for  30578 th iteration => 0.0101174830277\n",
      "Loss for  30579 th iteration => 0.0101171219476\n",
      "Loss for  30580 th iteration => 0.0101169065641\n",
      "Loss for  30581 th iteration => 0.0101164341466\n",
      "Loss for  30582 th iteration => 0.0101160226379\n",
      "Loss for  30583 th iteration => 0.0101156619154\n",
      "Loss for  30584 th iteration => 0.0101154475211\n",
      "Loss for  30585 th iteration => 0.0101149742765\n",
      "Loss for  30586 th iteration => 0.0101145626623\n",
      "Loss for  30587 th iteration => 0.0101142022699\n",
      "Loss for  30588 th iteration => 0.0101139889013\n",
      "Loss for  30589 th iteration => 0.0101135148295\n",
      "Loss for  30590 th iteration => 0.0101131031007\n",
      "Loss for  30591 th iteration => 0.0101127430115\n",
      "Loss for  30592 th iteration => 0.0101125307041\n",
      "Loss for  30593 th iteration => 0.0101120558051\n",
      "Loss for  30594 th iteration => 0.0101116439528\n",
      "Loss for  30595 th iteration => 0.0101112841661\n",
      "Loss for  30596 th iteration => 0.0101111191729\n",
      "Loss for  30597 th iteration => 0.0101105529737\n",
      "Loss for  30598 th iteration => 0.0101101852297\n",
      "Loss for  30599 th iteration => 0.0101098254282\n",
      "Loss for  30600 th iteration => 0.0101096155583\n",
      "Loss for  30601 th iteration => 0.0101091392697\n",
      "Loss for  30602 th iteration => 0.0101087269082\n",
      "Loss for  30603 th iteration => 0.0101083675471\n",
      "Loss for  30604 th iteration => 0.010108205411\n",
      "Loss for  30605 th iteration => 0.0101076365461\n",
      "Loss for  30606 th iteration => 0.010107269011\n",
      "Loss for  30607 th iteration => 0.0101069095747\n",
      "Loss for  30608 th iteration => 0.010106749413\n",
      "Loss for  30609 th iteration => 0.010106178957\n",
      "Loss for  30610 th iteration => 0.0101058115261\n",
      "Loss for  30611 th iteration => 0.0101054520015\n",
      "Loss for  30612 th iteration => 0.0101052938336\n",
      "Loss for  30613 th iteration => 0.0101047217802\n",
      "Loss for  30614 th iteration => 0.0101043544534\n",
      "Loss for  30615 th iteration => 0.0101039948276\n",
      "Loss for  30616 th iteration => 0.0101038386723\n",
      "Loss for  30617 th iteration => 0.0101032650153\n",
      "Loss for  30618 th iteration => 0.0101028977926\n",
      "Loss for  30619 th iteration => 0.0101025380797\n",
      "Loss for  30620 th iteration => 0.0101023349529\n",
      "Loss for  30621 th iteration => 0.010101855594\n",
      "Loss for  30622 th iteration => 0.0101014415327\n",
      "Loss for  30623 th iteration => 0.0101010821491\n",
      "Loss for  30624 th iteration => 0.0101009291435\n",
      "Loss for  30625 th iteration => 0.0101003527103\n",
      "Loss for  30626 th iteration => 0.0100999856959\n",
      "Loss for  30627 th iteration => 0.0100996261566\n",
      "Loss for  30628 th iteration => 0.0100994752503\n",
      "Loss for  30629 th iteration => 0.0100988971808\n",
      "Loss for  30630 th iteration => 0.0100985302704\n",
      "Loss for  30631 th iteration => 0.0100981705648\n",
      "Loss for  30632 th iteration => 0.0100980217726\n",
      "Loss for  30633 th iteration => 0.0100974420627\n",
      "Loss for  30634 th iteration => 0.0100970752562\n",
      "Loss for  30635 th iteration => 0.0100967154502\n",
      "Loss for  30636 th iteration => 0.0100965179171\n",
      "Loss for  30637 th iteration => 0.0100960360697\n",
      "Loss for  30638 th iteration => 0.0100956206423\n",
      "Loss for  30639 th iteration => 0.0100952610433\n",
      "Loss for  30640 th iteration => 0.0100951156139\n",
      "Loss for  30641 th iteration => 0.0100945330491\n",
      "Loss for  30642 th iteration => 0.0100941664505\n",
      "Loss for  30643 th iteration => 0.0100938066465\n",
      "Loss for  30644 th iteration => 0.0100936118815\n",
      "Loss for  30645 th iteration => 0.0100931286743\n",
      "Loss for  30646 th iteration => 0.0100927126588\n",
      "Loss for  30647 th iteration => 0.0100923530869\n",
      "Loss for  30648 th iteration => 0.010092211149\n",
      "Loss for  30649 th iteration => 0.0100916256795\n",
      "Loss for  30650 th iteration => 0.0100912592886\n",
      "Loss for  30651 th iteration => 0.0100908994668\n",
      "Loss for  30652 th iteration => 0.0100907597738\n",
      "Loss for  30653 th iteration => 0.0100901726159\n",
      "Loss for  30654 th iteration => 0.0100898063286\n",
      "Loss for  30655 th iteration => 0.0100894463577\n",
      "Loss for  30656 th iteration => 0.0100891410749\n",
      "Loss for  30657 th iteration => 0.0100888853092\n",
      "Loss for  30658 th iteration => 0.0100883537727\n",
      "Loss for  30659 th iteration => 0.0100879939566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  30660 th iteration => 0.0100876891458\n",
      "Loss for  30661 th iteration => 0.0100874340323\n",
      "Loss for  30662 th iteration => 0.0100869016272\n",
      "Loss for  30663 th iteration => 0.0100865419566\n",
      "Loss for  30664 th iteration => 0.010086237619\n",
      "Loss for  30665 th iteration => 0.0100859831751\n",
      "Loss for  30666 th iteration => 0.0100854498918\n",
      "Loss for  30667 th iteration => 0.0100850903577\n",
      "Loss for  30668 th iteration => 0.0100847864947\n",
      "Loss for  30669 th iteration => 0.0100845327371\n",
      "Loss for  30670 th iteration => 0.0100839985665\n",
      "Loss for  30671 th iteration => 0.01008363916\n",
      "Loss for  30672 th iteration => 0.0100833357729\n",
      "Loss for  30673 th iteration => 0.0100830827177\n",
      "Loss for  30674 th iteration => 0.0100825476509\n",
      "Loss for  30675 th iteration => 0.0100821883635\n",
      "Loss for  30676 th iteration => 0.0100818854535\n",
      "Loss for  30677 th iteration => 0.0100816331164\n",
      "Loss for  30678 th iteration => 0.010081097145\n",
      "Loss for  30679 th iteration => 0.0100807379683\n",
      "Loss for  30680 th iteration => 0.0100804355367\n",
      "Loss for  30681 th iteration => 0.0100801839324\n",
      "Loss for  30682 th iteration => 0.0100796470485\n",
      "Loss for  30683 th iteration => 0.0100792879743\n",
      "Loss for  30684 th iteration => 0.0100789860223\n",
      "Loss for  30685 th iteration => 0.0100787351655\n",
      "Loss for  30686 th iteration => 0.0100781973612\n",
      "Loss for  30687 th iteration => 0.0100778383817\n",
      "Loss for  30688 th iteration => 0.0100775369105\n",
      "Loss for  30689 th iteration => 0.0100772868148\n",
      "Loss for  30690 th iteration => 0.010076748083\n",
      "Loss for  30691 th iteration => 0.0100763891904\n",
      "Loss for  30692 th iteration => 0.0100760882011\n",
      "Loss for  30693 th iteration => 0.0100758388801\n",
      "Loss for  30694 th iteration => 0.0100752992136\n",
      "Loss for  30695 th iteration => 0.0100749404004\n",
      "Loss for  30696 th iteration => 0.0100746398942\n",
      "Loss for  30697 th iteration => 0.0100743913607\n",
      "Loss for  30698 th iteration => 0.0100738507528\n",
      "Loss for  30699 th iteration => 0.0100734920118\n",
      "Loss for  30700 th iteration => 0.0100731919898\n",
      "Loss for  30701 th iteration => 0.0100729442562\n",
      "Loss for  30702 th iteration => 0.0100724027006\n",
      "Loss for  30703 th iteration => 0.0100720440245\n",
      "Loss for  30704 th iteration => 0.0100717444879\n",
      "Loss for  30705 th iteration => 0.0100714975661\n",
      "Loss for  30706 th iteration => 0.0100709550566\n",
      "Loss for  30707 th iteration => 0.0100705964386\n",
      "Loss for  30708 th iteration => 0.0100702973883\n",
      "Loss for  30709 th iteration => 0.0100700512899\n",
      "Loss for  30710 th iteration => 0.0100695078208\n",
      "Loss for  30711 th iteration => 0.010069149254\n",
      "Loss for  30712 th iteration => 0.0100688506911\n",
      "Loss for  30713 th iteration => 0.0100686054271\n",
      "Loss for  30714 th iteration => 0.0100680609928\n",
      "Loss for  30715 th iteration => 0.0100677024707\n",
      "Loss for  30716 th iteration => 0.0100674043963\n",
      "Loss for  30717 th iteration => 0.0100671599774\n",
      "Loss for  30718 th iteration => 0.0100666145726\n",
      "Loss for  30719 th iteration => 0.0100662560887\n",
      "Loss for  30720 th iteration => 0.0100659585038\n",
      "Loss for  30721 th iteration => 0.0100657149402\n",
      "Loss for  30722 th iteration => 0.0100651685599\n",
      "Loss for  30723 th iteration => 0.010064810108\n",
      "Loss for  30724 th iteration => 0.0100645130136\n",
      "Loss for  30725 th iteration => 0.0100642703151\n",
      "Loss for  30726 th iteration => 0.0100637229546\n",
      "Loss for  30727 th iteration => 0.0100633645285\n",
      "Loss for  30728 th iteration => 0.0100630679256\n",
      "Loss for  30729 th iteration => 0.0100628261018\n",
      "Loss for  30730 th iteration => 0.0100622777565\n",
      "Loss for  30731 th iteration => 0.0100619193503\n",
      "Loss for  30732 th iteration => 0.0100616232397\n",
      "Loss for  30733 th iteration => 0.0100613822997\n",
      "Loss for  30734 th iteration => 0.0100608329653\n",
      "Loss for  30735 th iteration => 0.0100604745731\n",
      "Loss for  30736 th iteration => 0.0100601789559\n",
      "Loss for  30737 th iteration => 0.0100599389085\n",
      "Loss for  30738 th iteration => 0.010059388581\n",
      "Loss for  30739 th iteration => 0.0100590301971\n",
      "Loss for  30740 th iteration => 0.0100587350742\n",
      "Loss for  30741 th iteration => 0.0100584959278\n",
      "Loss for  30742 th iteration => 0.0100579446033\n",
      "Loss for  30743 th iteration => 0.0100575862222\n",
      "Loss for  30744 th iteration => 0.0100572915944\n",
      "Loss for  30745 th iteration => 0.0100570533572\n",
      "Loss for  30746 th iteration => 0.010056501032\n",
      "Loss for  30747 th iteration => 0.0100561426483\n",
      "Loss for  30748 th iteration => 0.0100558485166\n",
      "Loss for  30749 th iteration => 0.0100556111963\n",
      "Loss for  30750 th iteration => 0.010055057867\n",
      "Loss for  30751 th iteration => 0.0100546994754\n",
      "Loss for  30752 th iteration => 0.0100544058405\n",
      "Loss for  30753 th iteration => 0.0100541694447\n",
      "Loss for  30754 th iteration => 0.0100536151081\n",
      "Loss for  30755 th iteration => 0.0100532567033\n",
      "Loss for  30756 th iteration => 0.0100529635663\n",
      "Loss for  30757 th iteration => 0.0100527281022\n",
      "Loss for  30758 th iteration => 0.0100521727551\n",
      "Loss for  30759 th iteration => 0.0100518143321\n",
      "Loss for  30760 th iteration => 0.0100515216937\n",
      "Loss for  30761 th iteration => 0.0100512871682\n",
      "Loss for  30762 th iteration => 0.0100507308078\n",
      "Loss for  30763 th iteration => 0.0100503723617\n",
      "Loss for  30764 th iteration => 0.0100500802227\n",
      "Loss for  30765 th iteration => 0.0100498466425\n",
      "Loss for  30766 th iteration => 0.010049289266\n",
      "Loss for  30767 th iteration => 0.010048930792\n",
      "Loss for  30768 th iteration => 0.0100486391532\n",
      "Loss for  30769 th iteration => 0.0100484065247\n",
      "Loss for  30770 th iteration => 0.0100478481297\n",
      "Loss for  30771 th iteration => 0.0100474896229\n",
      "Loss for  30772 th iteration => 0.0100471984851\n",
      "Loss for  30773 th iteration => 0.0100469668145\n",
      "Loss for  30774 th iteration => 0.0100464073985\n",
      "Loss for  30775 th iteration => 0.0100460488543\n",
      "Loss for  30776 th iteration => 0.0100457582184\n",
      "Loss for  30777 th iteration => 0.0100455275115\n",
      "Loss for  30778 th iteration => 0.0100449670723\n",
      "Loss for  30779 th iteration => 0.0100446084863\n",
      "Loss for  30780 th iteration => 0.0100443183529\n",
      "Loss for  30781 th iteration => 0.0100440886155\n",
      "Loss for  30782 th iteration => 0.0100435271509\n",
      "Loss for  30783 th iteration => 0.0100431685186\n",
      "Loss for  30784 th iteration => 0.0100428788885\n",
      "Loss for  30785 th iteration => 0.010042650126\n",
      "Loss for  30786 th iteration => 0.0100420876342\n",
      "Loss for  30787 th iteration => 0.0100417289513\n",
      "Loss for  30788 th iteration => 0.0100414398252\n",
      "Loss for  30789 th iteration => 0.0100412120428\n",
      "Loss for  30790 th iteration => 0.010040648522\n",
      "Loss for  30791 th iteration => 0.0100402897841\n",
      "Loss for  30792 th iteration => 0.0100400011629\n",
      "Loss for  30793 th iteration => 0.0100397743656\n",
      "Loss for  30794 th iteration => 0.0100392098141\n",
      "Loss for  30795 th iteration => 0.0100388510171\n",
      "Loss for  30796 th iteration => 0.0100385629014\n",
      "Loss for  30797 th iteration => 0.010038337094\n",
      "Loss for  30798 th iteration => 0.0100377715103\n",
      "Loss for  30799 th iteration => 0.0100374126502\n",
      "Loss for  30800 th iteration => 0.0100371250406\n",
      "Loss for  30801 th iteration => 0.0100369002278\n",
      "Loss for  30802 th iteration => 0.0100363336104\n",
      "Loss for  30803 th iteration => 0.0100359746832\n",
      "Loss for  30804 th iteration => 0.0100356875804\n",
      "Loss for  30805 th iteration => 0.0100354637666\n",
      "Loss for  30806 th iteration => 0.0100348961144\n",
      "Loss for  30807 th iteration => 0.010034537116\n",
      "Loss for  30808 th iteration => 0.0100342505208\n",
      "Loss for  30809 th iteration => 0.0100340277102\n",
      "Loss for  30810 th iteration => 0.0100334590219\n",
      "Loss for  30811 th iteration => 0.0100330999486\n",
      "Loss for  30812 th iteration => 0.0100328138616\n",
      "Loss for  30813 th iteration => 0.0100325920582\n",
      "Loss for  30814 th iteration => 0.0100320223328\n",
      "Loss for  30815 th iteration => 0.0100316631808\n",
      "Loss for  30816 th iteration => 0.0100313776027\n",
      "Loss for  30817 th iteration => 0.0100311568104\n",
      "Loss for  30818 th iteration => 0.010030586047\n",
      "Loss for  30819 th iteration => 0.0100302268126\n",
      "Loss for  30820 th iteration => 0.0100299417439\n",
      "Loss for  30821 th iteration => 0.0100297219666\n",
      "Loss for  30822 th iteration => 0.0100291501643\n",
      "Loss for  30823 th iteration => 0.0100287908438\n",
      "Loss for  30824 th iteration => 0.0100285062853\n",
      "Loss for  30825 th iteration => 0.0100282875264\n",
      "Loss for  30826 th iteration => 0.0100277146844\n",
      "Loss for  30827 th iteration => 0.0100273552743\n",
      "Loss for  30828 th iteration => 0.0100270712265\n",
      "Loss for  30829 th iteration => 0.0100268534895\n",
      "Loss for  30830 th iteration => 0.0100262796073\n",
      "Loss for  30831 th iteration => 0.0100259201041\n",
      "Loss for  30832 th iteration => 0.0100256365676\n",
      "Loss for  30833 th iteration => 0.0100254198557\n",
      "Loss for  30834 th iteration => 0.0100248449327\n",
      "Loss for  30835 th iteration => 0.010024485333\n",
      "Loss for  30836 th iteration => 0.0100242023085\n",
      "Loss for  30837 th iteration => 0.0100239866248\n",
      "Loss for  30838 th iteration => 0.0100234106605\n",
      "Loss for  30839 th iteration => 0.0100230509609\n",
      "Loss for  30840 th iteration => 0.0100227684488\n",
      "Loss for  30841 th iteration => 0.0100225537964\n",
      "Loss for  30842 th iteration => 0.0100219767905\n",
      "Loss for  30843 th iteration => 0.0100216169876\n",
      "Loss for  30844 th iteration => 0.0100213349887\n",
      "Loss for  30845 th iteration => 0.0100211213704\n",
      "Loss for  30846 th iteration => 0.0100205433226\n",
      "Loss for  30847 th iteration => 0.0100201834132\n",
      "Loss for  30848 th iteration => 0.0100199019279\n",
      "Loss for  30849 th iteration => 0.0100196893464\n",
      "Loss for  30850 th iteration => 0.0100191102565\n",
      "Loss for  30851 th iteration => 0.0100187502374\n",
      "Loss for  30852 th iteration => 0.0100184696116\n",
      "Loss for  30853 th iteration => 0.0100180457742\n",
      "Loss for  30854 th iteration => 0.0100178905119\n",
      "Loss for  30855 th iteration => 0.0100173173547\n",
      "Loss for  30856 th iteration => 0.0100170379\n",
      "Loss for  30857 th iteration => 0.0100166127571\n",
      "Loss for  30858 th iteration => 0.0100164596062\n",
      "Loss for  30859 th iteration => 0.0100158848742\n",
      "Loss for  30860 th iteration => 0.0100156065868\n",
      "Loss for  30861 th iteration => 0.0100151801451\n",
      "Loss for  30862 th iteration => 0.0100150290946\n",
      "Loss for  30863 th iteration => 0.0100144527956\n",
      "Loss for  30864 th iteration => 0.0100141756717\n",
      "Loss for  30865 th iteration => 0.010013747938\n",
      "Loss for  30866 th iteration => 0.0100135989772\n",
      "Loss for  30867 th iteration => 0.0100130212304\n",
      "Loss for  30868 th iteration => 0.0100126683478\n",
      "Loss for  30869 th iteration => 0.01001239161\n",
      "Loss for  30870 th iteration => 0.0100121693617\n",
      "Loss for  30871 th iteration => 0.0100115902886\n",
      "Loss for  30872 th iteration => 0.0100112371092\n",
      "Loss for  30873 th iteration => 0.0100109609006\n",
      "Loss for  30874 th iteration => 0.0100107401364\n",
      "Loss for  30875 th iteration => 0.010010159747\n",
      "Loss for  30876 th iteration => 0.0100098062738\n",
      "Loss for  30877 th iteration => 0.0100095305943\n",
      "Loss for  30878 th iteration => 0.0100093113017\n",
      "Loss for  30879 th iteration => 0.0100087296056\n",
      "Loss for  30880 th iteration => 0.0100083758412\n",
      "Loss for  30881 th iteration => 0.0100081006908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  30882 th iteration => 0.0100078828575\n",
      "Loss for  30883 th iteration => 0.0100072998641\n",
      "Loss for  30884 th iteration => 0.0100069458111\n",
      "Loss for  30885 th iteration => 0.0100066711899\n",
      "Loss for  30886 th iteration => 0.0100064548039\n",
      "Loss for  30887 th iteration => 0.0100058705225\n",
      "Loss for  30888 th iteration => 0.0100055161831\n",
      "Loss for  30889 th iteration => 0.0100052420911\n",
      "Loss for  30890 th iteration => 0.0100050271411\n",
      "Loss for  30891 th iteration => 0.0100044415805\n",
      "Loss for  30892 th iteration => 0.0100040869571\n",
      "Loss for  30893 th iteration => 0.0100038133944\n",
      "Loss for  30894 th iteration => 0.0100035998692\n",
      "Loss for  30895 th iteration => 0.010003013038\n",
      "Loss for  30896 th iteration => 0.0100026581327\n",
      "Loss for  30897 th iteration => 0.0100023850993\n",
      "Loss for  30898 th iteration => 0.0100021729881\n",
      "Loss for  30899 th iteration => 0.0100015848949\n",
      "Loss for  30900 th iteration => 0.0100012297096\n",
      "Loss for  30901 th iteration => 0.0100009572056\n",
      "Loss for  30902 th iteration => 0.0100007464979\n",
      "Loss for  30903 th iteration => 0.0100001571509\n",
      "Loss for  30904 th iteration => 0.00999980168764\n",
      "Loss for  30905 th iteration => 0.00999952971303\n",
      "Loss for  30906 th iteration => 0.00999932039877\n",
      "Loss for  30907 th iteration => 0.00999872980605\n",
      "Loss for  30908 th iteration => 0.00999837406644\n",
      "Loss for  30909 th iteration => 0.00999810262134\n",
      "Loss for  30910 th iteration => 0.00999789469067\n",
      "Loss for  30911 th iteration => 0.00999730286004\n",
      "Loss for  30912 th iteration => 0.00999694684576\n",
      "Loss for  30913 th iteration => 0.00999667593024\n",
      "Loss for  30914 th iteration => 0.00999646937365\n",
      "Loss for  30915 th iteration => 0.00999587631275\n",
      "Loss for  30916 th iteration => 0.00999552002533\n",
      "Loss for  30917 th iteration => 0.00999524963947\n",
      "Loss for  30918 th iteration => 0.00999504444775\n",
      "Loss for  30919 th iteration => 0.00999445016403\n",
      "Loss for  30920 th iteration => 0.00999409360491\n",
      "Loss for  30921 th iteration => 0.00999382374878\n",
      "Loss for  30922 th iteration => 0.00999361991298\n",
      "Loss for  30923 th iteration => 0.00999302441372\n",
      "Loss for  30924 th iteration => 0.00999266758421\n",
      "Loss for  30925 th iteration => 0.00999239825791\n",
      "Loss for  30926 th iteration => 0.00999219576938\n",
      "Loss for  30927 th iteration => 0.00999159906167\n",
      "Loss for  30928 th iteration => 0.009991241963\n",
      "Loss for  30929 th iteration => 0.00999097318324\n",
      "Loss for  30930 th iteration => 0.00999069010964\n",
      "Loss for  30931 th iteration => 0.00999025574773\n",
      "Loss for  30932 th iteration => 0.00998981668958\n",
      "Loss for  30933 th iteration => 0.00998954919223\n",
      "Loss for  30934 th iteration => 0.00998911535411\n",
      "Loss for  30935 th iteration => 0.00998898346345\n",
      "Loss for  30936 th iteration => 0.00998839176148\n",
      "Loss for  30937 th iteration => 0.00998812568575\n",
      "Loss for  30938 th iteration => 0.00998769020857\n",
      "Loss for  30939 th iteration => 0.00998756080536\n",
      "Loss for  30940 th iteration => 0.00998696723585\n",
      "Loss for  30941 th iteration => 0.00998670257025\n",
      "Loss for  30942 th iteration => 0.00998626547158\n",
      "Loss for  30943 th iteration => 0.00998613853139\n",
      "Loss for  30944 th iteration => 0.00998554311234\n",
      "Loss for  30945 th iteration => 0.00998527984579\n",
      "Loss for  30946 th iteration => 0.00998484114257\n",
      "Loss for  30947 th iteration => 0.00998471664176\n",
      "Loss for  30948 th iteration => 0.00998411953903\n",
      "Loss for  30949 th iteration => 0.00998376860804\n",
      "Loss for  30950 th iteration => 0.00998350479294\n",
      "Loss for  30951 th iteration => 0.00998329524403\n",
      "Loss for  30952 th iteration => 0.00998269668969\n",
      "Loss for  30953 th iteration => 0.00998234532483\n",
      "Loss for  30954 th iteration => 0.00998208205174\n",
      "Loss for  30955 th iteration => 0.0099818742273\n",
      "Loss for  30956 th iteration => 0.00998127423696\n",
      "Loss for  30957 th iteration => 0.00998092244454\n",
      "Loss for  30958 th iteration => 0.00998065971309\n",
      "Loss for  30959 th iteration => 0.0099804535919\n",
      "Loss for  30960 th iteration => 0.00997985218069\n",
      "Loss for  30961 th iteration => 0.00997949996675\n",
      "Loss for  30962 th iteration => 0.0099792377766\n",
      "Loss for  30963 th iteration => 0.00997903333811\n",
      "Loss for  30964 th iteration => 0.00997843052075\n",
      "Loss for  30965 th iteration => 0.00997807789107\n",
      "Loss for  30966 th iteration => 0.00997781624189\n",
      "Loss for  30967 th iteration => 0.00997761346622\n",
      "Loss for  30968 th iteration => 0.00997700925699\n",
      "Loss for  30969 th iteration => 0.00997665621711\n",
      "Loss for  30970 th iteration => 0.00997639510859\n",
      "Loss for  30971 th iteration => 0.00997619397652\n",
      "Loss for  30972 th iteration => 0.00997558838927\n",
      "Loss for  30973 th iteration => 0.00997523494448\n",
      "Loss for  30974 th iteration => 0.00997497437632\n",
      "Loss for  30975 th iteration => 0.00997477486924\n",
      "Loss for  30976 th iteration => 0.00997416791743\n",
      "Loss for  30977 th iteration => 0.00997381407281\n",
      "Loss for  30978 th iteration => 0.00997355404473\n",
      "Loss for  30979 th iteration => 0.00997335614465\n",
      "Loss for  30980 th iteration => 0.00997274784135\n",
      "Loss for  30981 th iteration => 0.00997239360174\n",
      "Loss for  30982 th iteration => 0.00997213411346\n",
      "Loss for  30983 th iteration => 0.00997193780295\n",
      "Loss for  30984 th iteration => 0.00997132816086\n",
      "Loss for  30985 th iteration => 0.00997097353089\n",
      "Loss for  30986 th iteration => 0.00997071458217\n",
      "Loss for  30987 th iteration => 0.00997051984437\n",
      "Loss for  30988 th iteration => 0.00996990887582\n",
      "Loss for  30989 th iteration => 0.00996955385991\n",
      "Loss for  30990 th iteration => 0.00996929545051\n",
      "Loss for  30991 th iteration => 0.00996910226911\n",
      "Loss for  30992 th iteration => 0.0099684899861\n",
      "Loss for  30993 th iteration => 0.00996813458846\n",
      "Loss for  30994 th iteration => 0.00996787717675\n",
      "Loss for  30995 th iteration => 0.00996743583346\n",
      "Loss for  30996 th iteration => 0.00996732152322\n",
      "Loss for  30997 th iteration => 0.00996671561155\n",
      "Loss for  30998 th iteration => 0.00996645979946\n",
      "Loss for  30999 th iteration => 0.00996601657983\n",
      "Loss for  31000 th iteration => 0.00996590502348\n",
      "Loss for  31001 th iteration => 0.00996529703721\n",
      "Loss for  31002 th iteration => 0.00996504280786\n",
      "Loss for  31003 th iteration => 0.00996459773703\n",
      "Loss for  31004 th iteration => 0.00996448890065\n",
      "Loss for  31005 th iteration => 0.00996387886499\n",
      "Loss for  31006 th iteration => 0.00996362620208\n",
      "Loss for  31007 th iteration => 0.00996317930433\n",
      "Loss for  31008 th iteration => 0.0099630731551\n",
      "Loss for  31009 th iteration => 0.00996246117494\n",
      "Loss for  31010 th iteration => 0.00996211179968\n",
      "Loss for  31011 th iteration => 0.00996185822718\n",
      "Loss for  31012 th iteration => 0.00996165789431\n",
      "Loss for  31013 th iteration => 0.0099610443752\n",
      "Loss for  31014 th iteration => 0.00996069446766\n",
      "Loss for  31015 th iteration => 0.00996044144521\n",
      "Loss for  31016 th iteration => 0.00996024300779\n",
      "Loss for  31017 th iteration => 0.00995962796935\n",
      "Loss for  31018 th iteration => 0.00995927753829\n",
      "Loss for  31019 th iteration => 0.00995902506528\n",
      "Loss for  31020 th iteration => 0.009958828496\n",
      "Loss for  31021 th iteration => 0.00995821195724\n",
      "Loss for  31022 th iteration => 0.00995786101109\n",
      "Loss for  31023 th iteration => 0.00995760908691\n",
      "Loss for  31024 th iteration => 0.00995741435939\n",
      "Loss for  31025 th iteration => 0.00995679633875\n",
      "Loss for  31026 th iteration => 0.0099564448856\n",
      "Loss for  31027 th iteration => 0.00995619350966\n",
      "Loss for  31028 th iteration => 0.00995600059837\n",
      "Loss for  31029 th iteration => 0.00995538111375\n",
      "Loss for  31030 th iteration => 0.00995502916136\n",
      "Loss for  31031 th iteration => 0.0099547783331\n",
      "Loss for  31032 th iteration => 0.00995458721335\n",
      "Loss for  31033 th iteration => 0.00995396628208\n",
      "Loss for  31034 th iteration => 0.00995361383791\n",
      "Loss for  31035 th iteration => 0.0099533635568\n",
      "Loss for  31036 th iteration => 0.00995317420471\n",
      "Loss for  31037 th iteration => 0.00995255184363\n",
      "Loss for  31038 th iteration => 0.00995219891483\n",
      "Loss for  31039 th iteration => 0.00995194918035\n",
      "Loss for  31040 th iteration => 0.00995176157281\n",
      "Loss for  31041 th iteration => 0.00995113779824\n",
      "Loss for  31042 th iteration => 0.00995078439167\n",
      "Loss for  31043 th iteration => 0.00995053520333\n",
      "Loss for  31044 th iteration => 0.009950349318\n",
      "Loss for  31045 th iteration => 0.00994972414578\n",
      "Loss for  31046 th iteration => 0.00994937026803\n",
      "Loss for  31047 th iteration => 0.00994912213905\n",
      "Loss for  31048 th iteration => 0.00994867379998\n",
      "Loss for  31049 th iteration => 0.00994857523298\n",
      "Loss for  31050 th iteration => 0.00994795643911\n",
      "Loss for  31051 th iteration => 0.00994771004773\n",
      "Loss for  31052 th iteration => 0.00994725964876\n",
      "Loss for  31053 th iteration => 0.00994716403937\n",
      "Loss for  31054 th iteration => 0.00994654301259\n",
      "Loss for  31055 th iteration => 0.0099462983378\n",
      "Loss for  31056 th iteration => 0.00994584590986\n",
      "Loss for  31057 th iteration => 0.00994575321691\n",
      "Loss for  31058 th iteration => 0.00994512998795\n",
      "Loss for  31059 th iteration => 0.00994488700948\n",
      "Loss for  31060 th iteration => 0.00994443258246\n",
      "Loss for  31061 th iteration => 0.0099443427661\n",
      "Loss for  31062 th iteration => 0.00994371758052\n",
      "Loss for  31063 th iteration => 0.00994336935677\n",
      "Loss for  31064 th iteration => 0.00994312502499\n",
      "Loss for  31065 th iteration => 0.00994293279431\n",
      "Loss for  31066 th iteration => 0.0099423060056\n",
      "Loss for  31067 th iteration => 0.00994195717173\n",
      "Loss for  31068 th iteration => 0.00994171339532\n",
      "Loss for  31069 th iteration => 0.00994152319141\n",
      "Loss for  31070 th iteration => 0.00994089482224\n",
      "Loss for  31071 th iteration => 0.00994054538891\n",
      "Loss for  31072 th iteration => 0.00994030216704\n",
      "Loss for  31073 th iteration => 0.00994011395796\n",
      "Loss for  31074 th iteration => 0.0099394840303\n",
      "Loss for  31075 th iteration => 0.00993913400777\n",
      "Loss for  31076 th iteration => 0.00993889133963\n",
      "Loss for  31077 th iteration => 0.00993870509452\n",
      "Loss for  31078 th iteration => 0.00993807362967\n",
      "Loss for  31079 th iteration => 0.0099377230278\n",
      "Loss for  31080 th iteration => 0.0099374809126\n",
      "Loss for  31081 th iteration => 0.0099372966016\n",
      "Loss for  31082 th iteration => 0.00993666362021\n",
      "Loss for  31083 th iteration => 0.00993631244847\n",
      "Loss for  31084 th iteration => 0.00993607088548\n",
      "Loss for  31085 th iteration => 0.00993588847971\n",
      "Loss for  31086 th iteration => 0.0099352540018\n",
      "Loss for  31087 th iteration => 0.0099349022693\n",
      "Loss for  31088 th iteration => 0.00993466125778\n",
      "Loss for  31089 th iteration => 0.00993448072933\n",
      "Loss for  31090 th iteration => 0.00993384477429\n",
      "Loss for  31091 th iteration => 0.00993349248979\n",
      "Loss for  31092 th iteration => 0.00993325202905\n",
      "Loss for  31093 th iteration => 0.00993307335091\n",
      "Loss for  31094 th iteration => 0.00993243593756\n",
      "Loss for  31095 th iteration => 0.00993208310947\n",
      "Loss for  31096 th iteration => 0.00993184417893\n",
      "Loss for  31097 th iteration => 0.00993138867998\n",
      "Loss for  31098 th iteration => 0.00993130537131\n",
      "Loss for  31099 th iteration => 0.00993067402376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  31100 th iteration => 0.00993043693964\n",
      "Loss for  31101 th iteration => 0.00992997923688\n",
      "Loss for  31102 th iteration => 0.00992989904223\n",
      "Loss for  31103 th iteration => 0.00992926534\n",
      "Loss for  31104 th iteration => 0.00992903007811\n",
      "Loss for  31105 th iteration => 0.00992857020697\n",
      "Loss for  31106 th iteration => 0.00992849307953\n",
      "Loss for  31107 th iteration => 0.00992785705764\n",
      "Loss for  31108 th iteration => 0.00992762359462\n",
      "Loss for  31109 th iteration => 0.0099271615893\n",
      "Loss for  31110 th iteration => 0.00992708748382\n",
      "Loss for  31111 th iteration => 0.00992644982787\n",
      "Loss for  31112 th iteration => 0.00992610235359\n",
      "Loss for  31113 th iteration => 0.00992586675937\n",
      "Loss for  31114 th iteration => 0.00992568236237\n",
      "Loss for  31115 th iteration => 0.00992504305739\n",
      "Loss for  31116 th iteration => 0.00992469490969\n",
      "Loss for  31117 th iteration => 0.00992445987428\n",
      "Loss for  31118 th iteration => 0.00992427760534\n",
      "Loss for  31119 th iteration => 0.00992363667636\n",
      "Loss for  31120 th iteration => 0.00992328786736\n",
      "Loss for  31121 th iteration => 0.00992305338977\n",
      "Loss for  31122 th iteration => 0.00992287321338\n",
      "Loss for  31123 th iteration => 0.00992223068466\n",
      "Loss for  31124 th iteration => 0.00992188122603\n",
      "Loss for  31125 th iteration => 0.00992164730529\n",
      "Loss for  31126 th iteration => 0.00992146918712\n",
      "Loss for  31127 th iteration => 0.00992082508215\n",
      "Loss for  31128 th iteration => 0.00992047498513\n",
      "Loss for  31129 th iteration => 0.0099202416203\n",
      "Loss for  31130 th iteration => 0.00992006552716\n",
      "Loss for  31131 th iteration => 0.00991941986872\n",
      "Loss for  31132 th iteration => 0.00991906914411\n",
      "Loss for  31133 th iteration => 0.00991883633429\n",
      "Loss for  31134 th iteration => 0.00991866223409\n",
      "Loss for  31135 th iteration => 0.00991801504423\n",
      "Loss for  31136 th iteration => 0.00991766370243\n",
      "Loss for  31137 th iteration => 0.0099174316511\n",
      "Loss for  31138 th iteration => 0.009916971519\n",
      "Loss for  31139 th iteration => 0.0099168993683\n",
      "Loss for  31140 th iteration => 0.00991625855565\n",
      "Loss for  31141 th iteration => 0.00991602846559\n",
      "Loss for  31142 th iteration => 0.00991556597475\n",
      "Loss for  31143 th iteration => 0.00991549711293\n",
      "Loss for  31144 th iteration => 0.00991485381087\n",
      "Loss for  31145 th iteration => 0.0099146256544\n",
      "Loss for  31146 th iteration => 0.00991416084513\n",
      "Loss for  31147 th iteration => 0.00991409521926\n",
      "Loss for  31148 th iteration => 0.00991344946744\n",
      "Loss for  31149 th iteration => 0.00991322321786\n",
      "Loss for  31150 th iteration => 0.00991275612909\n",
      "Loss for  31151 th iteration => 0.009912693688\n",
      "Loss for  31152 th iteration => 0.00991204597699\n",
      "Loss for  31153 th iteration => 0.00991169968714\n",
      "Loss for  31154 th iteration => 0.00991147175408\n",
      "Loss for  31155 th iteration => 0.00991129262635\n",
      "Loss for  31156 th iteration => 0.00991064320919\n",
      "Loss for  31157 th iteration => 0.00991029618204\n",
      "Loss for  31158 th iteration => 0.00991006881083\n",
      "Loss for  31159 th iteration => 0.00990989192473\n",
      "Loss for  31160 th iteration => 0.00990924082904\n",
      "Loss for  31161 th iteration => 0.00990889307832\n",
      "Loss for  31162 th iteration => 0.0099086662678\n",
      "Loss for  31163 th iteration => 0.0099084915839\n",
      "Loss for  31164 th iteration => 0.00990783883643\n",
      "Loss for  31165 th iteration => 0.00990749037535\n",
      "Loss for  31166 th iteration => 0.0099072641244\n",
      "Loss for  31167 th iteration => 0.00990709160457\n",
      "Loss for  31168 th iteration => 0.00990643723124\n",
      "Loss for  31169 th iteration => 0.00990608807253\n",
      "Loss for  31170 th iteration => 0.00990586238005\n",
      "Loss for  31171 th iteration => 0.00990569198744\n",
      "Loss for  31172 th iteration => 0.00990503601334\n",
      "Loss for  31173 th iteration => 0.00990468616926\n",
      "Loss for  31174 th iteration => 0.00990446103419\n",
      "Loss for  31175 th iteration => 0.00990429273315\n",
      "Loss for  31176 th iteration => 0.00990363518263\n",
      "Loss for  31177 th iteration => 0.00990328466496\n",
      "Loss for  31178 th iteration => 0.00990306052907\n",
      "Loss for  31179 th iteration => 0.00990259436807\n",
      "Loss for  31180 th iteration => 0.00990253492401\n",
      "Loss for  31181 th iteration => 0.00990188345535\n",
      "Loss for  31182 th iteration => 0.00990166136424\n",
      "Loss for  31183 th iteration => 0.00990119273367\n",
      "Loss for  31184 th iteration => 0.00990113669808\n",
      "Loss for  31185 th iteration => 0.00990048264727\n",
      "Loss for  31186 th iteration => 0.00990026257084\n",
      "Loss for  31187 th iteration => 0.00989979151441\n",
      "Loss for  31188 th iteration => 0.00989973883009\n",
      "Loss for  31189 th iteration => 0.00989908225521\n",
      "Loss for  31190 th iteration => 0.00989873728649\n",
      "Loss for  31191 th iteration => 0.00989851648533\n",
      "Loss for  31192 th iteration => 0.00989834142722\n",
      "Loss for  31193 th iteration => 0.00989768308569\n",
      "Loss for  31194 th iteration => 0.009897337317\n",
      "Loss for  31195 th iteration => 0.0098971170803\n",
      "Loss for  31196 th iteration => 0.00989694438014\n",
      "Loss for  31197 th iteration => 0.0098962843022\n",
      "Loss for  31198 th iteration => 0.00989593774888\n",
      "Loss for  31199 th iteration => 0.00989571807531\n",
      "Loss for  31200 th iteration => 0.00989554768969\n",
      "Loss for  31201 th iteration => 0.00989488590464\n",
      "Loss for  31202 th iteration => 0.00989453858145\n",
      "Loss for  31203 th iteration => 0.00989431946973\n",
      "Loss for  31204 th iteration => 0.00989415135668\n",
      "Loss for  31205 th iteration => 0.00989348789289\n",
      "Loss for  31206 th iteration => 0.00989313981406\n",
      "Loss for  31207 th iteration => 0.00989292126295\n",
      "Loss for  31208 th iteration => 0.00989275538188\n",
      "Loss for  31209 th iteration => 0.00989209026683\n",
      "Loss for  31210 th iteration => 0.00989174144607\n",
      "Loss for  31211 th iteration => 0.00989152345437\n",
      "Loss for  31212 th iteration => 0.00989135976604\n",
      "Loss for  31213 th iteration => 0.00989069302635\n",
      "Loss for  31214 th iteration => 0.00989034347686\n",
      "Loss for  31215 th iteration => 0.00989012621273\n",
      "Loss for  31216 th iteration => 0.00988965513467\n",
      "Loss for  31217 th iteration => 0.00988960651182\n",
      "Loss for  31218 th iteration => 0.0098889458023\n",
      "Loss for  31219 th iteration => 0.00988873066607\n",
      "Loss for  31220 th iteration => 0.00988825700684\n",
      "Loss for  31221 th iteration => 0.00988821191533\n",
      "Loss for  31222 th iteration => 0.00988754852897\n",
      "Loss for  31223 th iteration => 0.00988733548811\n",
      "Loss for  31224 th iteration => 0.00988685929487\n",
      "Loss for  31225 th iteration => 0.00988681767319\n",
      "Loss for  31226 th iteration => 0.00988615176563\n",
      "Loss for  31227 th iteration => 0.00988580758634\n",
      "Loss for  31228 th iteration => 0.00988559392735\n",
      "Loss for  31229 th iteration => 0.00988542389248\n",
      "Loss for  31230 th iteration => 0.00988475617952\n",
      "Loss for  31231 th iteration => 0.00988441115093\n",
      "Loss for  31232 th iteration => 0.00988419805764\n",
      "Loss for  31233 th iteration => 0.0098840304641\n",
      "Loss for  31234 th iteration => 0.00988336097788\n",
      "Loss for  31235 th iteration => 0.00988301511648\n",
      "Loss for  31236 th iteration => 0.00988280258744\n",
      "Loss for  31237 th iteration => 0.00988263738896\n",
      "Loss for  31238 th iteration => 0.00988196616059\n",
      "Loss for  31239 th iteration => 0.00988161948229\n",
      "Loss for  31240 th iteration => 0.00988140751609\n",
      "Loss for  31241 th iteration => 0.00988124466792\n",
      "Loss for  31242 th iteration => 0.00988057172754\n",
      "Loss for  31243 th iteration => 0.00988022424766\n",
      "Loss for  31244 th iteration => 0.00988001284295\n",
      "Loss for  31245 th iteration => 0.00987985230184\n",
      "Loss for  31246 th iteration => 0.00987917767862\n",
      "Loss for  31247 th iteration => 0.00987882941192\n",
      "Loss for  31248 th iteration => 0.0098786185674\n",
      "Loss for  31249 th iteration => 0.00987846029151\n",
      "Loss for  31250 th iteration => 0.00987778401372\n",
      "Loss for  31251 th iteration => 0.00987743497443\n",
      "Loss for  31252 th iteration => 0.00987722527281\n",
      "Loss for  31253 th iteration => 0.00987674834994\n",
      "Loss for  31254 th iteration => 0.00987671155173\n",
      "Loss for  31255 th iteration => 0.00987604083121\n",
      "Loss for  31256 th iteration => 0.00987583332013\n",
      "Loss for  31257 th iteration => 0.00987535373331\n",
      "Loss for  31258 th iteration => 0.00987532055323\n",
      "Loss for  31259 th iteration => 0.00987464708862\n",
      "Loss for  31260 th iteration => 0.00987444173377\n",
      "Loss for  31261 th iteration => 0.00987395953264\n",
      "Loss for  31262 th iteration => 0.00987392990609\n",
      "Loss for  31263 th iteration => 0.00987325437215\n",
      "Loss for  31264 th iteration => 0.0098729105586\n",
      "Loss for  31265 th iteration => 0.00987270404057\n",
      "Loss for  31266 th iteration => 0.0098725397173\n",
      "Loss for  31267 th iteration => 0.00987186235498\n",
      "Loss for  31268 th iteration => 0.00987151765257\n",
      "Loss for  31269 th iteration => 0.00987131170043\n",
      "Loss for  31270 th iteration => 0.00987114987794\n",
      "Loss for  31271 th iteration => 0.00987047114939\n",
      "Loss for  31272 th iteration => 0.00987032344598\n",
      "Loss for  31273 th iteration => 0.00986993174148\n",
      "Loss for  31274 th iteration => 0.00986973783122\n",
      "Loss for  31275 th iteration => 0.0098691023527\n",
      "Loss for  31276 th iteration => 0.00986889896319\n",
      "Loss for  31277 th iteration => 0.00986856427059\n",
      "Loss for  31278 th iteration => 0.00986819305845\n",
      "Loss for  31279 th iteration => 0.00986786070726\n",
      "Loss for  31280 th iteration => 0.00986747590859\n",
      "Loss for  31281 th iteration => 0.00986705264905\n",
      "Loss for  31282 th iteration => 0.00986691348446\n",
      "Loss for  31283 th iteration => 0.00986649385656\n",
      "Loss for  31284 th iteration => 0.00986605587523\n",
      "Loss for  31285 th iteration => 0.00986568205183\n",
      "Loss for  31286 th iteration => 0.00986549409774\n",
      "Loss for  31287 th iteration => 0.00986512677612\n",
      "Loss for  31288 th iteration => 0.00986463659477\n",
      "Loss for  31289 th iteration => 0.00986431175193\n",
      "Loss for  31290 th iteration => 0.00986407548049\n",
      "Loss for  31291 th iteration => 0.0098637594839\n",
      "Loss for  31292 th iteration => 0.00986321805606\n",
      "Loss for  31293 th iteration => 0.00986294174871\n",
      "Loss for  31294 th iteration => 0.00986265789466\n",
      "Loss for  31295 th iteration => 0.00986226184714\n",
      "Loss for  31296 th iteration => 0.00986192937524\n",
      "Loss for  31297 th iteration => 0.00986157198471\n",
      "Loss for  31298 th iteration => 0.00986125675101\n",
      "Loss for  31299 th iteration => 0.00986076182604\n",
      "Loss for  31300 th iteration => 0.00986054060941\n",
      "Loss for  31301 th iteration => 0.0098600670106\n",
      "Loss for  31302 th iteration => 0.00985983772925\n",
      "Loss for  31303 th iteration => 0.0098595201396\n",
      "Loss for  31304 th iteration => 0.00985915351417\n",
      "Loss for  31305 th iteration => 0.0098586771073\n",
      "Loss for  31306 th iteration => 0.00985844949864\n",
      "Loss for  31307 th iteration => 0.00985813247835\n",
      "Loss for  31308 th iteration => 0.00985776677053\n",
      "Loss for  31309 th iteration => 0.00985728762063\n",
      "Loss for  31310 th iteration => 0.00985706165228\n",
      "Loss for  31311 th iteration => 0.00985674518959\n",
      "Loss for  31312 th iteration => 0.0098563803793\n",
      "Loss for  31313 th iteration => 0.00985589854933\n",
      "Loss for  31314 th iteration => 0.00985567418992\n",
      "Loss for  31315 th iteration => 0.00985535835616\n",
      "Loss for  31316 th iteration => 0.00985486133698\n",
      "Loss for  31317 th iteration => 0.00985464285728\n",
      "Loss for  31318 th iteration => 0.00985428710755\n",
      "Loss for  31319 th iteration => 0.00985397301799\n",
      "Loss for  31320 th iteration => 0.00985347324465\n",
      "Loss for  31321 th iteration => 0.00985325658915\n",
      "Loss for  31322 th iteration => 0.00985277913789\n",
      "Loss for  31323 th iteration => 0.00985255535552\n",
      "Loss for  31324 th iteration => 0.00985223871354\n",
      "Loss for  31325 th iteration => 0.00985187148712\n",
      "Loss for  31326 th iteration => 0.00985139125769\n",
      "Loss for  31327 th iteration => 0.0098511691267\n",
      "Loss for  31328 th iteration => 0.00985085304763\n",
      "Loss for  31329 th iteration => 0.00985048673635\n",
      "Loss for  31330 th iteration => 0.00985000379274\n",
      "Loss for  31331 th iteration => 0.00984978328128\n",
      "Loss for  31332 th iteration => 0.00984946775353\n",
      "Loss for  31333 th iteration => 0.00984910233765\n",
      "Loss for  31334 th iteration => 0.00984861674179\n",
      "Loss for  31335 th iteration => 0.009848397819\n",
      "Loss for  31336 th iteration => 0.00984808306727\n",
      "Loss for  31337 th iteration => 0.00984758133426\n",
      "Loss for  31338 th iteration => 0.00984736686805\n",
      "Loss for  31339 th iteration => 0.00984701273586\n",
      "Loss for  31340 th iteration => 0.00984669971469\n",
      "Loss for  31341 th iteration => 0.00984619525656\n",
      "Loss for  31342 th iteration => 0.00984598303222\n",
      "Loss for  31343 th iteration => 0.00984550187313\n",
      "Loss for  31344 th iteration => 0.00984528348164\n",
      "Loss for  31345 th iteration => 0.00984496775655\n",
      "Loss for  31346 th iteration => 0.00984459992142\n",
      "Loss for  31347 th iteration => 0.00984411600909\n",
      "Loss for  31348 th iteration => 0.00984389925014\n",
      "Loss for  31349 th iteration => 0.00984358408238\n",
      "Loss for  31350 th iteration => 0.00984321716148\n",
      "Loss for  31351 th iteration => 0.009842730559\n",
      "Loss for  31352 th iteration => 0.00984251540117\n",
      "Loss for  31353 th iteration => 0.00984220077933\n",
      "Loss for  31354 th iteration => 0.00984183475321\n",
      "Loss for  31355 th iteration => 0.00984134552164\n",
      "Loss for  31356 th iteration => 0.00984113193449\n",
      "Loss for  31357 th iteration => 0.00984081820239\n",
      "Loss for  31358 th iteration => 0.00984031189789\n",
      "Loss for  31359 th iteration => 0.00984010150256\n",
      "Loss for  31360 th iteration => 0.00983961933566\n",
      "Loss for  31361 th iteration => 0.00983940469579\n",
      "Loss for  31362 th iteration => 0.00983908933601\n",
      "Loss for  31363 th iteration => 0.00983872002782\n",
      "Loss for  31364 th iteration => 0.0098382350709\n",
      "Loss for  31365 th iteration => 0.0098380220755\n",
      "Loss for  31366 th iteration => 0.00983770727867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  31367 th iteration => 0.00983733890276\n",
      "Loss for  31368 th iteration => 0.00983685122001\n",
      "Loss for  31369 th iteration => 0.00983663983715\n",
      "Loss for  31370 th iteration => 0.00983632559165\n",
      "Loss for  31371 th iteration => 0.0098359581282\n",
      "Loss for  31372 th iteration => 0.00983546778174\n",
      "Loss for  31373 th iteration => 0.00983525798048\n",
      "Loss for  31374 th iteration => 0.00983494427508\n",
      "Loss for  31375 th iteration => 0.00983457770493\n",
      "Loss for  31376 th iteration => 0.00983408475488\n",
      "Loss for  31377 th iteration => 0.00983387650524\n",
      "Loss for  31378 th iteration => 0.00983356386601\n",
      "Loss for  31379 th iteration => 0.00983305291968\n",
      "Loss for  31380 th iteration => 0.00983284693796\n",
      "Loss for  31381 th iteration => 0.00983236108195\n",
      "Loss for  31382 th iteration => 0.00983215175464\n",
      "Loss for  31383 th iteration => 0.00983183729991\n",
      "Loss for  31384 th iteration => 0.00983146744725\n",
      "Loss for  31385 th iteration => 0.00983097882491\n",
      "Loss for  31386 th iteration => 0.00983077112389\n",
      "Loss for  31387 th iteration => 0.00983045722679\n",
      "Loss for  31388 th iteration => 0.0098300883058\n",
      "Loss for  31389 th iteration => 0.00982959698048\n",
      "Loss for  31390 th iteration => 0.00982939087424\n",
      "Loss for  31391 th iteration => 0.0098290775233\n",
      "Loss for  31392 th iteration => 0.00982870951443\n",
      "Loss for  31393 th iteration => 0.0098282155474\n",
      "Loss for  31394 th iteration => 0.00982801100542\n",
      "Loss for  31395 th iteration => 0.00982769818955\n",
      "Loss for  31396 th iteration => 0.00982733107389\n",
      "Loss for  31397 th iteration => 0.0098268345245\n",
      "Loss for  31398 th iteration => 0.00982663151719\n",
      "Loss for  31399 th iteration => 0.0098263199152\n",
      "Loss for  31400 th iteration => 0.00982580445818\n",
      "Loss for  31401 th iteration => 0.00982560278781\n",
      "Loss for  31402 th iteration => 0.00982511335648\n",
      "Loss for  31403 th iteration => 0.00982490924917\n",
      "Loss for  31404 th iteration => 0.00982459567433\n",
      "Loss for  31405 th iteration => 0.00982422527873\n",
      "Loss for  31406 th iteration => 0.00982373310079\n",
      "Loss for  31407 th iteration => 0.00982353060356\n",
      "Loss for  31408 th iteration => 0.00982321758172\n",
      "Loss for  31409 th iteration => 0.00982284811843\n",
      "Loss for  31410 th iteration => 0.0098223532565\n",
      "Loss for  31411 th iteration => 0.00982215233819\n",
      "Loss for  31412 th iteration => 0.00982183985804\n",
      "Loss for  31413 th iteration => 0.00982147130771\n",
      "Loss for  31414 th iteration => 0.00982097382239\n",
      "Loss for  31415 th iteration => 0.00982077445283\n",
      "Loss for  31416 th iteration => 0.0098204625034\n",
      "Loss for  31417 th iteration => 0.00982009484734\n",
      "Loss for  31418 th iteration => 0.00981959479728\n",
      "Loss for  31419 th iteration => 0.00981939694722\n",
      "Loss for  31420 th iteration => 0.00981908633973\n",
      "Loss for  31421 th iteration => 0.00981856648257\n",
      "Loss for  31422 th iteration => 0.00981836903898\n",
      "Loss for  31423 th iteration => 0.00981787612646\n",
      "Loss for  31424 th iteration => 0.00981767715632\n",
      "Loss for  31425 th iteration => 0.0098173644397\n",
      "Loss for  31426 th iteration => 0.00981699350882\n",
      "Loss for  31427 th iteration => 0.00981649786609\n",
      "Loss for  31428 th iteration => 0.00981630049146\n",
      "Loss for  31429 th iteration => 0.00981598832384\n",
      "Loss for  31430 th iteration => 0.00981561832693\n",
      "Loss for  31431 th iteration => 0.00981512001598\n",
      "Loss for  31432 th iteration => 0.00981492420602\n",
      "Loss for  31433 th iteration => 0.00981461257619\n",
      "Loss for  31434 th iteration => 0.00981424349408\n",
      "Loss for  31435 th iteration => 0.00981374257492\n",
      "Loss for  31436 th iteration => 0.00981354829974\n",
      "Loss for  31437 th iteration => 0.00981323719684\n",
      "Loss for  31438 th iteration => 0.00981286901105\n",
      "Loss for  31439 th iteration => 0.00981236554174\n",
      "Loss for  31440 th iteration => 0.00981217284914\n",
      "Loss for  31441 th iteration => 0.00981168403021\n",
      "Loss for  31442 th iteration => 0.00981151810397\n",
      "Loss for  31443 th iteration => 0.00981114572847\n",
      "Loss for  31444 th iteration => 0.00981064930542\n",
      "Loss for  31445 th iteration => 0.00981045544551\n",
      "Loss for  31446 th iteration => 0.00981014359319\n",
      "Loss for  31447 th iteration => 0.00980977217267\n",
      "Loss for  31448 th iteration => 0.00980927303643\n",
      "Loss for  31449 th iteration => 0.00980908075722\n",
      "Loss for  31450 th iteration => 0.00980876944978\n",
      "Loss for  31451 th iteration => 0.00980839896462\n",
      "Loss for  31452 th iteration => 0.00980789717655\n",
      "Loss for  31453 th iteration => 0.00980770644751\n",
      "Loss for  31454 th iteration => 0.00980739567386\n",
      "Loss for  31455 th iteration => 0.0098070261051\n",
      "Loss for  31456 th iteration => 0.00980652172459\n",
      "Loss for  31457 th iteration => 0.00980633251615\n",
      "Loss for  31458 th iteration => 0.00980602226554\n",
      "Loss for  31459 th iteration => 0.00980565359485\n",
      "Loss for  31460 th iteration => 0.00980514701759\n",
      "Loss for  31461 th iteration => 0.00980480756538\n",
      "Loss for  31462 th iteration => 0.00980461719498\n",
      "Loss for  31463 th iteration => 0.00980430564887\n",
      "Loss for  31464 th iteration => 0.00980393282914\n",
      "Loss for  31465 th iteration => 0.00980343287449\n",
      "Loss for  31466 th iteration => 0.00980324410105\n",
      "Loss for  31467 th iteration => 0.00980293310702\n",
      "Loss for  31468 th iteration => 0.00980256124331\n",
      "Loss for  31469 th iteration => 0.00980205859277\n",
      "Loss for  31470 th iteration => 0.00980187138512\n",
      "Loss for  31471 th iteration => 0.00980156093184\n",
      "Loss for  31472 th iteration => 0.00980119000473\n",
      "Loss for  31473 th iteration => 0.00980068471902\n",
      "Loss for  31474 th iteration => 0.00980049904696\n",
      "Loss for  31475 th iteration => 0.00980018912343\n",
      "Loss for  31476 th iteration => 0.00979981911415\n",
      "Loss for  31477 th iteration => 0.00979931125206\n",
      "Loss for  31478 th iteration => 0.0097991270863\n",
      "Loss for  31479 th iteration => 0.00979881782663\n",
      "Loss for  31480 th iteration => 0.00979828755959\n",
      "Loss for  31481 th iteration => 0.00979810027505\n",
      "Loss for  31482 th iteration => 0.00979759953024\n",
      "Loss for  31483 th iteration => 0.0097974142241\n",
      "Loss for  31484 th iteration => 0.00979710353433\n",
      "Loss for  31485 th iteration => 0.00979673030916\n",
      "Loss for  31486 th iteration => 0.00979622682334\n",
      "Loss for  31487 th iteration => 0.00979604309915\n",
      "Loss for  31488 th iteration => 0.00979573295728\n",
      "Loss for  31489 th iteration => 0.00979536068922\n",
      "Loss for  31490 th iteration => 0.00979485452448\n",
      "Loss for  31491 th iteration => 0.00979467235138\n",
      "Loss for  31492 th iteration => 0.00979436274619\n",
      "Loss for  31493 th iteration => 0.00979399141603\n",
      "Loss for  31494 th iteration => 0.00979348343802\n",
      "Loss for  31495 th iteration => 0.00979346189303\n",
      "Loss for  31496 th iteration => 0.00979300455645\n",
      "Loss for  31497 th iteration => 0.00979260275745\n",
      "Loss for  31498 th iteration => 0.0097921301727\n",
      "Loss for  31499 th iteration => 0.00979190710892\n",
      "Loss for  31500 th iteration => 0.00979162100386\n",
      "Loss for  31501 th iteration => 0.00979140126405\n",
      "Loss for  31502 th iteration => 0.00979077876905\n",
      "Loss for  31503 th iteration => 0.00979050990517\n",
      "Loss for  31504 th iteration => 0.00979026975136\n",
      "Loss for  31505 th iteration => 0.00979000961468\n",
      "Loss for  31506 th iteration => 0.00978942746666\n",
      "Loss for  31507 th iteration => 0.00978911364491\n",
      "Loss for  31508 th iteration => 0.00978891859072\n",
      "Loss for  31509 th iteration => 0.0097886183784\n",
      "Loss for  31510 th iteration => 0.0097880762727\n",
      "Loss for  31511 th iteration => 0.00978771971214\n",
      "Loss for  31512 th iteration => 0.00978740236076\n",
      "Loss for  31513 th iteration => 0.00978719346554\n",
      "Loss for  31514 th iteration => 0.00978692317693\n",
      "Loss for  31515 th iteration => 0.00978632716356\n",
      "Loss for  31516 th iteration => 0.0097860495997\n",
      "Loss for  31517 th iteration => 0.00978580097309\n",
      "Loss for  31518 th iteration => 0.0097855742282\n",
      "Loss for  31519 th iteration => 0.00978493778704\n",
      "Loss for  31520 th iteration => 0.00978459813772\n",
      "Loss for  31521 th iteration => 0.00978452244297\n",
      "Loss for  31522 th iteration => 0.00978411574287\n",
      "Loss for  31523 th iteration => 0.00978357014486\n",
      "Loss for  31524 th iteration => 0.0097832269467\n",
      "Loss for  31525 th iteration => 0.00978298925207\n",
      "Loss for  31526 th iteration => 0.00978291779258\n",
      "Loss for  31527 th iteration => 0.00978220287739\n",
      "Loss for  31528 th iteration => 0.00978185828679\n",
      "Loss for  31529 th iteration => 0.00978152408757\n",
      "Loss for  31530 th iteration => 0.00978144831805\n",
      "Loss for  31531 th iteration => 0.00978103782583\n",
      "Loss for  31532 th iteration => 0.00978049149386\n",
      "Loss for  31533 th iteration => 0.00978015355575\n",
      "Loss for  31534 th iteration => 0.00978008331012\n",
      "Loss for  31535 th iteration => 0.00977967334589\n",
      "Loss for  31536 th iteration => 0.00977912507072\n",
      "Loss for  31537 th iteration => 0.00977878349545\n",
      "Loss for  31538 th iteration => 0.00977871863195\n",
      "Loss for  31539 th iteration => 0.00977830918568\n",
      "Loss for  31540 th iteration => 0.00977775901736\n",
      "Loss for  31541 th iteration => 0.00977741520452\n",
      "Loss for  31542 th iteration => 0.00977718159139\n",
      "Loss for  31543 th iteration => 0.00977691182787\n",
      "Loss for  31544 th iteration => 0.0097765980136\n",
      "Loss for  31545 th iteration => 0.00977604915199\n",
      "Loss for  31546 th iteration => 0.00977571312505\n",
      "Loss for  31547 th iteration => 0.0097756480376\n",
      "Loss for  31548 th iteration => 0.00977523475411\n",
      "Loss for  31549 th iteration => 0.00977468394663\n",
      "Loss for  31550 th iteration => 0.00977434426881\n",
      "Loss for  31551 th iteration => 0.00977428456619\n",
      "Loss for  31552 th iteration => 0.00977387181373\n",
      "Loss for  31553 th iteration => 0.00977331911053\n",
      "Loss for  31554 th iteration => 0.00977297630522\n",
      "Loss for  31555 th iteration => 0.00977274600224\n",
      "Loss for  31556 th iteration => 0.00977268439477\n",
      "Loss for  31557 th iteration => 0.0097719555564\n",
      "Loss for  31558 th iteration => 0.00977181372855\n",
      "Loss for  31559 th iteration => 0.00977129108344\n",
      "Loss for  31560 th iteration => 0.00977139923655\n",
      "Loss for  31561 th iteration => 0.00977061582763\n",
      "Loss for  31562 th iteration => 0.00977041675405\n",
      "Loss for  31563 th iteration => 0.00976994519262\n",
      "Loss for  31564 th iteration => 0.0097700084381\n",
      "Loss for  31565 th iteration => 0.00976927603272\n",
      "Loss for  31566 th iteration => 0.00976902060855\n",
      "Loss for  31567 th iteration => 0.0097686029394\n",
      "Loss for  31568 th iteration => 0.00976832862069\n",
      "Loss for  31569 th iteration => 0.0097682256969\n",
      "Loss for  31570 th iteration => 0.00976762509387\n",
      "Loss for  31571 th iteration => 0.0097672632816\n",
      "Loss for  31572 th iteration => 0.00976692781354\n",
      "Loss for  31573 th iteration => 0.00976689123596\n",
      "Loss for  31574 th iteration => 0.0097662304043\n",
      "Loss for  31575 th iteration => 0.00976592355637\n",
      "Loss for  31576 th iteration => 0.00976552983712\n",
      "Loss for  31577 th iteration => 0.00976537087421\n",
      "Loss for  31578 th iteration => 0.00976501814448\n",
      "Loss for  31579 th iteration => 0.00976458380296\n",
      "Loss for  31580 th iteration => 0.00976413589699\n",
      "Loss for  31581 th iteration => 0.00976391782157\n",
      "Loss for  31582 th iteration => 0.00976374844609\n",
      "Loss for  31583 th iteration => 0.0097631096323\n",
      "Loss for  31584 th iteration => 0.00976277118977\n",
      "Loss for  31585 th iteration => 0.0097625539091\n",
      "Loss for  31586 th iteration => 0.00976239124389\n",
      "Loss for  31587 th iteration => 0.00976174799862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  31588 th iteration => 0.00976140692304\n",
      "Loss for  31589 th iteration => 0.00976119270385\n",
      "Loss for  31590 th iteration => 0.00976073372982\n",
      "Loss for  31591 th iteration => 0.0097606861057\n",
      "Loss for  31592 th iteration => 0.00976004367487\n",
      "Loss for  31593 th iteration => 0.00975970709461\n",
      "Loss for  31594 th iteration => 0.00975949351706\n",
      "Loss for  31595 th iteration => 0.00975932975822\n",
      "Loss for  31596 th iteration => 0.0097586828814\n",
      "Loss for  31597 th iteration => 0.00975834365233\n",
      "Loss for  31598 th iteration => 0.00975813127009\n",
      "Loss for  31599 th iteration => 0.00975778474702\n",
      "Loss for  31600 th iteration => 0.00975751078187\n",
      "Loss for  31601 th iteration => 0.00975698059102\n",
      "Loss for  31602 th iteration => 0.00975677263501\n",
      "Loss for  31603 th iteration => 0.00975630740525\n",
      "Loss for  31604 th iteration => 0.00975626992057\n",
      "Loss for  31605 th iteration => 0.00975561964362\n",
      "Loss for  31606 th iteration => 0.00975528234101\n",
      "Loss for  31607 th iteration => 0.00975507330706\n",
      "Loss for  31608 th iteration => 0.00975491467451\n",
      "Loss for  31609 th iteration => 0.00975426005579\n",
      "Loss for  31610 th iteration => 0.00975392015703\n",
      "Loss for  31611 th iteration => 0.00975371310328\n",
      "Loss for  31612 th iteration => 0.00975336566398\n",
      "Loss for  31613 th iteration => 0.00975309347342\n",
      "Loss for  31614 th iteration => 0.00975255835203\n",
      "Loss for  31615 th iteration => 0.00975235630532\n",
      "Loss for  31616 th iteration => 0.00975188530781\n",
      "Loss for  31617 th iteration => 0.0097518573348\n",
      "Loss for  31618 th iteration => 0.00975119953171\n",
      "Loss for  31619 th iteration => 0.00975086167126\n",
      "Loss for  31620 th iteration => 0.00975065713767\n",
      "Loss for  31621 th iteration => 0.00975050319896\n",
      "Loss for  31622 th iteration => 0.0097498420848\n",
      "Loss for  31623 th iteration => 0.00974969100873\n",
      "Loss for  31624 th iteration => 0.00974931489476\n",
      "Loss for  31625 th iteration => 0.00974900011415\n",
      "Loss for  31626 th iteration => 0.00974862998025\n",
      "Loss for  31627 th iteration => 0.00974829977363\n",
      "Loss for  31628 th iteration => 0.00974784054216\n",
      "Loss for  31629 th iteration => 0.00974774663653\n",
      "Loss for  31630 th iteration => 0.00974729824705\n",
      "Loss for  31631 th iteration => 0.00974691130077\n",
      "Loss for  31632 th iteration => 0.00974650198835\n",
      "Loss for  31633 th iteration => 0.00974636056905\n",
      "Loss for  31634 th iteration => 0.00974583866967\n",
      "Loss for  31635 th iteration => 0.00974564858737\n",
      "Loss for  31636 th iteration => 0.0097451668131\n",
      "Loss for  31637 th iteration => 0.00974483022052\n",
      "Loss for  31638 th iteration => 0.00974464562419\n",
      "Loss for  31639 th iteration => 0.00974426395162\n",
      "Loss for  31640 th iteration => 0.00974383203005\n",
      "Loss for  31641 th iteration => 0.00974344018887\n",
      "Loss for  31642 th iteration => 0.00974331290097\n",
      "Loss for  31643 th iteration => 0.00974275167825\n",
      "Loss for  31644 th iteration => 0.0097426257906\n",
      "Loss for  31645 th iteration => 0.00974205331475\n",
      "Loss for  31646 th iteration => 0.00974183140994\n",
      "Loss for  31647 th iteration => 0.00974152163636\n",
      "Loss for  31648 th iteration => 0.00974116425333\n",
      "Loss for  31649 th iteration => 0.00974069441069\n",
      "Loss for  31650 th iteration => 0.00974047531294\n",
      "Loss for  31651 th iteration => 0.00974016668713\n",
      "Loss for  31652 th iteration => 0.00973968130282\n",
      "Loss for  31653 th iteration => 0.00973946711937\n",
      "Loss for  31654 th iteration => 0.00973900088486\n",
      "Loss for  31655 th iteration => 0.00973878228651\n",
      "Loss for  31656 th iteration => 0.00973847279826\n",
      "Loss for  31657 th iteration => 0.0097381143535\n",
      "Loss for  31658 th iteration => 0.00973764283032\n",
      "Loss for  31659 th iteration => 0.00973742701936\n",
      "Loss for  31660 th iteration => 0.00973711850696\n",
      "Loss for  31661 th iteration => 0.00973676188965\n",
      "Loss for  31662 th iteration => 0.00973628587642\n",
      "Loss for  31663 th iteration => 0.00973595041041\n",
      "Loss for  31664 th iteration => 0.00973573503325\n",
      "Loss for  31665 th iteration => 0.00973542580386\n",
      "Loss for  31666 th iteration => 0.00973506625118\n",
      "Loss for  31667 th iteration => 0.00973459320191\n",
      "Loss for  31668 th iteration => 0.00973438059496\n",
      "Loss for  31669 th iteration => 0.00973407233623\n",
      "Loss for  31670 th iteration => 0.00973371460507\n",
      "Loss for  31671 th iteration => 0.00973323647119\n",
      "Loss for  31672 th iteration => 0.00973290184054\n",
      "Loss for  31673 th iteration => 0.00973268964456\n",
      "Loss for  31674 th iteration => 0.00973238066259\n",
      "Loss for  31675 th iteration => 0.00973201998887\n",
      "Loss for  31676 th iteration => 0.0097315454762\n",
      "Loss for  31677 th iteration => 0.00973133603428\n",
      "Loss for  31678 th iteration => 0.00973102801816\n",
      "Loss for  31679 th iteration => 0.0097306691607\n",
      "Loss for  31680 th iteration => 0.00973018955461\n",
      "Loss for  31681 th iteration => 0.00972998279811\n",
      "Loss for  31682 th iteration => 0.00972967651124\n",
      "Loss for  31683 th iteration => 0.00972917913254\n",
      "Loss for  31684 th iteration => 0.0097289754696\n",
      "Loss for  31685 th iteration => 0.00972849975664\n",
      "Loss for  31686 th iteration => 0.00972829334479\n",
      "Loss for  31687 th iteration => 0.00972798552361\n",
      "Loss for  31688 th iteration => 0.00972762546257\n",
      "Loss for  31689 th iteration => 0.00972714467318\n",
      "Loss for  31690 th iteration => 0.00972694093509\n",
      "Loss for  31691 th iteration => 0.00972663410568\n",
      "Loss for  31692 th iteration => 0.00972613481533\n",
      "Loss for  31693 th iteration => 0.00972593279803\n",
      "Loss for  31694 th iteration => 0.00972545592403\n",
      "Loss for  31695 th iteration => 0.00972525251463\n",
      "Loss for  31696 th iteration => 0.00972494488143\n",
      "Loss for  31697 th iteration => 0.009724583612\n",
      "Loss for  31698 th iteration => 0.00972410167722\n",
      "Loss for  31699 th iteration => 0.00972390093058\n",
      "Loss for  31700 th iteration => 0.00972359423212\n",
      "Loss for  31701 th iteration => 0.00972323472996\n",
      "Loss for  31702 th iteration => 0.00972274848773\n",
      "Loss for  31703 th iteration => 0.00972241401816\n",
      "Loss for  31704 th iteration => 0.00972221354536\n",
      "Loss for  31705 th iteration => 0.0097219060806\n",
      "Loss for  31706 th iteration => 0.00972154356977\n",
      "Loss for  31707 th iteration => 0.00972106060521\n",
      "Loss for  31708 th iteration => 0.00972086278604\n",
      "Loss for  31709 th iteration => 0.00972055625376\n",
      "Loss for  31710 th iteration => 0.00972019550965\n",
      "Loss for  31711 th iteration => 0.00971970763059\n",
      "Loss for  31712 th iteration => 0.00971951239936\n",
      "Loss for  31713 th iteration => 0.00971920756574\n",
      "Loss for  31714 th iteration => 0.0097186997165\n",
      "Loss for  31715 th iteration => 0.00971850527957\n",
      "Loss for  31716 th iteration => 0.00971802151689\n",
      "Loss for  31717 th iteration => 0.00971782650549\n",
      "Loss for  31718 th iteration => 0.00971752009728\n",
      "Loss for  31719 th iteration => 0.00971715804408\n",
      "Loss for  31720 th iteration => 0.00971666937089\n",
      "Loss for  31721 th iteration => 0.00971647694208\n",
      "Loss for  31722 th iteration => 0.00971617144153\n",
      "Loss for  31723 th iteration => 0.00971581111404\n",
      "Loss for  31724 th iteration => 0.00971531916767\n",
      "Loss for  31725 th iteration => 0.00971498433741\n",
      "Loss for  31726 th iteration => 0.00971479208058\n",
      "Loss for  31727 th iteration => 0.00971448578141\n",
      "Loss for  31728 th iteration => 0.00971412239474\n",
      "Loss for  31729 th iteration => 0.0097136330177\n",
      "Loss for  31730 th iteration => 0.00971344333956\n",
      "Loss for  31731 th iteration => 0.00971313794771\n",
      "Loss for  31732 th iteration => 0.0097127762897\n",
      "Loss for  31733 th iteration => 0.00971228260138\n",
      "Loss for  31734 th iteration => 0.00971194901848\n",
      "Loss for  31735 th iteration => 0.00971175950597\n",
      "Loss for  31736 th iteration => 0.00971145331513\n",
      "Loss for  31737 th iteration => 0.00971108860188\n",
      "Loss for  31738 th iteration => 0.0097105985242\n",
      "Loss for  31739 th iteration => 0.00971041158658\n",
      "Loss for  31740 th iteration => 0.00971010630276\n",
      "Loss for  31741 th iteration => 0.00970974332117\n",
      "Loss for  31742 th iteration => 0.00970924846458\n",
      "Loss for  31743 th iteration => 0.00970906403843\n",
      "Loss for  31744 th iteration => 0.00970875989688\n",
      "Loss for  31745 th iteration => 0.00970824290503\n",
      "Loss for  31746 th iteration => 0.00970805656653\n",
      "Loss for  31747 th iteration => 0.00970756599587\n",
      "Loss for  31748 th iteration => 0.00970738169076\n",
      "Loss for  31749 th iteration => 0.00970707647708\n",
      "Loss for  31750 th iteration => 0.00970671211253\n",
      "Loss for  31751 th iteration => 0.00970621675705\n",
      "Loss for  31752 th iteration => 0.00970603496286\n",
      "Loss for  31753 th iteration => 0.00970573063396\n",
      "Loss for  31754 th iteration => 0.00970536796475\n",
      "Loss for  31755 th iteration => 0.00970486893164\n",
      "Loss for  31756 th iteration => 0.00970453535887\n",
      "Loss for  31757 th iteration => 0.00970435364386\n",
      "Loss for  31758 th iteration => 0.00970404848913\n",
      "Loss for  31759 th iteration => 0.00970368272452\n",
      "Loss for  31760 th iteration => 0.00970318693888\n",
      "Loss for  31761 th iteration => 0.00970300773535\n",
      "Loss for  31762 th iteration => 0.00970270346668\n",
      "Loss for  31763 th iteration => 0.00970233940336\n",
      "Loss for  31764 th iteration => 0.00970183950728\n",
      "Loss for  31765 th iteration => 0.00970181870327\n",
      "Loss for  31766 th iteration => 0.00970137154699\n",
      "Loss for  31767 th iteration => 0.00970097559323\n",
      "Loss for  31768 th iteration => 0.00970051324972\n",
      "Loss for  31769 th iteration => 0.00970028930719\n",
      "Loss for  31770 th iteration => 0.00970001263792\n",
      "Loss for  31771 th iteration => 0.00969979671957\n",
      "Loss for  31772 th iteration => 0.00969918736122\n",
      "Loss for  31773 th iteration => 0.0096989162816\n",
      "Loss for  31774 th iteration => 0.00969852631714\n",
      "Loss for  31775 th iteration => 0.00969839682613\n",
      "Loss for  31776 th iteration => 0.00969805491168\n",
      "Loss for  31777 th iteration => 0.00969754698629\n",
      "Loss for  31778 th iteration => 0.00969719728112\n",
      "Loss for  31779 th iteration => 0.00969702717875\n",
      "Loss for  31780 th iteration => 0.00969673261356\n",
      "Loss for  31781 th iteration => 0.0096961783425\n",
      "Loss for  31782 th iteration => 0.0096958694928\n",
      "Loss for  31783 th iteration => 0.00969549591597\n",
      "Loss for  31784 th iteration => 0.00969537610492\n",
      "Loss for  31785 th iteration => 0.00969500575567\n",
      "Loss for  31786 th iteration => 0.0096945440653\n",
      "Loss for  31787 th iteration => 0.00969412509132\n",
      "Loss for  31788 th iteration => 0.00969405048102\n",
      "Loss for  31789 th iteration => 0.00969365058738\n",
      "Loss for  31790 th iteration => 0.00969311351827\n",
      "Loss for  31791 th iteration => 0.00969277673502\n",
      "Loss for  31792 th iteration => 0.00969254368482\n",
      "Loss for  31793 th iteration => 0.0096922778168\n",
      "Loss for  31794 th iteration => 0.00969196980791\n",
      "Loss for  31795 th iteration => 0.00969143266912\n",
      "Loss for  31796 th iteration => 0.0096911018441\n",
      "Loss for  31797 th iteration => 0.00969103328435\n",
      "Loss for  31798 th iteration => 0.00969063069794\n",
      "Loss for  31799 th iteration => 0.00969009060105\n",
      "Loss for  31800 th iteration => 0.00968975416238\n",
      "Loss for  31801 th iteration => 0.00968952401381\n",
      "Loss for  31802 th iteration => 0.00968925898484\n",
      "Loss for  31803 th iteration => 0.00968895090638\n",
      "Loss for  31804 th iteration => 0.00968841077818\n",
      "Loss for  31805 th iteration => 0.0096880805499\n",
      "Loss for  31806 th iteration => 0.00968801787178\n",
      "Loss for  31807 th iteration => 0.00968761258794\n",
      "Loss for  31808 th iteration => 0.00968706952949\n",
      "Loss for  31809 th iteration => 0.00968673365774\n",
      "Loss for  31810 th iteration => 0.00968667884284\n",
      "Loss for  31811 th iteration => 0.00968627454448\n",
      "Loss for  31812 th iteration => 0.00968573065319\n",
      "Loss for  31813 th iteration => 0.0096855897698\n",
      "Loss for  31814 th iteration => 0.009685077023\n",
      "Loss for  31815 th iteration => 0.00968518413755\n",
      "Loss for  31816 th iteration => 0.00968441501164\n",
      "Loss for  31817 th iteration => 0.00968421566209\n",
      "Loss for  31818 th iteration => 0.00968375472134\n",
      "Loss for  31819 th iteration => 0.00968363971079\n",
      "Loss for  31820 th iteration => 0.00968327695192\n",
      "Loss for  31821 th iteration => 0.00968284232683\n",
      "Loss for  31822 th iteration => 0.00968243837486\n",
      "Loss for  31823 th iteration => 0.00968215578228\n",
      "Loss for  31824 th iteration => 0.00968207437053\n",
      "Loss for  31825 th iteration => 0.00968146973214\n",
      "Loss for  31826 th iteration => 0.00968112270389\n",
      "Loss for  31827 th iteration => 0.00968077991511\n",
      "Loss for  31828 th iteration => 0.00968046923508\n",
      "Loss for  31829 th iteration => 0.00968038744495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  31830 th iteration => 0.00967980693873\n",
      "Loss for  31831 th iteration => 0.00967940854924\n",
      "Loss for  31832 th iteration => 0.0096791476339\n",
      "Loss for  31833 th iteration => 0.00967883956262\n",
      "Loss for  31834 th iteration => 0.00967867407105\n",
      "Loss for  31835 th iteration => 0.0096780379137\n",
      "Loss for  31836 th iteration => 0.00967783014278\n",
      "Loss for  31837 th iteration => 0.00967737464036\n",
      "Loss for  31838 th iteration => 0.00967733321886\n",
      "Loss for  31839 th iteration => 0.00967669902504\n",
      "Loss for  31840 th iteration => 0.00967636514296\n",
      "Loss for  31841 th iteration => 0.00967615675692\n",
      "Loss for  31842 th iteration => 0.00967581418256\n",
      "Loss for  31843 th iteration => 0.00967554671411\n",
      "Loss for  31844 th iteration => 0.00967502378955\n",
      "Loss for  31845 th iteration => 0.00967469293685\n",
      "Loss for  31846 th iteration => 0.00967448664962\n",
      "Loss for  31847 th iteration => 0.00967432755334\n",
      "Loss for  31848 th iteration => 0.00967368619815\n",
      "Loss for  31849 th iteration => 0.00967335161503\n",
      "Loss for  31850 th iteration => 0.00967314800711\n",
      "Loss for  31851 th iteration => 0.00967280454116\n",
      "Loss for  31852 th iteration => 0.00967253863414\n",
      "Loss for  31853 th iteration => 0.00967201198057\n",
      "Loss for  31854 th iteration => 0.00967168050337\n",
      "Loss for  31855 th iteration => 0.00967147822375\n",
      "Loss for  31856 th iteration => 0.00967132347618\n",
      "Loss for  31857 th iteration => 0.00967067520232\n",
      "Loss for  31858 th iteration => 0.00967034005243\n",
      "Loss for  31859 th iteration => 0.0096701410592\n",
      "Loss for  31860 th iteration => 0.00966979675183\n",
      "Loss for  31861 th iteration => 0.00966953225664\n",
      "Loss for  31862 th iteration => 0.00966900244948\n",
      "Loss for  31863 th iteration => 0.00966886032448\n",
      "Loss for  31864 th iteration => 0.00966848618302\n",
      "Loss for  31865 th iteration => 0.00966829610674\n",
      "Loss for  31866 th iteration => 0.00966769016451\n",
      "Loss for  31867 th iteration => 0.00966748768231\n",
      "Loss for  31868 th iteration => 0.00966703616127\n",
      "Loss for  31869 th iteration => 0.00966694608496\n",
      "Loss for  31870 th iteration => 0.00966650153499\n",
      "Loss for  31871 th iteration => 0.00966612058127\n",
      "Loss for  31872 th iteration => 0.00966571907091\n",
      "Loss for  31873 th iteration => 0.00966558017716\n",
      "Loss for  31874 th iteration => 0.00966506763051\n",
      "Loss for  31875 th iteration => 0.00966487808818\n",
      "Loss for  31876 th iteration => 0.0096644060548\n",
      "Loss for  31877 th iteration => 0.00966407061595\n",
      "Loss for  31878 th iteration => 0.00966389700401\n",
      "Loss for  31879 th iteration => 0.00966351553101\n",
      "Loss for  31880 th iteration => 0.00966309374456\n",
      "Loss for  31881 th iteration => 0.00966270220007\n",
      "Loss for  31882 th iteration => 0.00966243965738\n",
      "Loss for  31883 th iteration => 0.00966216893296\n",
      "Loss for  31884 th iteration => 0.00966191027633\n",
      "Loss for  31885 th iteration => 0.00966133766391\n",
      "Loss for  31886 th iteration => 0.0096611235301\n",
      "Loss for  31887 th iteration => 0.00966082075248\n",
      "Loss for  31888 th iteration => 0.00966046900564\n",
      "Loss for  31889 th iteration => 0.00966000196462\n",
      "Loss for  31890 th iteration => 0.00965966996164\n",
      "Loss for  31891 th iteration => 0.00965945725592\n",
      "Loss for  31892 th iteration => 0.0096591541309\n",
      "Loss for  31893 th iteration => 0.00965880028252\n",
      "Loss for  31894 th iteration => 0.00965833257971\n",
      "Loss for  31895 th iteration => 0.00965812362117\n",
      "Loss for  31896 th iteration => 0.00965767108531\n",
      "Loss for  31897 th iteration => 0.00965748806677\n",
      "Loss for  31898 th iteration => 0.00965713210255\n",
      "Loss for  31899 th iteration => 0.00965666589618\n",
      "Loss for  31900 th iteration => 0.00965645810939\n",
      "Loss for  31901 th iteration => 0.00965615587424\n",
      "Loss for  31902 th iteration => 0.00965580233883\n",
      "Loss for  31903 th iteration => 0.00965533084041\n",
      "Loss for  31904 th iteration => 0.00965499984595\n",
      "Loss for  31905 th iteration => 0.00965479341319\n",
      "Loss for  31906 th iteration => 0.0096544908075\n",
      "Loss for  31907 th iteration => 0.00965413513331\n",
      "Loss for  31908 th iteration => 0.00965366377929\n",
      "Loss for  31909 th iteration => 0.00965346079501\n",
      "Loss for  31910 th iteration => 0.00965315940452\n",
      "Loss for  31911 th iteration => 0.00965280611496\n",
      "Loss for  31912 th iteration => 0.00965233107875\n",
      "Loss for  31913 th iteration => 0.00965199883773\n",
      "Loss for  31914 th iteration => 0.00965179711601\n",
      "Loss for  31915 th iteration => 0.00965149532407\n",
      "Loss for  31916 th iteration => 0.00965113984105\n",
      "Loss for  31917 th iteration => 0.00965066412823\n",
      "Loss for  31918 th iteration => 0.00965033442555\n",
      "Loss for  31919 th iteration => 0.00965013399459\n",
      "Loss for  31920 th iteration => 0.00964983181286\n",
      "Loss for  31921 th iteration => 0.00964947416115\n",
      "Loss for  31922 th iteration => 0.0096489996634\n",
      "Loss for  31923 th iteration => 0.00964880263539\n",
      "Loss for  31924 th iteration => 0.00964850165434\n",
      "Loss for  31925 th iteration => 0.00964814636552\n",
      "Loss for  31926 th iteration => 0.00964766720603\n",
      "Loss for  31927 th iteration => 0.00964733635415\n",
      "Loss for  31928 th iteration => 0.00964714052938\n",
      "Loss for  31929 th iteration => 0.00964683912903\n",
      "Loss for  31930 th iteration => 0.0096464816195\n",
      "Loss for  31931 th iteration => 0.00964600246937\n",
      "Loss for  31932 th iteration => 0.00964580998253\n",
      "Loss for  31933 th iteration => 0.00964550976021\n",
      "Loss for  31934 th iteration => 0.00964515457472\n",
      "Loss for  31935 th iteration => 0.00964467209838\n",
      "Loss for  31936 th iteration => 0.00964434025572\n",
      "Loss for  31937 th iteration => 0.00964414889046\n",
      "Loss for  31938 th iteration => 0.00964384822144\n",
      "Loss for  31939 th iteration => 0.00964349076721\n",
      "Loss for  31940 th iteration => 0.00964300773353\n",
      "Loss for  31941 th iteration => 0.00964267856453\n",
      "Loss for  31942 th iteration => 0.00964248835425\n",
      "Loss for  31943 th iteration => 0.00964218725205\n",
      "Loss for  31944 th iteration => 0.00964182755738\n",
      "Loss for  31945 th iteration => 0.00964134596888\n",
      "Loss for  31946 th iteration => 0.00964115906229\n",
      "Loss for  31947 th iteration => 0.00964085912771\n",
      "Loss for  31948 th iteration => 0.00964050174291\n",
      "Loss for  31949 th iteration => 0.00964001546086\n",
      "Loss for  31950 th iteration => 0.00963968536871\n",
      "Loss for  31951 th iteration => 0.00963949953834\n",
      "Loss for  31952 th iteration => 0.00963919914421\n",
      "Loss for  31953 th iteration => 0.00963883947287\n",
      "Loss for  31954 th iteration => 0.00963835364127\n",
      "Loss for  31955 th iteration => 0.00963817105615\n",
      "Loss for  31956 th iteration => 0.00963787180938\n",
      "Loss for  31957 th iteration => 0.00963751441347\n",
      "Loss for  31958 th iteration => 0.00963702499966\n",
      "Loss for  31959 th iteration => 0.00963669412545\n",
      "Loss for  31960 th iteration => 0.00963651254307\n",
      "Loss for  31961 th iteration => 0.00963621281226\n",
      "Loss for  31962 th iteration => 0.00963585308728\n",
      "Loss for  31963 th iteration => 0.00963536326111\n",
      "Loss for  31964 th iteration => 0.0096351848761\n",
      "Loss for  31965 th iteration => 0.00963470527292\n",
      "Loss for  31966 th iteration => 0.00963455437292\n",
      "Loss for  31967 th iteration => 0.00963419231527\n",
      "Loss for  31968 th iteration => 0.009633704727\n",
      "Loss for  31969 th iteration => 0.00963352735455\n",
      "Loss for  31970 th iteration => 0.0096332282825\n",
      "Loss for  31971 th iteration => 0.00963286849341\n",
      "Loss for  31972 th iteration => 0.0096323755806\n",
      "Loss for  31973 th iteration => 0.00963204680643\n",
      "Loss for  31974 th iteration => 0.00963187040481\n",
      "Loss for  31975 th iteration => 0.00963157084065\n",
      "Loss for  31976 th iteration => 0.00963120871382\n",
      "Loss for  31977 th iteration => 0.00963071782135\n",
      "Loss for  31978 th iteration => 0.00963070059798\n",
      "Loss for  31979 th iteration => 0.00963025666033\n",
      "Loss for  31980 th iteration => 0.00962986651735\n",
      "Loss for  31981 th iteration => 0.00962940958719\n",
      "Loss for  31982 th iteration => 0.00962919020775\n",
      "Loss for  31983 th iteration => 0.00962891886311\n",
      "Loss for  31984 th iteration => 0.00962870621028\n",
      "Loss for  31985 th iteration => 0.00962810335041\n",
      "Loss for  31986 th iteration => 0.00962783557553\n",
      "Loss for  31987 th iteration => 0.00962745152085\n",
      "Loss for  31988 th iteration => 0.00962732532563\n",
      "Loss for  31989 th iteration => 0.00962699118037\n",
      "Loss for  31990 th iteration => 0.0096264851429\n",
      "Loss for  31991 th iteration => 0.00962614127789\n",
      "Loss for  31992 th iteration => 0.00962597436045\n",
      "Loss for  31993 th iteration => 0.0096256891917\n",
      "Loss for  31994 th iteration => 0.0096251353713\n",
      "Loss for  31995 th iteration => 0.00962483458923\n",
      "Loss for  31996 th iteration => 0.00962445987494\n",
      "Loss for  31997 th iteration => 0.00962435026572\n",
      "Loss for  31998 th iteration => 0.00962398395701\n",
      "Loss for  31999 th iteration => 0.00962352873187\n",
      "Loss for  32000 th iteration => 0.00962310762466\n",
      "Loss for  32001 th iteration => 0.00962287800403\n",
      "Loss for  32002 th iteration => 0.00962261682784\n",
      "Loss for  32003 th iteration => 0.00962231436566\n",
      "Loss for  32004 th iteration => 0.00962178183568\n",
      "Loss for  32005 th iteration => 0.00962145419157\n",
      "Loss for  32006 th iteration => 0.00962139154043\n",
      "Loss for  32007 th iteration => 0.00962099485606\n",
      "Loss for  32008 th iteration => 0.00962045865721\n",
      "Loss for  32009 th iteration => 0.00962012604077\n",
      "Loss for  32010 th iteration => 0.00961990027724\n",
      "Loss for  32011 th iteration => 0.00961964007548\n",
      "Loss for  32012 th iteration => 0.00961933962024\n",
      "Loss for  32013 th iteration => 0.00961880333897\n",
      "Loss for  32014 th iteration => 0.0096186717457\n",
      "Loss for  32015 th iteration => 0.0096184371458\n",
      "Loss for  32016 th iteration => 0.00961799418441\n",
      "Loss for  32017 th iteration => 0.00961750635607\n",
      "Loss for  32018 th iteration => 0.00961731227482\n",
      "Loss for  32019 th iteration => 0.00961685753374\n",
      "Loss for  32020 th iteration => 0.00961691915703\n",
      "Loss for  32021 th iteration => 0.00961620921326\n",
      "Loss for  32022 th iteration => 0.00961595656888\n",
      "Loss for  32023 th iteration => 0.00961555757682\n",
      "Loss for  32024 th iteration => 0.0096152807214\n",
      "Loss for  32025 th iteration => 0.0096152000976\n",
      "Loss for  32026 th iteration => 0.00961460154072\n",
      "Loss for  32027 th iteration => 0.00961426046552\n",
      "Loss for  32028 th iteration => 0.0096139205947\n",
      "Loss for  32029 th iteration => 0.00961361616294\n",
      "Loss for  32030 th iteration => 0.00961353580668\n",
      "Loss for  32031 th iteration => 0.00961296323745\n",
      "Loss for  32032 th iteration => 0.00961256681543\n",
      "Loss for  32033 th iteration => 0.00961231277597\n",
      "Loss for  32034 th iteration => 0.00961200535148\n",
      "Loss for  32035 th iteration => 0.00961184908182\n",
      "Loss for  32036 th iteration => 0.0096112154164\n",
      "Loss for  32037 th iteration => 0.00961088744137\n",
      "Loss for  32038 th iteration => 0.00961068235694\n",
      "Loss for  32039 th iteration => 0.00961052645708\n",
      "Loss for  32040 th iteration => 0.00960989510795\n",
      "Loss for  32041 th iteration => 0.00960956255923\n",
      "Loss for  32042 th iteration => 0.00960936309333\n",
      "Loss for  32043 th iteration => 0.00960890846415\n",
      "Loss for  32044 th iteration => 0.00960887661923\n",
      "Loss for  32045 th iteration => 0.00960824254403\n",
      "Loss for  32046 th iteration => 0.00960791194321\n",
      "Loss for  32047 th iteration => 0.00960771323103\n",
      "Loss for  32048 th iteration => 0.00960737277457\n",
      "Loss for  32049 th iteration => 0.00960711199401\n",
      "Loss for  32050 th iteration => 0.00960659053419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  32051 th iteration => 0.00960626191748\n",
      "Loss for  32052 th iteration => 0.00960606456442\n",
      "Loss for  32053 th iteration => 0.00960572417355\n",
      "Loss for  32054 th iteration => 0.00960546189775\n",
      "Loss for  32055 th iteration => 0.00960493916957\n",
      "Loss for  32056 th iteration => 0.00960480169927\n",
      "Loss for  32057 th iteration => 0.00960443124743\n",
      "Loss for  32058 th iteration => 0.00960424301494\n",
      "Loss for  32059 th iteration => 0.00960364461309\n",
      "Loss for  32060 th iteration => 0.00960344496036\n",
      "Loss for  32061 th iteration => 0.00960299934656\n",
      "Loss for  32062 th iteration => 0.00960291130853\n",
      "Loss for  32063 th iteration => 0.00960247249699\n",
      "Loss for  32064 th iteration => 0.0096020947084\n",
      "Loss for  32065 th iteration => 0.0096016995299\n",
      "Loss for  32066 th iteration => 0.00960142224727\n",
      "Loss for  32067 th iteration => 0.00960119826702\n",
      "Loss for  32068 th iteration => 0.00960086850229\n",
      "Loss for  32069 th iteration => 0.0096004049371\n",
      "Loss for  32070 th iteration => 0.00960006753597\n",
      "Loss for  32071 th iteration => 0.00959990456052\n",
      "Loss for  32072 th iteration => 0.00959952332598\n",
      "Loss for  32073 th iteration => 0.00959911026822\n",
      "Loss for  32074 th iteration => 0.00959871846872\n",
      "Loss for  32075 th iteration => 0.0095984615846\n",
      "Loss for  32076 th iteration => 0.00959819353268\n",
      "Loss for  32077 th iteration => 0.00959794507641\n",
      "Loss for  32078 th iteration => 0.00959737243014\n",
      "Loss for  32079 th iteration => 0.00959704494852\n",
      "Loss for  32080 th iteration => 0.00959683458525\n",
      "Loss for  32081 th iteration => 0.00959653661938\n",
      "Loss for  32082 th iteration => 0.00959618607865\n",
      "Loss for  32083 th iteration => 0.00959572411603\n",
      "Loss for  32084 th iteration => 0.00959539797265\n",
      "Loss for  32085 th iteration => 0.00959518982584\n",
      "Loss for  32086 th iteration => 0.00959489180605\n",
      "Loss for  32087 th iteration => 0.00959453983319\n",
      "Loss for  32088 th iteration => 0.00959407721092\n",
      "Loss for  32089 th iteration => 0.00959387323545\n",
      "Loss for  32090 th iteration => 0.00959342373773\n",
      "Loss for  32091 th iteration => 0.00959338154345\n",
      "Loss for  32092 th iteration => 0.00959276067362\n",
      "Loss for  32093 th iteration => 0.00959243134294\n",
      "Loss for  32094 th iteration => 0.0095922293493\n",
      "Loss for  32095 th iteration => 0.00959193270986\n",
      "Loss for  32096 th iteration => 0.00959158219059\n",
      "Loss for  32097 th iteration => 0.00959111390188\n",
      "Loss for  32098 th iteration => 0.00959078608648\n",
      "Loss for  32099 th iteration => 0.00959058614658\n",
      "Loss for  32100 th iteration => 0.00959028939817\n",
      "Loss for  32101 th iteration => 0.00958993734806\n",
      "Loss for  32102 th iteration => 0.00958946768599\n",
      "Loss for  32103 th iteration => 0.00958914138592\n",
      "Loss for  32104 th iteration => 0.00958894349418\n",
      "Loss for  32105 th iteration => 0.00958864663634\n",
      "Loss for  32106 th iteration => 0.00958829305685\n",
      "Loss for  32107 th iteration => 0.00958782243283\n",
      "Loss for  32108 th iteration => 0.00958762899303\n",
      "Loss for  32109 th iteration => 0.00958716973226\n",
      "Loss for  32110 th iteration => 0.00958700441296\n",
      "Loss for  32111 th iteration => 0.00958664927119\n",
      "Loss for  32112 th iteration => 0.00958617878219\n",
      "Loss for  32113 th iteration => 0.00958598684063\n",
      "Loss for  32114 th iteration => 0.00958569125932\n",
      "Loss for  32115 th iteration => 0.00958533893506\n",
      "Loss for  32116 th iteration => 0.00958486334425\n",
      "Loss for  32117 th iteration => 0.00958453577988\n",
      "Loss for  32118 th iteration => 0.00958434574021\n",
      "Loss for  32119 th iteration => 0.00958405000041\n",
      "Loss for  32120 th iteration => 0.00958369605941\n",
      "Loss for  32121 th iteration => 0.00958321922432\n",
      "Loss for  32122 th iteration => 0.0095828933262\n",
      "Loss for  32123 th iteration => 0.00958270518855\n",
      "Loss for  32124 th iteration => 0.00958240929182\n",
      "Loss for  32125 th iteration => 0.0095820537394\n",
      "Loss for  32126 th iteration => 0.00958157615997\n",
      "Loss for  32127 th iteration => 0.00958139216242\n",
      "Loss for  32128 th iteration => 0.00958092458175\n",
      "Loss for  32129 th iteration => 0.00958091827972\n",
      "Loss for  32130 th iteration => 0.00958026332473\n",
      "Loss for  32131 th iteration => 0.00957993478665\n",
      "Loss for  32132 th iteration => 0.00957975229763\n",
      "Loss for  32133 th iteration => 0.0095794575897\n",
      "Loss for  32134 th iteration => 0.00957910317354\n",
      "Loss for  32135 th iteration => 0.00957862074373\n",
      "Loss for  32136 th iteration => 0.00957829401116\n",
      "Loss for  32137 th iteration => 0.00957811329354\n",
      "Loss for  32138 th iteration => 0.00957781838516\n",
      "Loss for  32139 th iteration => 0.0095774622807\n",
      "Loss for  32140 th iteration => 0.00957697871741\n",
      "Loss for  32141 th iteration => 0.00957665377798\n",
      "Loss for  32142 th iteration => 0.00957647483671\n",
      "Loss for  32143 th iteration => 0.00957617973119\n",
      "Loss for  32144 th iteration => 0.00957582194703\n",
      "Loss for  32145 th iteration => 0.00957533837834\n",
      "Loss for  32146 th iteration => 0.00957516319944\n",
      "Loss for  32147 th iteration => 0.00957486942632\n",
      "Loss for  32148 th iteration => 0.00957451434659\n",
      "Loss for  32149 th iteration => 0.00957402739737\n",
      "Loss for  32150 th iteration => 0.0095738507568\n",
      "Loss for  32151 th iteration => 0.00957353836347\n",
      "Loss for  32152 th iteration => 0.0095733692013\n",
      "Loss for  32153 th iteration => 0.00957273660974\n",
      "Loss for  32154 th iteration => 0.00957251109674\n",
      "Loss for  32155 th iteration => 0.00957209068316\n",
      "Loss for  32156 th iteration => 0.00957200172583\n",
      "Loss for  32157 th iteration => 0.00957163563703\n",
      "Loss for  32158 th iteration => 0.00957117501335\n",
      "Loss for  32159 th iteration => 0.00957079708174\n",
      "Loss for  32160 th iteration => 0.00957050723256\n",
      "Loss for  32161 th iteration => 0.00957031461423\n",
      "Loss for  32162 th iteration => 0.00957003036876\n",
      "Loss for  32163 th iteration => 0.00956950654494\n",
      "Loss for  32164 th iteration => 0.00956916792759\n",
      "Loss for  32165 th iteration => 0.00956886275507\n",
      "Loss for  32166 th iteration => 0.0095686658259\n",
      "Loss for  32167 th iteration => 0.00956841015692\n",
      "Loss for  32168 th iteration => 0.00956783334957\n",
      "Loss for  32169 th iteration => 0.00956756784631\n",
      "Loss for  32170 th iteration => 0.00956733071729\n",
      "Loss for  32171 th iteration => 0.00956712403861\n",
      "Loss for  32172 th iteration => 0.00956650436246\n",
      "Loss for  32173 th iteration => 0.00956617464143\n",
      "Loss for  32174 th iteration => 0.00956594760609\n",
      "Loss for  32175 th iteration => 0.00956569003942\n",
      "Loss for  32176 th iteration => 0.00956539532163\n",
      "Loss for  32177 th iteration => 0.00956486642335\n",
      "Loss for  32178 th iteration => 0.00956453794664\n",
      "Loss for  32179 th iteration => 0.00956431279554\n",
      "Loss for  32180 th iteration => 0.00956405571532\n",
      "Loss for  32181 th iteration => 0.00956375968802\n",
      "Loss for  32182 th iteration => 0.00956322965664\n",
      "Loss for  32183 th iteration => 0.00956309816576\n",
      "Loss for  32184 th iteration => 0.00956269447543\n",
      "Loss for  32185 th iteration => 0.00956259779837\n",
      "Loss for  32186 th iteration => 0.00956194797471\n",
      "Loss for  32187 th iteration => 0.00956175382399\n",
      "Loss for  32188 th iteration => 0.00956130557736\n",
      "Loss for  32189 th iteration => 0.00956119040656\n",
      "Loss for  32190 th iteration => 0.009560840335\n",
      "Loss for  32191 th iteration => 0.00956041260368\n",
      "Loss for  32192 th iteration => 0.00956002225073\n",
      "Loss for  32193 th iteration => 0.00955974057776\n",
      "Loss for  32194 th iteration => 0.00955949216874\n",
      "Loss for  32195 th iteration => 0.00955924757549\n",
      "Loss for  32196 th iteration => 0.00955874039945\n",
      "Loss for  32197 th iteration => 0.00955839852466\n",
      "Loss for  32198 th iteration => 0.00955809854624\n",
      "Loss for  32199 th iteration => 0.00955784146975\n",
      "Loss for  32200 th iteration => 0.00955763770329\n",
      "Loss for  32201 th iteration => 0.00955705925665\n",
      "Loss for  32202 th iteration => 0.00955681445585\n",
      "Loss for  32203 th iteration => 0.00955639008649\n",
      "Loss for  32204 th iteration => 0.00955647123184\n",
      "Loss for  32205 th iteration => 0.00955572785497\n",
      "Loss for  32206 th iteration => 0.00955539885114\n",
      "Loss for  32207 th iteration => 0.00955507514295\n",
      "Loss for  32208 th iteration => 0.00955487493508\n",
      "Loss for  32209 th iteration => 0.00955471938376\n",
      "Loss for  32210 th iteration => 0.00955409356868\n",
      "Loss for  32211 th iteration => 0.00955376467501\n",
      "Loss for  32212 th iteration => 0.00955344190003\n",
      "Loss for  32213 th iteration => 0.00955324491756\n",
      "Loss for  32214 th iteration => 0.00955308976621\n",
      "Loss for  32215 th iteration => 0.0095524598302\n",
      "Loss for  32216 th iteration => 0.00955213179203\n",
      "Loss for  32217 th iteration => 0.00955193888839\n",
      "Loss for  32218 th iteration => 0.00955160077638\n",
      "Loss for  32219 th iteration => 0.00955134463635\n",
      "Loss for  32220 th iteration => 0.00955082781511\n",
      "Loss for  32221 th iteration => 0.00955068388361\n",
      "Loss for  32222 th iteration => 0.00955032499457\n",
      "Loss for  32223 th iteration => 0.00955013598228\n",
      "Loss for  32224 th iteration => 0.00954954804523\n",
      "Loss for  32225 th iteration => 0.00954934537504\n",
      "Loss for  32226 th iteration => 0.00954890543126\n",
      "Loss for  32227 th iteration => 0.00954881657331\n",
      "Loss for  32228 th iteration => 0.00954839091391\n",
      "Loss for  32229 th iteration => 0.00954800936681\n",
      "Loss for  32230 th iteration => 0.00954762517074\n",
      "Loss for  32231 th iteration => 0.00954733849339\n",
      "Loss for  32232 th iteration => 0.00954712899091\n",
      "Loss for  32233 th iteration => 0.00954679813567\n",
      "Loss for  32234 th iteration => 0.00954634532254\n",
      "Loss for  32235 th iteration => 0.00954600293894\n",
      "Loss for  32236 th iteration => 0.00954570274983\n",
      "Loss for  32237 th iteration => 0.00954548160566\n",
      "Loss for  32238 th iteration => 0.00954519287148\n",
      "Loss for  32239 th iteration => 0.00954466877639\n",
      "Loss for  32240 th iteration => 0.00954442236648\n",
      "Loss for  32241 th iteration => 0.00954399974843\n",
      "Loss for  32242 th iteration => 0.00954393260055\n",
      "Loss for  32243 th iteration => 0.00954347201567\n",
      "Loss for  32244 th iteration => 0.00954301532129\n",
      "Loss for  32245 th iteration => 0.00954269049099\n",
      "Loss for  32246 th iteration => 0.00954248393227\n",
      "Loss for  32247 th iteration => 0.00954219060818\n",
      "Loss for  32248 th iteration => 0.00954184407231\n",
      "Loss for  32249 th iteration => 0.00954138531359\n",
      "Loss for  32250 th iteration => 0.00954106101368\n",
      "Loss for  32251 th iteration => 0.00954085725947\n",
      "Loss for  32252 th iteration => 0.00954056410095\n",
      "Loss for  32253 th iteration => 0.0095402166412\n",
      "Loss for  32254 th iteration => 0.00953975585075\n",
      "Loss for  32255 th iteration => 0.00953943211856\n",
      "Loss for  32256 th iteration => 0.00953923113189\n",
      "Loss for  32257 th iteration => 0.00953893812721\n",
      "Loss for  32258 th iteration => 0.00953858972339\n",
      "Loss for  32259 th iteration => 0.00953812693259\n",
      "Loss for  32260 th iteration => 0.00953780380386\n",
      "Loss for  32261 th iteration => 0.00953760554912\n",
      "Loss for  32262 th iteration => 0.00953731268704\n",
      "Loss for  32263 th iteration => 0.00953696331989\n",
      "Loss for  32264 th iteration => 0.00953649855894\n",
      "Loss for  32265 th iteration => 0.00953617606785\n",
      "Loss for  32266 th iteration => 0.00953598051076\n",
      "Loss for  32267 th iteration => 0.00953568778049\n",
      "Loss for  32268 th iteration => 0.00953533743165\n",
      "Loss for  32269 th iteration => 0.00953487072963\n",
      "Loss for  32270 th iteration => 0.00953454890886\n",
      "Loss for  32271 th iteration => 0.00953435601639\n",
      "Loss for  32272 th iteration => 0.00953406340763\n",
      "Loss for  32273 th iteration => 0.00953371205959\n",
      "Loss for  32274 th iteration => 0.00953324401988\n",
      "Loss for  32275 th iteration => 0.00953305638077\n",
      "Loss for  32276 th iteration => 0.00953259792389\n",
      "Loss for  32277 th iteration => 0.00953243955757\n",
      "Loss for  32278 th iteration => 0.00953208715957\n",
      "Loss for  32279 th iteration => 0.00953161794534\n",
      "Loss for  32280 th iteration => 0.00953143213001\n",
      "Loss for  32281 th iteration => 0.00953097267109\n",
      "Loss for  32282 th iteration => 0.0095309614051\n",
      "Loss for  32283 th iteration => 0.00953031778012\n",
      "Loss for  32284 th iteration => 0.00952999248409\n",
      "Loss for  32285 th iteration => 0.00952980903875\n",
      "Loss for  32286 th iteration => 0.00952951816432\n",
      "Loss for  32287 th iteration => 0.00952916788731\n",
      "Loss for  32288 th iteration => 0.00952869200426\n",
      "Loss for  32289 th iteration => 0.00952836764329\n",
      "Loss for  32290 th iteration => 0.00952818662086\n",
      "Loss for  32291 th iteration => 0.00952789578607\n",
      "Loss for  32292 th iteration => 0.00952754436155\n",
      "Loss for  32293 th iteration => 0.00952706677223\n",
      "Loss for  32294 th iteration => 0.00952674336669\n",
      "Loss for  32295 th iteration => 0.00952656474462\n",
      "Loss for  32296 th iteration => 0.00952627394291\n",
      "Loss for  32297 th iteration => 0.00952592136049\n",
      "Loss for  32298 th iteration => 0.00952544208384\n",
      "Loss for  32299 th iteration => 0.00952511965315\n",
      "Loss for  32300 th iteration => 0.00952494340969\n",
      "Loss for  32301 th iteration => 0.00952465263475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  32302 th iteration => 0.0095242988846\n",
      "Loss for  32303 th iteration => 0.00952381793887\n",
      "Loss for  32304 th iteration => 0.00952349650155\n",
      "Loss for  32305 th iteration => 0.00952332261572\n",
      "Loss for  32306 th iteration => 0.00952303186154\n",
      "Loss for  32307 th iteration => 0.0095226769343\n",
      "Loss for  32308 th iteration => 0.00952219574499\n",
      "Loss for  32309 th iteration => 0.00952217854547\n",
      "Loss for  32310 th iteration => 0.00952156221213\n",
      "Loss for  32311 th iteration => 0.00952154786381\n",
      "Loss for  32312 th iteration => 0.00952091858618\n",
      "Loss for  32313 th iteration => 0.00952069797661\n",
      "Loss for  32314 th iteration => 0.00952043877316\n",
      "Loss for  32315 th iteration => 0.00952004160922\n",
      "Loss for  32316 th iteration => 0.00951983151597\n",
      "Loss for  32317 th iteration => 0.00951937502115\n",
      "Loss for  32318 th iteration => 0.00951900036175\n",
      "Loss for  32319 th iteration => 0.00951871592347\n",
      "Loss for  32320 th iteration => 0.00951852634566\n",
      "Loss for  32321 th iteration => 0.00951824440455\n",
      "Loss for  32322 th iteration => 0.00951772396494\n",
      "Loss for  32323 th iteration => 0.0095173887112\n",
      "Loss for  32324 th iteration => 0.00951725010992\n",
      "Loss for  32325 th iteration => 0.00951673371741\n",
      "Loss for  32326 th iteration => 0.00951664276337\n",
      "Loss for  32327 th iteration => 0.00951606737008\n",
      "Loss for  32328 th iteration => 0.00951580681304\n",
      "Loss for  32329 th iteration => 0.0095155727335\n",
      "Loss for  32330 th iteration => 0.00951537125248\n",
      "Loss for  32331 th iteration => 0.0095147525365\n",
      "Loss for  32332 th iteration => 0.00951461680265\n",
      "Loss for  32333 th iteration => 0.00951412060813\n",
      "Loss for  32334 th iteration => 0.0095142214123\n",
      "Loss for  32335 th iteration => 0.00951348404276\n",
      "Loss for  32336 th iteration => 0.00951328744803\n",
      "Loss for  32337 th iteration => 0.00951284682797\n",
      "Loss for  32338 th iteration => 0.00951262360166\n",
      "Loss for  32339 th iteration => 0.00951249131579\n",
      "Loss for  32340 th iteration => 0.00951195880181\n",
      "Loss for  32341 th iteration => 0.00951157830143\n",
      "Loss for  32342 th iteration => 0.0095112911867\n",
      "Loss for  32343 th iteration => 0.0095109438429\n",
      "Loss for  32344 th iteration => 0.00951073588224\n",
      "Loss for  32345 th iteration => 0.00951048434912\n",
      "Loss for  32346 th iteration => 0.00950996376723\n",
      "Loss for  32347 th iteration => 0.00950967236266\n",
      "Loss for  32348 th iteration => 0.00950929977828\n",
      "Loss for  32349 th iteration => 0.00950914998511\n",
      "Loss for  32350 th iteration => 0.00950881320563\n",
      "Loss for  32351 th iteration => 0.00950840367491\n",
      "Loss for  32352 th iteration => 0.00950797042152\n",
      "Loss for  32353 th iteration => 0.00950776944808\n",
      "Loss for  32354 th iteration => 0.00950743610152\n",
      "Loss for  32355 th iteration => 0.00950717873105\n",
      "Loss for  32356 th iteration => 0.00950667386617\n",
      "Loss for  32357 th iteration => 0.00950635208247\n",
      "Loss for  32358 th iteration => 0.00950615448831\n",
      "Loss for  32359 th iteration => 0.00950582054608\n",
      "Loss for  32360 th iteration => 0.00950556373009\n",
      "Loss for  32361 th iteration => 0.00950505603832\n",
      "Loss for  32362 th iteration => 0.00950473434004\n",
      "Loss for  32363 th iteration => 0.00950454005251\n",
      "Loss for  32364 th iteration => 0.00950420553779\n",
      "Loss for  32365 th iteration => 0.00950394921312\n",
      "Loss for  32366 th iteration => 0.00950343874827\n",
      "Loss for  32367 th iteration => 0.00950311719166\n",
      "Loss for  32368 th iteration => 0.00950292616389\n",
      "Loss for  32369 th iteration => 0.00950277903641\n",
      "Loss for  32370 th iteration => 0.00950215115108\n",
      "Loss for  32371 th iteration => 0.00950200127128\n",
      "Loss for  32372 th iteration => 0.00950151828183\n",
      "Loss for  32373 th iteration => 0.00950147644387\n",
      "Loss for  32374 th iteration => 0.00950100401474\n",
      "Loss for  32375 th iteration => 0.0095006772048\n",
      "Loss for  32376 th iteration => 0.00950024828873\n",
      "Loss for  32377 th iteration => 0.00950001423476\n",
      "Loss for  32378 th iteration => 0.00949975680049\n",
      "Loss for  32379 th iteration => 0.00949947539098\n",
      "Loss for  32380 th iteration => 0.00949898171369\n",
      "Loss for  32381 th iteration => 0.00949868879268\n",
      "Loss for  32382 th iteration => 0.00949834647364\n",
      "Loss for  32383 th iteration => 0.00949817158507\n",
      "Loss for  32384 th iteration => 0.00949784071707\n",
      "Loss for  32385 th iteration => 0.00949736662271\n",
      "Loss for  32386 th iteration => 0.00949707867311\n",
      "Loss for  32387 th iteration => 0.00949670321906\n",
      "Loss for  32388 th iteration => 0.00949659367964\n",
      "Loss for  32389 th iteration => 0.00949617225302\n",
      "Loss for  32390 th iteration => 0.00949581201787\n",
      "Loss for  32391 th iteration => 0.00949538098776\n",
      "Loss for  32392 th iteration => 0.00949517652446\n",
      "Loss for  32393 th iteration => 0.00949488637221\n",
      "Loss for  32394 th iteration => 0.00949454391998\n",
      "Loss for  32395 th iteration => 0.00949408923325\n",
      "Loss for  32396 th iteration => 0.00949376662284\n",
      "Loss for  32397 th iteration => 0.00949356550842\n",
      "Loss for  32398 th iteration => 0.00949327572603\n",
      "Loss for  32399 th iteration => 0.00949293281281\n",
      "Loss for  32400 th iteration => 0.00949247561354\n",
      "Loss for  32401 th iteration => 0.00949215285791\n",
      "Loss for  32402 th iteration => 0.00949195503236\n",
      "Loss for  32403 th iteration => 0.00949166559983\n",
      "Loss for  32404 th iteration => 0.00949132218953\n",
      "Loss for  32405 th iteration => 0.00949086252899\n",
      "Loss for  32406 th iteration => 0.00949053969016\n",
      "Loss for  32407 th iteration => 0.00949034509576\n",
      "Loss for  32408 th iteration => 0.00949005599392\n",
      "Loss for  32409 th iteration => 0.009489712052\n",
      "Loss for  32410 th iteration => 0.00948924997948\n",
      "Loss for  32411 th iteration => 0.00948892711689\n",
      "Loss for  32412 th iteration => 0.00948873569812\n",
      "Loss for  32413 th iteration => 0.00948844690857\n",
      "Loss for  32414 th iteration => 0.00948810240198\n",
      "Loss for  32415 th iteration => 0.00948763796489\n",
      "Loss for  32416 th iteration => 0.0094873151355\n",
      "Loss for  32417 th iteration => 0.00948712683894\n",
      "Loss for  32418 th iteration => 0.00948683834404\n",
      "Loss for  32419 th iteration => 0.00948649324116\n",
      "Loss for  32420 th iteration => 0.00948602648509\n",
      "Loss for  32421 th iteration => 0.00948570374347\n",
      "Loss for  32422 th iteration => 0.00948551851773\n",
      "Loss for  32423 th iteration => 0.00948523030057\n",
      "Loss for  32424 th iteration => 0.00948488457114\n",
      "Loss for  32425 th iteration => 0.00948441553994\n",
      "Loss for  32426 th iteration => 0.0094840929384\n",
      "Loss for  32427 th iteration => 0.00948391073402\n",
      "Loss for  32428 th iteration => 0.00948362277838\n",
      "Loss for  32429 th iteration => 0.00948327639346\n",
      "Loss for  32430 th iteration => 0.0094828051293\n",
      "Loss for  32431 th iteration => 0.00948248271795\n",
      "Loss for  32432 th iteration => 0.00948230348735\n",
      "Loss for  32433 th iteration => 0.00948201577765\n",
      "Loss for  32434 th iteration => 0.00948166870959\n",
      "Loss for  32435 th iteration => 0.00948119525302\n",
      "Loss for  32436 th iteration => 0.00948087307987\n",
      "Loss for  32437 th iteration => 0.00948069677726\n",
      "Loss for  32438 th iteration => 0.00948040929859\n",
      "Loss for  32439 th iteration => 0.00948006152092\n",
      "Loss for  32440 th iteration => 0.00947958591098\n",
      "Loss for  32441 th iteration => 0.00947926402201\n",
      "Loss for  32442 th iteration => 0.0094790906033\n",
      "Loss for  32443 th iteration => 0.00947880334135\n",
      "Loss for  32444 th iteration => 0.00947845482879\n",
      "Loss for  32445 th iteration => 0.00947797764938\n",
      "Loss for  32446 th iteration => 0.00947780407844\n",
      "Loss for  32447 th iteration => 0.0094774974723\n",
      "Loss for  32448 th iteration => 0.00947733306874\n",
      "Loss for  32449 th iteration => 0.00947671300434\n",
      "Loss for  32450 th iteration => 0.00947649031314\n",
      "Loss for  32451 th iteration => 0.00947607799428\n",
      "Loss for  32452 th iteration => 0.00947598912666\n",
      "Loss for  32453 th iteration => 0.00947545080454\n",
      "Loss for  32454 th iteration => 0.00947536421123\n",
      "Loss for  32455 th iteration => 0.00947481276331\n",
      "Loss for  32456 th iteration => 0.00947452048396\n",
      "Loss for  32457 th iteration => 0.00947433958276\n",
      "Loss for  32458 th iteration => 0.00947387059753\n",
      "Loss for  32459 th iteration => 0.00947373793247\n",
      "Loss for  32460 th iteration => 0.00947321007802\n",
      "Loss for  32461 th iteration => 0.0094729127642\n",
      "Loss for  32462 th iteration => 0.00947255686907\n",
      "Loss for  32463 th iteration => 0.00947244551138\n",
      "Loss for  32464 th iteration => 0.00947209222633\n",
      "Loss for  32465 th iteration => 0.00947164848581\n",
      "Loss for  32466 th iteration => 0.00947124213379\n",
      "Loss for  32467 th iteration => 0.00947101838385\n",
      "Loss for  32468 th iteration => 0.00947076618856\n",
      "Loss for  32469 th iteration => 0.00947047417633\n",
      "Loss for  32470 th iteration => 0.00946995599062\n",
      "Loss for  32471 th iteration => 0.00946963592057\n",
      "Loss for  32472 th iteration => 0.00946941486067\n",
      "Loss for  32473 th iteration => 0.00946916327133\n",
      "Loss for  32474 th iteration => 0.00946887170973\n",
      "Loss for  32475 th iteration => 0.00946835226294\n",
      "Loss for  32476 th iteration => 0.00946822393267\n",
      "Loss for  32477 th iteration => 0.00946782924688\n",
      "Loss for  32478 th iteration => 0.00946773378582\n",
      "Loss for  32479 th iteration => 0.00946709647166\n",
      "Loss for  32480 th iteration => 0.00946690357153\n",
      "Loss for  32481 th iteration => 0.00946646659174\n",
      "Loss for  32482 th iteration => 0.00946635124329\n",
      "Loss for  32483 th iteration => 0.00946601447794\n",
      "Loss for  32484 th iteration => 0.00946558703891\n",
      "Loss for  32485 th iteration => 0.00946520961494\n",
      "Loss for  32486 th iteration => 0.0094649257776\n",
      "Loss for  32487 th iteration => 0.00946469068209\n",
      "Loss for  32488 th iteration => 0.00946444766124\n",
      "Loss for  32489 th iteration => 0.00946395357503\n",
      "Loss for  32490 th iteration => 0.00946361013388\n",
      "Loss for  32491 th iteration => 0.00946332259865\n",
      "Loss for  32492 th iteration => 0.00946295464093\n",
      "Loss for  32493 th iteration => 0.00946298848167\n",
      "Loss for  32494 th iteration => 0.00946229549875\n",
      "Loss for  32495 th iteration => 0.00946206648739\n",
      "Loss for  32496 th iteration => 0.0094616349214\n",
      "Loss for  32497 th iteration => 0.00946143990006\n",
      "Loss for  32498 th iteration => 0.00946128953387\n",
      "Loss for  32499 th iteration => 0.00946067213351\n",
      "Loss for  32500 th iteration => 0.00946034955771\n",
      "Loss for  32501 th iteration => 0.00946003141445\n",
      "Loss for  32502 th iteration => 0.00945983974727\n",
      "Loss for  32503 th iteration => 0.00945969325767\n",
      "Loss for  32504 th iteration => 0.00945907213298\n",
      "Loss for  32505 th iteration => 0.00945892479945\n",
      "Loss for  32506 th iteration => 0.00945844490688\n",
      "Loss for  32507 th iteration => 0.00945840246476\n",
      "Loss for  32508 th iteration => 0.00945793510677\n",
      "Loss for  32509 th iteration => 0.00945761189032\n",
      "Loss for  32510 th iteration => 0.00945718688639\n",
      "Loss for  32511 th iteration => 0.00945695355674\n",
      "Loss for  32512 th iteration => 0.00945669816798\n",
      "Loss for  32513 th iteration => 0.00945642009991\n",
      "Loss for  32514 th iteration => 0.00945593200706\n",
      "Loss for  32515 th iteration => 0.00945564028079\n",
      "Loss for  32516 th iteration => 0.00945530153761\n",
      "Loss for  32517 th iteration => 0.00945498640284\n",
      "Loss for  32518 th iteration => 0.00945481888088\n",
      "Loss for  32519 th iteration => 0.00945445133442\n",
      "Loss for  32520 th iteration => 0.00945404665332\n",
      "Loss for  32521 th iteration => 0.00945367026203\n",
      "Loss for  32522 th iteration => 0.00945342013586\n",
      "Loss for  32523 th iteration => 0.00945316210073\n",
      "Loss for  32524 th iteration => 0.00945291799148\n",
      "Loss for  32525 th iteration => 0.00945236094666\n",
      "Loss for  32526 th iteration => 0.00945204364388\n",
      "Loss for  32527 th iteration => 0.0094518405958\n",
      "Loss for  32528 th iteration => 0.00945155224522\n",
      "Loss for  32529 th iteration => 0.00945121176413\n",
      "Loss for  32530 th iteration => 0.00945076125095\n",
      "Loss for  32531 th iteration => 0.00945044300579\n",
      "Loss for  32532 th iteration => 0.00945024385009\n",
      "Loss for  32533 th iteration => 0.00944995607412\n",
      "Loss for  32534 th iteration => 0.00944961558661\n",
      "Loss for  32535 th iteration => 0.00944916208192\n",
      "Loss for  32536 th iteration => 0.00944884298608\n",
      "Loss for  32537 th iteration => 0.00944864764\n",
      "Loss for  32538 th iteration => 0.0094483604105\n",
      "Loss for  32539 th iteration => 0.00944801986477\n",
      "Loss for  32540 th iteration => 0.00944756343947\n",
      "Loss for  32541 th iteration => 0.00944724358091\n",
      "Loss for  32542 th iteration => 0.00944705196488\n",
      "Loss for  32543 th iteration => 0.00944676525484\n",
      "Loss for  32544 th iteration => 0.00944642460128\n",
      "Loss for  32545 th iteration => 0.00944596532354\n",
      "Loss for  32546 th iteration => 0.00944564478661\n",
      "Loss for  32547 th iteration => 0.00944545735797\n",
      "Loss for  32548 th iteration => 0.00944500982058\n",
      "Loss for  32549 th iteration => 0.00944485156233\n",
      "Loss for  32550 th iteration => 0.00944450676671\n",
      "Loss for  32551 th iteration => 0.00944404650634\n",
      "Loss for  32552 th iteration => 0.00944386355792\n",
      "Loss for  32553 th iteration => 0.00944341175063\n",
      "Loss for  32554 th iteration => 0.00944325754829\n",
      "Loss for  32555 th iteration => 0.00944291259966\n",
      "Loss for  32556 th iteration => 0.0094424493102\n",
      "Loss for  32557 th iteration => 0.00944213290795\n",
      "Loss for  32558 th iteration => 0.00944195063901\n",
      "Loss for  32559 th iteration => 0.00944166405338\n",
      "Loss for  32560 th iteration => 0.00944131893965\n",
      "Loss for  32561 th iteration => 0.0094408528901\n",
      "Loss for  32562 th iteration => 0.0094405359115\n",
      "Loss for  32563 th iteration => 0.00944035719117\n",
      "Loss for  32564 th iteration => 0.00944007106703\n",
      "Loss for  32565 th iteration => 0.00943972574333\n",
      "Loss for  32566 th iteration => 0.0094392569958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  32567 th iteration => 0.00943893951728\n",
      "Loss for  32568 th iteration => 0.00943876427583\n",
      "Loss for  32569 th iteration => 0.00943847858966\n",
      "Loss for  32570 th iteration => 0.00943813301301\n",
      "Loss for  32571 th iteration => 0.00943766162719\n",
      "Loss for  32572 th iteration => 0.00943734372207\n",
      "Loss for  32573 th iteration => 0.0094371718924\n",
      "Loss for  32574 th iteration => 0.00943688662165\n",
      "Loss for  32575 th iteration => 0.00943654075089\n",
      "Loss for  32576 th iteration => 0.00943606780405\n",
      "Loss for  32577 th iteration => 0.00943589625468\n",
      "Loss for  32578 th iteration => 0.00943559308834\n",
      "Loss for  32579 th iteration => 0.0094354293192\n",
      "Loss for  32580 th iteration => 0.00943481448093\n",
      "Loss for  32581 th iteration => 0.00943459290327\n",
      "Loss for  32582 th iteration => 0.00943418501512\n",
      "Loss for  32583 th iteration => 0.0094340970165\n",
      "Loss for  32584 th iteration => 0.00943356324457\n",
      "Loss for  32585 th iteration => 0.00943347752832\n",
      "Loss for  32586 th iteration => 0.00943293124783\n",
      "Loss for  32587 th iteration => 0.00943263913776\n",
      "Loss for  32588 th iteration => 0.00943230607646\n",
      "Loss for  32589 th iteration => 0.00943214984465\n",
      "Loss for  32590 th iteration => 0.0094318680864\n",
      "Loss for  32591 th iteration => 0.00943133997043\n",
      "Loss for  32592 th iteration => 0.00943104822851\n",
      "Loss for  32593 th iteration => 0.00943069087481\n",
      "Loss for  32594 th iteration => 0.00943058565018\n",
      "Loss for  32595 th iteration => 0.009430233923\n",
      "Loss for  32596 th iteration => 0.00942979522267\n",
      "Loss for  32597 th iteration => 0.00942938844915\n",
      "Loss for  32598 th iteration => 0.00942916882641\n",
      "Loss for  32599 th iteration => 0.00942891872729\n",
      "Loss for  32600 th iteration => 0.00942863195937\n",
      "Loss for  32601 th iteration => 0.00942811695654\n",
      "Loss for  32602 th iteration => 0.00942798541952\n",
      "Loss for  32603 th iteration => 0.00942759633746\n",
      "Loss for  32604 th iteration => 0.00942750121958\n",
      "Loss for  32605 th iteration => 0.00942687203224\n",
      "Loss for  32606 th iteration => 0.00942667759888\n",
      "Loss for  32607 th iteration => 0.00942624665481\n",
      "Loss for  32608 th iteration => 0.00942602412103\n",
      "Loss for  32609 th iteration => 0.00942590364444\n",
      "Loss for  32610 th iteration => 0.0094253714015\n",
      "Loss for  32611 th iteration => 0.00942500160794\n",
      "Loss for  32612 th iteration => 0.00942471511608\n",
      "Loss for  32613 th iteration => 0.00942437721237\n",
      "Loss for  32614 th iteration => 0.00942417108294\n",
      "Loss for  32615 th iteration => 0.0094239321993\n",
      "Loss for  32616 th iteration => 0.00942341016481\n",
      "Loss for  32617 th iteration => 0.00942313092717\n",
      "Loss for  32618 th iteration => 0.00942275472822\n",
      "Loss for  32619 th iteration => 0.00942261984702\n",
      "Loss for  32620 th iteration => 0.00942228417985\n",
      "Loss for  32621 th iteration => 0.00942188565498\n",
      "Loss for  32622 th iteration => 0.00942145064923\n",
      "Loss for  32623 th iteration => 0.00942126009394\n",
      "Loss for  32624 th iteration => 0.00942082027604\n",
      "Loss for  32625 th iteration => 0.00942079417145\n",
      "Loss for  32626 th iteration => 0.00942018103722\n",
      "Loss for  32627 th iteration => 0.00941986118097\n",
      "Loss for  32628 th iteration => 0.00941954564004\n",
      "Loss for  32629 th iteration => 0.00941935594843\n",
      "Loss for  32630 th iteration => 0.00941902770018\n",
      "Loss for  32631 th iteration => 0.00941877636296\n",
      "Loss for  32632 th iteration => 0.0094182739328\n",
      "Loss for  32633 th iteration => 0.00941813657485\n",
      "Loss for  32634 th iteration => 0.00941778711547\n",
      "Loss for  32635 th iteration => 0.00941760258577\n",
      "Loss for  32636 th iteration => 0.00941703018511\n",
      "Loss for  32637 th iteration => 0.00941683122798\n",
      "Loss for  32638 th iteration => 0.00941640532019\n",
      "Loss for  32639 th iteration => 0.0094161818669\n",
      "Loss for  32640 th iteration => 0.00941592473904\n",
      "Loss for  32641 th iteration => 0.00941564898225\n",
      "Loss for  32642 th iteration => 0.00941516152299\n",
      "Loss for  32643 th iteration => 0.00941487537981\n",
      "Loss for  32644 th iteration => 0.00941453900879\n",
      "Loss for  32645 th iteration => 0.00941436857383\n",
      "Loss for  32646 th iteration => 0.00941404206733\n",
      "Loss for  32647 th iteration => 0.00941357487167\n",
      "Loss for  32648 th iteration => 0.00941329268065\n",
      "Loss for  32649 th iteration => 0.00941292227028\n",
      "Loss for  32650 th iteration => 0.00941281861089\n",
      "Loss for  32651 th iteration => 0.00941240186005\n",
      "Loss for  32652 th iteration => 0.00941204873108\n",
      "Loss for  32653 th iteration => 0.00941162186303\n",
      "Loss for  32654 th iteration => 0.00941142375689\n",
      "Loss for  32655 th iteration => 0.0094109906751\n",
      "Loss for  32656 th iteration => 0.00941082215277\n",
      "Loss for  32657 th iteration => 0.0094104822786\n",
      "Loss for  32658 th iteration => 0.00941003414557\n",
      "Loss for  32659 th iteration => 0.00940972008107\n",
      "Loss for  32660 th iteration => 0.00940952434094\n",
      "Loss for  32661 th iteration => 0.00940923951374\n",
      "Loss for  32662 th iteration => 0.00940890002422\n",
      "Loss for  32663 th iteration => 0.00940844849786\n",
      "Loss for  32664 th iteration => 0.00940813298225\n",
      "Loss for  32665 th iteration => 0.00940794148829\n",
      "Loss for  32666 th iteration => 0.0094076573712\n",
      "Loss for  32667 th iteration => 0.00940731820139\n",
      "Loss for  32668 th iteration => 0.00940686336904\n",
      "Loss for  32669 th iteration => 0.00940654651199\n",
      "Loss for  32670 th iteration => 0.00940635916634\n",
      "Loss for  32671 th iteration => 0.00940607572578\n",
      "Loss for  32672 th iteration => 0.00940573681343\n",
      "Loss for  32673 th iteration => 0.00940527875904\n",
      "Loss for  32674 th iteration => 0.00940496066577\n",
      "Loss for  32675 th iteration => 0.00940477862543\n",
      "Loss for  32676 th iteration => 0.00940433050672\n",
      "Loss for  32677 th iteration => 0.00940417824365\n",
      "Loss for  32678 th iteration => 0.00940383563816\n",
      "Loss for  32679 th iteration => 0.0094033759191\n",
      "Loss for  32680 th iteration => 0.00940306187935\n",
      "Loss for  32681 th iteration => 0.00940288118209\n",
      "Loss for  32682 th iteration => 0.00940259772633\n",
      "Loss for  32683 th iteration => 0.00940225533955\n",
      "Loss for  32684 th iteration => 0.00940179246155\n",
      "Loss for  32685 th iteration => 0.00940147725464\n",
      "Loss for  32686 th iteration => 0.00940130053989\n",
      "Loss for  32687 th iteration => 0.00940101770684\n",
      "Loss for  32688 th iteration => 0.00940067548089\n",
      "Loss for  32689 th iteration => 0.00940020952231\n",
      "Loss for  32690 th iteration => 0.00939989324617\n",
      "Loss for  32691 th iteration => 0.0093997204261\n",
      "Loss for  32692 th iteration => 0.00939943818575\n",
      "Loss for  32693 th iteration => 0.00939909606514\n",
      "Loss for  32694 th iteration => 0.0093986271013\n",
      "Loss for  32695 th iteration => 0.0093983098499\n",
      "Loss for  32696 th iteration => 0.00939814152374\n",
      "Loss for  32697 th iteration => 0.00939768077493\n",
      "Loss for  32698 th iteration => 0.00939769596677\n",
      "Loss for  32699 th iteration => 0.00939704811289\n",
      "Loss for  32700 th iteration => 0.0093968716337\n",
      "Loss for  32701 th iteration => 0.0093965790047\n",
      "Loss for  32702 th iteration => 0.00939622991329\n",
      "Loss for  32703 th iteration => 0.00939598871632\n",
      "Loss for  32704 th iteration => 0.00939558095091\n",
      "Loss for  32705 th iteration => 0.00939518056936\n",
      "Loss for  32706 th iteration => 0.00939493489574\n",
      "Loss for  32707 th iteration => 0.00939471546344\n",
      "Loss for  32708 th iteration => 0.00939447818104\n",
      "Loss for  32709 th iteration => 0.00939393780587\n",
      "Loss for  32710 th iteration => 0.00939364243112\n",
      "Loss for  32711 th iteration => 0.00939331385479\n",
      "Loss for  32712 th iteration => 0.00939315652963\n",
      "Loss for  32713 th iteration => 0.00939269690388\n",
      "Loss for  32714 th iteration => 0.0093925430492\n",
      "Loss for  32715 th iteration => 0.00939207045247\n",
      "Loss for  32716 th iteration => 0.00939170528575\n",
      "Loss for  32717 th iteration => 0.00939145064407\n",
      "Loss for  32718 th iteration => 0.00939122563198\n",
      "Loss for  32719 th iteration => 0.00939102270975\n",
      "Loss for  32720 th iteration => 0.0093904205035\n",
      "Loss for  32721 th iteration => 0.00939029287799\n",
      "Loss for  32722 th iteration => 0.0093899043357\n",
      "Loss for  32723 th iteration => 0.00938981024955\n",
      "Loss for  32724 th iteration => 0.0093891857792\n",
      "Loss for  32725 th iteration => 0.00938899358912\n",
      "Loss for  32726 th iteration => 0.00938856553406\n",
      "Loss for  32727 th iteration => 0.00938834590314\n",
      "Loss for  32728 th iteration => 0.00938822477884\n",
      "Loss for  32729 th iteration => 0.00938769706504\n",
      "Loss for  32730 th iteration => 0.00938733066758\n",
      "Loss for  32731 th iteration => 0.00938704566367\n",
      "Loss for  32732 th iteration => 0.00938671140754\n",
      "Loss for  32733 th iteration => 0.00938650536907\n",
      "Loss for  32734 th iteration => 0.00938627039038\n",
      "Loss for  32735 th iteration => 0.00938575039883\n",
      "Loss for  32736 th iteration => 0.00938547526935\n",
      "Loss for  32737 th iteration => 0.00938509956233\n",
      "Loss for  32738 th iteration => 0.00938486000319\n",
      "Loss for  32739 th iteration => 0.00938474085361\n",
      "Loss for  32740 th iteration => 0.0093842401297\n",
      "Loss for  32741 th iteration => 0.00938380718763\n",
      "Loss for  32742 th iteration => 0.00938349349682\n",
      "Loss for  32743 th iteration => 0.00938330324166\n",
      "Loss for  32744 th iteration => 0.00938297710763\n",
      "Loss for  32745 th iteration => 0.0093827280696\n",
      "Loss for  32746 th iteration => 0.00938223021375\n",
      "Loss for  32747 th iteration => 0.00938191462736\n",
      "Loss for  32748 th iteration => 0.00938172968113\n",
      "Loss for  32749 th iteration => 0.0093814023154\n",
      "Loss for  32750 th iteration => 0.00938115583745\n",
      "Loss for  32751 th iteration => 0.00938065742588\n",
      "Loss for  32752 th iteration => 0.00938051384376\n",
      "Loss for  32753 th iteration => 0.00938004241285\n",
      "Loss for  32754 th iteration => 0.00938000316278\n",
      "Loss for  32755 th iteration => 0.00937954131018\n",
      "Loss for  32756 th iteration => 0.00937922115688\n",
      "Loss for  32757 th iteration => 0.00937880398188\n",
      "Loss for  32758 th iteration => 0.00937857333621\n",
      "Loss for  32759 th iteration => 0.00937832735311\n",
      "Loss for  32760 th iteration => 0.00937805015145\n",
      "Loss for  32761 th iteration => 0.00937757022036\n",
      "Loss for  32762 th iteration => 0.00937728000402\n",
      "Loss for  32763 th iteration => 0.00937695037791\n",
      "Loss for  32764 th iteration => 0.00937663512328\n",
      "Loss for  32765 th iteration => 0.00937647976526\n",
      "Loss for  32766 th iteration => 0.00937611277973\n",
      "Loss for  32767 th iteration => 0.00937571657332\n",
      "Loss for  32768 th iteration => 0.00937534047204\n",
      "Loss for  32769 th iteration => 0.00937509829048\n",
      "Loss for  32770 th iteration => 0.00937484398176\n",
      "Loss for  32771 th iteration => 0.00937461110876\n",
      "Loss for  32772 th iteration => 0.00937405389453\n",
      "Loss for  32773 th iteration => 0.00937373908424\n",
      "Loss for  32774 th iteration => 0.00937354441237\n",
      "Loss for  32775 th iteration => 0.00937311263995\n",
      "Loss for  32776 th iteration => 0.0093730787928\n",
      "Loss for  32777 th iteration => 0.00937248014563\n",
      "Loss for  32778 th iteration => 0.00937216348028\n",
      "Loss for  32779 th iteration => 0.00937185173184\n",
      "Loss for  32780 th iteration => 0.0093716600848\n",
      "Loss for  32781 th iteration => 0.00937137789703\n",
      "Loss for  32782 th iteration => 0.00937104085507\n",
      "Loss for  32783 th iteration => 0.00937059035091\n",
      "Loss for  32784 th iteration => 0.00937027664543\n",
      "Loss for  32785 th iteration => 0.00937008960674\n",
      "Loss for  32786 th iteration => 0.00936980826678\n",
      "Loss for  32787 th iteration => 0.00936947186438\n",
      "Loss for  32788 th iteration => 0.00936901773352\n",
      "Loss for  32789 th iteration => 0.0093687021987\n",
      "Loss for  32790 th iteration => 0.00936852099993\n",
      "Loss for  32791 th iteration => 0.0093680764443\n",
      "Loss for  32792 th iteration => 0.00936792520132\n",
      "Loss for  32793 th iteration => 0.00936758557363\n",
      "Loss for  32794 th iteration => 0.00936712929989\n",
      "Loss for  32795 th iteration => 0.00936681709828\n",
      "Loss for  32796 th iteration => 0.00936663766845\n",
      "Loss for  32797 th iteration => 0.00936635667635\n",
      "Loss for  32798 th iteration => 0.00936601762052\n",
      "Loss for  32799 th iteration => 0.00936555781941\n",
      "Loss for  32800 th iteration => 0.00936524390537\n",
      "Loss for  32801 th iteration => 0.00936506885883\n",
      "Loss for  32802 th iteration => 0.00936478863945\n",
      "Loss for  32803 th iteration => 0.00936445008566\n",
      "Loss for  32804 th iteration => 0.00936398685048\n",
      "Loss for  32805 th iteration => 0.00936367134167\n",
      "Loss for  32806 th iteration => 0.0093635021287\n",
      "Loss for  32807 th iteration => 0.00936304638112\n",
      "Loss for  32808 th iteration => 0.00936290748531\n",
      "Loss for  32809 th iteration => 0.00936256554377\n",
      "Loss for  32810 th iteration => 0.0093621003946\n",
      "Loss for  32811 th iteration => 0.00936178849507\n",
      "Loss for  32812 th iteration => 0.00936162058394\n",
      "Loss for  32813 th iteration => 0.00936134055634\n",
      "Loss for  32814 th iteration => 0.00936099906175\n",
      "Loss for  32815 th iteration => 0.00936053367979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  32816 th iteration => 0.00936036273175\n",
      "Loss for  32817 th iteration => 0.00936006867941\n",
      "Loss for  32818 th iteration => 0.00935972545464\n",
      "Loss for  32819 th iteration => 0.00935948340411\n",
      "Loss for  32820 th iteration => 0.00935907908321\n",
      "Loss for  32821 th iteration => 0.00935868087698\n",
      "Loss for  32822 th iteration => 0.00935843940311\n",
      "Loss for  32823 th iteration => 0.00935822194125\n",
      "Loss for  32824 th iteration => 0.00935798552774\n",
      "Loss for  32825 th iteration => 0.00935744789876\n",
      "Loss for  32826 th iteration => 0.00935715471504\n",
      "Loss for  32827 th iteration => 0.00935682994705\n",
      "Loss for  32828 th iteration => 0.00935667402614\n",
      "Loss for  32829 th iteration => 0.00935640802104\n",
      "Loss for  32830 th iteration => 0.00935587599689\n",
      "Loss for  32831 th iteration => 0.0093557788375\n",
      "Loss for  32832 th iteration => 0.00935524765314\n",
      "Loss for  32833 th iteration => 0.00935514767604\n",
      "Loss for  32834 th iteration => 0.00935478593695\n",
      "Loss for  32835 th iteration => 0.00935451079056\n",
      "Loss for  32836 th iteration => 0.00935399791612\n",
      "Loss for  32837 th iteration => 0.00935377277701\n",
      "Loss for  32838 th iteration => 0.00935348636444\n",
      "Loss for  32839 th iteration => 0.00935329907597\n",
      "Loss for  32840 th iteration => 0.00935277230673\n",
      "Loss for  32841 th iteration => 0.00935248466045\n",
      "Loss for  32842 th iteration => 0.00935215664521\n",
      "Loss for  32843 th iteration => 0.00935184040555\n",
      "Loss for  32844 th iteration => 0.00935165060406\n",
      "Loss for  32845 th iteration => 0.00935136912669\n",
      "Loss for  32846 th iteration => 0.00935093091934\n",
      "Loss for  32847 th iteration => 0.0093505518431\n",
      "Loss for  32848 th iteration => 0.00935031546973\n",
      "Loss for  32849 th iteration => 0.00935001681349\n",
      "Loss for  32850 th iteration => 0.00934988193732\n",
      "Loss for  32851 th iteration => 0.00934927402734\n",
      "Loss for  32852 th iteration => 0.00934895890752\n",
      "Loss for  32853 th iteration => 0.00934864698787\n",
      "Loss for  32854 th iteration => 0.00934845849437\n",
      "Loss for  32855 th iteration => 0.00934813384464\n",
      "Loss for  32856 th iteration => 0.00934788754232\n",
      "Loss for  32857 th iteration => 0.00934739342574\n",
      "Loss for  32858 th iteration => 0.00934707903848\n",
      "Loss for  32859 th iteration => 0.00934689643453\n",
      "Loss for  32860 th iteration => 0.00934657036387\n",
      "Loss for  32861 th iteration => 0.0093463272194\n",
      "Loss for  32862 th iteration => 0.00934583252849\n",
      "Loss for  32863 th iteration => 0.0093456883616\n",
      "Loss for  32864 th iteration => 0.00934522039353\n",
      "Loss for  32865 th iteration => 0.00934518110391\n",
      "Loss for  32866 th iteration => 0.00934472517066\n",
      "Loss for  32867 th iteration => 0.00934440463698\n",
      "Loss for  32868 th iteration => 0.00934399307877\n",
      "Loss for  32869 th iteration => 0.00934375952892\n",
      "Loss for  32870 th iteration => 0.00934338233522\n",
      "Loss for  32871 th iteration => 0.00934325840907\n",
      "Loss for  32872 th iteration => 0.00934288940016\n",
      "Loss for  32873 th iteration => 0.00934247692097\n",
      "Loss for  32874 th iteration => 0.0093421535036\n",
      "Loss for  32875 th iteration => 0.00934183383005\n",
      "Loss for  32876 th iteration => 0.00934168592579\n",
      "Loss for  32877 th iteration => 0.00934131911862\n",
      "Loss for  32878 th iteration => 0.00934092897476\n",
      "Loss for  32879 th iteration => 0.00934055089771\n",
      "Loss for  32880 th iteration => 0.00934031377594\n",
      "Loss for  32881 th iteration => 0.00933991088079\n",
      "Loss for  32882 th iteration => 0.00933985200947\n",
      "Loss for  32883 th iteration => 0.00933940365925\n",
      "Loss for  32884 th iteration => 0.00933896219392\n",
      "Loss for  32885 th iteration => 0.00933865105202\n",
      "Loss for  32886 th iteration => 0.00933845833907\n",
      "Loss for  32887 th iteration => 0.00933817855938\n",
      "Loss for  32888 th iteration => 0.00933784556631\n",
      "Loss for  32889 th iteration => 0.00933740000068\n",
      "Loss for  32890 th iteration => 0.0093370863605\n",
      "Loss for  32891 th iteration => 0.00933690037609\n",
      "Loss for  32892 th iteration => 0.00933646444559\n",
      "Loss for  32893 th iteration => 0.00933630806011\n",
      "Loss for  32894 th iteration => 0.00933597235985\n",
      "Loss for  32895 th iteration => 0.00933552409061\n",
      "Loss for  32896 th iteration => 0.00933521297483\n",
      "Loss for  32897 th iteration => 0.00933502900148\n",
      "Loss for  32898 th iteration => 0.00933474998535\n",
      "Loss for  32899 th iteration => 0.00933441524915\n",
      "Loss for  32900 th iteration => 0.00933396302052\n",
      "Loss for  32901 th iteration => 0.00933364957813\n",
      "Loss for  32902 th iteration => 0.00933347212712\n",
      "Loss for  32903 th iteration => 0.00933302821744\n",
      "Loss for  32904 th iteration => 0.0093328807852\n",
      "Loss for  32905 th iteration => 0.00933254322251\n",
      "Loss for  32906 th iteration => 0.00933208845782\n",
      "Loss for  32907 th iteration => 0.00933177774216\n",
      "Loss for  32908 th iteration => 0.00933160216212\n",
      "Loss for  32909 th iteration => 0.00933132379542\n",
      "Loss for  32910 th iteration => 0.00933098710518\n",
      "Loss for  32911 th iteration => 0.00933052851028\n",
      "Loss for  32912 th iteration => 0.0093302156244\n",
      "Loss for  32913 th iteration => 0.00933004599503\n",
      "Loss for  32914 th iteration => 0.00932959483459\n",
      "Loss for  32915 th iteration => 0.0093294558967\n",
      "Loss for  32916 th iteration => 0.00932911627151\n",
      "Loss for  32917 th iteration => 0.00932865529439\n",
      "Loss for  32918 th iteration => 0.00932834531971\n",
      "Loss for  32919 th iteration => 0.009328177815\n",
      "Loss for  32920 th iteration => 0.00932789999358\n",
      "Loss for  32921 th iteration => 0.00932756115765\n",
      "Loss for  32922 th iteration => 0.00932709910964\n",
      "Loss for  32923 th iteration => 0.00932692868884\n",
      "Loss for  32924 th iteration => 0.00932663732264\n",
      "Loss for  32925 th iteration => 0.00932629541743\n",
      "Loss for  32926 th iteration => 0.00932605645831\n",
      "Loss for  32927 th iteration => 0.00932565446631\n",
      "Loss for  32928 th iteration => 0.00932525971445\n",
      "Loss for  32929 th iteration => 0.00932501767429\n",
      "Loss for  32930 th iteration => 0.00932480283776\n",
      "Loss for  32931 th iteration => 0.00932456864644\n",
      "Loss for  32932 th iteration => 0.00932403563841\n",
      "Loss for  32933 th iteration => 0.00932374304483\n",
      "Loss for  32934 th iteration => 0.00932342039168\n",
      "Loss for  32935 th iteration => 0.00932310874281\n",
      "Loss for  32936 th iteration => 0.00932296976612\n",
      "Loss for  32937 th iteration => 0.00932266168724\n",
      "Loss for  32938 th iteration => 0.0093221965893\n",
      "Loss for  32939 th iteration => 0.00932201903073\n",
      "Loss for  32940 th iteration => 0.00932159942492\n",
      "Loss for  32941 th iteration => 0.00932152840529\n",
      "Loss for  32942 th iteration => 0.00932100108941\n",
      "Loss for  32943 th iteration => 0.00932071705728\n",
      "Loss for  32944 th iteration => 0.00932029639689\n",
      "Loss for  32945 th iteration => 0.00932007585675\n",
      "Loss for  32946 th iteration => 0.00931979146143\n",
      "Loss for  32947 th iteration => 0.00931960428264\n",
      "Loss for  32948 th iteration => 0.0093190798332\n",
      "Loss for  32949 th iteration => 0.00931879541648\n",
      "Loss for  32950 th iteration => 0.00931846872281\n",
      "Loss for  32951 th iteration => 0.00931815621481\n",
      "Loss for  32952 th iteration => 0.00931814098389\n",
      "Loss for  32953 th iteration => 0.00931751756367\n",
      "Loss for  32954 th iteration => 0.00931725199957\n",
      "Loss for  32955 th iteration => 0.00931687556374\n",
      "Loss for  32956 th iteration => 0.00931664105987\n",
      "Loss for  32957 th iteration => 0.00931634628935\n",
      "Loss for  32958 th iteration => 0.00931621191387\n",
      "Loss for  32959 th iteration => 0.00931560589476\n",
      "Loss for  32960 th iteration => 0.00931529300998\n",
      "Loss for  32961 th iteration => 0.00931498350547\n",
      "Loss for  32962 th iteration => 0.00931479724682\n",
      "Loss for  32963 th iteration => 0.00931447580507\n",
      "Loss for  32964 th iteration => 0.00931423082545\n",
      "Loss for  32965 th iteration => 0.00931373958585\n",
      "Loss for  32966 th iteration => 0.00931360199338\n",
      "Loss for  32967 th iteration => 0.00931326305577\n",
      "Loss for  32968 th iteration => 0.00931296805447\n",
      "Loss for  32969 th iteration => 0.00931263847901\n",
      "Loss for  32970 th iteration => 0.00931232608751\n",
      "Loss for  32971 th iteration => 0.00931191315727\n",
      "Loss for  32972 th iteration => 0.00931168553743\n",
      "Loss for  32973 th iteration => 0.00931144220109\n",
      "Loss for  32974 th iteration => 0.00931105237126\n",
      "Loss for  32975 th iteration => 0.00931081611809\n",
      "Loss for  32976 th iteration => 0.00931041061156\n",
      "Loss for  32977 th iteration => 0.00931008656346\n",
      "Loss for  32978 th iteration => 0.00930977161453\n",
      "Loss for  32979 th iteration => 0.00930962101429\n",
      "Loss for  32980 th iteration => 0.00930925932351\n",
      "Loss for  32981 th iteration => 0.00930887063308\n",
      "Loss for  32982 th iteration => 0.00930849686444\n",
      "Loss for  32983 th iteration => 0.00930825976258\n",
      "Loss for  32984 th iteration => 0.00930786025129\n",
      "Loss for  32985 th iteration => 0.00930780002308\n",
      "Loss for  32986 th iteration => 0.00930735673674\n",
      "Loss for  32987 th iteration => 0.00930691861879\n",
      "Loss for  32988 th iteration => 0.00930660917251\n",
      "Loss for  32989 th iteration => 0.00930641635507\n",
      "Loss for  32990 th iteration => 0.00930599046864\n",
      "Loss for  32991 th iteration => 0.00930595900701\n",
      "Loss for  32992 th iteration => 0.00930536694859\n",
      "Loss for  32993 th iteration => 0.00930505474212\n",
      "Loss for  32994 th iteration => 0.00930474707435\n",
      "Loss for  32995 th iteration => 0.0093045582621\n",
      "Loss for  32996 th iteration => 0.00930428119358\n",
      "Loss for  32997 th iteration => 0.00930394942924\n",
      "Loss for  32998 th iteration => 0.00930350367649\n",
      "Loss for  32999 th iteration => 0.00930319308074\n",
      "Loss for  33000 th iteration => 0.00930301123599\n",
      "Loss for  33001 th iteration => 0.00930257475906\n",
      "Loss for  33002 th iteration => 0.00930242404436\n",
      "Loss for  33003 th iteration => 0.00930208991957\n",
      "Loss for  33004 th iteration => 0.00930164113301\n",
      "Loss for  33005 th iteration => 0.00930133253637\n",
      "Loss for  33006 th iteration => 0.00930115312746\n",
      "Loss for  33007 th iteration => 0.00930087707735\n",
      "Loss for  33008 th iteration => 0.00930054417752\n",
      "Loss for  33009 th iteration => 0.00930009117718\n",
      "Loss for  33010 th iteration => 0.0092997798576\n",
      "Loss for  33011 th iteration => 0.0092996081462\n",
      "Loss for  33012 th iteration => 0.00929916204556\n",
      "Loss for  33013 th iteration => 0.00929902120518\n",
      "Loss for  33014 th iteration => 0.00929868580196\n",
      "Loss for  33015 th iteration => 0.00929822996471\n",
      "Loss for  33016 th iteration => 0.00929792088537\n",
      "Loss for  33017 th iteration => 0.00929775076025\n",
      "Loss for  33018 th iteration => 0.00929730364922\n",
      "Loss for  33019 th iteration => 0.00929731352397\n",
      "Loss for  33020 th iteration => 0.00929668206059\n",
      "Loss for  33021 th iteration => 0.00929651090917\n",
      "Loss for  33022 th iteration => 0.00929607528001\n",
      "Loss for  33023 th iteration => 0.00929602413132\n",
      "Loss for  33024 th iteration => 0.00929564450595\n",
      "Loss for  33025 th iteration => 0.00929524612036\n",
      "Loss for  33026 th iteration => 0.00929485482997\n",
      "Loss for  33027 th iteration => 0.00929461126841\n",
      "Loss for  33028 th iteration => 0.00929439803826\n",
      "Loss for  33029 th iteration => 0.00929398121194\n",
      "Loss for  33030 th iteration => 0.00929382212719\n",
      "Loss for  33031 th iteration => 0.00929334637694\n",
      "Loss for  33032 th iteration => 0.00929302771862\n",
      "Loss for  33033 th iteration => 0.00929271338651\n",
      "Loss for  33034 th iteration => 0.0092925750902\n",
      "Loss for  33035 th iteration => 0.00929208410365\n",
      "Loss for  33036 th iteration => 0.00929199926482\n",
      "Loss for  33037 th iteration => 0.00929144798069\n",
      "Loss for  33038 th iteration => 0.00929120074377\n",
      "Loss for  33039 th iteration => 0.0092908174586\n",
      "Loss for  33040 th iteration => 0.00929075301507\n",
      "Loss for  33041 th iteration => 0.00929038575037\n",
      "Loss for  33042 th iteration => 0.00928988589733\n",
      "Loss for  33043 th iteration => 0.00928975397759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  33044 th iteration => 0.00928928348735\n",
      "Loss for  33045 th iteration => 0.00928938088258\n",
      "Loss for  33046 th iteration => 0.00928867797643\n",
      "Loss for  33047 th iteration => 0.00928848248607\n",
      "Loss for  33048 th iteration => 0.00928807123466\n",
      "Loss for  33049 th iteration => 0.00928784377695\n",
      "Loss for  33050 th iteration => 0.00928757062591\n",
      "Loss for  33051 th iteration => 0.00928737986574\n",
      "Loss for  33052 th iteration => 0.00928686317718\n",
      "Loss for  33053 th iteration => 0.00928657333625\n",
      "Loss for  33054 th iteration => 0.00928625634574\n",
      "Loss for  33055 th iteration => 0.00928593588677\n",
      "Loss for  33056 th iteration => 0.00928576080599\n",
      "Loss for  33057 th iteration => 0.00928547619654\n",
      "Loss for  33058 th iteration => 0.00928504814285\n",
      "Loss for  33059 th iteration => 0.00928466594533\n",
      "Loss for  33060 th iteration => 0.00928444123441\n",
      "Loss for  33061 th iteration => 0.00928403039584\n",
      "Loss for  33062 th iteration => 0.00928395085225\n",
      "Loss for  33063 th iteration => 0.00928358514823\n",
      "Loss for  33064 th iteration => 0.00928309787254\n",
      "Loss for  33065 th iteration => 0.00928278713823\n",
      "Loss for  33066 th iteration => 0.00928248041903\n",
      "Loss for  33067 th iteration => 0.00928230083931\n",
      "Loss for  33068 th iteration => 0.00928215934519\n",
      "Loss for  33069 th iteration => 0.00928155893521\n",
      "Loss for  33070 th iteration => 0.00928141494942\n",
      "Loss for  33071 th iteration => 0.0092809525107\n",
      "Loss for  33072 th iteration => 0.00928078280521\n",
      "Loss for  33073 th iteration => 0.00928060335004\n",
      "Loss for  33074 th iteration => 0.00928014759037\n",
      "Loss for  33075 th iteration => 0.00927974528052\n",
      "Loss for  33076 th iteration => 0.0092795108013\n",
      "Loss for  33077 th iteration => 0.00927913878587\n",
      "Loss for  33078 th iteration => 0.009278879032\n",
      "Loss for  33079 th iteration => 0.00927867798275\n",
      "Loss for  33080 th iteration => 0.00927836460033\n",
      "Loss for  33081 th iteration => 0.00927793143735\n",
      "Loss for  33082 th iteration => 0.00927760834213\n",
      "Loss for  33083 th iteration => 0.00927732521145\n",
      "Loss for  33084 th iteration => 0.00927712086463\n",
      "Loss for  33085 th iteration => 0.00927684997761\n",
      "Loss for  33086 th iteration => 0.00927634333571\n",
      "Loss for  33087 th iteration => 0.00927611733989\n",
      "Loss for  33088 th iteration => 0.00927570763809\n",
      "Loss for  33089 th iteration => 0.00927551278319\n",
      "Loss for  33090 th iteration => 0.00927523836948\n",
      "Loss for  33091 th iteration => 0.00927490893321\n",
      "Loss for  33092 th iteration => 0.00927447003402\n",
      "Loss for  33093 th iteration => 0.00927416255962\n",
      "Loss for  33094 th iteration => 0.00927397521913\n",
      "Loss for  33095 th iteration => 0.00927354660037\n",
      "Loss for  33096 th iteration => 0.00927339267417\n",
      "Loss for  33097 th iteration => 0.00927306133513\n",
      "Loss for  33098 th iteration => 0.00927261891118\n",
      "Loss for  33099 th iteration => 0.00927231272152\n",
      "Loss for  33100 th iteration => 0.00927212873167\n",
      "Loss for  33101 th iteration => 0.00927169732192\n",
      "Loss for  33102 th iteration => 0.00927168407519\n",
      "Loss for  33103 th iteration => 0.00927107845443\n",
      "Loss for  33104 th iteration => 0.00927076894368\n",
      "Loss for  33105 th iteration => 0.00927059411676\n",
      "Loss for  33106 th iteration => 0.00927015340738\n",
      "Loss for  33107 th iteration => 0.00927001079551\n",
      "Loss for  33108 th iteration => 0.00926967901924\n",
      "Loss for  33109 th iteration => 0.00926922864768\n",
      "Loss for  33110 th iteration => 0.00926892072182\n",
      "Loss for  33111 th iteration => 0.00926874865827\n",
      "Loss for  33112 th iteration => 0.00926830556433\n",
      "Loss for  33113 th iteration => 0.00926816705619\n",
      "Loss for  33114 th iteration => 0.00926783315716\n",
      "Loss for  33115 th iteration => 0.00926737956474\n",
      "Loss for  33116 th iteration => 0.00926707329462\n",
      "Loss for  33117 th iteration => 0.00926690384976\n",
      "Loss for  33118 th iteration => 0.00926645852003\n",
      "Loss for  33119 th iteration => 0.00926647181058\n",
      "Loss for  33120 th iteration => 0.00926584284159\n",
      "Loss for  33121 th iteration => 0.00926567174908\n",
      "Loss for  33122 th iteration => 0.00926538543475\n",
      "Loss for  33123 th iteration => 0.00926504381202\n",
      "Loss for  33124 th iteration => 0.00926481429762\n",
      "Loss for  33125 th iteration => 0.00926441457241\n",
      "Loss for  33126 th iteration => 0.00926402786636\n",
      "Loss for  33127 th iteration => 0.00926378322584\n",
      "Loss for  33128 th iteration => 0.00926357558996\n",
      "Loss for  33129 th iteration => 0.00926315602358\n",
      "Loss for  33130 th iteration => 0.00926300456669\n",
      "Loss for  33131 th iteration => 0.00926252673281\n",
      "Loss for  33132 th iteration => 0.0092622129839\n",
      "Loss for  33133 th iteration => 0.00926189674563\n",
      "Loss for  33134 th iteration => 0.0092617652446\n",
      "Loss for  33135 th iteration => 0.00926127029713\n",
      "Loss for  33136 th iteration => 0.00926119431674\n",
      "Loss for  33137 th iteration => 0.00926064114545\n",
      "Loss for  33138 th iteration => 0.00926057989459\n",
      "Loss for  33139 th iteration => 0.00926002772818\n",
      "Loss for  33140 th iteration => 0.00926011976511\n",
      "Loss for  33141 th iteration => 0.0092594235617\n",
      "Loss for  33142 th iteration => 0.00925923211602\n",
      "Loss for  33143 th iteration => 0.00925882076611\n",
      "Loss for  33144 th iteration => 0.00925859770109\n",
      "Loss for  33145 th iteration => 0.00925832237368\n",
      "Loss for  33146 th iteration => 0.00925813328355\n",
      "Loss for  33147 th iteration => 0.00925762053164\n",
      "Loss for  33148 th iteration => 0.00925733405689\n",
      "Loss for  33149 th iteration => 0.00925701763799\n",
      "Loss for  33150 th iteration => 0.00925670038943\n",
      "Loss for  33151 th iteration => 0.00925652409402\n",
      "Loss for  33152 th iteration => 0.00925624121157\n",
      "Loss for  33153 th iteration => 0.00925581724205\n",
      "Loss for  33154 th iteration => 0.00925543777333\n",
      "Loss for  33155 th iteration => 0.0092552142632\n",
      "Loss for  33156 th iteration => 0.0092548055114\n",
      "Loss for  33157 th iteration => 0.0092547256615\n",
      "Loss for  33158 th iteration => 0.00925436302603\n",
      "Loss for  33159 th iteration => 0.00925388029019\n",
      "Loss for  33160 th iteration => 0.00925357148274\n",
      "Loss for  33161 th iteration => 0.00925326503036\n",
      "Loss for  33162 th iteration => 0.00925308601493\n",
      "Loss for  33163 th iteration => 0.00925294571317\n",
      "Loss for  33164 th iteration => 0.00925234980413\n",
      "Loss for  33165 th iteration => 0.00925220686985\n",
      "Loss for  33166 th iteration => 0.00925174727776\n",
      "Loss for  33167 th iteration => 0.0092515761612\n",
      "Loss for  33168 th iteration => 0.00925128236685\n",
      "Loss for  33169 th iteration => 0.00925106140201\n",
      "Loss for  33170 th iteration => 0.00925054776632\n",
      "Loss for  33171 th iteration => 0.0092503139053\n",
      "Loss for  33172 th iteration => 0.00924994516484\n",
      "Loss for  33173 th iteration => 0.0092496830971\n",
      "Loss for  33174 th iteration => 0.0092494863302\n",
      "Loss for  33175 th iteration => 0.00924917413193\n",
      "Loss for  33176 th iteration => 0.00924874553533\n",
      "Loss for  33177 th iteration => 0.00924842264054\n",
      "Loss for  33178 th iteration => 0.00924814287073\n",
      "Loss for  33179 th iteration => 0.00924779258052\n",
      "Loss for  33180 th iteration => 0.00924768991911\n",
      "Loss for  33181 th iteration => 0.00924728801183\n",
      "Loss for  33182 th iteration => 0.00924694314605\n",
      "Loss for  33183 th iteration => 0.00924653303161\n",
      "Loss for  33184 th iteration => 0.00924634042982\n",
      "Loss for  33185 th iteration => 0.00924591939093\n",
      "Loss for  33186 th iteration => 0.00924576279697\n",
      "Loss for  33187 th iteration => 0.0092454329431\n",
      "Loss for  33188 th iteration => 0.00924499748297\n",
      "Loss for  33189 th iteration => 0.00924469333777\n",
      "Loss for  33190 th iteration => 0.00924450533777\n",
      "Loss for  33191 th iteration => 0.00924408040062\n",
      "Loss for  33192 th iteration => 0.00924406019035\n",
      "Loss for  33193 th iteration => 0.0092434660345\n",
      "Loss for  33194 th iteration => 0.00924315804252\n",
      "Loss for  33195 th iteration => 0.00924298016232\n",
      "Loss for  33196 th iteration => 0.00924254499627\n",
      "Loss for  33197 th iteration => 0.00924224253486\n",
      "Loss for  33198 th iteration => 0.00924223001543\n",
      "Loss for  33199 th iteration => 0.00924162705523\n",
      "Loss for  33200 th iteration => 0.00924131995524\n",
      "Loss for  33201 th iteration => 0.00924114609065\n",
      "Loss for  33202 th iteration => 0.00924070699634\n",
      "Loss for  33203 th iteration => 0.00924056790856\n",
      "Loss for  33204 th iteration => 0.00924023670959\n",
      "Loss for  33205 th iteration => 0.00923978878704\n",
      "Loss for  33206 th iteration => 0.00923948273609\n",
      "Loss for  33207 th iteration => 0.00923931257914\n",
      "Loss for  33208 th iteration => 0.00923887013121\n",
      "Loss for  33209 th iteration => 0.00923873569534\n",
      "Loss for  33210 th iteration => 0.00923840271264\n",
      "Loss for  33211 th iteration => 0.00923795123477\n",
      "Loss for  33212 th iteration => 0.00923764632669\n",
      "Loss for  33213 th iteration => 0.00923747968908\n",
      "Loss for  33214 th iteration => 0.00923703408008\n",
      "Loss for  33215 th iteration => 0.00923690417417\n",
      "Loss for  33216 th iteration => 0.00923656935553\n",
      "Loss for  33217 th iteration => 0.00923611632926\n",
      "Loss for  33218 th iteration => 0.00923595344018\n",
      "Loss for  33219 th iteration => 0.00923566215939\n",
      "Loss for  33220 th iteration => 0.00923532819713\n",
      "Loss for  33221 th iteration => 0.0092350959255\n",
      "Loss for  33222 th iteration => 0.00923470012555\n",
      "Loss for  33223 th iteration => 0.00923431320513\n",
      "Loss for  33224 th iteration => 0.00923407566525\n",
      "Loss for  33225 th iteration => 0.00923386462186\n",
      "Loss for  33226 th iteration => 0.00923345113479\n",
      "Loss for  33227 th iteration => 0.00923329851415\n",
      "Loss for  33228 th iteration => 0.00923282364064\n",
      "Loss for  33229 th iteration => 0.00923251015221\n",
      "Loss for  33230 th iteration => 0.00923219996715\n",
      "Loss for  33231 th iteration => 0.00923206654312\n",
      "Loss for  33232 th iteration => 0.00923157616896\n",
      "Loss for  33233 th iteration => 0.00923150054241\n",
      "Loss for  33234 th iteration => 0.0092309518914\n",
      "Loss for  33235 th iteration => 0.0092308889842\n",
      "Loss for  33236 th iteration => 0.00923034428818\n",
      "Loss for  33237 th iteration => 0.00923043330025\n",
      "Loss for  33238 th iteration => 0.00922974034204\n",
      "Loss for  33239 th iteration => 0.00922954815641\n",
      "Loss for  33240 th iteration => 0.0092291414956\n",
      "Loss for  33241 th iteration => 0.0092289189352\n",
      "Loss for  33242 th iteration => 0.00922864935894\n",
      "Loss for  33243 th iteration => 0.00922845790125\n",
      "Loss for  33244 th iteration => 0.00922794910079\n",
      "Loss for  33245 th iteration => 0.00922766144029\n",
      "Loss for  33246 th iteration => 0.00922735014977\n",
      "Loss for  33247 th iteration => 0.00922703255528\n",
      "Loss for  33248 th iteration => 0.00922686266549\n",
      "Loss for  33249 th iteration => 0.00922657755923\n",
      "Loss for  33250 th iteration => 0.00922615758098\n",
      "Loss for  33251 th iteration => 0.00922577650983\n",
      "Loss for  33252 th iteration => 0.00922555853838\n",
      "Loss for  33253 th iteration => 0.00922514894628\n",
      "Loss for  33254 th iteration => 0.00922525264505\n",
      "Loss for  33255 th iteration => 0.00922453815491\n",
      "Loss for  33256 th iteration => 0.00922439728365\n",
      "Loss for  33257 th iteration => 0.00922393933827\n",
      "Loss for  33258 th iteration => 0.0092237682593\n",
      "Loss for  33259 th iteration => 0.00922347612398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  33260 th iteration => 0.00922325686883\n",
      "Loss for  33261 th iteration => 0.0092227471807\n",
      "Loss for  33262 th iteration => 0.00922251491187\n",
      "Loss for  33263 th iteration => 0.00922214828236\n",
      "Loss for  33264 th iteration => 0.00922188555917\n",
      "Loss for  33265 th iteration => 0.009221554641\n",
      "Loss for  33266 th iteration => 0.00922151595166\n",
      "Loss for  33267 th iteration => 0.00922095597623\n",
      "Loss for  33268 th iteration => 0.0092206342651\n",
      "Loss for  33269 th iteration => 0.00922035700873\n",
      "Loss for  33270 th iteration => 0.00922000546761\n",
      "Loss for  33271 th iteration => 0.00921976325929\n",
      "Loss for  33272 th iteration => 0.00921964655482\n",
      "Loss for  33273 th iteration => 0.00921916460488\n",
      "Loss for  33274 th iteration => 0.00921875527982\n",
      "Loss for  33275 th iteration => 0.00921856557932\n",
      "Loss for  33276 th iteration => 0.00921814415385\n",
      "Loss for  33277 th iteration => 0.00921799024348\n",
      "Loss for  33278 th iteration => 0.00921766370509\n",
      "Loss for  33279 th iteration => 0.00921723090994\n",
      "Loss for  33280 th iteration => 0.00921692596271\n",
      "Loss for  33281 th iteration => 0.00921674169781\n",
      "Loss for  33282 th iteration => 0.00921631514684\n",
      "Loss for  33283 th iteration => 0.00921616694384\n",
      "Loss for  33284 th iteration => 0.00921583918022\n",
      "Loss for  33285 th iteration => 0.00921540218281\n",
      "Loss for  33286 th iteration => 0.00921509749079\n",
      "Loss for  33287 th iteration => 0.00921491839256\n",
      "Loss for  33288 th iteration => 0.00921448699134\n",
      "Loss for  33289 th iteration => 0.00921434432314\n",
      "Loss for  33290 th iteration => 0.00921401525755\n",
      "Loss for  33291 th iteration => 0.00921357416454\n",
      "Loss for  33292 th iteration => 0.0092132698572\n",
      "Loss for  33293 th iteration => 0.00921309566947\n",
      "Loss for  33294 th iteration => 0.00921265968016\n",
      "Loss for  33295 th iteration => 0.00921252238223\n",
      "Loss for  33296 th iteration => 0.00921219194179\n",
      "Loss for  33297 th iteration => 0.00921174685496\n",
      "Loss for  33298 th iteration => 0.00921144305542\n",
      "Loss for  33299 th iteration => 0.00921127353418\n",
      "Loss for  33300 th iteration => 0.00921083320647\n",
      "Loss for  33301 th iteration => 0.00921070112191\n",
      "Loss for  33302 th iteration => 0.00921036923742\n",
      "Loss for  33303 th iteration => 0.00920992025388\n",
      "Loss for  33304 th iteration => 0.00920961707919\n",
      "Loss for  33305 th iteration => 0.009209451992\n",
      "Loss for  33306 th iteration => 0.00920900756372\n",
      "Loss for  33307 th iteration => 0.00920888054293\n",
      "Loss for  33308 th iteration => 0.00920854714869\n",
      "Loss for  33309 th iteration => 0.00920809687154\n",
      "Loss for  33310 th iteration => 0.00920793348707\n",
      "Loss for  33311 th iteration => 0.00920764638603\n",
      "Loss for  33312 th iteration => 0.00920731088122\n",
      "Loss for  33313 th iteration => 0.009207084495\n",
      "Loss for  33314 th iteration => 0.00920668791879\n",
      "Loss for  33315 th iteration => 0.00920630479756\n",
      "Loss for  33316 th iteration => 0.00920606600136\n",
      "Loss for  33317 th iteration => 0.00920586024278\n",
      "Loss for  33318 th iteration => 0.00920544408977\n",
      "Loss for  33319 th iteration => 0.00920529848736\n",
      "Loss for  33320 th iteration => 0.00920482283527\n",
      "Loss for  33321 th iteration => 0.00920469057607\n",
      "Loss for  33322 th iteration => 0.00920421608397\n",
      "Loss for  33323 th iteration => 0.00920423458775\n",
      "Loss for  33324 th iteration => 0.00920360881627\n",
      "Loss for  33325 th iteration => 0.00920346163886\n",
      "Loss for  33326 th iteration => 0.00920300030412\n",
      "Loss for  33327 th iteration => 0.00920284570296\n",
      "Loss for  33328 th iteration => 0.00920239284899\n",
      "Loss for  33329 th iteration => 0.00920239610791\n",
      "Loss for  33330 th iteration => 0.0092017980377\n",
      "Loss for  33331 th iteration => 0.00920151074492\n",
      "Loss for  33332 th iteration => 0.00920120256981\n",
      "Loss for  33333 th iteration => 0.0092008833493\n",
      "Loss for  33334 th iteration => 0.00920061098047\n",
      "Loss for  33335 th iteration => 0.00920053091486\n",
      "Loss for  33336 th iteration => 0.00920001688668\n",
      "Loss for  33337 th iteration => 0.00919963576048\n",
      "Loss for  33338 th iteration => 0.00919942132091\n",
      "Loss for  33339 th iteration => 0.0091990089488\n",
      "Loss for  33340 th iteration => 0.00919883017387\n",
      "Loss for  33341 th iteration => 0.00919868438402\n",
      "Loss for  33342 th iteration => 0.0091981010966\n",
      "Loss for  33343 th iteration => 0.00919779594857\n",
      "Loss for  33344 th iteration => 0.0091974908229\n",
      "Loss for  33345 th iteration => 0.00919718706687\n",
      "Loss for  33346 th iteration => 0.0091970141111\n",
      "Loss for  33347 th iteration => 0.0091968729694\n",
      "Loss for  33348 th iteration => 0.0091962845728\n",
      "Loss for  33349 th iteration => 0.00919614228214\n",
      "Loss for  33350 th iteration => 0.00919568948532\n",
      "Loss for  33351 th iteration => 0.00919551669041\n",
      "Loss for  33352 th iteration => 0.0091952332887\n",
      "Loss for  33353 th iteration => 0.00919501009242\n",
      "Loss for  33354 th iteration => 0.00919450472841\n",
      "Loss for  33355 th iteration => 0.00919427072347\n",
      "Loss for  33356 th iteration => 0.0091939095539\n",
      "Loss for  33357 th iteration => 0.00919364499837\n",
      "Loss for  33358 th iteration => 0.00919331935609\n",
      "Loss for  33359 th iteration => 0.00919328316753\n",
      "Loss for  33360 th iteration => 0.00919272464304\n",
      "Loss for  33361 th iteration => 0.00919240089161\n",
      "Loss for  33362 th iteration => 0.00919212939411\n",
      "Loss for  33363 th iteration => 0.00919177572369\n",
      "Loss for  33364 th iteration => 0.00919153829419\n",
      "Loss for  33365 th iteration => 0.00919142516884\n",
      "Loss for  33366 th iteration => 0.00919094437605\n",
      "Loss for  33367 th iteration => 0.00919053272665\n",
      "Loss for  33368 th iteration => 0.0091903490639\n",
      "Loss for  33369 th iteration => 0.00918992630089\n",
      "Loss for  33370 th iteration => 0.00918962627727\n",
      "Loss for  33371 th iteration => 0.0091896018292\n",
      "Loss for  33372 th iteration => 0.00918901859958\n",
      "Loss for  33373 th iteration => 0.00918871404702\n",
      "Loss for  33374 th iteration => 0.00918841378296\n",
      "Loss for  33375 th iteration => 0.00918823087995\n",
      "Loss for  33376 th iteration => 0.00918780752238\n",
      "Loss for  33377 th iteration => 0.00918779458654\n",
      "Loss for  33378 th iteration => 0.00918720086449\n",
      "Loss for  33379 th iteration => 0.00918689642852\n",
      "Loss for  33380 th iteration => 0.00918659597619\n",
      "Loss for  33381 th iteration => 0.00918641900731\n",
      "Loss for  33382 th iteration => 0.00918598977187\n",
      "Loss for  33383 th iteration => 0.00918598775417\n",
      "Loss for  33384 th iteration => 0.00918538383087\n",
      "Loss for  33385 th iteration => 0.00918507951151\n",
      "Loss for  33386 th iteration => 0.00918477901961\n",
      "Loss for  33387 th iteration => 0.00918460769221\n",
      "Loss for  33388 th iteration => 0.0091841730104\n",
      "Loss for  33389 th iteration => 0.00918418134575\n",
      "Loss for  33390 th iteration => 0.00918356749855\n",
      "Loss for  33391 th iteration => 0.00918326329583\n",
      "Loss for  33392 th iteration => 0.00918296290565\n",
      "Loss for  33393 th iteration => 0.00918279694126\n",
      "Loss for  33394 th iteration => 0.00918235722336\n",
      "Loss for  33395 th iteration => 0.00918237537426\n",
      "Loss for  33396 th iteration => 0.00918175436643\n",
      "Loss for  33397 th iteration => 0.00918158612726\n",
      "Loss for  33398 th iteration => 0.00918116189149\n",
      "Loss for  33399 th iteration => 0.00918111186919\n",
      "Loss for  33400 th iteration => 0.00918056711318\n",
      "Loss for  33401 th iteration => 0.00918052886395\n",
      "Loss for  33402 th iteration => 0.00917997262792\n",
      "Loss for  33403 th iteration => 0.00917972953919\n",
      "Loss for  33404 th iteration => 0.00917937931482\n",
      "Loss for  33405 th iteration => 0.00917926173066\n",
      "Loss for  33406 th iteration => 0.00917878475007\n",
      "Loss for  33407 th iteration => 0.00917867933155\n",
      "Loss for  33408 th iteration => 0.00917819194777\n",
      "Loss for  33409 th iteration => 0.00917805160624\n",
      "Loss for  33410 th iteration => 0.00917761437862\n",
      "Loss for  33411 th iteration => 0.00917757177206\n",
      "Loss for  33412 th iteration => 0.0091770355182\n",
      "Loss for  33413 th iteration => 0.00917677892324\n",
      "Loss for  33414 th iteration => 0.00917645558602\n",
      "Loss for  33415 th iteration => 0.00917614206393\n",
      "Loss for  33416 th iteration => 0.00917577853468\n",
      "Loss for  33417 th iteration => 0.00917578184123\n",
      "Loss for  33418 th iteration => 0.00917518854615\n",
      "Loss for  33419 th iteration => 0.0091749000145\n",
      "Loss for  33420 th iteration => 0.00917459659027\n",
      "Loss for  33421 th iteration => 0.00917427601644\n",
      "Loss for  33422 th iteration => 0.00917400646668\n",
      "Loss for  33423 th iteration => 0.00917376033077\n",
      "Loss for  33424 th iteration => 0.00917358550539\n",
      "Loss for  33425 th iteration => 0.00917303510332\n",
      "Loss for  33426 th iteration => 0.009172825885\n",
      "Loss for  33427 th iteration => 0.00917241236417\n",
      "Loss for  33428 th iteration => 0.00917211440762\n",
      "Loss for  33429 th iteration => 0.00917204197055\n",
      "Loss for  33430 th iteration => 0.00917168345278\n",
      "Loss for  33431 th iteration => 0.00917120784126\n",
      "Loss for  33432 th iteration => 0.0091709044515\n",
      "Loss for  33433 th iteration => 0.00917060108381\n",
      "Loss for  33434 th iteration => 0.00917030178673\n",
      "Loss for  33435 th iteration => 0.00917024144005\n",
      "Loss for  33436 th iteration => 0.0091698782892\n",
      "Loss for  33437 th iteration => 0.00916940224403\n",
      "Loss for  33438 th iteration => 0.00916926276662\n",
      "Loss for  33439 th iteration => 0.00916881071289\n",
      "Loss for  33440 th iteration => 0.00916864185681\n",
      "Loss for  33441 th iteration => 0.00916847609184\n",
      "Loss for  33442 th iteration => 0.00916802384775\n",
      "Loss for  33443 th iteration => 0.00916763302109\n",
      "Loss for  33444 th iteration => 0.00916740141092\n",
      "Loss for  33445 th iteration => 0.0091670413957\n",
      "Loss for  33446 th iteration => 0.00916677911007\n",
      "Loss for  33447 th iteration => 0.009166455397\n",
      "Loss for  33448 th iteration => 0.00916642363232\n",
      "Loss for  33449 th iteration => 0.00916586353046\n",
      "Loss for  33450 th iteration => 0.00916554178798\n",
      "Loss for  33451 th iteration => 0.00916527182507\n",
      "Loss for  33452 th iteration => 0.00916492004695\n",
      "Loss for  33453 th iteration => 0.00916468408863\n",
      "Loss for  33454 th iteration => 0.00916457647181\n",
      "Loss for  33455 th iteration => 0.00916409384261\n",
      "Loss for  33456 th iteration => 0.00916368383933\n",
      "Loss for  33457 th iteration => 0.00916350206853\n",
      "Loss for  33458 th iteration => 0.00916307995626\n",
      "Loss for  33459 th iteration => 0.00916278070429\n",
      "Loss for  33460 th iteration => 0.00916276313014\n",
      "Loss for  33461 th iteration => 0.00916217745543\n",
      "Loss for  33462 th iteration => 0.00916187509365\n",
      "Loss for  33463 th iteration => 0.00916170023859\n",
      "Loss for  33464 th iteration => 0.00916127229184\n",
      "Loss for  33465 th iteration => 0.00916097171594\n",
      "Loss for  33466 th iteration => 0.00916096677324\n",
      "Loss for  33467 th iteration => 0.00916037013892\n",
      "Loss for  33468 th iteration => 0.00916006745311\n",
      "Loss for  33469 th iteration => 0.00915976884367\n",
      "Loss for  33470 th iteration => 0.00915959521005\n",
      "Loss for  33471 th iteration => 0.00915916397552\n",
      "Loss for  33472 th iteration => 0.00915903106583\n",
      "Loss for  33473 th iteration => 0.00915870240045\n",
      "Loss for  33474 th iteration => 0.00915826095253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  33475 th iteration => 0.00915796188261\n",
      "Loss for  33476 th iteration => 0.00915779462191\n",
      "Loss for  33477 th iteration => 0.00915735729614\n",
      "Loss for  33478 th iteration => 0.00915723059528\n",
      "Loss for  33479 th iteration => 0.00915690109438\n",
      "Loss for  33480 th iteration => 0.00915645588309\n",
      "Loss for  33481 th iteration => 0.00915629533655\n",
      "Loss for  33482 th iteration => 0.0091560082091\n",
      "Loss for  33483 th iteration => 0.00915567749836\n",
      "Loss for  33484 th iteration => 0.009155277816\n",
      "Loss for  33485 th iteration => 0.00915523815473\n",
      "Loss for  33486 th iteration => 0.00915468419345\n",
      "Loss for  33487 th iteration => 0.00915444643735\n",
      "Loss for  33488 th iteration => 0.00915424324241\n",
      "Loss for  33489 th iteration => 0.00915383091661\n",
      "Loss for  33490 th iteration => 0.00915350492871\n",
      "Loss for  33491 th iteration => 0.00915339860004\n",
      "Loss for  33492 th iteration => 0.00915291253604\n",
      "Loss for  33493 th iteration => 0.00915259968075\n",
      "Loss for  33494 th iteration => 0.00915247765323\n",
      "Loss for  33495 th iteration => 0.00915198572955\n",
      "Loss for  33496 th iteration => 0.00915173275295\n",
      "Loss for  33497 th iteration => 0.00915155982644\n",
      "Loss for  33498 th iteration => 0.0091511461189\n",
      "Loss for  33499 th iteration => 0.00915093287425\n",
      "Loss for  33500 th iteration => 0.00915057334505\n",
      "Loss for  33501 th iteration => 0.00915047403082\n",
      "Loss for  33502 th iteration => 0.00914988935392\n",
      "Loss for  33503 th iteration => 0.00914969474923\n",
      "Loss for  33504 th iteration => 0.00914930105595\n",
      "Loss for  33505 th iteration => 0.00914907361721\n",
      "Loss for  33506 th iteration => 0.0091487170469\n",
      "Loss for  33507 th iteration => 0.009148723787\n",
      "Loss for  33508 th iteration => 0.00914812957225\n",
      "Loss for  33509 th iteration => 0.00914783843786\n",
      "Loss for  33510 th iteration => 0.00914754115368\n",
      "Loss for  33511 th iteration => 0.00914721790944\n",
      "Loss for  33512 th iteration => 0.00914695477726\n",
      "Loss for  33513 th iteration => 0.00914688014634\n",
      "Loss for  33514 th iteration => 0.0091463694748\n",
      "Loss for  33515 th iteration => 0.00914598392993\n",
      "Loss for  33516 th iteration => 0.00914578094851\n",
      "Loss for  33517 th iteration => 0.00914536462032\n",
      "Loss for  33518 th iteration => 0.00914506807647\n",
      "Loss for  33519 th iteration => 0.0091450062964\n",
      "Loss for  33520 th iteration => 0.00914464484571\n",
      "Loss for  33521 th iteration => 0.00914417144728\n",
      "Loss for  33522 th iteration => 0.00914403418035\n",
      "Loss for  33523 th iteration => 0.00914358325115\n",
      "Loss for  33524 th iteration => 0.00914341664706\n",
      "Loss for  33525 th iteration => 0.00914324996275\n",
      "Loss for  33526 th iteration => 0.00914280159221\n",
      "Loss for  33527 th iteration => 0.00914241217401\n",
      "Loss for  33528 th iteration => 0.00914218235144\n",
      "Loss for  33529 th iteration => 0.00914182387795\n",
      "Loss for  33530 th iteration => 0.00914156324796\n",
      "Loss for  33531 th iteration => 0.00914124033417\n",
      "Loss for  33532 th iteration => 0.00914120978507\n",
      "Loss for  33533 th iteration => 0.00914065261691\n",
      "Loss for  33534 th iteration => 0.00914033226518\n",
      "Loss for  33535 th iteration => 0.00914006423527\n",
      "Loss for  33536 th iteration => 0.00913971372448\n",
      "Loss for  33537 th iteration => 0.00913947810803\n",
      "Loss for  33538 th iteration => 0.00913937281445\n",
      "Loss for  33539 th iteration => 0.00913889284704\n",
      "Loss for  33540 th iteration => 0.00913848386152\n",
      "Loss for  33541 th iteration => 0.00913830439128\n",
      "Loss for  33542 th iteration => 0.00913788410348\n",
      "Loss for  33543 th iteration => 0.00913758438618\n",
      "Loss for  33544 th iteration => 0.00913744004292\n",
      "Loss for  33545 th iteration => 0.00913711600922\n",
      "Loss for  33546 th iteration => 0.00913668530928\n",
      "Loss for  33547 th iteration => 0.00913638745027\n",
      "Loss for  33548 th iteration => 0.00913621056083\n",
      "Loss for  33549 th iteration => 0.00913578528815\n",
      "Loss for  33550 th iteration => 0.00913564799808\n",
      "Loss for  33551 th iteration => 0.0091353235795\n",
      "Loss for  33552 th iteration => 0.00913488786584\n",
      "Loss for  33553 th iteration => 0.00913458898827\n",
      "Loss for  33554 th iteration => 0.00913441948148\n",
      "Loss for  33555 th iteration => 0.00913398799873\n",
      "Loss for  33556 th iteration => 0.00913369176749\n",
      "Loss for  33557 th iteration => 0.00913369628217\n",
      "Loss for  33558 th iteration => 0.00913309111548\n",
      "Loss for  33559 th iteration => 0.00913279134823\n",
      "Loss for  33560 th iteration => 0.00913262896867\n",
      "Loss for  33561 th iteration => 0.00913219159217\n",
      "Loss for  33562 th iteration => 0.00913189377083\n",
      "Loss for  33563 th iteration => 0.00913191126673\n",
      "Loss for  33564 th iteration => 0.00913129769146\n",
      "Loss for  33565 th iteration => 0.00913113086358\n",
      "Loss for  33566 th iteration => 0.00913085450738\n",
      "Loss for  33567 th iteration => 0.00913051898058\n",
      "Loss for  33568 th iteration => 0.00913012296828\n",
      "Loss for  33569 th iteration => 0.00913008427032\n",
      "Loss for  33570 th iteration => 0.00912953577779\n",
      "Loss for  33571 th iteration => 0.00912929313066\n",
      "Loss for  33572 th iteration => 0.00912894858645\n",
      "Loss for  33573 th iteration => 0.00912883003588\n",
      "Loss for  33574 th iteration => 0.00912836010052\n",
      "Loss for  33575 th iteration => 0.00912807171209\n",
      "Loss for  33576 th iteration => 0.00912795732144\n",
      "Loss for  33577 th iteration => 0.00912745801906\n",
      "Loss for  33578 th iteration => 0.00912736282161\n",
      "Loss for  33579 th iteration => 0.00912701781081\n",
      "Loss for  33580 th iteration => 0.00912675880259\n",
      "Loss for  33581 th iteration => 0.0091262640215\n",
      "Loss for  33582 th iteration => 0.0091261526402\n",
      "Loss for  33583 th iteration => 0.00912566840336\n",
      "Loss for  33584 th iteration => 0.00912544765444\n",
      "Loss for  33585 th iteration => 0.00912518508314\n",
      "Loss for  33586 th iteration => 0.00912499579462\n",
      "Loss for  33587 th iteration => 0.00912450320102\n",
      "Loss for  33588 th iteration => 0.00912421760917\n",
      "Loss for  33589 th iteration => 0.00912391793439\n",
      "Loss for  33590 th iteration => 0.00912360002787\n",
      "Loss for  33591 th iteration => 0.00912333530884\n",
      "Loss for  33592 th iteration => 0.00912325776125\n",
      "Loss for  33593 th iteration => 0.00912275249837\n",
      "Loss for  33594 th iteration => 0.00912237189928\n",
      "Loss for  33595 th iteration => 0.00912216711706\n",
      "Loss for  33596 th iteration => 0.00912175592144\n",
      "Loss for  33597 th iteration => 0.00912146033051\n",
      "Loss for  33598 th iteration => 0.0091213938424\n",
      "Loss for  33599 th iteration => 0.00912103710444\n",
      "Loss for  33600 th iteration => 0.0091205661818\n",
      "Loss for  33601 th iteration => 0.0091204317459\n",
      "Loss for  33602 th iteration => 0.00911998110795\n",
      "Loss for  33603 th iteration => 0.00911981610837\n",
      "Loss for  33604 th iteration => 0.0091195317744\n",
      "Loss for  33605 th iteration => 0.00911931618154\n",
      "Loss for  33606 th iteration => 0.00911881621159\n",
      "Loss for  33607 th iteration => 0.00911858877177\n",
      "Loss for  33608 th iteration => 0.00911823103352\n",
      "Loss for  33609 th iteration => 0.00911797264001\n",
      "Loss for  33610 th iteration => 0.00911764831106\n",
      "Loss for  33611 th iteration => 0.00911761702582\n",
      "Loss for  33612 th iteration => 0.00911706595194\n",
      "Loss for  33613 th iteration => 0.00911674755325\n",
      "Loss for  33614 th iteration => 0.00911648068261\n",
      "Loss for  33615 th iteration => 0.0091161319877\n",
      "Loss for  33616 th iteration => 0.00911589546643\n",
      "Loss for  33617 th iteration => 0.0091156659756\n",
      "Loss for  33618 th iteration => 0.00911543848777\n",
      "Loss for  33619 th iteration => 0.00911490960437\n",
      "Loss for  33620 th iteration => 0.0091146122818\n",
      "Loss for  33621 th iteration => 0.00911442958213\n",
      "Loss for  33622 th iteration => 0.009114013614\n",
      "Loss for  33623 th iteration => 0.00911371847956\n",
      "Loss for  33624 th iteration => 0.00911369795423\n",
      "Loss for  33625 th iteration => 0.00911312058087\n",
      "Loss for  33626 th iteration => 0.00911282157249\n",
      "Loss for  33627 th iteration => 0.00911264747952\n",
      "Loss for  33628 th iteration => 0.00911222493138\n",
      "Loss for  33629 th iteration => 0.00911192672397\n",
      "Loss for  33630 th iteration => 0.00911178885704\n",
      "Loss for  33631 th iteration => 0.0091114646931\n",
      "Loss for  33632 th iteration => 0.00911103274443\n",
      "Loss for  33633 th iteration => 0.00911073634587\n",
      "Loss for  33634 th iteration => 0.0091105654888\n",
      "Loss for  33635 th iteration => 0.00911013749799\n",
      "Loss for  33636 th iteration => 0.00910984424766\n",
      "Loss for  33637 th iteration => 0.00910984516485\n",
      "Loss for  33638 th iteration => 0.00910924520949\n",
      "Loss for  33639 th iteration => 0.00910894743323\n",
      "Loss for  33640 th iteration => 0.00910878457389\n",
      "Loss for  33641 th iteration => 0.00910835030375\n",
      "Loss for  33642 th iteration => 0.00910805455743\n",
      "Loss for  33643 th iteration => 0.00910807007154\n",
      "Loss for  33644 th iteration => 0.00910746019577\n",
      "Loss for  33645 th iteration => 0.00910729543581\n",
      "Loss for  33646 th iteration => 0.00910701891807\n",
      "Loss for  33647 th iteration => 0.00910668656813\n",
      "Loss for  33648 th iteration => 0.00910629123388\n",
      "Loss for  33649 th iteration => 0.00910625385212\n",
      "Loss for  33650 th iteration => 0.00910570761688\n",
      "Loss for  33651 th iteration => 0.00910546673306\n",
      "Loss for  33652 th iteration => 0.00910512300167\n",
      "Loss for  33653 th iteration => 0.00910500584578\n",
      "Loss for  33654 th iteration => 0.00910453816413\n",
      "Loss for  33655 th iteration => 0.00910425030789\n",
      "Loss for  33656 th iteration => 0.00910413789442\n",
      "Loss for  33657 th iteration => 0.00910364104783\n",
      "Loss for  33658 th iteration => 0.0091035441594\n",
      "Loss for  33659 th iteration => 0.0091030492996\n",
      "Loss for  33660 th iteration => 0.00910309759909\n",
      "Loss for  33661 th iteration => 0.00910245309292\n",
      "Loss for  33662 th iteration => 0.00910234099852\n",
      "Loss for  33663 th iteration => 0.00910186131286\n",
      "Loss for  33664 th iteration => 0.00910163864004\n",
      "Loss for  33665 th iteration => 0.00910128315579\n",
      "Loss for  33666 th iteration => 0.00910128589452\n",
      "Loss for  33667 th iteration => 0.0091007022718\n",
      "Loss for  33668 th iteration => 0.00910041528281\n",
      "Loss for  33669 th iteration => 0.00910012011767\n",
      "Loss for  33670 th iteration => 0.00909980071948\n",
      "Loss for  33671 th iteration => 0.00909953905165\n",
      "Loss for  33672 th iteration => 0.00909929332\n",
      "Loss for  33673 th iteration => 0.00909912841775\n",
      "Loss for  33674 th iteration => 0.00909857852041\n",
      "Loss for  33675 th iteration => 0.00909837864636\n",
      "Loss for  33676 th iteration => 0.00909796808265\n",
      "Loss for  33677 th iteration => 0.00909767014984\n",
      "Loss for  33678 th iteration => 0.0090975013177\n",
      "Loss for  33679 th iteration => 0.00909736081455\n",
      "Loss for  33680 th iteration => 0.00909678595434\n",
      "Loss for  33681 th iteration => 0.00909664825282\n",
      "Loss for  33682 th iteration => 0.00909620397037\n",
      "Loss for  33683 th iteration => 0.00909603457248\n",
      "Loss for  33684 th iteration => 0.00909562469319\n",
      "Loss for  33685 th iteration => 0.00909566894631\n",
      "Loss for  33686 th iteration => 0.00909504518497\n",
      "Loss for  33687 th iteration => 0.00909481434256\n",
      "Loss for  33688 th iteration => 0.00909446309396\n",
      "Loss for  33689 th iteration => 0.00909420124476\n",
      "Loss for  33690 th iteration => 0.00909388102336\n",
      "Loss for  33691 th iteration => 0.00909359346297\n",
      "Loss for  33692 th iteration => 0.009093561105\n",
      "Loss for  33693 th iteration => 0.00909298201088\n",
      "Loss for  33694 th iteration => 0.00909272213078\n",
      "Loss for  33695 th iteration => 0.00909236948412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  33696 th iteration => 0.00909213996579\n",
      "Loss for  33697 th iteration => 0.00909175794381\n",
      "Loss for  33698 th iteration => 0.009091833166\n",
      "Loss for  33699 th iteration => 0.0090911567907\n",
      "Loss for  33700 th iteration => 0.00909085860281\n",
      "Loss for  33701 th iteration => 0.00909056298169\n",
      "Loss for  33702 th iteration => 0.00909038290485\n",
      "Loss for  33703 th iteration => 0.00908996725801\n",
      "Loss for  33704 th iteration => 0.00908967377199\n",
      "Loss for  33705 th iteration => 0.00908965686283\n",
      "Loss for  33706 th iteration => 0.00908907884596\n",
      "Loss for  33707 th iteration => 0.00908878116493\n",
      "Loss for  33708 th iteration => 0.00908861034596\n",
      "Loss for  33709 th iteration => 0.00908818783912\n",
      "Loss for  33710 th iteration => 0.00908789061316\n",
      "Loss for  33711 th iteration => 0.00908775652407\n",
      "Loss for  33712 th iteration => 0.00908743321907\n",
      "Loss for  33713 th iteration => 0.00908700182579\n",
      "Loss for  33714 th iteration => 0.00908670631911\n",
      "Loss for  33715 th iteration => 0.00908653948468\n",
      "Loss for  33716 th iteration => 0.00908611121843\n",
      "Loss for  33717 th iteration => 0.00908581812197\n",
      "Loss for  33718 th iteration => 0.00908582523787\n",
      "Loss for  33719 th iteration => 0.00908522354496\n",
      "Loss for  33720 th iteration => 0.00908492631957\n",
      "Loss for  33721 th iteration => 0.00908476807118\n",
      "Loss for  33722 th iteration => 0.00908433327538\n",
      "Loss for  33723 th iteration => 0.00908403705822\n",
      "Loss for  33724 th iteration => 0.00908405984748\n",
      "Loss for  33725 th iteration => 0.00908344994527\n",
      "Loss for  33726 th iteration => 0.009083282878\n",
      "Loss for  33727 th iteration => 0.00908286951174\n",
      "Loss for  33728 th iteration => 0.00908281966115\n",
      "Loss for  33729 th iteration => 0.00908228669022\n",
      "Loss for  33730 th iteration => 0.00908207173883\n",
      "Loss for  33731 th iteration => 0.00908188515854\n",
      "Loss for  33732 th iteration => 0.00908146356622\n",
      "Loss for  33733 th iteration => 0.00908112370872\n",
      "Loss for  33734 th iteration => 0.00908100811986\n",
      "Loss for  33735 th iteration => 0.00908054344719\n",
      "Loss for  33736 th iteration => 0.00908025074607\n",
      "Loss for  33737 th iteration => 0.00908014946558\n",
      "Loss for  33738 th iteration => 0.00907965122645\n",
      "Loss for  33739 th iteration => 0.00907955216533\n",
      "Loss for  33740 th iteration => 0.0090790606881\n",
      "Loss for  33741 th iteration => 0.00907911129765\n",
      "Loss for  33742 th iteration => 0.00907846938733\n",
      "Loss for  33743 th iteration => 0.00907835511326\n",
      "Loss for  33744 th iteration => 0.00907788229013\n",
      "Loss for  33745 th iteration => 0.00907765340702\n",
      "Loss for  33746 th iteration => 0.00907730451161\n",
      "Loss for  33747 th iteration => 0.00907714498008\n",
      "Loss for  33748 th iteration => 0.00907689306083\n",
      "Loss for  33749 th iteration => 0.00907643608041\n",
      "Loss for  33750 th iteration => 0.00907615039523\n",
      "Loss for  33751 th iteration => 0.00907582459101\n",
      "Loss for  33752 th iteration => 0.00907557134032\n",
      "Loss for  33753 th iteration => 0.00907521632686\n",
      "Loss for  33754 th iteration => 0.00907527281621\n",
      "Loss for  33755 th iteration => 0.00907460934967\n",
      "Loss for  33756 th iteration => 0.00907457997828\n",
      "Loss for  33757 th iteration => 0.00907402047454\n",
      "Loss for  33758 th iteration => 0.00907385351087\n",
      "Loss for  33759 th iteration => 0.00907344170238\n",
      "Loss for  33760 th iteration => 0.00907348522074\n",
      "Loss for  33761 th iteration => 0.00907286747083\n",
      "Loss for  33762 th iteration => 0.00907263879592\n",
      "Loss for  33763 th iteration => 0.00907228829214\n",
      "Loss for  33764 th iteration => 0.00907202848067\n",
      "Loss for  33765 th iteration => 0.00907170913165\n",
      "Loss for  33766 th iteration => 0.00907142008301\n",
      "Loss for  33767 th iteration => 0.00907138743663\n",
      "Loss for  33768 th iteration => 0.00907081476886\n",
      "Loss for  33769 th iteration => 0.00907055600698\n",
      "Loss for  33770 th iteration => 0.00907020502767\n",
      "Loss for  33771 th iteration => 0.00906997674712\n",
      "Loss for  33772 th iteration => 0.00906959542154\n",
      "Loss for  33773 th iteration => 0.00906940132313\n",
      "Loss for  33774 th iteration => 0.00906926268831\n",
      "Loss for  33775 th iteration => 0.00906870324799\n",
      "Loss for  33776 th iteration => 0.00906840673474\n",
      "Loss for  33777 th iteration => 0.00906822843818\n",
      "Loss for  33778 th iteration => 0.00906781617093\n",
      "Loss for  33779 th iteration => 0.00906751949283\n",
      "Loss for  33780 th iteration => 0.00906722963106\n",
      "Loss for  33781 th iteration => 0.00906720722946\n",
      "Loss for  33782 th iteration => 0.00906663538482\n",
      "Loss for  33783 th iteration => 0.00906634025392\n",
      "Loss for  33784 th iteration => 0.00906616739263\n",
      "Loss for  33785 th iteration => 0.00906574870381\n",
      "Loss for  33786 th iteration => 0.0090654541639\n",
      "Loss for  33787 th iteration => 0.00906545034048\n",
      "Loss for  33788 th iteration => 0.00906486493273\n",
      "Loss for  33789 th iteration => 0.00906456843973\n",
      "Loss for  33790 th iteration => 0.0090642749619\n",
      "Loss for  33791 th iteration => 0.00906410700612\n",
      "Loss for  33792 th iteration => 0.0090636821548\n",
      "Loss for  33793 th iteration => 0.00906339077337\n",
      "Loss for  33794 th iteration => 0.00906339579844\n",
      "Loss for  33795 th iteration => 0.00906279877976\n",
      "Loss for  33796 th iteration => 0.00906250313976\n",
      "Loss for  33797 th iteration => 0.00906234456028\n",
      "Loss for  33798 th iteration => 0.00906191282984\n",
      "Loss for  33799 th iteration => 0.00906161750695\n",
      "Loss for  33800 th iteration => 0.00906163966745\n",
      "Loss for  33801 th iteration => 0.00906103315469\n",
      "Loss for  33802 th iteration => 0.00906086744829\n",
      "Loss for  33803 th iteration => 0.00906045527474\n",
      "Loss for  33804 th iteration => 0.00906040581214\n",
      "Loss for  33805 th iteration => 0.00905987572519\n",
      "Loss for  33806 th iteration => 0.00905966086849\n",
      "Loss for  33807 th iteration => 0.0090594762705\n",
      "Loss for  33808 th iteration => 0.00905905657708\n",
      "Loss for  33809 th iteration => 0.00905871850535\n",
      "Loss for  33810 th iteration => 0.00905845437832\n",
      "Loss for  33811 th iteration => 0.00905829023204\n",
      "Loss for  33812 th iteration => 0.00905784882524\n",
      "Loss for  33813 th iteration => 0.00905756413719\n",
      "Loss for  33814 th iteration => 0.0090574305529\n",
      "Loss for  33815 th iteration => 0.00905698917619\n",
      "Loss for  33816 th iteration => 0.00905681702575\n",
      "Loss for  33817 th iteration => 0.00905658532317\n",
      "Loss for  33818 th iteration => 0.00905619648103\n",
      "Loss for  33819 th iteration => 0.00905586435563\n",
      "Loss for  33820 th iteration => 0.00905558063959\n",
      "Loss for  33821 th iteration => 0.00905519736466\n",
      "Loss for  33822 th iteration => 0.00905497137451\n",
      "Loss for  33823 th iteration => 0.00905462412298\n",
      "Loss for  33824 th iteration => 0.00905462930564\n",
      "Loss for  33825 th iteration => 0.00905405037483\n",
      "Loss for  33826 th iteration => 0.0090537597312\n",
      "Loss for  33827 th iteration => 0.00905347427975\n",
      "Loss for  33828 th iteration => 0.0090531510807\n",
      "Loss for  33829 th iteration => 0.00905289819483\n",
      "Loss for  33830 th iteration => 0.00905254772875\n",
      "Loss for  33831 th iteration => 0.0090526025474\n",
      "Loss for  33832 th iteration => 0.0090519420167\n",
      "Loss for  33833 th iteration => 0.00905191321422\n",
      "Loss for  33834 th iteration => 0.00905135501922\n",
      "Loss for  33835 th iteration => 0.00905118934027\n",
      "Loss for  33836 th iteration => 0.0090507803986\n",
      "Loss for  33837 th iteration => 0.0090508248472\n",
      "Loss for  33838 th iteration => 0.0090502078817\n",
      "Loss for  33839 th iteration => 0.00904998027804\n",
      "Loss for  33840 th iteration => 0.00904963165426\n",
      "Loss for  33841 th iteration => 0.00904937281708\n",
      "Loss for  33842 th iteration => 0.00904905544315\n",
      "Loss for  33843 th iteration => 0.00904876761396\n",
      "Loss for  33844 th iteration => 0.00904873826512\n",
      "Loss for  33845 th iteration => 0.00904816476562\n",
      "Loss for  33846 th iteration => 0.00904790817044\n",
      "Loss for  33847 th iteration => 0.00904755788077\n",
      "Loss for  33848 th iteration => 0.0090473318557\n",
      "Loss for  33849 th iteration => 0.00904695113184\n",
      "Loss for  33850 th iteration => 0.00904675889961\n",
      "Loss for  33851 th iteration => 0.00904662340139\n",
      "Loss for  33852 th iteration => 0.0090460631901\n",
      "Loss for  33853 th iteration => 0.0090457680176\n",
      "Loss for  33854 th iteration => 0.00904559240986\n",
      "Loss for  33855 th iteration => 0.00904518046402\n",
      "Loss for  33856 th iteration => 0.00904488524385\n",
      "Loss for  33857 th iteration => 0.00904459558787\n",
      "Loss for  33858 th iteration => 0.00904457863823\n",
      "Loss for  33859 th iteration => 0.00904400546362\n",
      "Loss for  33860 th iteration => 0.00904371127136\n",
      "Loss for  33861 th iteration => 0.00904354178965\n",
      "Loss for  33862 th iteration => 0.00904312313035\n",
      "Loss for  33863 th iteration => 0.0090428283961\n",
      "Loss for  33864 th iteration => 0.00904269695897\n",
      "Loss for  33865 th iteration => 0.00904237677968\n",
      "Loss for  33866 th iteration => 0.0090419486499\n",
      "Loss for  33867 th iteration => 0.00904165568686\n",
      "Loss for  33868 th iteration => 0.0090414918425\n",
      "Loss for  33869 th iteration => 0.00904106670944\n",
      "Loss for  33870 th iteration => 0.00904077419289\n",
      "Loss for  33871 th iteration => 0.0090407869774\n",
      "Loss for  33872 th iteration => 0.00904018779101\n",
      "Loss for  33873 th iteration => 0.00904002644902\n",
      "Loss for  33874 th iteration => 0.00903961335956\n",
      "Loss for  33875 th iteration => 0.00903956263243\n",
      "Loss for  33876 th iteration => 0.00903903582221\n",
      "Loss for  33877 th iteration => 0.00903882536183\n",
      "Loss for  33878 th iteration => 0.00903863431169\n",
      "Loss for  33879 th iteration => 0.00903822331414\n",
      "Loss for  33880 th iteration => 0.00903788405811\n",
      "Loss for  33881 th iteration => 0.00903762383017\n",
      "Loss for  33882 th iteration => 0.00903745421958\n",
      "Loss for  33883 th iteration => 0.00903702073347\n",
      "Loss for  33884 th iteration => 0.00903673484938\n",
      "Loss for  33885 th iteration => 0.00903660075675\n",
      "Loss for  33886 th iteration => 0.00903615948692\n",
      "Loss for  33887 th iteration => 0.00903599258959\n",
      "Loss for  33888 th iteration => 0.00903575448378\n",
      "Loss for  33889 th iteration => 0.00903537487468\n",
      "Loss for  33890 th iteration => 0.00903504020947\n",
      "Loss for  33891 th iteration => 0.00903476239671\n",
      "Loss for  33892 th iteration => 0.00903437859883\n",
      "Loss for  33893 th iteration => 0.00903415571836\n",
      "Loss for  33894 th iteration => 0.00903380653671\n",
      "Loss for  33895 th iteration => 0.00903365082904\n",
      "Loss for  33896 th iteration => 0.00903339684103\n",
      "Loss for  33897 th iteration => 0.00903294915083\n",
      "Loss for  33898 th iteration => 0.0090326637289\n",
      "Loss for  33899 th iteration => 0.00903234309247\n",
      "Loss for  33900 th iteration => 0.00903209036945\n",
      "Loss for  33901 th iteration => 0.00903173917142\n",
      "Loss for  33902 th iteration => 0.00903179246766\n",
      "Loss for  33903 th iteration => 0.00903113768171\n",
      "Loss for  33904 th iteration => 0.00903094855465\n",
      "Loss for  33905 th iteration => 0.00903054034191\n",
      "Loss for  33906 th iteration => 0.00903024606316\n",
      "Loss for  33907 th iteration => 0.009029951805\n",
      "Loss for  33908 th iteration => 0.00902966462137\n",
      "Loss for  33909 th iteration => 0.00902977821506\n",
      "Loss for  33910 th iteration => 0.00902908420814\n",
      "Loss for  33911 th iteration => 0.00902894472601\n",
      "Loss for  33912 th iteration => 0.00902851101468\n",
      "Loss for  33913 th iteration => 0.00902833960193\n",
      "Loss for  33914 th iteration => 0.00902793783197\n",
      "Loss for  33915 th iteration => 0.00902774046504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  33916 th iteration => 0.00902761752965\n",
      "Loss for  33917 th iteration => 0.00902713616875\n",
      "Loss for  33918 th iteration => 0.00902679654954\n",
      "Loss for  33919 th iteration => 0.00902653163693\n",
      "Loss for  33920 th iteration => 0.00902622324806\n",
      "Loss for  33921 th iteration => 0.00902592724594\n",
      "Loss for  33922 th iteration => 0.0090256545568\n",
      "Loss for  33923 th iteration => 0.00902558941883\n",
      "Loss for  33924 th iteration => 0.00902508175241\n",
      "Loss for  33925 th iteration => 0.00902472543501\n",
      "Loss for  33926 th iteration => 0.00902450834624\n",
      "Loss for  33927 th iteration => 0.00902412161338\n",
      "Loss for  33928 th iteration => 0.00902393496038\n",
      "Loss for  33929 th iteration => 0.00902353495591\n",
      "Loss for  33930 th iteration => 0.00902351397804\n",
      "Loss for  33931 th iteration => 0.0090229463554\n",
      "Loss for  33932 th iteration => 0.0090226540734\n",
      "Loss for  33933 th iteration => 0.00902248328378\n",
      "Loss for  33934 th iteration => 0.00902206806086\n",
      "Loss for  33935 th iteration => 0.00902177490835\n",
      "Loss for  33936 th iteration => 0.00902164258832\n",
      "Loss for  33937 th iteration => 0.00902132423777\n",
      "Loss for  33938 th iteration => 0.00902089895012\n",
      "Loss for  33939 th iteration => 0.00902060739118\n",
      "Loss for  33940 th iteration => 0.00902044313619\n",
      "Loss for  33941 th iteration => 0.00902002104531\n",
      "Loss for  33942 th iteration => 0.00901972927585\n",
      "Loss for  33943 th iteration => 0.0090197411429\n",
      "Loss for  33944 th iteration => 0.00901914600607\n",
      "Loss for  33945 th iteration => 0.00901885245248\n",
      "Loss for  33946 th iteration => 0.00901856186318\n",
      "Loss for  33947 th iteration => 0.00901840365859\n",
      "Loss for  33948 th iteration => 0.00901797493732\n",
      "Loss for  33949 th iteration => 0.00901768507942\n",
      "Loss for  33950 th iteration => 0.00901770787854\n",
      "Loss for  33951 th iteration => 0.00901710542947\n",
      "Loss for  33952 th iteration => 0.00901694045468\n",
      "Loss for  33953 th iteration => 0.0090166772087\n",
      "Loss for  33954 th iteration => 0.0090163430829\n",
      "Loss for  33955 th iteration => 0.00901595934993\n",
      "Loss for  33956 th iteration => 0.00901574556044\n",
      "Loss for  33957 th iteration => 0.00901556637152\n",
      "Loss for  33958 th iteration => 0.00901514683698\n",
      "Loss for  33959 th iteration => 0.00901498459195\n",
      "Loss for  33960 th iteration => 0.00901456560266\n",
      "Loss for  33961 th iteration => 0.00901454583424\n",
      "Loss for  33962 th iteration => 0.00901398127668\n",
      "Loss for  33963 th iteration => 0.00901380383072\n",
      "Loss for  33964 th iteration => 0.00901339709572\n",
      "Loss for  33965 th iteration => 0.00901321209273\n",
      "Loss for  33966 th iteration => 0.00901281296585\n",
      "Loss for  33967 th iteration => 0.00901262292092\n",
      "Loss for  33968 th iteration => 0.00901239045059\n",
      "Loss for  33969 th iteration => 0.0090120321213\n",
      "Loss for  33970 th iteration => 0.00901165595256\n",
      "Loss for  33971 th iteration => 0.00901133620003\n",
      "Loss for  33972 th iteration => 0.00901108532544\n",
      "Loss for  33973 th iteration => 0.00901073436225\n",
      "Loss for  33974 th iteration => 0.00901062054869\n",
      "Loss for  33975 th iteration => 0.00901030068485\n",
      "Loss for  33976 th iteration => 0.00900994896386\n",
      "Loss for  33977 th iteration => 0.00900954270058\n",
      "Loss for  33978 th iteration => 0.00900924976914\n",
      "Loss for  33979 th iteration => 0.00900895685809\n",
      "Loss for  33980 th iteration => 0.0090086676952\n",
      "Loss for  33981 th iteration => 0.00900860985625\n",
      "Loss for  33982 th iteration => 0.00900825909393\n",
      "Loss for  33983 th iteration => 0.00900779981911\n",
      "Loss for  33984 th iteration => 0.00900766372565\n",
      "Loss for  33985 th iteration => 0.00900722940605\n",
      "Loss for  33986 th iteration => 0.00900706124695\n",
      "Loss for  33987 th iteration => 0.00900666138611\n",
      "Loss for  33988 th iteration => 0.00900670941081\n",
      "Loss for  33989 th iteration => 0.00900609358644\n",
      "Loss for  33990 th iteration => 0.00900586322158\n",
      "Loss for  33991 th iteration => 0.00900552304975\n",
      "Loss for  33992 th iteration => 0.00900526133364\n",
      "Loss for  33993 th iteration => 0.00900495252632\n",
      "Loss for  33994 th iteration => 0.00900466119093\n",
      "Loss for  33995 th iteration => 0.00900464908857\n",
      "Loss for  33996 th iteration => 0.0090040643372\n",
      "Loss for  33997 th iteration => 0.00900381654093\n",
      "Loss for  33998 th iteration => 0.00900346302746\n",
      "Loss for  33999 th iteration => 0.00900324590723\n",
      "Loss for  34000 th iteration => 0.00900286185475\n",
      "Loss for  34001 th iteration => 0.00900267678483\n",
      "Loss for  34002 th iteration => 0.0090025498546\n",
      "Loss for  34003 th iteration => 0.0090019833534\n",
      "Loss for  34004 th iteration => 0.00900169090426\n",
      "Loss for  34005 th iteration => 0.00900140084961\n",
      "Loss for  34006 th iteration => 0.0090012305171\n",
      "Loss for  34007 th iteration => 0.00900081667637\n",
      "Loss for  34008 th iteration => 0.00900052663769\n",
      "Loss for  34009 th iteration => 0.00900052570205\n",
      "Loss for  34010 th iteration => 0.00899994529165\n",
      "Loss for  34011 th iteration => 0.00899965297103\n",
      "Loss for  34012 th iteration => 0.00899936322707\n",
      "Loss for  34013 th iteration => 0.00899920014531\n",
      "Loss for  34014 th iteration => 0.00899877912987\n",
      "Loss for  34015 th iteration => 0.0089984897714\n",
      "Loss for  34016 th iteration => 0.00899850201073\n",
      "Loss for  34017 th iteration => 0.00899790814519\n",
      "Loss for  34018 th iteration => 0.00899774874849\n",
      "Loss for  34019 th iteration => 0.00899733878804\n",
      "Loss for  34020 th iteration => 0.00899728949322\n",
      "Loss for  34021 th iteration => 0.00899676708471\n",
      "Loss for  34022 th iteration => 0.00899655728623\n",
      "Loss for  34023 th iteration => 0.00899637044965\n",
      "Loss for  34024 th iteration => 0.0089959614662\n",
      "Loss for  34025 th iteration => 0.00899562621384\n",
      "Loss for  34026 th iteration => 0.0089953660427\n",
      "Loss for  34027 th iteration => 0.00899520171828\n",
      "Loss for  34028 th iteration => 0.00899476947941\n",
      "Loss for  34029 th iteration => 0.00899448604634\n",
      "Loss for  34030 th iteration => 0.00899435490543\n",
      "Loss for  34031 th iteration => 0.00899391979826\n",
      "Loss for  34032 th iteration => 0.00899374775954\n",
      "Loss for  34033 th iteration => 0.00899336629345\n",
      "Loss for  34034 th iteration => 0.00899328636923\n",
      "Loss for  34035 th iteration => 0.00899281147187\n",
      "Loss for  34036 th iteration => 0.00899253007669\n",
      "Loss for  34037 th iteration => 0.00899215478177\n",
      "Loss for  34038 th iteration => 0.00899192870848\n",
      "Loss for  34039 th iteration => 0.00899158701226\n",
      "Loss for  34040 th iteration => 0.00899133107924\n",
      "Loss for  34041 th iteration => 0.00899128205627\n",
      "Loss for  34042 th iteration => 0.00899073258859\n",
      "Loss for  34043 th iteration => 0.00899045626702\n",
      "Loss for  34044 th iteration => 0.00899013184574\n",
      "Loss for  34045 th iteration => 0.00898988834576\n",
      "Loss for  34046 th iteration => 0.0089895312533\n",
      "Loss for  34047 th iteration => 0.00898932292232\n",
      "Loss for  34048 th iteration => 0.00898920649181\n",
      "Loss for  34049 th iteration => 0.00898875901903\n",
      "Loss for  34050 th iteration => 0.00898850873768\n",
      "Loss for  34051 th iteration => 0.00898807426557\n",
      "Loss for  34052 th iteration => 0.00898790863384\n",
      "Loss for  34053 th iteration => 0.0089875063561\n",
      "Loss for  34054 th iteration => 0.0089873119994\n",
      "Loss for  34055 th iteration => 0.00898718184176\n",
      "Loss for  34056 th iteration => 0.00898671515859\n",
      "Loss for  34057 th iteration => 0.00898637552887\n",
      "Loss for  34058 th iteration => 0.00898611565321\n",
      "Loss for  34059 th iteration => 0.00898580749149\n",
      "Loss for  34060 th iteration => 0.00898551629103\n",
      "Loss for  34061 th iteration => 0.0089852397554\n",
      "Loss for  34062 th iteration => 0.00898505922898\n",
      "Loss for  34063 th iteration => 0.00898479299708\n",
      "Loss for  34064 th iteration => 0.0089843243933\n",
      "Loss for  34065 th iteration => 0.00898410834507\n",
      "Loss for  34066 th iteration => 0.00898372560702\n",
      "Loss for  34067 th iteration => 0.00898354020958\n",
      "Loss for  34068 th iteration => 0.0089831404797\n",
      "Loss for  34069 th iteration => 0.00898312162781\n",
      "Loss for  34070 th iteration => 0.00898256313196\n",
      "Loss for  34071 th iteration => 0.00898227192606\n",
      "Loss for  34072 th iteration => 0.00898198269161\n",
      "Loss for  34073 th iteration => 0.00898181005973\n",
      "Loss for  34074 th iteration => 0.00898140140469\n",
      "Loss for  34075 th iteration => 0.00898111027746\n",
      "Loss for  34076 th iteration => 0.00898097720256\n",
      "Loss for  34077 th iteration => 0.00898066242798\n",
      "Loss for  34078 th iteration => 0.00898024262911\n",
      "Loss for  34079 th iteration => 0.00897995316975\n",
      "Loss for  34080 th iteration => 0.00897978875066\n",
      "Loss for  34081 th iteration => 0.00897937249168\n",
      "Loss for  34082 th iteration => 0.0089790814892\n",
      "Loss for  34083 th iteration => 0.00897879657597\n",
      "Loss for  34084 th iteration => 0.00897880047288\n",
      "Loss for  34085 th iteration => 0.00897821423236\n",
      "Loss for  34086 th iteration => 0.00897792474944\n",
      "Loss for  34087 th iteration => 0.00897776814405\n",
      "Loss for  34088 th iteration => 0.00897734447869\n",
      "Loss for  34089 th iteration => 0.00897705360372\n",
      "Loss for  34090 th iteration => 0.00897676882849\n",
      "Loss for  34091 th iteration => 0.00897678679162\n",
      "Loss for  34092 th iteration => 0.00897619090082\n",
      "Loss for  34093 th iteration => 0.00897603079216\n",
      "Loss for  34094 th iteration => 0.00897576541961\n",
      "Loss for  34095 th iteration => 0.00897543708433\n",
      "Loss for  34096 th iteration => 0.00897505538218\n",
      "Loss for  34097 th iteration => 0.00897484496536\n",
      "Loss for  34098 th iteration => 0.00897466578648\n",
      "Loss for  34099 th iteration => 0.00897425085189\n",
      "Loss for  34100 th iteration => 0.00897408989841\n",
      "Loss for  34101 th iteration => 0.00897382393031\n",
      "Loss for  34102 th iteration => 0.0089735059493\n",
      "Loss for  34103 th iteration => 0.00897309566795\n",
      "Loss for  34104 th iteration => 0.00897291923365\n",
      "Loss for  34105 th iteration => 0.00897251667368\n",
      "Loss for  34106 th iteration => 0.00897233264657\n",
      "Loss for  34107 th iteration => 0.00897193772974\n",
      "Loss for  34108 th iteration => 0.0089717479832\n",
      "Loss for  34109 th iteration => 0.00897151940052\n",
      "Loss for  34110 th iteration => 0.00897116293295\n",
      "Loss for  34111 th iteration => 0.00897079189072\n",
      "Loss for  34112 th iteration => 0.008970472389\n",
      "Loss for  34113 th iteration => 0.00897022654408\n",
      "Loss for  34114 th iteration => 0.00896987444654\n",
      "Loss for  34115 th iteration => 0.00896976560876\n",
      "Loss for  34116 th iteration => 0.00896944667319\n",
      "Loss for  34117 th iteration => 0.00896910066188\n",
      "Loss for  34118 th iteration => 0.00896869636199\n",
      "Loss for  34119 th iteration => 0.00896840604217\n",
      "Loss for  34120 th iteration => 0.00896811574237\n",
      "Loss for  34121 th iteration => 0.00896782546257\n",
      "Loss for  34122 th iteration => 0.00896754176617\n",
      "Loss for  34123 th iteration => 0.00896765799721\n",
      "Loss for  34124 th iteration => 0.00896697099489\n",
      "Loss for  34125 th iteration => 0.00896683157549\n",
      "Loss for  34126 th iteration => 0.00896640582304\n",
      "Loss for  34127 th iteration => 0.00896623423254\n",
      "Loss for  34128 th iteration => 0.00896584065708\n",
      "Loss for  34129 th iteration => 0.00896564019393\n",
      "Loss for  34130 th iteration => 0.00896552933306\n",
      "Loss for  34131 th iteration => 0.00896504623549\n",
      "Loss for  34132 th iteration => 0.00896471527818\n",
      "Loss for  34133 th iteration => 0.00896444948999\n",
      "Loss for  34134 th iteration => 0.00896414998272\n",
      "Loss for  34135 th iteration => 0.00896385288761\n",
      "Loss for  34136 th iteration => 0.00896358471978\n",
      "Loss for  34137 th iteration => 0.00896338358806\n",
      "Loss for  34138 th iteration => 0.00896316666021\n",
      "Loss for  34139 th iteration => 0.00896266642802\n",
      "Loss for  34140 th iteration => 0.00896245905947\n",
      "Loss for  34141 th iteration => 0.00896207040111\n",
      "Loss for  34142 th iteration => 0.00896189366283\n",
      "Loss for  34143 th iteration => 0.00896148846223\n",
      "Loss for  34144 th iteration => 0.0089613316685\n",
      "Loss for  34145 th iteration => 0.0089610640719\n",
      "Loss for  34146 th iteration => 0.00896062453084\n",
      "Loss for  34147 th iteration => 0.00896033551174\n",
      "Loss for  34148 th iteration => 0.00896017186267\n",
      "Loss for  34149 th iteration => 0.00895975812888\n",
      "Loss for  34150 th iteration => 0.00895946837842\n",
      "Loss for  34151 th iteration => 0.0089591835589\n",
      "Loss for  34152 th iteration => 0.00895918679563\n",
      "Loss for  34153 th iteration => 0.00895860483634\n",
      "Loss for  34154 th iteration => 0.00895831536181\n",
      "Loss for  34155 th iteration => 0.00895816028828\n",
      "Loss for  34156 th iteration => 0.00895773881543\n",
      "Loss for  34157 th iteration => 0.00895744919147\n",
      "Loss for  34158 th iteration => 0.0089571637147\n",
      "Loss for  34159 th iteration => 0.00895718241931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  34160 th iteration => 0.008956589354\n",
      "Loss for  34161 th iteration => 0.00895642876652\n",
      "Loss for  34162 th iteration => 0.00895602582222\n",
      "Loss for  34163 th iteration => 0.00895597766665\n",
      "Loss for  34164 th iteration => 0.00895545885578\n",
      "Loss for  34165 th iteration => 0.00895524735276\n",
      "Loss for  34166 th iteration => 0.00895507116207\n",
      "Loss for  34167 th iteration => 0.00895465787816\n",
      "Loss for  34168 th iteration => 0.00895449547216\n",
      "Loss for  34169 th iteration => 0.00895408281659\n",
      "Loss for  34170 th iteration => 0.00895406201433\n",
      "Loss for  34171 th iteration => 0.00895350757821\n",
      "Loss for  34172 th iteration => 0.00895332987668\n",
      "Loss for  34173 th iteration => 0.00895293105757\n",
      "Loss for  34174 th iteration => 0.0089527457556\n",
      "Loss for  34175 th iteration => 0.00895235458691\n",
      "Loss for  34176 th iteration => 0.00895216176204\n",
      "Loss for  34177 th iteration => 0.00895178140377\n",
      "Loss for  34178 th iteration => 0.0089517371986\n",
      "Loss for  34179 th iteration => 0.0089512150729\n",
      "Loss for  34180 th iteration => 0.00895089334063\n",
      "Loss for  34181 th iteration => 0.00895065223602\n",
      "Loss for  34182 th iteration => 0.00895029761597\n",
      "Loss for  34183 th iteration => 0.0089500908077\n",
      "Loss for  34184 th iteration => 0.00894980661408\n",
      "Loss for  34185 th iteration => 0.00894969614085\n",
      "Loss for  34186 th iteration => 0.00894912774363\n",
      "Loss for  34187 th iteration => 0.00894883867022\n",
      "Loss for  34188 th iteration => 0.00894854961664\n",
      "Loss for  34189 th iteration => 0.00894826058289\n",
      "Loss for  34190 th iteration => 0.00894797235796\n",
      "Loss for  34191 th iteration => 0.00894779626856\n",
      "Loss for  34192 th iteration => 0.00894769870631\n",
      "Loss for  34193 th iteration => 0.00894712123695\n",
      "Loss for  34194 th iteration => 0.00894698261335\n",
      "Loss for  34195 th iteration => 0.00894655862147\n",
      "Loss for  34196 th iteration => 0.00894638774143\n",
      "Loss for  34197 th iteration => 0.00894599601027\n",
      "Loss for  34198 th iteration => 0.00894579675928\n",
      "Loss for  34199 th iteration => 0.00894568740629\n",
      "Loss for  34200 th iteration => 0.0089452046444\n",
      "Loss for  34201 th iteration => 0.00894487570078\n",
      "Loss for  34202 th iteration => 0.00894461037183\n",
      "Loss for  34203 th iteration => 0.00894431295655\n",
      "Loss for  34204 th iteration => 0.00894401624317\n",
      "Loss for  34205 th iteration => 0.00894375063401\n",
      "Loss for  34206 th iteration => 0.00894354870917\n",
      "Loss for  34207 th iteration => 0.00894333558627\n",
      "Loss for  34208 th iteration => 0.00894283469001\n",
      "Loss for  34209 th iteration => 0.00894262964229\n",
      "Loss for  34210 th iteration => 0.0089422411385\n",
      "Loss for  34211 th iteration => 0.00894206679186\n",
      "Loss for  34212 th iteration => 0.00894166217468\n",
      "Loss for  34213 th iteration => 0.00894150569649\n",
      "Loss for  34214 th iteration => 0.00894124137234\n",
      "Loss for  34215 th iteration => 0.00894080191188\n",
      "Loss for  34216 th iteration => 0.00894051347456\n",
      "Loss for  34217 th iteration => 0.00894035272137\n",
      "Loss for  34218 th iteration => 0.00893993927519\n",
      "Loss for  34219 th iteration => 0.00893965078637\n",
      "Loss for  34220 th iteration => 0.00893936561453\n",
      "Loss for  34221 th iteration => 0.00893937351071\n",
      "Loss for  34222 th iteration => 0.00893879098976\n",
      "Loss for  34223 th iteration => 0.00893850257156\n",
      "Loss for  34224 th iteration => 0.00893821686202\n",
      "Loss for  34225 th iteration => 0.00893806124053\n",
      "Loss for  34226 th iteration => 0.00893764037095\n",
      "Loss for  34227 th iteration => 0.0089373538341\n",
      "Loss for  34228 th iteration => 0.00893737842659\n",
      "Loss for  34229 th iteration => 0.00893678550186\n",
      "Loss for  34230 th iteration => 0.00893662376833\n",
      "Loss for  34231 th iteration => 0.0089362230481\n",
      "Loss for  34232 th iteration => 0.00893617667477\n",
      "Loss for  34233 th iteration => 0.00893566003342\n",
      "Loss for  34234 th iteration => 0.00893544619168\n",
      "Loss for  34235 th iteration => 0.00893509841898\n",
      "Loss for  34236 th iteration => 0.00893503572487\n",
      "Loss for  34237 th iteration => 0.00893453786449\n",
      "Loss for  34238 th iteration => 0.008934438195\n",
      "Loss for  34239 th iteration => 0.00893414407126\n",
      "Loss for  34240 th iteration => 0.00893383267054\n",
      "Loss for  34241 th iteration => 0.00893344566443\n",
      "Loss for  34242 th iteration => 0.00893322444692\n",
      "Loss for  34243 th iteration => 0.00893289793944\n",
      "Loss for  34244 th iteration => 0.00893261663869\n",
      "Loss for  34245 th iteration => 0.0089323499915\n",
      "Loss for  34246 th iteration => 0.00893201027515\n",
      "Loss for  34247 th iteration => 0.00893196410527\n",
      "Loss for  34248 th iteration => 0.00893141875622\n",
      "Loss for  34249 th iteration => 0.00893114859216\n",
      "Loss for  34250 th iteration => 0.00893082539231\n",
      "Loss for  34251 th iteration => 0.00893058831615\n",
      "Loss for  34252 th iteration => 0.00893023218153\n",
      "Loss for  34253 th iteration => 0.00893002939196\n",
      "Loss for  34254 th iteration => 0.00892974650689\n",
      "Loss for  34255 th iteration => 0.00892963953736\n",
      "Loss for  34256 th iteration => 0.00892906991041\n",
      "Loss for  34257 th iteration => 0.0089289387467\n",
      "Loss for  34258 th iteration => 0.00892850966507\n",
      "Loss for  34259 th iteration => 0.00892834606252\n",
      "Loss for  34260 th iteration => 0.00892794942177\n",
      "Loss for  34261 th iteration => 0.00892775836398\n",
      "Loss for  34262 th iteration => 0.00892763355998\n",
      "Loss for  34263 th iteration => 0.00892716730076\n",
      "Loss for  34264 th iteration => 0.00892683380648\n",
      "Loss for  34265 th iteration => 0.00892657522059\n",
      "Loss for  34266 th iteration => 0.00892627342478\n",
      "Loss for  34267 th iteration => 0.00892598328597\n",
      "Loss for  34268 th iteration => 0.0089257133676\n",
      "Loss for  34269 th iteration => 0.00892551423088\n",
      "Loss for  34270 th iteration => 0.00892529581005\n",
      "Loss for  34271 th iteration => 0.00892480608552\n",
      "Loss for  34272 th iteration => 0.00892459714652\n",
      "Loss for  34273 th iteration => 0.00892421473218\n",
      "Loss for  34274 th iteration => 0.00892403665134\n",
      "Loss for  34275 th iteration => 0.00892363779128\n",
      "Loss for  34276 th iteration => 0.00892335437426\n",
      "Loss for  34277 th iteration => 0.00892333927951\n",
      "Loss for  34278 th iteration => 0.00892278101422\n",
      "Loss for  34279 th iteration => 0.0089224936166\n",
      "Loss for  34280 th iteration => 0.00892220942063\n",
      "Loss for  34281 th iteration => 0.00892204198858\n",
      "Loss for  34282 th iteration => 0.00892163445423\n",
      "Loss for  34283 th iteration => 0.00892134817752\n",
      "Loss for  34284 th iteration => 0.00892135255539\n",
      "Loss for  34285 th iteration => 0.008920778055\n",
      "Loss for  34286 th iteration => 0.00892049078194\n",
      "Loss for  34287 th iteration => 0.00892020530821\n",
      "Loss for  34288 th iteration => 0.00892004789933\n",
      "Loss for  34289 th iteration => 0.00891963199554\n",
      "Loss for  34290 th iteration => 0.0089193447966\n",
      "Loss for  34291 th iteration => 0.00891906275007\n",
      "Loss for  34292 th iteration => 0.00891907806989\n",
      "Loss for  34293 th iteration => 0.00891849097651\n",
      "Loss for  34294 th iteration => 0.00891833411792\n",
      "Loss for  34295 th iteration => 0.00891806974244\n",
      "Loss for  34296 th iteration => 0.00891774848448\n",
      "Loss for  34297 th iteration => 0.00891737023782\n",
      "Loss for  34298 th iteration => 0.00891716097145\n",
      "Loss for  34299 th iteration => 0.00891698569351\n",
      "Loss for  34300 th iteration => 0.00891657664138\n",
      "Loss for  34301 th iteration => 0.00891641446185\n",
      "Loss for  34302 th iteration => 0.0089160058816\n",
      "Loss for  34303 th iteration => 0.00891598419802\n",
      "Loss for  34304 th iteration => 0.00891543589751\n",
      "Loss for  34305 th iteration => 0.00891525836191\n",
      "Loss for  34306 th iteration => 0.00891486417413\n",
      "Loss for  34307 th iteration => 0.00891467900968\n",
      "Loss for  34308 th iteration => 0.00891429250008\n",
      "Loss for  34309 th iteration => 0.00891409978495\n",
      "Loss for  34310 th iteration => 0.00891372272449\n",
      "Loss for  34311 th iteration => 0.0089136790803\n",
      "Loss for  34312 th iteration => 0.00891316366007\n",
      "Loss for  34313 th iteration => 0.00891284134449\n",
      "Loss for  34314 th iteration => 0.00891260570262\n",
      "Loss for  34315 th iteration => 0.00891225035186\n",
      "Loss for  34316 th iteration => 0.00891204774423\n",
      "Loss for  34317 th iteration => 0.00891166284546\n",
      "Loss for  34318 th iteration => 0.00891175934418\n",
      "Loss for  34319 th iteration => 0.00891109361497\n",
      "Loss for  34320 th iteration => 0.00891096205563\n",
      "Loss for  34321 th iteration => 0.00891053567373\n",
      "Loss for  34322 th iteration => 0.00891037160115\n",
      "Loss for  34323 th iteration => 0.00890997773307\n",
      "Loss for  34324 th iteration => 0.0089097819\n",
      "Loss for  34325 th iteration => 0.00890953186931\n",
      "Loss for  34326 th iteration => 0.00890932223214\n",
      "Loss for  34327 th iteration => 0.00890886666119\n",
      "Loss for  34328 th iteration => 0.00890860742939\n",
      "Loss for  34329 th iteration => 0.00890830858039\n",
      "Loss for  34330 th iteration => 0.00890801772557\n",
      "Loss for  34331 th iteration => 0.00890775050564\n",
      "Loss for  34332 th iteration => 0.00890742914637\n",
      "Loss for  34333 th iteration => 0.00890731316344\n",
      "Loss for  34334 th iteration => 0.0089069794335\n",
      "Loss for  34335 th iteration => 0.00890663921442\n",
      "Loss for  34336 th iteration => 0.00890625578209\n",
      "Loss for  34337 th iteration => 0.00890608101456\n",
      "Loss for  34338 th iteration => 0.00890568474037\n",
      "Loss for  34339 th iteration => 0.00890539842999\n",
      "Loss for  34340 th iteration => 0.00890511566787\n",
      "Loss for  34341 th iteration => 0.00890509838936\n",
      "Loss for  34342 th iteration => 0.00890454508641\n",
      "Loss for  34343 th iteration => 0.00890425884568\n",
      "Loss for  34344 th iteration => 0.008903975039\n",
      "Loss for  34345 th iteration => 0.00890380801575\n",
      "Loss for  34346 th iteration => 0.00890340313322\n",
      "Loss for  34347 th iteration => 0.00890311752669\n",
      "Loss for  34348 th iteration => 0.00890296540944\n",
      "Loss for  34349 th iteration => 0.00890270369909\n",
      "Loss for  34350 th iteration => 0.00890226405068\n",
      "Loss for  34351 th iteration => 0.00890197851662\n",
      "Loss for  34352 th iteration => 0.00890182231711\n",
      "Loss for  34353 th iteration => 0.00890140871176\n",
      "Loss for  34354 th iteration => 0.00890112266821\n",
      "Loss for  34355 th iteration => 0.00890083956588\n",
      "Loss for  34356 th iteration => 0.00890085572645\n",
      "Loss for  34357 th iteration => 0.00890027163421\n",
      "Loss for  34358 th iteration => 0.00890011456808\n",
      "Loss for  34359 th iteration => 0.00889971423464\n",
      "Loss for  34360 th iteration => 0.00889966723547\n",
      "Loss for  34361 th iteration => 0.00889915553068\n",
      "Loss for  34362 th iteration => 0.00889894598023\n",
      "Loss for  34363 th iteration => 0.00889859814458\n",
      "Loss for  34364 th iteration => 0.00889853550565\n",
      "Loss for  34365 th iteration => 0.0088980397349\n",
      "Loss for  34366 th iteration => 0.00889794471226\n",
      "Loss for  34367 th iteration => 0.00889749976842\n",
      "Loss for  34368 th iteration => 0.00889748794074\n",
      "Loss for  34369 th iteration => 0.00889695702156\n",
      "Loss for  34370 th iteration => 0.00889673995316\n",
      "Loss for  34371 th iteration => 0.00889641404874\n",
      "Loss for  34372 th iteration => 0.00889613653875\n",
      "Loss for  34373 th iteration => 0.00889587084952\n",
      "Loss for  34374 th iteration => 0.00889553363689\n",
      "Loss for  34375 th iteration => 0.00889548587174\n",
      "Loss for  34376 th iteration => 0.00889494890425\n",
      "Loss for  34377 th iteration => 0.00889468114646\n",
      "Loss for  34378 th iteration => 0.00889436004138\n",
      "Loss for  34379 th iteration => 0.00889412555781\n",
      "Loss for  34380 th iteration => 0.00889377133357\n",
      "Loss for  34381 th iteration => 0.00889356996673\n",
      "Loss for  34382 th iteration => 0.00889318620205\n",
      "Loss for  34383 th iteration => 0.00889328413486\n",
      "Loss for  34384 th iteration => 0.00889261925056\n",
      "Loss for  34385 th iteration => 0.00889248870206\n",
      "Loss for  34386 th iteration => 0.00889206366637\n",
      "Loss for  34387 th iteration => 0.00889190054138\n",
      "Loss for  34388 th iteration => 0.00889150808145\n",
      "Loss for  34389 th iteration => 0.00889131253333\n",
      "Loss for  34390 th iteration => 0.00889095616607\n",
      "Loss for  34391 th iteration => 0.0088909645676\n",
      "Loss for  34392 th iteration => 0.00889040168275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  34393 th iteration => 0.00889014321232\n",
      "Loss for  34394 th iteration => 0.00888984595512\n",
      "Loss for  34395 th iteration => 0.00888955580391\n",
      "Loss for  34396 th iteration => 0.00888929023224\n",
      "Loss for  34397 th iteration => 0.00888896854154\n",
      "Loss for  34398 th iteration => 0.00888873904758\n",
      "Loss for  34399 th iteration => 0.00888863900732\n",
      "Loss for  34400 th iteration => 0.00888818361696\n",
      "Loss for  34401 th iteration => 0.00888780070231\n",
      "Loss for  34402 th iteration => 0.00888762776592\n",
      "Loss for  34403 th iteration => 0.00888723221438\n",
      "Loss for  34404 th iteration => 0.00888694707151\n",
      "Loss for  34405 th iteration => 0.00888666291151\n",
      "Loss for  34406 th iteration => 0.00888665182756\n",
      "Loss for  34407 th iteration => 0.00888609719442\n",
      "Loss for  34408 th iteration => 0.00888581212074\n",
      "Loss for  34409 th iteration => 0.00888552889071\n",
      "Loss for  34410 th iteration => 0.00888536430034\n",
      "Loss for  34411 th iteration => 0.00888495989012\n",
      "Loss for  34412 th iteration => 0.00888467488937\n",
      "Loss for  34413 th iteration => 0.00888439270332\n",
      "Loss for  34414 th iteration => 0.00888439662502\n",
      "Loss for  34415 th iteration => 0.00888382544076\n",
      "Loss for  34416 th iteration => 0.00888354050918\n",
      "Loss for  34417 th iteration => 0.00888325853384\n",
      "Loss for  34418 th iteration => 0.00888310166891\n",
      "Loss for  34419 th iteration => 0.00888268870597\n",
      "Loss for  34420 th iteration => 0.00888240385013\n",
      "Loss for  34421 th iteration => 0.00888226191493\n",
      "Loss for  34422 th iteration => 0.00888200777444\n",
      "Loss for  34423 th iteration => 0.00888168268962\n",
      "Loss for  34424 th iteration => 0.00888128704797\n",
      "Loss for  34425 th iteration => 0.00888123889387\n",
      "Loss for  34426 th iteration => 0.008880731983\n",
      "Loss for  34427 th iteration => 0.00888051870919\n",
      "Loss for  34428 th iteration => 0.00888017493432\n",
      "Loss for  34429 th iteration => 0.00887993864355\n",
      "Loss for  34430 th iteration => 0.00887979536072\n",
      "Loss for  34431 th iteration => 0.00887935801206\n",
      "Loss for  34432 th iteration => 0.0088792310845\n",
      "Loss for  34433 th iteration => 0.00887894061463\n",
      "Loss for  34434 th iteration => 0.00887865776866\n",
      "Loss for  34435 th iteration => 0.00887822667459\n",
      "Loss for  34436 th iteration => 0.00887808291388\n",
      "Loss for  34437 th iteration => 0.00887765964956\n",
      "Loss for  34438 th iteration => 0.00887750818718\n",
      "Loss for  34439 th iteration => 0.00887709290258\n",
      "Loss for  34440 th iteration => 0.00887684098043\n",
      "Loss for  34441 th iteration => 0.00887679652515\n",
      "Loss for  34442 th iteration => 0.00887625642902\n",
      "Loss for  34443 th iteration => 0.00887599139211\n",
      "Loss for  34444 th iteration => 0.0088756698853\n",
      "Loss for  34445 th iteration => 0.00887543818107\n",
      "Loss for  34446 th iteration => 0.00887508349725\n",
      "Loss for  34447 th iteration => 0.0088748851697\n",
      "Loss for  34448 th iteration => 0.0088746028223\n",
      "Loss for  34449 th iteration => 0.00887450224721\n",
      "Loss for  34450 th iteration => 0.00887393804182\n",
      "Loss for  34451 th iteration => 0.00887380602627\n",
      "Loss for  34452 th iteration => 0.00887338482546\n",
      "Loss for  34453 th iteration => 0.00887322019263\n",
      "Loss for  34454 th iteration => 0.00887283160724\n",
      "Loss for  34455 th iteration => 0.00887263493816\n",
      "Loss for  34456 th iteration => 0.00887239278002\n",
      "Loss for  34457 th iteration => 0.00887218304048\n",
      "Loss for  34458 th iteration => 0.00887172991257\n",
      "Loss for  34459 th iteration => 0.00887146980536\n",
      "Loss for  34460 th iteration => 0.00887117654887\n",
      "Loss for  34461 th iteration => 0.00887088472547\n",
      "Loss for  34462 th iteration => 0.00887062318882\n",
      "Loss for  34463 th iteration => 0.00887029999996\n",
      "Loss for  34464 th iteration => 0.00887019253097\n",
      "Loss for  34465 th iteration => 0.0088698591962\n",
      "Loss for  34466 th iteration => 0.00886952125989\n",
      "Loss for  34467 th iteration => 0.00886913657595\n",
      "Loss for  34468 th iteration => 0.00886896776949\n",
      "Loss for  34469 th iteration => 0.00886857065625\n",
      "Loss for  34470 th iteration => 0.00886828669169\n",
      "Loss for  34471 th iteration => 0.00886800361922\n",
      "Loss for  34472 th iteration => 0.00886784967075\n",
      "Loss for  34473 th iteration => 0.00886758844552\n",
      "Loss for  34474 th iteration => 0.00886715642035\n",
      "Loss for  34475 th iteration => 0.00886687378465\n",
      "Loss for  34476 th iteration => 0.00886671392266\n",
      "Loss for  34477 th iteration => 0.00886630770414\n",
      "Loss for  34478 th iteration => 0.00886602388069\n",
      "Loss for  34479 th iteration => 0.00886574203752\n",
      "Loss for  34480 th iteration => 0.008865753084\n",
      "Loss for  34481 th iteration => 0.00886517792885\n",
      "Loss for  34482 th iteration => 0.00886489417411\n",
      "Loss for  34483 th iteration => 0.00886461250545\n",
      "Loss for  34484 th iteration => 0.00886446084044\n",
      "Loss for  34485 th iteration => 0.00886404588255\n",
      "Loss for  34486 th iteration => 0.00886376220024\n",
      "Loss for  34487 th iteration => 0.00886348239661\n",
      "Loss for  34488 th iteration => 0.0088635081424\n",
      "Loss for  34489 th iteration => 0.00886292300591\n",
      "Loss for  34490 th iteration => 0.00886276261909\n",
      "Loss for  34491 th iteration => 0.00886237114733\n",
      "Loss for  34492 th iteration => 0.00886232411967\n",
      "Loss for  34493 th iteration => 0.0088618163205\n",
      "Loss for  34494 th iteration => 0.0088616033259\n",
      "Loss for  34495 th iteration => 0.00886126358734\n",
      "Loss for  34496 th iteration => 0.00886120254236\n",
      "Loss for  34497 th iteration => 0.00886071646901\n",
      "Loss for  34498 th iteration => 0.00886060982917\n",
      "Loss for  34499 th iteration => 0.00886033256373\n",
      "Loss for  34500 th iteration => 0.0088600134939\n",
      "Loss for  34501 th iteration => 0.00885964311317\n",
      "Loss for  34502 th iteration => 0.00885941429525\n",
      "Loss for  34503 th iteration => 0.00885910483885\n",
      "Loss for  34504 th iteration => 0.00885881551544\n",
      "Loss for  34505 th iteration => 0.00885856633689\n",
      "Loss for  34506 th iteration => 0.00885821714883\n",
      "Loss for  34507 th iteration => 0.0088580297104\n",
      "Loss for  34508 th iteration => 0.00885779723633\n",
      "Loss for  34509 th iteration => 0.00885738098465\n",
      "Loss for  34510 th iteration => 0.00885705388623\n",
      "Loss for  34511 th iteration => 0.00885683009503\n",
      "Loss for  34512 th iteration => 0.00885646984764\n",
      "Loss for  34513 th iteration => 0.00885627920105\n",
      "Loss for  34514 th iteration => 0.00885588673096\n",
      "Loss for  34515 th iteration => 0.00885583930133\n",
      "Loss for  34516 th iteration => 0.0088554959492\n",
      "Loss for  34517 th iteration => 0.0088550522262\n",
      "Loss for  34518 th iteration => 0.0088549180218\n",
      "Loss for  34519 th iteration => 0.00885450138244\n",
      "Loss for  34520 th iteration => 0.00885433455356\n",
      "Loss for  34521 th iteration => 0.00885395053596\n",
      "Loss for  34522 th iteration => 0.00885375163966\n",
      "Loss for  34523 th iteration => 0.00885351883305\n",
      "Loss for  34524 th iteration => 0.00885330650004\n",
      "Loss for  34525 th iteration => 0.00885285355619\n",
      "Loss for  34526 th iteration => 0.00885259121901\n",
      "Loss for  34527 th iteration => 0.00885230256247\n",
      "Loss for  34528 th iteration => 0.00885200850439\n",
      "Loss for  34529 th iteration => 0.00885175157153\n",
      "Loss for  34530 th iteration => 0.00885142593627\n",
      "Loss for  34531 th iteration => 0.00885120560799\n",
      "Loss for  34532 th iteration => 0.00885111415595\n",
      "Loss for  34533 th iteration => 0.00885065436711\n",
      "Loss for  34534 th iteration => 0.00885026739221\n",
      "Loss for  34535 th iteration => 0.00885010324385\n",
      "Loss for  34536 th iteration => 0.00884970315092\n",
      "Loss for  34537 th iteration => 0.00884942037495\n",
      "Loss for  34538 th iteration => 0.0088491381711\n",
      "Loss for  34539 th iteration => 0.00884914348507\n",
      "Loss for  34540 th iteration => 0.00884857752853\n",
      "Loss for  34541 th iteration => 0.00884829482083\n",
      "Loss for  34542 th iteration => 0.00884801387286\n",
      "Loss for  34543 th iteration => 0.00884785893715\n",
      "Loss for  34544 th iteration => 0.00884744965056\n",
      "Loss for  34545 th iteration => 0.00884716701476\n",
      "Loss for  34546 th iteration => 0.00884688581365\n",
      "Loss for  34547 th iteration => 0.00884690769223\n",
      "Loss for  34548 th iteration => 0.00884632853887\n",
      "Loss for  34549 th iteration => 0.00884616963175\n",
      "Loss for  34550 th iteration => 0.00884577795442\n",
      "Loss for  34551 th iteration => 0.00884572893773\n",
      "Loss for  34552 th iteration => 0.00884522608524\n",
      "Loss for  34553 th iteration => 0.00884501438762\n",
      "Loss for  34554 th iteration => 0.00884467391685\n",
      "Loss for  34555 th iteration => 0.00884461163818\n",
      "Loss for  34556 th iteration => 0.00884412716113\n",
      "Loss for  34557 th iteration => 0.00884402266865\n",
      "Loss for  34558 th iteration => 0.0088435928734\n",
      "Loss for  34559 th iteration => 0.00884357328204\n",
      "Loss for  34560 th iteration => 0.00884305811332\n",
      "Loss for  34561 th iteration => 0.00884283105625\n",
      "Loss for  34562 th iteration => 0.00884252199139\n",
      "Loss for  34563 th iteration => 0.00884223425211\n",
      "Loss for  34564 th iteration => 0.00884198564015\n",
      "Loss for  34565 th iteration => 0.00884163978424\n",
      "Loss for  34566 th iteration => 0.00884135182037\n",
      "Loss for  34567 th iteration => 0.00884131398388\n",
      "Loss for  34568 th iteration => 0.00884080653863\n",
      "Loss for  34569 th iteration => 0.00884048094714\n",
      "Loss for  34570 th iteration => 0.00884025777055\n",
      "Loss for  34571 th iteration => 0.00883989893405\n",
      "Loss for  34572 th iteration => 0.00883970899657\n",
      "Loss for  34573 th iteration => 0.00883931949061\n",
      "Loss for  34574 th iteration => 0.00883904004473\n",
      "Loss for  34575 th iteration => 0.00883915288558\n",
      "Loss for  34576 th iteration => 0.00883848637368\n",
      "Loss for  34577 th iteration => 0.0088383532703\n",
      "Loss for  34578 th iteration => 0.00883793763715\n",
      "Loss for  34579 th iteration => 0.00883777183882\n",
      "Loss for  34580 th iteration => 0.00883738889658\n",
      "Loss for  34581 th iteration => 0.00883719056131\n",
      "Loss for  34582 th iteration => 0.00883684015281\n",
      "Loss for  34583 th iteration => 0.00883661451491\n",
      "Loss for  34584 th iteration => 0.00883654077678\n",
      "Loss for  34585 th iteration => 0.00883603442819\n",
      "Loss for  34586 th iteration => 0.00883574736521\n",
      "Loss for  34587 th iteration => 0.00883545375643\n",
      "Loss for  34588 th iteration => 0.00883519847262\n",
      "Loss for  34589 th iteration => 0.00883487323181\n",
      "Loss for  34590 th iteration => 0.00883464958235\n",
      "Loss for  34591 th iteration => 0.00883429660386\n",
      "Loss for  34592 th iteration => 0.00883436903701\n",
      "Loss for  34593 th iteration => 0.00883372031961\n",
      "Loss for  34594 th iteration => 0.00883343988692\n",
      "Loss for  34595 th iteration => 0.00883327457952\n",
      "Loss for  34596 th iteration => 0.00883287788556\n",
      "Loss for  34597 th iteration => 0.00883259616813\n",
      "Loss for  34598 th iteration => 0.00883231446951\n",
      "Loss for  34599 th iteration => 0.00883203628696\n",
      "Loss for  34600 th iteration => 0.00883203694172\n",
      "Loss for  34601 th iteration => 0.00883147481801\n",
      "Loss for  34602 th iteration => 0.00883119427538\n",
      "Loss for  34603 th iteration => 0.00883103909476\n",
      "Loss for  34604 th iteration => 0.00883063280354\n",
      "Loss for  34605 th iteration => 0.00883035122532\n",
      "Loss for  34606 th iteration => 0.00883006966593\n",
      "Loss for  34607 th iteration => 0.0088297916357\n",
      "Loss for  34608 th iteration => 0.00882981035138\n",
      "Loss for  34609 th iteration => 0.00882923413291\n",
      "Loss for  34610 th iteration => 0.00882907796324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  34611 th iteration => 0.00882882125288\n",
      "Loss for  34612 th iteration => 0.00882850267871\n",
      "Loss for  34613 th iteration => 0.00882813602068\n",
      "Loss for  34614 th iteration => 0.00882792569797\n",
      "Loss for  34615 th iteration => 0.0088275869274\n",
      "Loss for  34616 th iteration => 0.00882752404792\n",
      "Loss for  34617 th iteration => 0.00882704114654\n",
      "Loss for  34618 th iteration => 0.00882693849707\n",
      "Loss for  34619 th iteration => 0.00882665632436\n",
      "Loss for  34620 th iteration => 0.00882634602976\n",
      "Loss for  34621 th iteration => 0.00882597646241\n",
      "Loss for  34622 th iteration => 0.008825750853\n",
      "Loss for  34623 th iteration => 0.00882544253577\n",
      "Loss for  34624 th iteration => 0.00882515609801\n",
      "Loss for  34625 th iteration => 0.00882490837843\n",
      "Loss for  34626 th iteration => 0.00882456376054\n",
      "Loss for  34627 th iteration => 0.00882427699199\n",
      "Loss for  34628 th iteration => 0.00882423956169\n",
      "Loss for  34629 th iteration => 0.00882373357418\n",
      "Loss for  34630 th iteration => 0.00882340907644\n",
      "Loss for  34631 th iteration => 0.00882318697541\n",
      "Loss for  34632 th iteration => 0.00882282915905\n",
      "Loss for  34633 th iteration => 0.00882264036941\n",
      "Loss for  34634 th iteration => 0.00882225192333\n",
      "Loss for  34635 th iteration => 0.008821972941\n",
      "Loss for  34636 th iteration => 0.00882208756528\n",
      "Loss for  34637 th iteration => 0.00882142235594\n",
      "Loss for  34638 th iteration => 0.00882128931418\n",
      "Loss for  34639 th iteration => 0.00882087577684\n",
      "Loss for  34640 th iteration => 0.00882070998755\n",
      "Loss for  34641 th iteration => 0.00882032919256\n",
      "Loss for  34642 th iteration => 0.00882013081544\n",
      "Loss for  34643 th iteration => 0.00881978260395\n",
      "Loss for  34644 th iteration => 0.00881955546761\n",
      "Loss for  34645 th iteration => 0.00881948632165\n",
      "Loss for  34646 th iteration => 0.00881897886042\n",
      "Loss for  34647 th iteration => 0.00881869409207\n",
      "Loss for  34648 th iteration => 0.0088184002951\n",
      "Loss for  34649 th iteration => 0.00881814735222\n",
      "Loss for  34650 th iteration => 0.00881782187743\n",
      "Loss for  34651 th iteration => 0.00881760061356\n",
      "Loss for  34652 th iteration => 0.00881724531284\n",
      "Loss for  34653 th iteration => 0.00881732361439\n",
      "Loss for  34654 th iteration => 0.00881667396099\n",
      "Loss for  34655 th iteration => 0.00881639373423\n",
      "Loss for  34656 th iteration => 0.00881623098188\n",
      "Loss for  34657 th iteration => 0.00881583471969\n",
      "Loss for  34658 th iteration => 0.00881555407196\n",
      "Loss for  34659 th iteration => 0.0088152734429\n",
      "Loss for  34660 th iteration => 0.00881499368595\n",
      "Loss for  34661 th iteration => 0.00881499988105\n",
      "Loss for  34662 th iteration => 0.0088144369674\n",
      "Loss for  34663 th iteration => 0.00881415640577\n",
      "Loss for  34664 th iteration => 0.00881387881232\n",
      "Loss for  34665 th iteration => 0.00881372348304\n",
      "Loss for  34666 th iteration => 0.00881331763624\n",
      "Loss for  34667 th iteration => 0.00881303714551\n",
      "Loss for  34668 th iteration => 0.00881275691799\n",
      "Loss for  34669 th iteration => 0.00881264468005\n",
      "Loss for  34670 th iteration => 0.00881233808227\n",
      "Loss for  34671 th iteration => 0.00881192584903\n",
      "Loss for  34672 th iteration => 0.00881177121344\n",
      "Loss for  34673 th iteration => 0.0088115160146\n",
      "Loss for  34674 th iteration => 0.00881119651631\n",
      "Loss for  34675 th iteration => 0.0088108320937\n",
      "Loss for  34676 th iteration => 0.00881062167328\n",
      "Loss for  34677 th iteration => 0.00881028640551\n",
      "Loss for  34678 th iteration => 0.00881022331414\n",
      "Loss for  34679 th iteration => 0.00880974343339\n",
      "Loss for  34680 th iteration => 0.00880964041966\n",
      "Loss for  34681 th iteration => 0.00880936192198\n",
      "Loss for  34682 th iteration => 0.00880904886385\n",
      "Loss for  34683 th iteration => 0.00880868311934\n",
      "Loss for  34684 th iteration => 0.00880845578838\n",
      "Loss for  34685 th iteration => 0.00880815138319\n",
      "Loss for  34686 th iteration => 0.00880786313533\n",
      "Loss for  34687 th iteration => 0.0088076194154\n",
      "Loss for  34688 th iteration => 0.00880727268058\n",
      "Loss for  34689 th iteration => 0.00880698949835\n",
      "Loss for  34690 th iteration => 0.00880695371939\n",
      "Loss for  34691 th iteration => 0.00880644728102\n",
      "Loss for  34692 th iteration => 0.00880612222467\n",
      "Loss for  34693 th iteration => 0.00880590285925\n",
      "Loss for  34694 th iteration => 0.00880554444179\n",
      "Loss for  34695 th iteration => 0.00880535842918\n",
      "Loss for  34696 th iteration => 0.00880496900548\n",
      "Loss for  34697 th iteration => 0.00880469175365\n",
      "Loss for  34698 th iteration => 0.0088048124162\n",
      "Loss for  34699 th iteration => 0.00880414492158\n",
      "Loss for  34700 th iteration => 0.00880401048962\n",
      "Loss for  34701 th iteration => 0.00880360050945\n",
      "Loss for  34702 th iteration => 0.008803433305\n",
      "Loss for  34703 th iteration => 0.00880305609121\n",
      "Loss for  34704 th iteration => 0.0088028562752\n",
      "Loss for  34705 th iteration => 0.00880251166773\n",
      "Loss for  34706 th iteration => 0.00880228262181\n",
      "Loss for  34707 th iteration => 0.00880222195797\n",
      "Loss for  34708 th iteration => 0.00880170857084\n",
      "Loss for  34709 th iteration => 0.00880142745379\n",
      "Loss for  34710 th iteration => 0.00880113214807\n",
      "Loss for  34711 th iteration => 0.00880088287721\n",
      "Loss for  34712 th iteration => 0.00880055587321\n",
      "Loss for  34713 th iteration => 0.0088003383009\n",
      "Loss for  34714 th iteration => 0.00879998048098\n",
      "Loss for  34715 th iteration => 0.00880006821785\n",
      "Loss for  34716 th iteration => 0.00879941247614\n",
      "Loss for  34717 th iteration => 0.00879913302219\n",
      "Loss for  34718 th iteration => 0.00879897406706\n",
      "Loss for  34719 th iteration => 0.00879857646199\n",
      "Loss for  34720 th iteration => 0.0087982968953\n",
      "Loss for  34721 th iteration => 0.00879801734714\n",
      "Loss for  34722 th iteration => 0.00879773781751\n",
      "Loss for  34723 th iteration => 0.0087974632667\n",
      "Loss for  34724 th iteration => 0.0087974712683\n",
      "Loss for  34725 th iteration => 0.00879690470126\n",
      "Loss for  34726 th iteration => 0.00879675446155\n",
      "Loss for  34727 th iteration => 0.00879648985289\n",
      "Loss for  34728 th iteration => 0.00879618202134\n",
      "Loss for  34729 th iteration => 0.00879581480436\n",
      "Loss for  34730 th iteration => 0.00879560898203\n",
      "Loss for  34731 th iteration => 0.0087952699728\n",
      "Loss for  34732 th iteration => 0.00879520545587\n",
      "Loss for  34733 th iteration => 0.00879472505164\n",
      "Loss for  34734 th iteration => 0.00879446680591\n",
      "Loss for  34735 th iteration => 0.00879432122957\n",
      "Loss for  34736 th iteration => 0.00879389699891\n",
      "Loss for  34737 th iteration => 0.00879363508993\n",
      "Loss for  34738 th iteration => 0.00879332456981\n",
      "Loss for  34739 th iteration => 0.0087930893041\n",
      "Loss for  34740 th iteration => 0.00879293243557\n",
      "Loss for  34741 th iteration => 0.00879255454488\n",
      "Loss for  34742 th iteration => 0.00879234575653\n",
      "Loss for  34743 th iteration => 0.00879193369206\n",
      "Loss for  34744 th iteration => 0.00879201437927\n",
      "Loss for  34745 th iteration => 0.00879139360016\n",
      "Loss for  34746 th iteration => 0.00879119795099\n",
      "Loss for  34747 th iteration => 0.0087908515066\n",
      "Loss for  34748 th iteration => 0.0087906215293\n",
      "Loss for  34749 th iteration => 0.00879030939816\n",
      "Loss for  34750 th iteration => 0.00879004527156\n",
      "Loss for  34751 th iteration => 0.00878976732413\n",
      "Loss for  34752 th iteration => 0.00878963378902\n",
      "Loss for  34753 th iteration => 0.00878933292645\n",
      "Loss for  34754 th iteration => 0.00878905024453\n",
      "Loss for  34755 th iteration => 0.00878870574262\n",
      "Loss for  34756 th iteration => 0.00878845652888\n",
      "Loss for  34757 th iteration => 0.00878817919529\n",
      "Loss for  34758 th iteration => 0.00878786908605\n",
      "Loss for  34759 th iteration => 0.0087875214401\n",
      "Loss for  34760 th iteration => 0.00878729408143\n",
      "Loss for  34761 th iteration => 0.00878711175933\n",
      "Loss for  34762 th iteration => 0.00878683393139\n",
      "Loss for  34763 th iteration => 0.00878644105243\n",
      "Loss for  34764 th iteration => 0.00878615030594\n",
      "Loss for  34765 th iteration => 0.00878589840457\n",
      "Loss for  34766 th iteration => 0.00878557582756\n",
      "Loss for  34767 th iteration => 0.00878535575526\n",
      "Loss for  34768 th iteration => 0.00878500149744\n",
      "Loss for  34769 th iteration => 0.00878481488709\n",
      "Loss for  34770 th iteration => 0.00878469572803\n",
      "Loss for  34771 th iteration => 0.00878415954596\n",
      "Loss for  34772 th iteration => 0.00878388158775\n",
      "Loss for  34773 th iteration => 0.0087837176271\n",
      "Loss for  34774 th iteration => 0.00878332637941\n",
      "Loss for  34775 th iteration => 0.00878304776778\n",
      "Loss for  34776 th iteration => 0.00878276917453\n",
      "Loss for  34777 th iteration => 0.00878249059966\n",
      "Loss for  34778 th iteration => 0.00878221703386\n",
      "Loss for  34779 th iteration => 0.0087822163779\n",
      "Loss for  34780 th iteration => 0.00878166021782\n",
      "Loss for  34781 th iteration => 0.0087813838604\n",
      "Loss for  34782 th iteration => 0.0087812291425\n",
      "Loss for  34783 th iteration => 0.00878082751762\n",
      "Loss for  34784 th iteration => 0.00878054906095\n",
      "Loss for  34785 th iteration => 0.00878027062269\n",
      "Loss for  34786 th iteration => 0.00877999499374\n",
      "Loss for  34787 th iteration => 0.00878001626048\n",
      "Loss for  34788 th iteration => 0.00877944542799\n",
      "Loss for  34789 th iteration => 0.00877928925242\n",
      "Loss for  34790 th iteration => 0.00877903807671\n",
      "Loss for  34791 th iteration => 0.00877872013302\n",
      "Loss for  34792 th iteration => 0.00877835982449\n",
      "Loss for  34793 th iteration => 0.00877814928755\n",
      "Loss for  34794 th iteration => 0.00877781574846\n",
      "Loss for  34795 th iteration => 0.00877758160605\n",
      "Loss for  34796 th iteration => 0.00877744748232\n",
      "Loss for  34797 th iteration => 0.00877701630848\n",
      "Loss for  34798 th iteration => 0.00877689383672\n",
      "Loss for  34799 th iteration => 0.00877660981837\n",
      "Loss for  34800 th iteration => 0.00877633264411\n",
      "Loss for  34801 th iteration => 0.00877591034999\n",
      "Loss for  34802 th iteration => 0.00877577046386\n",
      "Loss for  34803 th iteration => 0.00877535606614\n",
      "Loss for  34804 th iteration => 0.00877520841151\n",
      "Loss for  34805 th iteration => 0.00877480543794\n",
      "Loss for  34806 th iteration => 0.00877454790899\n",
      "Loss for  34807 th iteration => 0.00877426821912\n",
      "Loss for  34808 th iteration => 0.0087742312957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  34809 th iteration => 0.00877372962232\n",
      "Loss for  34810 th iteration => 0.0087734053838\n",
      "Loss for  34811 th iteration => 0.00877318933473\n",
      "Loss for  34812 th iteration => 0.00877283160224\n",
      "Loss for  34813 th iteration => 0.00877264903639\n",
      "Loss for  34814 th iteration => 0.0087722612798\n",
      "Loss for  34815 th iteration => 0.00877198427873\n",
      "Loss for  34816 th iteration => 0.00877198524011\n",
      "Loss for  34817 th iteration => 0.00877156648101\n",
      "Loss for  34818 th iteration => 0.00877130870455\n",
      "Loss for  34819 th iteration => 0.00877090409371\n",
      "Loss for  34820 th iteration => 0.0087707355386\n",
      "Loss for  34821 th iteration => 0.00877036378515\n",
      "Loss for  34822 th iteration => 0.0087701625285\n",
      "Loss for  34823 th iteration => 0.00876982346922\n",
      "Loss for  34824 th iteration => 0.00876959017668\n",
      "Loss for  34825 th iteration => 0.00876953888456\n",
      "Loss for  34826 th iteration => 0.00876902280135\n",
      "Loss for  34827 th iteration => 0.00876874740484\n",
      "Loss for  34828 th iteration => 0.00876845040013\n",
      "Loss for  34829 th iteration => 0.00876820693139\n",
      "Loss for  34830 th iteration => 0.00876787814776\n",
      "Loss for  34831 th iteration => 0.00876766645613\n",
      "Loss for  34832 th iteration => 0.00876730604322\n",
      "Loss for  34833 th iteration => 0.00876712797948\n",
      "Loss for  34834 th iteration => 0.00876701507873\n",
      "Loss for  34835 th iteration => 0.00876646762922\n",
      "Loss for  34836 th iteration => 0.00876619048232\n",
      "Loss for  34837 th iteration => 0.00876603487428\n",
      "Loss for  34838 th iteration => 0.00876563775838\n",
      "Loss for  34839 th iteration => 0.00876536025022\n",
      "Loss for  34840 th iteration => 0.00876508276033\n",
      "Loss for  34841 th iteration => 0.00876480537124\n",
      "Loss for  34842 th iteration => 0.00876466507917\n",
      "Loss for  34843 th iteration => 0.00876441485878\n",
      "Loss for  34844 th iteration => 0.0087641024389\n",
      "Loss for  34845 th iteration => 0.00876371675378\n",
      "Loss for  34846 th iteration => 0.00876366619335\n",
      "Loss for  34847 th iteration => 0.00876317572604\n",
      "Loss for  34848 th iteration => 0.00876296726747\n",
      "Loss for  34849 th iteration => 0.00876263357101\n",
      "Loss for  34850 th iteration => 0.00876239872654\n",
      "Loss for  34851 th iteration => 0.00876226402279\n",
      "Loss for  34852 th iteration => 0.00876183540163\n",
      "Loss for  34853 th iteration => 0.00876171046621\n",
      "Loss for  34854 th iteration => 0.00876128469425\n",
      "Loss for  34855 th iteration => 0.00876129464934\n",
      "Loss for  34856 th iteration => 0.00876073314147\n",
      "Loss for  34857 th iteration => 0.0087605925376\n",
      "Loss for  34858 th iteration => 0.00876018073525\n",
      "Loss for  34859 th iteration => 0.00876003235239\n",
      "Loss for  34860 th iteration => 0.00875963390898\n",
      "Loss for  34861 th iteration => 0.008759376268\n",
      "Loss for  34862 th iteration => 0.00875909570904\n",
      "Loss for  34863 th iteration => 0.0087588075115\n",
      "Loss for  34864 th iteration => 0.00875881093996\n",
      "Loss for  34865 th iteration => 0.00875823727456\n",
      "Loss for  34866 th iteration => 0.00875802365699\n",
      "Loss for  34867 th iteration => 0.00875766534298\n",
      "Loss for  34868 th iteration => 0.00875748526857\n",
      "Loss for  34869 th iteration => 0.00875710061408\n",
      "Loss for  34870 th iteration => 0.00875682364203\n",
      "Loss for  34871 th iteration => 0.00875654682056\n",
      "Loss for  34872 th iteration => 0.00875644077776\n",
      "Loss for  34873 th iteration => 0.00875623163334\n",
      "Loss for  34874 th iteration => 0.00875587364099\n",
      "Loss for  34875 th iteration => 0.00875546956374\n",
      "Loss for  34876 th iteration => 0.00875530234984\n",
      "Loss for  34877 th iteration => 0.0087549312049\n",
      "Loss for  34878 th iteration => 0.00875473121533\n",
      "Loss for  34879 th iteration => 0.00875439283746\n",
      "Loss for  34880 th iteration => 0.00875416023634\n",
      "Loss for  34881 th iteration => 0.00875385745355\n",
      "Loss for  34882 th iteration => 0.00875384068418\n",
      "Loss for  34883 th iteration => 0.00875332063911\n",
      "Loss for  34884 th iteration => 0.00875302467465\n",
      "Loss for  34885 th iteration => 0.00875278211245\n",
      "Loss for  34886 th iteration => 0.00875245429934\n",
      "Loss for  34887 th iteration => 0.00875224358277\n",
      "Loss for  34888 th iteration => 0.0087518840726\n",
      "Loss for  34889 th iteration => 0.00875170519392\n",
      "Loss for  34890 th iteration => 0.00875144812052\n",
      "Loss for  34891 th iteration => 0.00875119440629\n",
      "Loss for  34892 th iteration => 0.00875077340007\n",
      "Loss for  34893 th iteration => 0.00875049832706\n",
      "Loss for  34894 th iteration => 0.00875034131034\n",
      "Loss for  34895 th iteration => 0.00874994644844\n",
      "Loss for  34896 th iteration => 0.00874966991883\n",
      "Loss for  34897 th iteration => 0.00874939340737\n",
      "Loss for  34898 th iteration => 0.00874911823466\n",
      "Loss for  34899 th iteration => 0.00874913344624\n",
      "Loss for  34900 th iteration => 0.00874857026807\n",
      "Loss for  34901 th iteration => 0.00874841748536\n",
      "Loss for  34902 th iteration => 0.00874803216627\n",
      "Loss for  34903 th iteration => 0.00874798217597\n",
      "Loss for  34904 th iteration => 0.00874749243199\n",
      "Loss for  34905 th iteration => 0.00874728611812\n",
      "Loss for  34906 th iteration => 0.00874695225386\n",
      "Loss for  34907 th iteration => 0.00874671977931\n",
      "Loss for  34908 th iteration => 0.00874658390113\n",
      "Loss for  34909 th iteration => 0.00874615792631\n",
      "Loss for  34910 th iteration => 0.00874603274498\n",
      "Loss for  34911 th iteration => 0.00874560938074\n",
      "Loss for  34912 th iteration => 0.00874561820994\n",
      "Loss for  34913 th iteration => 0.00874505952112\n",
      "Loss for  34914 th iteration => 0.00874491864881\n",
      "Loss for  34915 th iteration => 0.0087445090504\n",
      "Loss for  34916 th iteration => 0.00874436038813\n",
      "Loss for  34917 th iteration => 0.00874396389843\n",
      "Loss for  34918 th iteration => 0.00874370674767\n",
      "Loss for  34919 th iteration => 0.00874342766964\n",
      "Loss for  34920 th iteration => 0.00874313983388\n",
      "Loss for  34921 th iteration => 0.00874314410195\n",
      "Loss for  34922 th iteration => 0.00874257153822\n",
      "Loss for  34923 th iteration => 0.00874235952996\n",
      "Loss for  34924 th iteration => 0.00874200151531\n",
      "Loss for  34925 th iteration => 0.00874182311016\n",
      "Loss for  34926 th iteration => 0.0087414393024\n",
      "Loss for  34927 th iteration => 0.00874116330533\n",
      "Loss for  34928 th iteration => 0.00874088732627\n",
      "Loss for  34929 th iteration => 0.00874061654536\n",
      "Loss for  34930 th iteration => 0.00874073218424\n",
      "Loss for  34931 th iteration => 0.00874007505946\n",
      "Loss for  34932 th iteration => 0.00873994284801\n",
      "Loss for  34933 th iteration => 0.00873953871974\n",
      "Loss for  34934 th iteration => 0.00873937348977\n",
      "Loss for  34935 th iteration => 0.00873900236933\n",
      "Loss for  34936 th iteration => 0.00873880428891\n",
      "Loss for  34937 th iteration => 0.00873846600914\n",
      "Loss for  34938 th iteration => 0.00873823710454\n",
      "Loss for  34939 th iteration => 0.00873818261582\n",
      "Loss for  34940 th iteration => 0.00873767211504\n",
      "Loss for  34941 th iteration => 0.00873739779453\n",
      "Loss for  34942 th iteration => 0.00873710352658\n",
      "Loss for  34943 th iteration => 0.00873686127139\n",
      "Loss for  34944 th iteration => 0.00873653508837\n",
      "Loss for  34945 th iteration => 0.00873632474407\n",
      "Loss for  34946 th iteration => 0.00873596679936\n",
      "Loss for  34947 th iteration => 0.00873578996807\n",
      "Loss for  34948 th iteration => 0.00873567829914\n",
      "Loss for  34949 th iteration => 0.0087351343028\n",
      "Loss for  34950 th iteration => 0.00873485869476\n",
      "Loss for  34951 th iteration => 0.00873458582056\n",
      "Loss for  34952 th iteration => 0.00873442947597\n",
      "Loss for  34953 th iteration => 0.00873403469801\n",
      "Loss for  34954 th iteration => 0.00873375915861\n",
      "Loss for  34955 th iteration => 0.00873348363722\n",
      "Loss for  34956 th iteration => 0.00873321128438\n",
      "Loss for  34957 th iteration => 0.0087332274886\n",
      "Loss for  34958 th iteration => 0.00873266453933\n",
      "Loss for  34959 th iteration => 0.0087325126581\n",
      "Loss for  34960 th iteration => 0.00873226000253\n",
      "Loss for  34961 th iteration => 0.00873194982496\n",
      "Loss for  34962 th iteration => 0.00873159066451\n",
      "Loss for  34963 th iteration => 0.0087313846906\n",
      "Loss for  34964 th iteration => 0.00873105247176\n",
      "Loss for  34965 th iteration => 0.00873082078597\n",
      "Loss for  34966 th iteration => 0.00873068756139\n",
      "Loss for  34967 th iteration => 0.00873026248761\n",
      "Loss for  34968 th iteration => 0.00873013677942\n",
      "Loss for  34969 th iteration => 0.00872971614511\n",
      "Loss for  34970 th iteration => 0.0087297258149\n",
      "Loss for  34971 th iteration => 0.00872916798309\n",
      "Loss for  34972 th iteration => 0.00872902657235\n",
      "Loss for  34973 th iteration => 0.00872861946923\n",
      "Loss for  34974 th iteration => 0.00872847026539\n",
      "Loss for  34975 th iteration => 0.00872807588089\n",
      "Loss for  34976 th iteration => 0.00872781732779\n",
      "Loss for  34977 th iteration => 0.00872754162942\n",
      "Loss for  34978 th iteration => 0.00872725256365\n",
      "Loss for  34979 th iteration => 0.00872726318544\n",
      "Loss for  34980 th iteration => 0.00872668598851\n",
      "Loss for  34981 th iteration => 0.00872647741645\n",
      "Loss for  34982 th iteration => 0.00872611791649\n",
      "Loss for  34983 th iteration => 0.00872594297222\n",
      "Loss for  34984 th iteration => 0.00872555791238\n",
      "Loss for  34985 th iteration => 0.00872528290205\n",
      "Loss for  34986 th iteration => 0.00872500790962\n",
      "Loss for  34987 th iteration => 0.00872473738159\n",
      "Loss for  34988 th iteration => 0.00872486056688\n",
      "Loss for  34989 th iteration => 0.00872420119261\n",
      "Loss for  34990 th iteration => 0.0087240663711\n",
      "Loss for  34991 th iteration => 0.00872366681956\n",
      "Loss for  34992 th iteration => 0.00872349897067\n",
      "Loss for  34993 th iteration => 0.00872313243509\n",
      "Loss for  34994 th iteration => 0.00872293172779\n",
      "Loss for  34995 th iteration => 0.0087225980401\n",
      "Loss for  34996 th iteration => 0.00872236595717\n",
      "Loss for  34997 th iteration => 0.00872218312383\n",
      "Loss for  34998 th iteration => 0.00872193888854\n",
      "Loss for  34999 th iteration => 0.00872153372562\n",
      "Loss for  35000 th iteration => 0.00872123680391\n",
      "Loss for  35001 th iteration => 0.00872099916713\n",
      "Loss for  35002 th iteration => 0.00872067032283\n",
      "Loss for  35003 th iteration => 0.00872046460371\n",
      "Loss for  35004 th iteration => 0.00872010399111\n",
      "Loss for  35005 th iteration => 0.00871993108241\n",
      "Loss for  35006 th iteration => 0.00871967406037\n",
      "Loss for  35007 th iteration => 0.00871942339039\n",
      "Loss for  35008 th iteration => 0.0087190007616\n",
      "Loss for  35009 th iteration => 0.00871872773453\n",
      "Loss for  35010 th iteration => 0.00871857629827\n",
      "Loss for  35011 th iteration => 0.00871817970268\n",
      "Loss for  35012 th iteration => 0.0087179051473\n",
      "Loss for  35013 th iteration => 0.00871763060982\n",
      "Loss for  35014 th iteration => 0.00871735752834\n",
      "Loss for  35015 th iteration => 0.0087172203858\n",
      "Loss for  35016 th iteration => 0.00871697722436\n",
      "Loss for  35017 th iteration => 0.00871666191236\n",
      "Loss for  35018 th iteration => 0.00871628210889\n",
      "Loss for  35019 th iteration => 0.00871623465806\n",
      "Loss for  35020 th iteration => 0.00871574688414\n",
      "Loss for  35021 th iteration => 0.00871553833491\n",
      "Loss for  35022 th iteration => 0.00871521064665\n",
      "Loss for  35023 th iteration => 0.00871497528527\n",
      "Loss for  35024 th iteration => 0.00871467674663\n",
      "Loss for  35025 th iteration => 0.00871458806112\n",
      "Loss for  35026 th iteration => 0.00871414897764\n",
      "Loss for  35027 th iteration => 0.00871401355118\n",
      "Loss for  35028 th iteration => 0.00871377891397\n",
      "Loss for  35029 th iteration => 0.00871343358636\n",
      "Loss for  35030 th iteration => 0.00871311242011\n",
      "Loss for  35031 th iteration => 0.00871285219323\n",
      "Loss for  35032 th iteration => 0.00871259260051\n",
      "Loss for  35033 th iteration => 0.00871227788607\n",
      "Loss for  35034 th iteration => 0.00871197201655\n",
      "Loss for  35035 th iteration => 0.0087117111569\n",
      "Loss for  35036 th iteration => 0.0087114397528\n",
      "Loss for  35037 th iteration => 0.00871114912788\n",
      "Loss for  35038 th iteration => 0.00871117002723\n",
      "Loss for  35039 th iteration => 0.00871058675898\n",
      "Loss for  35040 th iteration => 0.00871052931923\n",
      "Loss for  35041 th iteration => 0.00871003564131\n",
      "Loss for  35042 th iteration => 0.00870997973217\n",
      "Loss for  35043 th iteration => 0.00870949253411\n",
      "Loss for  35044 th iteration => 0.00870930058508\n",
      "Loss for  35045 th iteration => 0.00870895991032\n",
      "Loss for  35046 th iteration => 0.00870873637081\n",
      "Loss for  35047 th iteration => 0.00870866991278\n",
      "Loss for  35048 th iteration => 0.00870817543316\n",
      "Loss for  35049 th iteration => 0.00870789910558\n",
      "Loss for  35050 th iteration => 0.00870761039043\n",
      "Loss for  35051 th iteration => 0.00870736631267\n",
      "Loss for  35052 th iteration => 0.00870704549981\n",
      "Loss for  35053 th iteration => 0.00870683351292\n",
      "Loss for  35054 th iteration => 0.00870648076023\n",
      "Loss for  35055 th iteration => 0.00870630091139\n",
      "Loss for  35056 th iteration => 0.00870604684279\n",
      "Loss for  35057 th iteration => 0.00870579532\n",
      "Loss for  35058 th iteration => 0.00870538115338\n",
      "Loss for  35059 th iteration => 0.00870510835829\n",
      "Loss for  35060 th iteration => 0.00870495142747\n",
      "Loss for  35061 th iteration => 0.0087045626131\n",
      "Loss for  35062 th iteration => 0.00870428890257\n",
      "Loss for  35063 th iteration => 0.00870401520981\n",
      "Loss for  35064 th iteration => 0.00870374159836\n",
      "Loss for  35065 th iteration => 0.00870360048536\n",
      "Loss for  35066 th iteration => 0.00870335265559\n",
      "Loss for  35067 th iteration => 0.00870304804137\n",
      "Loss for  35068 th iteration => 0.00870266508456\n",
      "Loss for  35069 th iteration => 0.00870261596431\n",
      "Loss for  35070 th iteration => 0.00870213281218\n",
      "Loss for  35071 th iteration => 0.00870192769401\n",
      "Loss for  35072 th iteration => 0.00870159830707\n",
      "Loss for  35073 th iteration => 0.00870136626671\n",
      "Loss for  35074 th iteration => 0.00870106440495\n",
      "Loss for  35075 th iteration => 0.00870097411018\n",
      "Loss for  35076 th iteration => 0.00870053410426\n",
      "Loss for  35077 th iteration => 0.00870040418519\n",
      "Loss for  35078 th iteration => 0.00870001817343\n",
      "Loss for  35079 th iteration => 0.00869996588797\n",
      "Loss for  35080 th iteration => 0.00869950109339\n",
      "Loss for  35081 th iteration => 0.00869924717739\n",
      "Loss for  35082 th iteration => 0.00869898304603\n",
      "Loss for  35083 th iteration => 0.00869867669214\n",
      "Loss for  35084 th iteration => 0.00869836842481\n",
      "Loss for  35085 th iteration => 0.00869811157464\n",
      "Loss for  35086 th iteration => 0.00869783789383\n",
      "Loss for  35087 th iteration => 0.00869754714713\n",
      "Loss for  35088 th iteration => 0.00869746447444\n",
      "Loss for  35089 th iteration => 0.00869708095211\n",
      "Loss for  35090 th iteration => 0.00869678104896\n",
      "Loss for  35091 th iteration => 0.00869642292334\n",
      "Loss for  35092 th iteration => 0.00869625032112\n",
      "Loss for  35093 th iteration => 0.00869586874376\n",
      "Loss for  35094 th iteration => 0.00869559557331\n",
      "Loss for  35095 th iteration => 0.00869532242053\n",
      "Loss for  35096 th iteration => 0.00869504928542\n",
      "Loss for  35097 th iteration => 0.00869478045433\n",
      "Loss for  35098 th iteration => 0.00869490243638\n",
      "Loss for  35099 th iteration => 0.00869424741482\n",
      "Loss for  35100 th iteration => 0.00869411445403\n",
      "Loss for  35101 th iteration => 0.00869371678546\n",
      "Loss for  35102 th iteration => 0.00869355069069\n",
      "Loss for  35103 th iteration => 0.00869318614266\n",
      "Loss for  35104 th iteration => 0.00869298708603\n",
      "Loss for  35105 th iteration => 0.00869265548734\n",
      "Loss for  35106 th iteration => 0.00869242363885\n",
      "Loss for  35107 th iteration => 0.00869212803649\n",
      "Loss for  35108 th iteration => 0.00869211620364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  35109 th iteration => 0.00869159860405\n",
      "Loss for  35110 th iteration => 0.00869130300714\n",
      "Loss for  35111 th iteration => 0.00869106778104\n",
      "Loss for  35112 th iteration => 0.00869074016749\n",
      "Loss for  35113 th iteration => 0.00869053695112\n",
      "Loss for  35114 th iteration => 0.00869017747818\n",
      "Loss for  35115 th iteration => 0.00869000611513\n",
      "Loss for  35116 th iteration => 0.00868962572452\n",
      "Loss for  35117 th iteration => 0.00868962893874\n",
      "Loss for  35118 th iteration => 0.00868908257685\n",
      "Loss for  35119 th iteration => 0.00868881004817\n",
      "Loss for  35120 th iteration => 0.00868866183641\n",
      "Loss for  35121 th iteration => 0.00868826704618\n",
      "Loss for  35122 th iteration => 0.00868799434304\n",
      "Loss for  35123 th iteration => 0.00868772165757\n",
      "Loss for  35124 th iteration => 0.00868744898978\n",
      "Loss for  35125 th iteration => 0.00868717793483\n",
      "Loss for  35126 th iteration => 0.00868720489162\n",
      "Loss for  35127 th iteration => 0.00868664283654\n",
      "Loss for  35128 th iteration => 0.00868648644064\n",
      "Loss for  35129 th iteration => 0.00868611246126\n",
      "Loss for  35130 th iteration => 0.00868606341539\n",
      "Loss for  35131 th iteration => 0.00868558033963\n",
      "Loss for  35132 th iteration => 0.0086853702005\n",
      "Loss for  35133 th iteration => 0.00868504785259\n",
      "Loss for  35134 th iteration => 0.00868481083734\n",
      "Loss for  35135 th iteration => 0.00868451746406\n",
      "Loss for  35136 th iteration => 0.00868442844374\n",
      "Loss for  35137 th iteration => 0.00868399600604\n",
      "Loss for  35138 th iteration => 0.00868385495541\n",
      "Loss for  35139 th iteration => 0.00868363070314\n",
      "Loss for  35140 th iteration => 0.00868327866435\n",
      "Loss for  35141 th iteration => 0.0086829669162\n",
      "Loss for  35142 th iteration => 0.00868270091396\n",
      "Loss for  35143 th iteration => 0.00868245084236\n",
      "Loss for  35144 th iteration => 0.00868213174937\n",
      "Loss for  35145 th iteration => 0.00868183170372\n",
      "Loss for  35146 th iteration => 0.00868156870276\n",
      "Loss for  35147 th iteration => 0.00868130317577\n",
      "Loss for  35148 th iteration => 0.0086810078013\n",
      "Loss for  35149 th iteration => 0.00868104116475\n",
      "Loss for  35150 th iteration => 0.00868045426064\n",
      "Loss for  35151 th iteration => 0.00868039829973\n",
      "Loss for  35152 th iteration => 0.00867990682905\n",
      "Loss for  35153 th iteration => 0.00867985240306\n",
      "Loss for  35154 th iteration => 0.00867936902209\n",
      "Loss for  35155 th iteration => 0.00867917433508\n",
      "Loss for  35156 th iteration => 0.00867884012054\n",
      "Loss for  35157 th iteration => 0.00867861252598\n",
      "Loss for  35158 th iteration => 0.00867831255309\n",
      "Loss for  35159 th iteration => 0.00867829829327\n",
      "Loss for  35160 th iteration => 0.00867778670662\n",
      "Loss for  35161 th iteration => 0.00867749515013\n",
      "Loss for  35162 th iteration => 0.00867725763407\n",
      "Loss for  35163 th iteration => 0.00867693395153\n",
      "Loss for  35164 th iteration => 0.00867672855319\n",
      "Loss for  35165 th iteration => 0.00867637290437\n",
      "Loss for  35166 th iteration => 0.00867619946484\n",
      "Loss for  35167 th iteration => 0.00867582145016\n",
      "Loss for  35168 th iteration => 0.00867555402367\n",
      "Loss for  35169 th iteration => 0.00867554897135\n",
      "Loss for  35170 th iteration => 0.00867501073766\n",
      "Loss for  35171 th iteration => 0.00867474003016\n",
      "Loss for  35172 th iteration => 0.00867458795817\n",
      "Loss for  35173 th iteration => 0.00867419780143\n",
      "Loss for  35174 th iteration => 0.00867392596828\n",
      "Loss for  35175 th iteration => 0.0086736541527\n",
      "Loss for  35176 th iteration => 0.00867338235466\n",
      "Loss for  35177 th iteration => 0.00867311298152\n",
      "Loss for  35178 th iteration => 0.00867313353647\n",
      "Loss for  35179 th iteration => 0.00867257669535\n",
      "Loss for  35180 th iteration => 0.00867242305308\n",
      "Loss for  35181 th iteration => 0.00867217970784\n",
      "Loss for  35182 th iteration => 0.00867186774999\n",
      "Loss for  35183 th iteration => 0.00867151772162\n",
      "Loss for  35184 th iteration => 0.00867130994938\n",
      "Loss for  35185 th iteration => 0.00867098700337\n",
      "Loss for  35186 th iteration => 0.00867075227283\n",
      "Loss for  35187 th iteration => 0.00867045862572\n",
      "Loss for  35188 th iteration => 0.00867036807637\n",
      "Loss for  35189 th iteration => 0.0086699357695\n",
      "Loss for  35190 th iteration => 0.00866979907386\n",
      "Loss for  35191 th iteration => 0.00866956952911\n",
      "Loss for  35192 th iteration => 0.00866922474953\n",
      "Loss for  35193 th iteration => 0.00866891025813\n",
      "Loss for  35194 th iteration => 0.00866864863255\n",
      "Loss for  35195 th iteration => 0.00866839597866\n",
      "Loss for  35196 th iteration => 0.00866808185221\n",
      "Loss for  35197 th iteration => 0.0086677804018\n",
      "Loss for  35198 th iteration => 0.00866752048515\n",
      "Loss for  35199 th iteration => 0.00866725363975\n",
      "Loss for  35200 th iteration => 0.00866696014739\n",
      "Loss for  35201 th iteration => 0.00866688875619\n",
      "Loss for  35202 th iteration => 0.00866650633692\n",
      "Loss for  35203 th iteration => 0.00866635085349\n",
      "Loss for  35204 th iteration => 0.00866586187921\n",
      "Loss for  35205 th iteration => 0.008665806673\n",
      "Loss for  35206 th iteration => 0.00866532645503\n",
      "Loss for  35207 th iteration => 0.00866513287475\n",
      "Loss for  35208 th iteration => 0.00866479930911\n",
      "Loss for  35209 th iteration => 0.00866457275483\n",
      "Loss for  35210 th iteration => 0.00866427214904\n",
      "Loss for  35211 th iteration => 0.00866401694819\n",
      "Loss for  35212 th iteration => 0.00866399023628\n",
      "Loss for  35213 th iteration => 0.00866345859329\n",
      "Loss for  35214 th iteration => 0.00866322219863\n",
      "Loss for  35215 th iteration => 0.00866289908885\n",
      "Loss for  35216 th iteration => 0.00866269486691\n",
      "Loss for  35217 th iteration => 0.00866233973648\n",
      "Loss for  35218 th iteration => 0.00866216752668\n",
      "Loss for  35219 th iteration => 0.00866179177797\n",
      "Loss for  35220 th iteration => 0.00866152173428\n",
      "Loss for  35221 th iteration => 0.0086615183097\n",
      "Loss for  35222 th iteration => 0.00866098363394\n",
      "Loss for  35223 th iteration => 0.00866071261621\n",
      "Loss for  35224 th iteration => 0.00866044327862\n",
      "Loss for  35225 th iteration => 0.0086602905301\n",
      "Loss for  35226 th iteration => 0.00865990231176\n",
      "Loss for  35227 th iteration => 0.00865963136073\n",
      "Loss for  35228 th iteration => 0.00865936042715\n",
      "Loss for  35229 th iteration => 0.008659089511\n",
      "Loss for  35230 th iteration => 0.00865882149792\n",
      "Loss for  35231 th iteration => 0.00865884062361\n",
      "Loss for  35232 th iteration => 0.00865828637157\n",
      "Loss for  35233 th iteration => 0.00865813326455\n",
      "Loss for  35234 th iteration => 0.00865789039341\n",
      "Loss for  35235 th iteration => 0.00865757959124\n",
      "Loss for  35236 th iteration => 0.00865723094002\n",
      "Loss for  35237 th iteration => 0.0086570235201\n",
      "Loss for  35238 th iteration => 0.00865670199852\n",
      "Loss for  35239 th iteration => 0.00865646757351\n",
      "Loss for  35240 th iteration => 0.00865617550075\n",
      "Loss for  35241 th iteration => 0.00865608416855\n",
      "Loss for  35242 th iteration => 0.0086556539405\n",
      "Loss for  35243 th iteration => 0.00865551702039\n",
      "Loss for  35244 th iteration => 0.00865528902269\n",
      "Loss for  35245 th iteration => 0.00865494471735\n",
      "Loss for  35246 th iteration => 0.00865463198514\n",
      "Loss for  35247 th iteration => 0.00865437029906\n",
      "Loss for  35248 th iteration => 0.00865411948938\n",
      "Loss for  35249 th iteration => 0.00865380592313\n",
      "Loss for  35250 th iteration => 0.00865350560933\n",
      "Loss for  35251 th iteration => 0.00865324628242\n",
      "Loss for  35252 th iteration => 0.00865298061855\n",
      "Loss for  35253 th iteration => 0.00865268694425\n",
      "Loss for  35254 th iteration => 0.00865261643407\n",
      "Loss for  35255 th iteration => 0.00865223615458\n",
      "Loss for  35256 th iteration => 0.00865208010226\n",
      "Loss for  35257 th iteration => 0.00865159320155\n",
      "Loss for  35258 th iteration => 0.00865153766322\n",
      "Loss for  35259 th iteration => 0.00865106015044\n",
      "Loss for  35260 th iteration => 0.00865086576819\n",
      "Loss for  35261 th iteration => 0.00865053476773\n",
      "Loss for  35262 th iteration => 0.00865030738155\n",
      "Loss for  35263 th iteration => 0.00865000937006\n",
      "Loss for  35264 th iteration => 0.00864975125251\n",
      "Loss for  35265 th iteration => 0.00864972962059\n",
      "Loss for  35266 th iteration => 0.00864919666122\n",
      "Loss for  35267 th iteration => 0.00864896291606\n",
      "Loss for  35268 th iteration => 0.00864863889064\n",
      "Loss for  35269 th iteration => 0.00864843734511\n",
      "Loss for  35270 th iteration => 0.00864808127247\n",
      "Loss for  35271 th iteration => 0.00864791176485\n",
      "Loss for  35272 th iteration => 0.00864753640473\n",
      "Loss for  35273 th iteration => 0.008647266218\n",
      "Loss for  35274 th iteration => 0.00864699895113\n",
      "Loss for  35275 th iteration => 0.00864699583867\n",
      "Loss for  35276 th iteration => 0.00864646073083\n",
      "Loss for  35277 th iteration => 0.00864619063038\n",
      "Loss for  35278 th iteration => 0.00864604109683\n",
      "Loss for  35279 th iteration => 0.00864565304439\n",
      "Loss for  35280 th iteration => 0.00864538297037\n",
      "Loss for  35281 th iteration => 0.00864511291369\n",
      "Loss for  35282 th iteration => 0.00864484287435\n",
      "Loss for  35283 th iteration => 0.00864457314311\n",
      "Loss for  35284 th iteration => 0.00864446323659\n",
      "Loss for  35285 th iteration => 0.00864417037805\n",
      "Loss for  35286 th iteration => 0.00864377377668\n",
      "Loss for  35287 th iteration => 0.00864362059068\n",
      "Loss for  35288 th iteration => 0.00864338063594\n",
      "Loss for  35289 th iteration => 0.00864306809794\n",
      "Loss for  35290 th iteration => 0.00864272191716\n",
      "Loss for  35291 th iteration => 0.00864251379182\n",
      "Loss for  35292 th iteration => 0.00864219476719\n",
      "Loss for  35293 th iteration => 0.0086419603256\n",
      "Loss for  35294 th iteration => 0.00864184130056\n",
      "Loss for  35295 th iteration => 0.00864141715119\n",
      "Loss for  35296 th iteration => 0.00864129722192\n",
      "Loss for  35297 th iteration => 0.00864087974512\n",
      "Loss for  35298 th iteration => 0.00864089912709\n",
      "Loss for  35299 th iteration => 0.00864034434049\n",
      "Loss for  35300 th iteration => 0.00864020860347\n",
      "Loss for  35301 th iteration => 0.00863980671202\n",
      "Loss for  35302 th iteration => 0.00863966313967\n",
      "Loss for  35303 th iteration => 0.00863927830509\n",
      "Loss for  35304 th iteration => 0.00863901644601\n",
      "Loss for  35305 th iteration => 0.0086387550588\n",
      "Loss for  35306 th iteration => 0.00863845871226\n",
      "Loss for  35307 th iteration => 0.00863823196232\n",
      "Loss for  35308 th iteration => 0.00863806328929\n",
      "Loss for  35309 th iteration => 0.00863781952204\n",
      "Loss for  35310 th iteration => 0.00863749494225\n",
      "Loss for  35311 th iteration => 0.0086372113542\n",
      "Loss for  35312 th iteration => 0.00863693289015\n",
      "Loss for  35313 th iteration => 0.0086365718179\n",
      "Loss for  35314 th iteration => 0.00863637610901\n",
      "Loss for  35315 th iteration => 0.00863604821603\n",
      "Loss for  35316 th iteration => 0.00863581948788\n",
      "Loss for  35317 th iteration => 0.00863552459851\n",
      "Loss for  35318 th iteration => 0.00863526468872\n",
      "Loss for  35319 th iteration => 0.008635249818\n",
      "Loss for  35320 th iteration => 0.00863471227096\n",
      "Loss for  35321 th iteration => 0.0086344816786\n",
      "Loss for  35322 th iteration => 0.00863415626602\n",
      "Loss for  35323 th iteration => 0.00863395788646\n",
      "Loss for  35324 th iteration => 0.00863360041368\n",
      "Loss for  35325 th iteration => 0.00863343408432\n",
      "Loss for  35326 th iteration => 0.00863305772222\n",
      "Loss for  35327 th iteration => 0.00863278842555\n",
      "Loss for  35328 th iteration => 0.00863252106617\n",
      "Loss for  35329 th iteration => 0.00863252337417\n",
      "Loss for  35330 th iteration => 0.00863198558235\n",
      "Loss for  35331 th iteration => 0.00863171634906\n",
      "Loss for  35332 th iteration => 0.00863144897609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  35333 th iteration => 0.0086313007464\n",
      "Loss for  35334 th iteration => 0.0086309113688\n",
      "Loss for  35335 th iteration => 0.00863064220149\n",
      "Loss for  35336 th iteration => 0.00863037305143\n",
      "Loss for  35337 th iteration => 0.0086301039186\n",
      "Loss for  35338 th iteration => 0.00862983738132\n",
      "Loss for  35339 th iteration => 0.00862986424878\n",
      "Loss for  35340 th iteration => 0.00862930942165\n",
      "Loss for  35341 th iteration => 0.00862915344462\n",
      "Loss for  35342 th iteration => 0.00862878610174\n",
      "Loss for  35343 th iteration => 0.00862873666789\n",
      "Loss for  35344 th iteration => 0.00862826109063\n",
      "Loss for  35345 th iteration => 0.00862805104536\n",
      "Loss for  35346 th iteration => 0.00862773570871\n",
      "Loss for  35347 th iteration => 0.00862749863634\n",
      "Loss for  35348 th iteration => 0.00862721190616\n",
      "Loss for  35349 th iteration => 0.0086271224407\n",
      "Loss for  35350 th iteration => 0.00862669955215\n",
      "Loss for  35351 th iteration => 0.00862655261719\n",
      "Loss for  35352 th iteration => 0.00862619283102\n",
      "Loss for  35353 th iteration => 0.00862613070773\n",
      "Loss for  35354 th iteration => 0.00862568467931\n",
      "Loss for  35355 th iteration => 0.00862541338428\n",
      "Loss for  35356 th iteration => 0.00862517572054\n",
      "Loss for  35357 th iteration => 0.00862485360441\n",
      "Loss for  35358 th iteration => 0.00862456191914\n",
      "Loss for  35359 th iteration => 0.00862429750092\n",
      "Loss for  35360 th iteration => 0.00862404047207\n",
      "Loss for  35361 th iteration => 0.00862374156593\n",
      "Loss for  35362 th iteration => 0.00862352042798\n",
      "Loss for  35363 th iteration => 0.0086233505575\n",
      "Loss for  35364 th iteration => 0.00862311457268\n",
      "Loss for  35365 th iteration => 0.00862278208477\n",
      "Loss for  35366 th iteration => 0.00862250517459\n",
      "Loss for  35367 th iteration => 0.00862222154163\n",
      "Loss for  35368 th iteration => 0.00862186368166\n",
      "Loss for  35369 th iteration => 0.00862166656171\n",
      "Loss for  35370 th iteration => 0.00862134187427\n",
      "Loss for  35371 th iteration => 0.00862111174177\n",
      "Loss for  35372 th iteration => 0.00862082005073\n",
      "Loss for  35373 th iteration => 0.00862055875029\n",
      "Loss for  35374 th iteration => 0.00862055402198\n",
      "Loss for  35375 th iteration => 0.00862000809893\n",
      "Loss for  35376 th iteration => 0.0086197806941\n",
      "Loss for  35377 th iteration => 0.00861945389463\n",
      "Loss for  35378 th iteration => 0.00861925869505\n",
      "Loss for  35379 th iteration => 0.00861889984287\n",
      "Loss for  35380 th iteration => 0.0086187366855\n",
      "Loss for  35381 th iteration => 0.00861835818327\n",
      "Loss for  35382 th iteration => 0.00861808978879\n",
      "Loss for  35383 th iteration => 0.00861782292387\n",
      "Loss for  35384 th iteration => 0.00861783372871\n",
      "Loss for  35385 th iteration => 0.00861729012911\n",
      "Loss for  35386 th iteration => 0.00861714133199\n",
      "Loss for  35387 th iteration => 0.00861689252687\n",
      "Loss for  35388 th iteration => 0.00861659277895\n",
      "Loss for  35389 th iteration => 0.00861624486651\n",
      "Loss for  35390 th iteration => 0.0086160416302\n",
      "Loss for  35391 th iteration => 0.00861572102591\n",
      "Loss for  35392 th iteration => 0.00861549060783\n",
      "Loss for  35393 th iteration => 0.00861519746137\n",
      "Loss for  35394 th iteration => 0.00861510636357\n",
      "Loss for  35395 th iteration => 0.00861467902277\n",
      "Loss for  35396 th iteration => 0.0086145455415\n",
      "Loss for  35397 th iteration => 0.00861417296593\n",
      "Loss for  35398 th iteration => 0.00861411580419\n",
      "Loss for  35399 th iteration => 0.00861366733569\n",
      "Loss for  35400 th iteration => 0.00861340889559\n",
      "Loss for  35401 th iteration => 0.0086131599814\n",
      "Loss for  35402 th iteration => 0.00861285199862\n",
      "Loss for  35403 th iteration => 0.0086125545418\n",
      "Loss for  35404 th iteration => 0.00861229726912\n",
      "Loss for  35405 th iteration => 0.00861203463889\n",
      "Loss for  35406 th iteration => 0.00861174270986\n",
      "Loss for  35407 th iteration => 0.00861151471049\n",
      "Loss for  35408 th iteration => 0.0086111906713\n",
      "Loss for  35409 th iteration => 0.00861115161936\n",
      "Loss for  35410 th iteration => 0.00861073719439\n",
      "Loss for  35411 th iteration => 0.00861062503561\n",
      "Loss for  35412 th iteration => 0.00861011698482\n",
      "Loss for  35413 th iteration => 0.00860996148059\n",
      "Loss for  35414 th iteration => 0.00860959677102\n",
      "Loss for  35415 th iteration => 0.00860940790662\n",
      "Loss for  35416 th iteration => 0.00860907653839\n",
      "Loss for  35417 th iteration => 0.00860885449432\n",
      "Loss for  35418 th iteration => 0.00860855628791\n",
      "Loss for  35419 th iteration => 0.00860830230117\n",
      "Loss for  35420 th iteration => 0.00860816742331\n",
      "Loss for  35421 th iteration => 0.00860786278737\n",
      "Loss for  35422 th iteration => 0.00860752005234\n",
      "Loss for  35423 th iteration => 0.00860720084029\n",
      "Loss for  35424 th iteration => 0.00860699962278\n",
      "Loss for  35425 th iteration => 0.00860664820168\n",
      "Loss for  35426 th iteration => 0.00860647918105\n",
      "Loss for  35427 th iteration => 0.00860610851134\n",
      "Loss for  35428 th iteration => 0.00860584086794\n",
      "Loss for  35429 th iteration => 0.00860557347035\n",
      "Loss for  35430 th iteration => 0.00860545129927\n",
      "Loss for  35431 th iteration => 0.00860516408891\n",
      "Loss for  35432 th iteration => 0.0086047753516\n",
      "Loss for  35433 th iteration => 0.00860450919585\n",
      "Loss for  35434 th iteration => 0.00860435954092\n",
      "Loss for  35435 th iteration => 0.00860397530082\n",
      "Loss for  35436 th iteration => 0.00860370778559\n",
      "Loss for  35437 th iteration => 0.00860344028741\n",
      "Loss for  35438 th iteration => 0.00860317280625\n",
      "Loss for  35439 th iteration => 0.00860290534212\n",
      "Loss for  35440 th iteration => 0.00860264078061\n",
      "Loss for  35441 th iteration => 0.00860266540322\n",
      "Loss for  35442 th iteration => 0.00860211526163\n",
      "Loss for  35443 th iteration => 0.00860196161322\n",
      "Loss for  35444 th iteration => 0.00860172682974\n",
      "Loss for  35445 th iteration => 0.00860141393317\n",
      "Loss for  35446 th iteration => 0.00860107363322\n",
      "Loss for  35447 th iteration => 0.00860086466994\n",
      "Loss for  35448 th iteration => 0.0086005516151\n",
      "Loss for  35449 th iteration => 0.0086003155325\n",
      "Loss for  35450 th iteration => 0.00860003074632\n",
      "Loss for  35451 th iteration => 0.00859994102011\n",
      "Loss for  35452 th iteration => 0.00859952166089\n",
      "Loss for  35453 th iteration => 0.00859937453112\n",
      "Loss for  35454 th iteration => 0.00859901832254\n",
      "Loss for  35455 th iteration => 0.00859895403727\n",
      "Loss for  35456 th iteration => 0.00859851352178\n",
      "Loss for  35457 th iteration => 0.00859824169178\n",
      "Loss for  35458 th iteration => 0.00859800794087\n",
      "Loss for  35459 th iteration => 0.00859768640715\n",
      "Loss for  35460 th iteration => 0.00859739769728\n",
      "Loss for  35461 th iteration => 0.00859713356918\n",
      "Loss for  35462 th iteration => 0.0085968796033\n",
      "Loss for  35463 th iteration => 0.00859658090071\n",
      "Loss for  35464 th iteration => 0.00859636216012\n",
      "Loss for  35465 th iteration => 0.00859619073404\n",
      "Loss for  35466 th iteration => 0.00859585616797\n",
      "Loss for  35467 th iteration => 0.0085957285534\n",
      "Loss for  35468 th iteration => 0.00859535408778\n",
      "Loss for  35469 th iteration => 0.00859507067458\n",
      "Loss for  35470 th iteration => 0.00859471607536\n",
      "Loss for  35471 th iteration => 0.0085945189708\n",
      "Loss for  35472 th iteration => 0.00859419760675\n",
      "Loss for  35473 th iteration => 0.00859396742776\n",
      "Loss for  35474 th iteration => 0.00859367912046\n",
      "Loss for  35475 th iteration => 0.00859341604418\n",
      "Loss for  35476 th iteration => 0.00859316170588\n",
      "Loss for  35477 th iteration => 0.0085930045197\n",
      "Loss for  35478 th iteration => 0.0085927629286\n",
      "Loss for  35479 th iteration => 0.00859231929215\n",
      "Loss for  35480 th iteration => 0.00859212778628\n",
      "Loss for  35481 th iteration => 0.00859176851962\n",
      "Loss for  35482 th iteration => 0.00859160910896\n",
      "Loss for  35483 th iteration => 0.00859123217738\n",
      "Loss for  35484 th iteration => 0.00859096544345\n",
      "Loss for  35485 th iteration => 0.00859069872645\n",
      "Loss for  35486 th iteration => 0.00859043263903\n",
      "Loss for  35487 th iteration => 0.00859031938172\n",
      "Loss for  35488 th iteration => 0.00859003119398\n",
      "Loss for  35489 th iteration => 0.00858963958323\n",
      "Loss for  35490 th iteration => 0.00858949257379\n",
      "Loss for  35491 th iteration => 0.00858924640906\n",
      "Loss for  35492 th iteration => 0.00858894526205\n",
      "Loss for  35493 th iteration => 0.00858860109814\n",
      "Loss for  35494 th iteration => 0.00858839745355\n",
      "Loss for  35495 th iteration => 0.00858808065632\n",
      "Loss for  35496 th iteration => 0.00858784977203\n",
      "Loss for  35497 th iteration => 0.00858756157652\n",
      "Loss for  35498 th iteration => 0.00858747072845\n",
      "Loss for  35499 th iteration => 0.00858704838016\n",
      "Loss for  35500 th iteration => 0.00858691166424\n",
      "Loss for  35501 th iteration => 0.0085866878042\n",
      "Loss for  35502 th iteration => 0.00858634762859\n",
      "Loss for  35503 th iteration => 0.00858604344526\n",
      "Loss for  35504 th iteration => 0.00858578132358\n",
      "Loss for  35505 th iteration => 0.00858553948554\n",
      "Loss for  35506 th iteration => 0.00858522771159\n",
      "Loss for  35507 th iteration => 0.008584935031\n",
      "Loss for  35508 th iteration => 0.00858467632132\n",
      "Loss for  35509 th iteration => 0.0085844185123\n",
      "Loss for  35510 th iteration => 0.00858412510184\n",
      "Loss for  35511 th iteration => 0.00858390201463\n",
      "Loss for  35512 th iteration => 0.00858373146974\n",
      "Loss for  35513 th iteration => 0.00858339312172\n",
      "Loss for  35514 th iteration => 0.00858327065376\n",
      "Loss for  35515 th iteration => 0.00858289329803\n",
      "Loss for  35516 th iteration => 0.00858261886232\n",
      "Loss for  35517 th iteration => 0.00858226185011\n",
      "Loss for  35518 th iteration => 0.00858206861767\n",
      "Loss for  35519 th iteration => 0.00858174494421\n",
      "Loss for  35520 th iteration => 0.00858151853484\n",
      "Loss for  35521 th iteration => 0.00858122801937\n",
      "Loss for  35522 th iteration => 0.00858096861253\n",
      "Loss for  35523 th iteration => 0.00858071107654\n",
      "Loss for  35524 th iteration => 0.00858042200878\n",
      "Loss for  35525 th iteration => 0.00858044374996\n",
      "Loss for  35526 th iteration => 0.00857987468593\n",
      "Loss for  35527 th iteration => 0.00857968141261\n",
      "Loss for  35528 th iteration => 0.00857932537913\n",
      "Loss for  35529 th iteration => 0.00857916429107\n",
      "Loss for  35530 th iteration => 0.00857879209696\n",
      "Loss for  35531 th iteration => 0.00857852612294\n",
      "Loss for  35532 th iteration => 0.00857826016575\n",
      "Loss for  35533 th iteration => 0.00857799422539\n",
      "Loss for  35534 th iteration => 0.00857772919968\n",
      "Loss for  35535 th iteration => 0.00857761304804\n",
      "Loss for  35536 th iteration => 0.00857732623389\n",
      "Loss for  35537 th iteration => 0.00857693586294\n",
      "Loss for  35538 th iteration => 0.00857691244145\n",
      "Loss for  35539 th iteration => 0.00857641926738\n",
      "Loss for  35540 th iteration => 0.00857624594299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  35541 th iteration => 0.00857590039799\n",
      "Loss for  35542 th iteration => 0.00857569963785\n",
      "Loss for  35543 th iteration => 0.00857538153774\n",
      "Loss for  35544 th iteration => 0.00857515346049\n",
      "Loss for  35545 th iteration => 0.00857486377082\n",
      "Loss for  35546 th iteration => 0.00857477278952\n",
      "Loss for  35547 th iteration => 0.00857434979939\n",
      "Loss for  35548 th iteration => 0.00857421758345\n",
      "Loss for  35549 th iteration => 0.00857398768095\n",
      "Loss for  35550 th iteration => 0.00857365497559\n",
      "Loss for  35551 th iteration => 0.00857334806782\n",
      "Loss for  35552 th iteration => 0.0085730901262\n",
      "Loss for  35553 th iteration => 0.008572845714\n",
      "Loss for  35554 th iteration => 0.00857253843527\n",
      "Loss for  35555 th iteration => 0.00857224498606\n",
      "Loss for  35556 th iteration => 0.00857198854362\n",
      "Loss for  35557 th iteration => 0.00857173004611\n",
      "Loss for  35558 th iteration => 0.00857143882361\n",
      "Loss for  35559 th iteration => 0.00857121507821\n",
      "Loss for  35560 th iteration => 0.00857089125022\n",
      "Loss for  35561 th iteration => 0.00857085564231\n",
      "Loss for  35562 th iteration => 0.00857034748659\n",
      "Loss for  35563 th iteration => 0.00857043131566\n",
      "Loss for  35564 th iteration => 0.00856983002461\n",
      "Loss for  35565 th iteration => 0.00856967384571\n",
      "Loss for  35566 th iteration => 0.00856931475149\n",
      "Loss for  35567 th iteration => 0.00856912512835\n",
      "Loss for  35568 th iteration => 0.00856879945737\n",
      "Loss for  35569 th iteration => 0.00856857657365\n",
      "Loss for  35570 th iteration => 0.00856828414322\n",
      "Loss for  35571 th iteration => 0.00856802818031\n",
      "Loss for  35572 th iteration => 0.00856776969478\n",
      "Loss for  35573 th iteration => 0.00856761260319\n",
      "Loss for  35574 th iteration => 0.00856736960648\n",
      "Loss for  35575 th iteration => 0.00856693736435\n",
      "Loss for  35576 th iteration => 0.0085667422601\n",
      "Loss for  35577 th iteration => 0.00856638958665\n",
      "Loss for  35578 th iteration => 0.00856622674745\n",
      "Loss for  35579 th iteration => 0.00856585694128\n",
      "Loss for  35580 th iteration => 0.00856559175679\n",
      "Loss for  35581 th iteration => 0.00856532658903\n",
      "Loss for  35582 th iteration => 0.008565061438\n",
      "Loss for  35583 th iteration => 0.008564797964\n",
      "Loss for  35584 th iteration => 0.00856468040155\n",
      "Loss for  35585 th iteration => 0.00856439489735\n",
      "Loss for  35586 th iteration => 0.00856400641879\n",
      "Loss for  35587 th iteration => 0.00856386271829\n",
      "Loss for  35588 th iteration => 0.00856360001801\n",
      "Loss for  35589 th iteration => 0.00856322597954\n",
      "Loss for  35590 th iteration => 0.00856305515156\n",
      "Loss for  35591 th iteration => 0.00856270874464\n",
      "Loss for  35592 th iteration => 0.00856251041433\n",
      "Loss for  35593 th iteration => 0.00856219151789\n",
      "Loss for  35594 th iteration => 0.00856196743082\n",
      "Loss for  35595 th iteration => 0.00856183890184\n",
      "Loss for  35596 th iteration => 0.008561427028\n",
      "Loss for  35597 th iteration => 0.00856131158203\n",
      "Loss for  35598 th iteration => 0.00856103729123\n",
      "Loss for  35599 th iteration => 0.00856077797313\n",
      "Loss for  35600 th iteration => 0.00856037359746\n",
      "Loss for  35601 th iteration => 0.00856024202371\n",
      "Loss for  35602 th iteration => 0.00855984568355\n",
      "Loss for  35603 th iteration => 0.00855970620217\n",
      "Loss for  35604 th iteration => 0.00855932852056\n",
      "Loss for  35605 th iteration => 0.00855907362541\n",
      "Loss for  35606 th iteration => 0.00855881517529\n",
      "Loss for  35607 th iteration => 0.00855852544077\n",
      "Loss for  35608 th iteration => 0.00855830180115\n",
      "Loss for  35609 th iteration => 0.00855797906666\n",
      "Loss for  35610 th iteration => 0.00855794329419\n",
      "Loss for  35611 th iteration => 0.00855753353149\n",
      "Loss for  35612 th iteration => 0.00855742249713\n",
      "Loss for  35613 th iteration => 0.00855692126382\n",
      "Loss for  35614 th iteration => 0.00855676519119\n",
      "Loss for  35615 th iteration => 0.00855640757681\n",
      "Loss for  35616 th iteration => 0.00855621801637\n",
      "Loss for  35617 th iteration => 0.00855589386799\n",
      "Loss for  35618 th iteration => 0.0085556710047\n",
      "Loss for  35619 th iteration => 0.00855538013834\n",
      "Loss for  35620 th iteration => 0.00855512415485\n",
      "Loss for  35621 th iteration => 0.00855486678076\n",
      "Loss for  35622 th iteration => 0.0085547093738\n",
      "Loss for  35623 th iteration => 0.00855446815789\n",
      "Loss for  35624 th iteration => 0.00855403640105\n",
      "Loss for  35625 th iteration => 0.00855384298224\n",
      "Loss for  35626 th iteration => 0.00855349016782\n",
      "Loss for  35627 th iteration => 0.00855332905158\n",
      "Loss for  35628 th iteration => 0.0085529600442\n",
      "Loss for  35629 th iteration => 0.00855269564524\n",
      "Loss for  35630 th iteration => 0.00855243126293\n",
      "Loss for  35631 th iteration => 0.00855216689726\n",
      "Loss for  35632 th iteration => 0.00855190330891\n",
      "Loss for  35633 th iteration => 0.00855178665995\n",
      "Loss for  35634 th iteration => 0.00855150232822\n",
      "Loss for  35635 th iteration => 0.00855111439025\n",
      "Loss for  35636 th iteration => 0.00855085179988\n",
      "Loss for  35637 th iteration => 0.00855082856687\n",
      "Loss for  35638 th iteration => 0.00855033694658\n",
      "Loss for  35639 th iteration => 0.00855016589541\n",
      "Loss for  35640 th iteration => 0.00854982128455\n",
      "Loss for  35641 th iteration => 0.00854962271085\n",
      "Loss for  35642 th iteration => 0.00854930563002\n",
      "Loss for  35643 th iteration => 0.00854907996125\n",
      "Loss for  35644 th iteration => 0.00854895464178\n",
      "Loss for  35645 th iteration => 0.00854854298329\n",
      "Loss for  35646 th iteration => 0.00854842690158\n",
      "Loss for  35647 th iteration => 0.00854801701581\n",
      "Loss for  35648 th iteration => 0.00854803124721\n",
      "Loss for  35649 th iteration => 0.00854749261156\n",
      "Loss for  35650 th iteration => 0.00854736058302\n",
      "Loss for  35651 th iteration => 0.00854696625123\n",
      "Loss for  35652 th iteration => 0.00854682631497\n",
      "Loss for  35653 th iteration => 0.0085464517004\n",
      "Loss for  35654 th iteration => 0.00854619511591\n",
      "Loss for  35655 th iteration => 0.008545939921\n",
      "Loss for  35656 th iteration => 0.00854564848422\n",
      "Loss for  35657 th iteration => 0.00854542811213\n",
      "Loss for  35658 th iteration => 0.00854510202357\n",
      "Loss for  35659 th iteration => 0.00854491818086\n",
      "Loss for  35660 th iteration => 0.00854471164055\n",
      "Loss for  35661 th iteration => 0.00854451010558\n",
      "Loss for  35662 th iteration => 0.00854417733815\n",
      "Loss for  35663 th iteration => 0.00854378833466\n",
      "Loss for  35664 th iteration => 0.0085436315794\n",
      "Loss for  35665 th iteration => 0.00854327624704\n",
      "Loss for  35666 th iteration => 0.0085430859854\n",
      "Loss for  35667 th iteration => 0.00854276413699\n",
      "Loss for  35668 th iteration => 0.00854254055479\n",
      "Loss for  35669 th iteration => 0.00854225200549\n",
      "Loss for  35670 th iteration => 0.00854199528622\n",
      "Loss for  35671 th iteration => 0.00854174006057\n",
      "Loss for  35672 th iteration => 0.00854158374004\n",
      "Loss for  35673 th iteration => 0.00854134415002\n",
      "Loss for  35674 th iteration => 0.00854091066734\n",
      "Loss for  35675 th iteration => 0.00854071962136\n",
      "Loss for  35676 th iteration => 0.00854036601565\n",
      "Loss for  35677 th iteration => 0.00854020728698\n",
      "Loss for  35678 th iteration => 0.00853983761992\n",
      "Loss for  35679 th iteration => 0.00853957401912\n",
      "Loss for  35680 th iteration => 0.00853931043488\n",
      "Loss for  35681 th iteration => 0.0085390468672\n",
      "Loss for  35682 th iteration => 0.00853878356949\n",
      "Loss for  35683 th iteration => 0.00853866983836\n",
      "Loss for  35684 th iteration => 0.00853838578123\n",
      "Loss for  35685 th iteration => 0.00853799888447\n",
      "Loss for  35686 th iteration => 0.00853785344655\n",
      "Loss for  35687 th iteration => 0.00853760934142\n",
      "Loss for  35688 th iteration => 0.00853731329403\n",
      "Loss for  35689 th iteration => 0.00853697302864\n",
      "Loss for  35690 th iteration => 0.00853677167449\n",
      "Loss for  35691 th iteration => 0.00853645892219\n",
      "Loss for  35692 th iteration => 0.00853623018352\n",
      "Loss for  35693 th iteration => 0.00853594482344\n",
      "Loss for  35694 th iteration => 0.00853569045317\n",
      "Loss for  35695 th iteration => 0.00853559784328\n",
      "Loss for  35696 th iteration => 0.0085351567512\n",
      "Loss for  35697 th iteration => 0.0085350701221\n",
      "Loss for  35698 th iteration => 0.00853463359643\n",
      "Loss for  35699 th iteration => 0.00853467848788\n",
      "Loss for  35700 th iteration => 0.00853410964906\n",
      "Loss for  35701 th iteration => 0.00853400699528\n",
      "Loss for  35702 th iteration => 0.00853359902837\n",
      "Loss for  35703 th iteration => 0.00853337605096\n",
      "Loss for  35704 th iteration => 0.00853308889901\n",
      "Loss for  35705 th iteration => 0.00853283086049\n",
      "Loss for  35706 th iteration => 0.00853257873857\n",
      "Loss for  35707 th iteration => 0.00853228584261\n",
      "Loss for  35708 th iteration => 0.00853206854813\n",
      "Loss for  35709 th iteration => 0.00853174173714\n",
      "Loss for  35710 th iteration => 0.00853171633795\n",
      "Loss for  35711 th iteration => 0.00853120751584\n",
      "Loss for  35712 th iteration => 0.008531293269\n",
      "Loss for  35713 th iteration => 0.00853069628591\n",
      "Loss for  35714 th iteration => 0.00853053587597\n",
      "Loss for  35715 th iteration => 0.0085301857726\n",
      "Loss for  35716 th iteration => 0.00852999187593\n",
      "Loss for  35717 th iteration => 0.00852967523648\n",
      "Loss for  35718 th iteration => 0.00852944803919\n",
      "Loss for  35719 th iteration => 0.00852916467854\n",
      "Loss for  35720 th iteration => 0.0085289043644\n",
      "Loss for  35721 th iteration => 0.00852865409975\n",
      "Loss for  35722 th iteration => 0.00852836221855\n",
      "Loss for  35723 th iteration => 0.00852839592147\n",
      "Loss for  35724 th iteration => 0.00852782283446\n",
      "Loss for  35725 th iteration => 0.00852763706743\n",
      "Loss for  35726 th iteration => 0.00852727977758\n",
      "Loss for  35727 th iteration => 0.00852712630409\n",
      "Loss for  35728 th iteration => 0.00852675469129\n",
      "Loss for  35729 th iteration => 0.00852649188465\n",
      "Loss for  35730 th iteration => 0.00852622909449\n",
      "Loss for  35731 th iteration => 0.0085259663208\n",
      "Loss for  35732 th iteration => 0.00852570356359\n",
      "Loss for  35733 th iteration => 0.00852544133438\n",
      "Loss for  35734 th iteration => 0.00852533204108\n",
      "Loss for  35735 th iteration => 0.00852504720422\n",
      "Loss for  35736 th iteration => 0.00852466150498\n",
      "Loss for  35737 th iteration => 0.00852451443596\n",
      "Loss for  35738 th iteration => 0.00852427686801\n",
      "Loss for  35739 th iteration => 0.00852397576191\n",
      "Loss for  35740 th iteration => 0.00852363885344\n",
      "Loss for  35741 th iteration => 0.00852343576202\n",
      "Loss for  35742 th iteration => 0.00852312635422\n",
      "Loss for  35743 th iteration => 0.0085228958906\n",
      "Loss for  35744 th iteration => 0.00852261386228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  35745 th iteration => 0.00852235778109\n",
      "Loss for  35746 th iteration => 0.00852227198372\n",
      "Loss for  35747 th iteration => 0.00852182969224\n",
      "Loss for  35748 th iteration => 0.00852174187887\n",
      "Loss for  35749 th iteration => 0.00852130803153\n",
      "Loss for  35750 th iteration => 0.00852135528187\n",
      "Loss for  35751 th iteration => 0.00852078578254\n",
      "Loss for  35752 th iteration => 0.00852068195273\n",
      "Loss for  35753 th iteration => 0.00852027656629\n",
      "Loss for  35754 th iteration => 0.00852004993885\n",
      "Loss for  35755 th iteration => 0.00851976803447\n",
      "Loss for  35756 th iteration => 0.00851950637116\n",
      "Loss for  35757 th iteration => 0.00851925947123\n",
      "Loss for  35758 th iteration => 0.00851896297587\n",
      "Loss for  35759 th iteration => 0.00851875087769\n",
      "Loss for  35760 th iteration => 0.00851842018522\n",
      "Loss for  35761 th iteration => 0.00851840378454\n",
      "Loss for  35762 th iteration => 0.00851789190232\n",
      "Loss for  35763 th iteration => 0.00851798078487\n",
      "Loss for  35764 th iteration => 0.00851738263037\n",
      "Loss for  35765 th iteration => 0.00851721827345\n",
      "Loss for  35766 th iteration => 0.00851687371072\n",
      "Loss for  35767 th iteration => 0.00851667589794\n",
      "Loss for  35768 th iteration => 0.00851636476799\n",
      "Loss for  35769 th iteration => 0.00851613368549\n",
      "Loss for  35770 th iteration => 0.00851585580316\n",
      "Loss for  35771 th iteration => 0.00851559163476\n",
      "Loss for  35772 th iteration => 0.00851534681719\n",
      "Loss for  35773 th iteration => 0.00851505040551\n",
      "Loss for  35774 th iteration => 0.00851509706384\n",
      "Loss for  35775 th iteration => 0.00851451332895\n",
      "Loss for  35776 th iteration => 0.008514332949\n",
      "Loss for  35777 th iteration => 0.00851397189498\n",
      "Loss for  35778 th iteration => 0.00851382377818\n",
      "Loss for  35779 th iteration => 0.00851344905503\n",
      "Loss for  35780 th iteration => 0.00851318705512\n",
      "Loss for  35781 th iteration => 0.0085129250716\n",
      "Loss for  35782 th iteration => 0.00851266310449\n",
      "Loss for  35783 th iteration => 0.00851240115376\n",
      "Loss for  35784 th iteration => 0.00851213921943\n",
      "Loss for  35785 th iteration => 0.0085118804475\n",
      "Loss for  35786 th iteration => 0.00851190499102\n",
      "Loss for  35787 th iteration => 0.0085113656029\n",
      "Loss for  35788 th iteration => 0.00851121451568\n",
      "Loss for  35789 th iteration => 0.008510986552\n",
      "Loss for  35790 th iteration => 0.0085106782917\n",
      "Loss for  35791 th iteration => 0.00851034612178\n",
      "Loss for  35792 th iteration => 0.00851013991445\n",
      "Loss for  35793 th iteration => 0.00850983564548\n",
      "Loss for  35794 th iteration => 0.00850976580648\n",
      "Loss for  35795 th iteration => 0.00850934208798\n",
      "Loss for  35796 th iteration => 0.00850920767137\n",
      "Loss for  35797 th iteration => 0.00850884826978\n",
      "Loss for  35798 th iteration => 0.00850864998353\n",
      "Loss for  35799 th iteration => 0.00850835419526\n",
      "Loss for  35800 th iteration => 0.00850809273675\n",
      "Loss for  35801 th iteration => 0.00850786107374\n",
      "Loss for  35802 th iteration => 0.00850768590572\n",
      "Loss for  35803 th iteration => 0.0085073674503\n",
      "Loss for  35804 th iteration => 0.00850714085813\n",
      "Loss for  35805 th iteration => 0.00850678245802\n",
      "Loss for  35806 th iteration => 0.00850658020521\n",
      "Loss for  35807 th iteration => 0.00850629136573\n",
      "Loss for  35808 th iteration => 0.00850602006887\n",
      "Loss for  35809 th iteration => 0.00850579996002\n",
      "Loss for  35810 th iteration => 0.00850546854141\n",
      "Loss for  35811 th iteration => 0.00850518064523\n",
      "Loss for  35812 th iteration => 0.00850492766876\n",
      "Loss for  35813 th iteration => 0.00850467303869\n",
      "Loss for  35814 th iteration => 0.00850438695849\n",
      "Loss for  35815 th iteration => 0.00850416851902\n",
      "Loss for  35816 th iteration => 0.00850408785709\n",
      "Loss for  35817 th iteration => 0.0085036619078\n",
      "Loss for  35818 th iteration => 0.00850331143642\n",
      "Loss for  35819 th iteration => 0.00850315411215\n",
      "Loss for  35820 th iteration => 0.00850278984477\n",
      "Loss for  35821 th iteration => 0.00850252849049\n",
      "Loss for  35822 th iteration => 0.00850226715253\n",
      "Loss for  35823 th iteration => 0.00850200583087\n",
      "Loss for  35824 th iteration => 0.00850174452552\n",
      "Loss for  35825 th iteration => 0.00850148323646\n",
      "Loss for  35826 th iteration => 0.0085012225633\n",
      "Loss for  35827 th iteration => 0.0085012338921\n",
      "Loss for  35828 th iteration => 0.00850070461859\n",
      "Loss for  35829 th iteration => 0.00850055929929\n",
      "Loss for  35830 th iteration => 0.00850019633777\n",
      "Loss for  35831 th iteration => 0.00850014471029\n",
      "Loss for  35832 th iteration => 0.00849968781746\n",
      "Loss for  35833 th iteration => 0.0084994875044\n",
      "Loss for  35834 th iteration => 0.00849917825496\n",
      "Loss for  35835 th iteration => 0.00849895047007\n",
      "Loss for  35836 th iteration => 0.00849866869805\n",
      "Loss for  35837 th iteration => 0.00849841356443\n",
      "Loss for  35838 th iteration => 0.00849815937384\n",
      "Loss for  35839 th iteration => 0.00849804323029\n",
      "Loss for  35840 th iteration => 0.00849765953324\n",
      "Loss for  35841 th iteration => 0.00849749206025\n",
      "Loss for  35842 th iteration => 0.00849716807478\n",
      "Loss for  35843 th iteration => 0.00849707629005\n",
      "Loss for  35844 th iteration => 0.0084965893453\n",
      "Loss for  35845 th iteration => 0.00849648591006\n",
      "Loss for  35846 th iteration => 0.00849608429961\n",
      "Loss for  35847 th iteration => 0.00849585916894\n",
      "Loss for  35848 th iteration => 0.00849557873257\n",
      "Loss for  35849 th iteration => 0.00849531846012\n",
      "Loss for  35850 th iteration => 0.00849507313243\n",
      "Loss for  35851 th iteration => 0.0084947779249\n",
      "Loss for  35852 th iteration => 0.00849456750032\n",
      "Loss for  35853 th iteration => 0.00849423756176\n",
      "Loss for  35854 th iteration => 0.00849406286841\n",
      "Loss for  35855 th iteration => 0.00849385563612\n",
      "Loss for  35856 th iteration => 0.00849345137931\n",
      "Loss for  35857 th iteration => 0.00849354158008\n",
      "Loss for  35858 th iteration => 0.00849294605684\n",
      "Loss for  35859 th iteration => 0.00849278491664\n",
      "Loss for  35860 th iteration => 0.0084924401244\n",
      "Loss for  35861 th iteration => 0.00849224543603\n",
      "Loss for  35862 th iteration => 0.00849193416732\n",
      "Loss for  35863 th iteration => 0.00849170611949\n",
      "Loss for  35864 th iteration => 0.00849142818659\n",
      "Loss for  35865 th iteration => 0.00849116696564\n",
      "Loss for  35866 th iteration => 0.0084909221832\n",
      "Loss for  35867 th iteration => 0.00849062797313\n",
      "Loss for  35868 th iteration => 0.00849041766306\n",
      "Loss for  35869 th iteration => 0.00849034649063\n",
      "Loss for  35870 th iteration => 0.00848991425873\n",
      "Loss for  35871 th iteration => 0.00848955584386\n",
      "Loss for  35872 th iteration => 0.00848940806734\n",
      "Loss for  35873 th iteration => 0.00848903644565\n",
      "Loss for  35874 th iteration => 0.00848877592283\n",
      "Loss for  35875 th iteration => 0.00848851541625\n",
      "Loss for  35876 th iteration => 0.0084882549259\n",
      "Loss for  35877 th iteration => 0.00848799445179\n",
      "Loss for  35878 th iteration => 0.0084877339939\n",
      "Loss for  35879 th iteration => 0.00848747355225\n",
      "Loss for  35880 th iteration => 0.00848721598794\n",
      "Loss for  35881 th iteration => 0.00848724245936\n",
      "Loss for  35882 th iteration => 0.00848670509243\n",
      "Loss for  35883 th iteration => 0.00848655469777\n",
      "Loss for  35884 th iteration => 0.00848632845348\n",
      "Loss for  35885 th iteration => 0.00848602069083\n",
      "Loss for  35886 th iteration => 0.00848569229861\n",
      "Loss for  35887 th iteration => 0.00848564368986\n",
      "Loss for  35888 th iteration => 0.00848520179267\n",
      "Loss for  35889 th iteration => 0.00848508826137\n",
      "Loss for  35890 th iteration => 0.00848471102356\n",
      "Loss for  35891 th iteration => 0.00848453328303\n",
      "Loss for  35892 th iteration => 0.00848421999569\n",
      "Loss for  35893 th iteration => 0.00848397874857\n",
      "Loss for  35894 th iteration => 0.00848372871339\n",
      "Loss for  35895 th iteration => 0.00848342465184\n",
      "Loss for  35896 th iteration => 0.00848323815009\n",
      "Loss for  35897 th iteration => 0.00848302184601\n",
      "Loss for  35898 th iteration => 0.00848265256083\n",
      "Loss for  35899 th iteration => 0.00848257289313\n",
      "Loss for  35900 th iteration => 0.00848216527138\n",
      "Loss for  35901 th iteration => 0.00848192143535\n",
      "Loss for  35902 th iteration => 0.00848167695424\n",
      "Loss for  35903 th iteration => 0.00848137824342\n",
      "Loss for  35904 th iteration => 0.00848106040166\n",
      "Loss for  35905 th iteration => 0.00848084015392\n",
      "Loss for  35906 th iteration => 0.00848055579588\n",
      "Loss for  35907 th iteration => 0.0084803022286\n",
      "Loss for  35908 th iteration => 0.00848005116599\n",
      "Loss for  35909 th iteration => 0.00847976446609\n",
      "Loss for  35910 th iteration => 0.00847954651297\n",
      "Loss for  35911 th iteration => 0.00847923020427\n",
      "Loss for  35912 th iteration => 0.00847928856294\n",
      "Loss for  35913 th iteration => 0.00847869466496\n",
      "Loss for  35914 th iteration => 0.00847854124869\n",
      "Loss for  35915 th iteration => 0.00847817727638\n",
      "Loss for  35916 th iteration => 0.00847791740913\n",
      "Loss for  35917 th iteration => 0.00847765755804\n",
      "Loss for  35918 th iteration => 0.0084773977231\n",
      "Loss for  35919 th iteration => 0.0084771379043\n",
      "Loss for  35920 th iteration => 0.00847687810166\n",
      "Loss for  35921 th iteration => 0.00847661831516\n",
      "Loss for  35922 th iteration => 0.00847635854481\n",
      "Loss for  35923 th iteration => 0.00847610136874\n",
      "Loss for  35924 th iteration => 0.00847611870255\n",
      "Loss for  35925 th iteration => 0.00847558786809\n",
      "Loss for  35926 th iteration => 0.00847544246282\n",
      "Loss for  35927 th iteration => 0.0084752067982\n",
      "Loss for  35928 th iteration => 0.00847490928356\n",
      "Loss for  35929 th iteration => 0.00847457711212\n",
      "Loss for  35930 th iteration => 0.00847437513358\n",
      "Loss for  35931 th iteration => 0.00847407057966\n",
      "Loss for  35932 th iteration => 0.00847384111337\n",
      "Loss for  35933 th iteration => 0.00847356405176\n",
      "Loss for  35934 th iteration => 0.00847330722206\n",
      "Loss for  35935 th iteration => 0.00847305809483\n",
      "Loss for  35936 th iteration => 0.00847294408411\n",
      "Loss for  35937 th iteration => 0.00847256607433\n",
      "Loss for  35938 th iteration => 0.00847239179927\n",
      "Loss for  35939 th iteration => 0.00847222059621\n",
      "Loss for  35940 th iteration => 0.00847184094414\n",
      "Loss for  35941 th iteration => 0.00847158873886\n",
      "Loss for  35942 th iteration => 0.00847130483577\n",
      "Loss for  35943 th iteration => 0.00847099634421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  35944 th iteration => 0.00847076697049\n",
      "Loss for  35945 th iteration => 0.00847049378974\n",
      "Loss for  35946 th iteration => 0.0084702292804\n",
      "Loss for  35947 th iteration => 0.00846999120132\n",
      "Loss for  35948 th iteration => 0.00846969176397\n",
      "Loss for  35949 th iteration => 0.00846948869709\n",
      "Loss for  35950 th iteration => 0.00846931053467\n",
      "Loss for  35951 th iteration => 0.00846900232369\n",
      "Loss for  35952 th iteration => 0.00846875598951\n",
      "Loss for  35953 th iteration => 0.00846839023982\n",
      "Loss for  35954 th iteration => 0.00846845267363\n",
      "Loss for  35955 th iteration => 0.00846789113844\n",
      "Loss for  35956 th iteration => 0.00846768788058\n",
      "Loss for  35957 th iteration => 0.0084673880962\n",
      "Loss for  35958 th iteration => 0.0084671515541\n",
      "Loss for  35959 th iteration => 0.00846688502986\n",
      "Loss for  35960 th iteration => 0.00846661538981\n",
      "Loss for  35961 th iteration => 0.00846638194039\n",
      "Loss for  35962 th iteration => 0.00846607938639\n",
      "Loss for  35963 th iteration => 0.00846587882878\n",
      "Loss for  35964 th iteration => 0.00846554354249\n",
      "Loss for  35965 th iteration => 0.00846537569596\n",
      "Loss for  35966 th iteration => 0.00846501531635\n",
      "Loss for  35967 th iteration => 0.00846502331735\n",
      "Loss for  35968 th iteration => 0.00846449950384\n",
      "Loss for  35969 th iteration => 0.00846424168532\n",
      "Loss for  35970 th iteration => 0.00846421879852\n",
      "Loss for  35971 th iteration => 0.00846373783181\n",
      "Loss for  35972 th iteration => 0.00846356884527\n",
      "Loss for  35973 th iteration => 0.00846323270717\n",
      "Loss for  35974 th iteration => 0.00846303600126\n",
      "Loss for  35975 th iteration => 0.00846272758528\n",
      "Loss for  35976 th iteration => 0.00846250328804\n",
      "Loss for  35977 th iteration => 0.00846222246678\n",
      "Loss for  35978 th iteration => 0.00846197070472\n",
      "Loss for  35979 th iteration => 0.00846171912534\n",
      "Loss for  35980 th iteration => 0.00846160425722\n",
      "Loss for  35981 th iteration => 0.00846122314914\n",
      "Loss for  35982 th iteration => 0.00846105959056\n",
      "Loss for  35983 th iteration => 0.00846087497403\n",
      "Loss for  35984 th iteration => 0.00846050900078\n",
      "Loss for  35985 th iteration => 0.00846024871079\n",
      "Loss for  35986 th iteration => 0.0084599734468\n",
      "Loss for  35987 th iteration => 0.00845966104389\n",
      "Loss for  35988 th iteration => 0.0084594368802\n",
      "Loss for  35989 th iteration => 0.00845915990008\n",
      "Loss for  35990 th iteration => 0.00845890048991\n",
      "Loss for  35991 th iteration => 0.00845865872113\n",
      "Loss for  35992 th iteration => 0.00845836427438\n",
      "Loss for  35993 th iteration => 0.00845815750816\n",
      "Loss for  35994 th iteration => 0.00845782823206\n",
      "Loss for  35995 th iteration => 0.00845765776097\n",
      "Loss for  35996 th iteration => 0.00845745120767\n",
      "Loss for  35997 th iteration => 0.00845704984247\n",
      "Loss for  35998 th iteration => 0.00845714290105\n",
      "Loss for  35999 th iteration => 0.00845654948021\n",
      "Loss for  36000 th iteration => 0.00845638792632\n",
      "Loss for  36001 th iteration => 0.0084560479487\n",
      "Loss for  36002 th iteration => 0.00845585278106\n",
      "Loss for  36003 th iteration => 0.00845554639076\n",
      "Loss for  36004 th iteration => 0.00845531780066\n",
      "Loss for  36005 th iteration => 0.0084550448074\n",
      "Loss for  36006 th iteration => 0.00845478298372\n",
      "Loss for  36007 th iteration => 0.00845454319961\n",
      "Loss for  36008 th iteration => 0.00845424832888\n",
      "Loss for  36009 th iteration => 0.00845404156838\n",
      "Loss for  36010 th iteration => 0.00845371664605\n",
      "Loss for  36011 th iteration => 0.00845380320371\n",
      "Loss for  36012 th iteration => 0.0084531850793\n",
      "Loss for  36013 th iteration => 0.00845292815726\n",
      "Loss for  36014 th iteration => 0.00845278459178\n",
      "Loss for  36015 th iteration => 0.00845241248682\n",
      "Loss for  36016 th iteration => 0.00845215417257\n",
      "Loss for  36017 th iteration => 0.00845189587432\n",
      "Loss for  36018 th iteration => 0.00845163759208\n",
      "Loss for  36019 th iteration => 0.00845137972056\n",
      "Loss for  36020 th iteration => 0.00845124807869\n",
      "Loss for  36021 th iteration => 0.00845087620257\n",
      "Loss for  36022 th iteration => 0.00845071715846\n",
      "Loss for  36023 th iteration => 0.00845053447522\n",
      "Loss for  36024 th iteration => 0.00845019250334\n",
      "Loss for  36025 th iteration => 0.00845001792861\n",
      "Loss for  36026 th iteration => 0.0084496789412\n",
      "Loss for  36027 th iteration => 0.00844963123668\n",
      "Loss for  36028 th iteration => 0.0084491655114\n",
      "Loss for  36029 th iteration => 0.00844897503832\n",
      "Loss for  36030 th iteration => 0.00844865089136\n",
      "Loss for  36031 th iteration => 0.00844845253794\n",
      "Loss for  36032 th iteration => 0.00844813631443\n",
      "Loss for  36033 th iteration => 0.00844793016491\n",
      "Loss for  36034 th iteration => 0.0084476217805\n",
      "Loss for  36035 th iteration => 0.00844740791804\n",
      "Loss for  36036 th iteration => 0.00844711389831\n",
      "Loss for  36037 th iteration => 0.00844678720436\n",
      "Loss for  36038 th iteration => 0.00844661401153\n",
      "Loss for  36039 th iteration => 0.00844625675323\n",
      "Loss for  36040 th iteration => 0.00844615259743\n",
      "Loss for  36041 th iteration => 0.00844575194731\n",
      "Loss for  36042 th iteration => 0.00844584192815\n",
      "Loss for  36043 th iteration => 0.0084452533329\n",
      "Loss for  36044 th iteration => 0.0084450942612\n",
      "Loss for  36045 th iteration => 0.00844475318802\n",
      "Loss for  36046 th iteration => 0.00844456042868\n",
      "Loss for  36047 th iteration => 0.00844425301578\n",
      "Loss for  36048 th iteration => 0.00844402676181\n",
      "Loss for  36049 th iteration => 0.00844375281719\n",
      "Loss for  36050 th iteration => 0.00844349325916\n",
      "Loss for  36051 th iteration => 0.00844325259325\n",
      "Loss for  36052 th iteration => 0.00844295991936\n",
      "Loss for  36053 th iteration => 0.00844275234495\n",
      "Loss for  36054 th iteration => 0.00844242810501\n",
      "Loss for  36055 th iteration => 0.00844251017275\n",
      "Loss for  36056 th iteration => 0.00844190033398\n",
      "Loss for  36057 th iteration => 0.0084416429893\n",
      "Loss for  36058 th iteration => 0.00844149882359\n",
      "Loss for  36059 th iteration => 0.00844112976497\n",
      "Loss for  36060 th iteration => 0.00844087212896\n",
      "Loss for  36061 th iteration => 0.00844061450889\n",
      "Loss for  36062 th iteration => 0.00844035690474\n",
      "Loss for  36063 th iteration => 0.00844009931652\n",
      "Loss for  36064 th iteration => 0.00843984174422\n",
      "Loss for  36065 th iteration => 0.00843958418785\n",
      "Loss for  36066 th iteration => 0.00843932672713\n",
      "Loss for  36067 th iteration => 0.00843920087381\n",
      "Loss for  36068 th iteration => 0.00843898453006\n",
      "Loss for  36069 th iteration => 0.00843867530543\n",
      "Loss for  36070 th iteration => 0.0084384716641\n",
      "Loss for  36071 th iteration => 0.00843829581004\n",
      "Loss for  36072 th iteration => 0.00843795264489\n",
      "Loss for  36073 th iteration => 0.00843765102521\n",
      "Loss for  36074 th iteration => 0.00843743138637\n",
      "Loss for  36075 th iteration => 0.00843713775156\n",
      "Loss for  36076 th iteration => 0.00843691025698\n",
      "Loss for  36077 th iteration => 0.00843662452105\n",
      "Loss for  36078 th iteration => 0.00843638925549\n",
      "Loss for  36079 th iteration => 0.00843611133359\n",
      "Loss for  36080 th iteration => 0.00843586838067\n",
      "Loss for  36081 th iteration => 0.00843559818908\n",
      "Loss for  36082 th iteration => 0.00843534763134\n",
      "Loss for  36083 th iteration => 0.00843508829385\n",
      "Loss for  36084 th iteration => 0.00843473263441\n",
      "Loss for  36085 th iteration => 0.0084346284197\n",
      "Loss for  36086 th iteration => 0.00843422892811\n",
      "Loss for  36087 th iteration => 0.00843431717202\n",
      "Loss for  36088 th iteration => 0.0084337316438\n",
      "Loss for  36089 th iteration => 0.00843357312276\n",
      "Loss for  36090 th iteration => 0.00843323289634\n",
      "Loss for  36091 th iteration => 0.00843304064526\n",
      "Loss for  36092 th iteration => 0.00843273412079\n",
      "Loss for  36093 th iteration => 0.00843250833389\n",
      "Loss for  36094 th iteration => 0.00843223531819\n",
      "Loss for  36095 th iteration => 0.00843197618721\n",
      "Loss for  36096 th iteration => 0.00843173648953\n",
      "Loss for  36097 th iteration => 0.00843144420383\n",
      "Loss for  36098 th iteration => 0.00843123763582\n",
      "Loss for  36099 th iteration => 0.00843091298027\n",
      "Loss for  36100 th iteration => 0.00843085763706\n",
      "Loss for  36101 th iteration => 0.00843052446168\n",
      "Loss for  36102 th iteration => 0.00843013115601\n",
      "Loss for  36103 th iteration => 0.00842987536894\n",
      "Loss for  36104 th iteration => 0.00842973133836\n",
      "Loss for  36105 th iteration => 0.00842936269638\n",
      "Loss for  36106 th iteration => 0.00842910576729\n",
      "Loss for  36107 th iteration => 0.00842884885405\n",
      "Loss for  36108 th iteration => 0.00842859195667\n",
      "Loss for  36109 th iteration => 0.00842833507514\n",
      "Loss for  36110 th iteration => 0.00842807820946\n",
      "Loss for  36111 th iteration => 0.00842782135963\n",
      "Loss for  36112 th iteration => 0.00842756584905\n",
      "Loss for  36113 th iteration => 0.00842747898754\n",
      "Loss for  36114 th iteration => 0.00842717116248\n",
      "Loss for  36115 th iteration => 0.00842696737143\n",
      "Loss for  36116 th iteration => 0.00842665968543\n",
      "Loss for  36117 th iteration => 0.00842658152336\n",
      "Loss for  36118 th iteration => 0.00842614957437\n",
      "Loss for  36119 th iteration => 0.00842592992524\n",
      "Loss for  36120 th iteration => 0.00842563766949\n",
      "Loss for  36121 th iteration => 0.00842541016042\n",
      "Loss for  36122 th iteration => 0.00842512580766\n",
      "Loss for  36123 th iteration => 0.00842489052347\n",
      "Loss for  36124 th iteration => 0.00842461398875\n",
      "Loss for  36125 th iteration => 0.00842437101318\n",
      "Loss for  36126 th iteration => 0.00842410221266\n",
      "Loss for  36127 th iteration => 0.00842385162834\n",
      "Loss for  36128 th iteration => 0.00842359424267\n",
      "Loss for  36129 th iteration => 0.00842323842436\n",
      "Loss for  36130 th iteration => 0.0084229833606\n",
      "Loss for  36131 th iteration => 0.00842288078881\n",
      "Loss for  36132 th iteration => 0.00842258035847\n",
      "Loss for  36133 th iteration => 0.00842247210568\n",
      "Loss for  36134 th iteration => 0.00842198592237\n",
      "Loss for  36135 th iteration => 0.00842182689923\n",
      "Loss for  36136 th iteration => 0.00842148858864\n",
      "Loss for  36137 th iteration => 0.00842129581205\n",
      "Loss for  36138 th iteration => 0.00842099122625\n",
      "Loss for  36139 th iteration => 0.00842076489127\n",
      "Loss for  36140 th iteration => 0.00842049383623\n",
      "Loss for  36141 th iteration => 0.00842023413547\n",
      "Loss for  36142 th iteration => 0.0084199964196\n",
      "Loss for  36143 th iteration => 0.00841970354322\n",
      "Loss for  36144 th iteration => 0.00841949897733\n",
      "Loss for  36145 th iteration => 0.00841917428546\n",
      "Loss for  36146 th iteration => 0.00841925907048\n",
      "Loss for  36147 th iteration => 0.00841865044974\n",
      "Loss for  36148 th iteration => 0.00841839416326\n",
      "Loss for  36149 th iteration => 0.00841813882657\n",
      "Loss for  36150 th iteration => 0.00841799691816\n",
      "Loss for  36151 th iteration => 0.00841762780432\n",
      "Loss for  36152 th iteration => 0.00841737157899\n",
      "Loss for  36153 th iteration => 0.00841711536945\n",
      "Loss for  36154 th iteration => 0.00841685917569\n",
      "Loss for  36155 th iteration => 0.00841660299771\n",
      "Loss for  36156 th iteration => 0.00841634693998\n",
      "Loss for  36157 th iteration => 0.00841621862236\n",
      "Loss for  36158 th iteration => 0.00841584765382\n",
      "Loss for  36159 th iteration => 0.00841569283963\n",
      "Loss for  36160 th iteration => 0.0084155097751\n",
      "Loss for  36161 th iteration => 0.00841517229143\n",
      "Loss for  36162 th iteration => 0.00841499747074\n",
      "Loss for  36163 th iteration => 0.00841466287017\n",
      "Loss for  36164 th iteration => 0.00841461557127\n",
      "Loss for  36165 th iteration => 0.00841415360111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  36166 th iteration => 0.0084139628511\n",
      "Loss for  36167 th iteration => 0.00841364314729\n",
      "Loss for  36168 th iteration => 0.00841344450439\n",
      "Loss for  36169 th iteration => 0.00841313273622\n",
      "Loss for  36170 th iteration => 0.00841292628498\n",
      "Loss for  36171 th iteration => 0.00841262236779\n",
      "Loss for  36172 th iteration => 0.00841240819164\n",
      "Loss for  36173 th iteration => 0.00841211804735\n",
      "Loss for  36174 th iteration => 0.0084117915585\n",
      "Loss for  36175 th iteration => 0.00841162237705\n",
      "Loss for  36176 th iteration => 0.0084112646973\n",
      "Loss for  36177 th iteration => 0.00841116529397\n",
      "Loss for  36178 th iteration => 0.00841076731445\n",
      "Loss for  36179 th iteration => 0.00841085673115\n",
      "Loss for  36180 th iteration => 0.00841027352227\n",
      "Loss for  36181 th iteration => 0.00841011131468\n",
      "Loss for  36182 th iteration => 0.00840977757642\n",
      "Loss for  36183 th iteration => 0.00840958163527\n",
      "Loss for  36184 th iteration => 0.00840928160162\n",
      "Loss for  36185 th iteration => 0.00840905212216\n",
      "Loss for  36186 th iteration => 0.00840878559889\n",
      "Loss for  36187 th iteration => 0.00840852277391\n",
      "Loss for  36188 th iteration => 0.00840828956924\n",
      "Loss for  36189 th iteration => 0.00840799358912\n",
      "Loss for  36190 th iteration => 0.00840779351367\n",
      "Loss for  36191 th iteration => 0.00840746463578\n",
      "Loss for  36192 th iteration => 0.0084074167688\n",
      "Loss for  36193 th iteration => 0.0084070841743\n",
      "Loss for  36194 th iteration => 0.0084066892367\n",
      "Loss for  36195 th iteration => 0.00840643366674\n",
      "Loss for  36196 th iteration => 0.00840617914623\n",
      "Loss for  36197 th iteration => 0.00840604081986\n",
      "Loss for  36198 th iteration => 0.0084056694465\n",
      "Loss for  36199 th iteration => 0.00840541420967\n",
      "Loss for  36200 th iteration => 0.00840528070655\n",
      "Loss for  36201 th iteration => 0.00840491620536\n",
      "Loss for  36202 th iteration => 0.00840475467029\n",
      "Loss for  36203 th iteration => 0.00840441819981\n",
      "Loss for  36204 th iteration => 0.00840422876756\n",
      "Loss for  36205 th iteration => 0.00840392019369\n",
      "Loss for  36206 th iteration => 0.0084037041001\n",
      "Loss for  36207 th iteration => 0.00840358706775\n",
      "Loss for  36208 th iteration => 0.00840318943291\n",
      "Loss for  36209 th iteration => 0.00840307186409\n",
      "Loss for  36210 th iteration => 0.00840268116165\n",
      "Loss for  36211 th iteration => 0.00840269458516\n",
      "Loss for  36212 th iteration => 0.00840217368863\n",
      "Loss for  36213 th iteration => 0.00840204005967\n",
      "Loss for  36214 th iteration => 0.00840166470701\n",
      "Loss for  36215 th iteration => 0.00840152312382\n",
      "Loss for  36216 th iteration => 0.00840116749157\n",
      "Loss for  36217 th iteration => 0.00840090742309\n",
      "Loss for  36218 th iteration => 0.00840067330131\n",
      "Loss for  36219 th iteration => 0.00840037806327\n",
      "Loss for  36220 th iteration => 0.00840017907401\n",
      "Loss for  36221 th iteration => 0.00839984890317\n",
      "Loss for  36222 th iteration => 0.00839983682712\n",
      "Loss for  36223 th iteration => 0.00839933940433\n",
      "Loss for  36224 th iteration => 0.00839920286074\n",
      "Loss for  36225 th iteration => 0.00839884532067\n",
      "Loss for  36226 th iteration => 0.00839877954295\n",
      "Loss for  36227 th iteration => 0.00839847736989\n",
      "Loss for  36228 th iteration => 0.00839815096233\n",
      "Loss for  36229 th iteration => 0.00839785980476\n",
      "Loss for  36230 th iteration => 0.0083976228517\n",
      "Loss for  36231 th iteration => 0.00839736512671\n",
      "Loss for  36232 th iteration => 0.00839709490536\n",
      "Loss for  36233 th iteration => 0.0083968704218\n",
      "Loss for  36234 th iteration => 0.00839656712192\n",
      "Loss for  36235 th iteration => 0.008396375691\n",
      "Loss for  36236 th iteration => 0.00839603949997\n",
      "Loss for  36237 th iteration => 0.0083958809353\n",
      "Loss for  36238 th iteration => 0.00839552153001\n",
      "Loss for  36239 th iteration => 0.00839526726389\n",
      "Loss for  36240 th iteration => 0.00839513474977\n",
      "Loss for  36241 th iteration => 0.00839490771615\n",
      "Loss for  36242 th iteration => 0.00839462016509\n",
      "Loss for  36243 th iteration => 0.00839426586239\n",
      "Loss for  36244 th iteration => 0.00839409597981\n",
      "Loss for  36245 th iteration => 0.00839389337838\n",
      "Loss for  36246 th iteration => 0.00839357335902\n",
      "Loss for  36247 th iteration => 0.00839327454388\n",
      "Loss for  36248 th iteration => 0.00839304890604\n",
      "Loss for  36249 th iteration => 0.00839277776198\n",
      "Loss for  36250 th iteration => 0.00839252458403\n",
      "Loss for  36251 th iteration => 0.00839228098097\n",
      "Loss for  36252 th iteration => 0.00839200089479\n",
      "Loss for  36253 th iteration => 0.00839194640526\n",
      "Loss for  36254 th iteration => 0.00839149322322\n",
      "Loss for  36255 th iteration => 0.00839143045063\n",
      "Loss for  36256 th iteration => 0.00839098953207\n",
      "Loss for  36257 th iteration => 0.00839082514212\n",
      "Loss for  36258 th iteration => 0.00839049691712\n",
      "Loss for  36259 th iteration => 0.0083902976042\n",
      "Loss for  36260 th iteration => 0.00839015278625\n",
      "Loss for  36261 th iteration => 0.00838986453555\n",
      "Loss for  36262 th iteration => 0.00838965180785\n",
      "Loss for  36263 th iteration => 0.00838926164965\n",
      "Loss for  36264 th iteration => 0.00838914093078\n",
      "Loss for  36265 th iteration => 0.00838874904913\n",
      "Loss for  36266 th iteration => 0.00838863013275\n",
      "Loss for  36267 th iteration => 0.00838823655105\n",
      "Loss for  36268 th iteration => 0.0083881194131\n",
      "Loss for  36269 th iteration => 0.00838773402102\n",
      "Loss for  36270 th iteration => 0.00838748118408\n",
      "Loss for  36271 th iteration => 0.00838724059655\n",
      "Loss for  36272 th iteration => 0.00838695438799\n",
      "Loss for  36273 th iteration => 0.00838674714391\n",
      "Loss for  36274 th iteration => 0.00838642827767\n",
      "Loss for  36275 th iteration => 0.0083863650932\n",
      "Loss for  36276 th iteration => 0.00838604071407\n",
      "Loss for  36277 th iteration => 0.00838565647414\n",
      "Loss for  36278 th iteration => 0.00838540216108\n",
      "Loss for  36279 th iteration => 0.00838514812138\n",
      "Loss for  36280 th iteration => 0.00838500363904\n",
      "Loss for  36281 th iteration => 0.00838464168762\n",
      "Loss for  36282 th iteration => 0.00838438743507\n",
      "Loss for  36283 th iteration => 0.00838413319809\n",
      "Loss for  36284 th iteration => 0.00838387897669\n",
      "Loss for  36285 th iteration => 0.00838362477086\n",
      "Loss for  36286 th iteration => 0.0083833705806\n",
      "Loss for  36287 th iteration => 0.00838311640591\n",
      "Loss for  36288 th iteration => 0.00838286272565\n",
      "Loss for  36289 th iteration => 0.00838273710691\n",
      "Loss for  36290 th iteration => 0.00838251901365\n",
      "Loss for  36291 th iteration => 0.00838221813574\n",
      "Loss for  36292 th iteration => 0.00838201521745\n",
      "Loss for  36293 th iteration => 0.00838171148222\n",
      "Loss for  36294 th iteration => 0.00838150154801\n",
      "Loss for  36295 th iteration => 0.00838133801776\n",
      "Loss for  36296 th iteration => 0.0083809882925\n",
      "Loss for  36297 th iteration => 0.00838070064309\n",
      "Loss for  36298 th iteration => 0.00838047383575\n",
      "Loss for  36299 th iteration => 0.00838019411345\n",
      "Loss for  36300 th iteration => 0.00837995950694\n",
      "Loss for  36301 th iteration => 0.00837968762631\n",
      "Loss for  36302 th iteration => 0.00837944530482\n",
      "Loss for  36303 th iteration => 0.00837918118156\n",
      "Loss for  36304 th iteration => 0.00837893122818\n",
      "Loss for  36305 th iteration => 0.00837867894945\n",
      "Loss for  36306 th iteration => 0.0083783271879\n",
      "Loss for  36307 th iteration => 0.00837807331404\n",
      "Loss for  36308 th iteration => 0.00837781972259\n",
      "Loss for  36309 th iteration => 0.00837772159931\n",
      "Loss for  36310 th iteration => 0.00837742323767\n",
      "Loss for  36311 th iteration => 0.0083773147731\n",
      "Loss for  36312 th iteration => 0.0083768358935\n",
      "Loss for  36313 th iteration => 0.00837667495267\n",
      "Loss for  36314 th iteration => 0.00837634397586\n",
      "Loss for  36315 th iteration => 0.00837614919157\n",
      "Loss for  36316 th iteration => 0.00837585202734\n",
      "Loss for  36317 th iteration => 0.00837562359803\n",
      "Loss for  36318 th iteration => 0.00837536004897\n",
      "Loss for  36319 th iteration => 0.00837509817056\n",
      "Loss for  36320 th iteration => 0.00837486804179\n",
      "Loss for  36321 th iteration => 0.00837457290773\n",
      "Loss for  36322 th iteration => 0.00837437600679\n",
      "Loss for  36323 th iteration => 0.00837404793879\n",
      "Loss for  36324 th iteration => 0.00837400139673\n",
      "Loss for  36325 th iteration => 0.00837353274852\n",
      "Loss for  36326 th iteration => 0.00837341896892\n",
      "Loss for  36327 th iteration => 0.00837302634573\n",
      "Loss for  36328 th iteration => 0.00837277278667\n",
      "Loss for  36329 th iteration => 0.00837251992616\n",
      "Loss for  36330 th iteration => 0.00837238478207\n",
      "Loss for  36331 th iteration => 0.00837201468086\n",
      "Loss for  36332 th iteration => 0.00837188183533\n",
      "Loss for  36333 th iteration => 0.00837152067569\n",
      "Loss for  36334 th iteration => 0.00837135974275\n",
      "Loss for  36335 th iteration => 0.00837102666769\n",
      "Loss for  36336 th iteration => 0.00837083778436\n",
      "Loss for  36337 th iteration => 0.00837053265753\n",
      "Loss for  36338 th iteration => 0.00837031595919\n",
      "Loss for  36339 th iteration => 0.00837003905994\n",
      "Loss for  36340 th iteration => 0.00836995926897\n",
      "Loss for  36341 th iteration => 0.00836955807885\n",
      "Loss for  36342 th iteration => 0.00836941975242\n",
      "Loss for  36343 th iteration => 0.00836908123498\n",
      "Loss for  36344 th iteration => 0.00836887928017\n",
      "Loss for  36345 th iteration => 0.00836874722859\n",
      "Loss for  36346 th iteration => 0.00836834026853\n",
      "Loss for  36347 th iteration => 0.00836812857753\n",
      "Loss for  36348 th iteration => 0.00836781390536\n",
      "Loss for  36349 th iteration => 0.00836755070647\n",
      "Loss for  36350 th iteration => 0.00836742905619\n",
      "Loss for  36351 th iteration => 0.00836707696923\n",
      "Loss for  36352 th iteration => 0.00836688454076\n",
      "Loss for  36353 th iteration => 0.00836660290727\n",
      "Loss for  36354 th iteration => 0.00836634054773\n",
      "Loss for  36355 th iteration => 0.00836612852586\n",
      "Loss for  36356 th iteration => 0.0083658045306\n",
      "Loss for  36357 th iteration => 0.00836552792372\n",
      "Loss for  36358 th iteration => 0.00836528014279\n",
      "Loss for  36359 th iteration => 0.00836503718864\n",
      "Loss for  36360 th iteration => 0.00836475671145\n",
      "Loss for  36361 th iteration => 0.00836465541138\n",
      "Loss for  36362 th iteration => 0.00836436379523\n",
      "Loss for  36363 th iteration => 0.0083640595442\n",
      "Loss for  36364 th iteration => 0.00836371465534\n",
      "Loss for  36365 th iteration => 0.00836346165074\n",
      "Loss for  36366 th iteration => 0.00836321010903\n",
      "Loss for  36367 th iteration => 0.00836306436259\n",
      "Loss for  36368 th iteration => 0.00836270508002\n",
      "Loss for  36369 th iteration => 0.00836245213551\n",
      "Loss for  36370 th iteration => 0.00836219920644\n",
      "Loss for  36371 th iteration => 0.00836194629282\n",
      "Loss for  36372 th iteration => 0.00836169339463\n",
      "Loss for  36373 th iteration => 0.00836144051188\n",
      "Loss for  36374 th iteration => 0.00836118770858\n",
      "Loss for  36375 th iteration => 0.00836106146717\n",
      "Loss for  36376 th iteration => 0.00836069199677\n",
      "Loss for  36377 th iteration => 0.00836069658928\n",
      "Loss for  36378 th iteration => 0.00836020053529\n",
      "Loss for  36379 th iteration => 0.00836016976086\n",
      "Loss for  36380 th iteration => 0.00835972539935\n",
      "Loss for  36381 th iteration => 0.00835963005971\n",
      "Loss for  36382 th iteration => 0.00835938305084\n",
      "Loss for  36383 th iteration => 0.00835909100145\n",
      "Loss for  36384 th iteration => 0.00835877611643\n",
      "Loss for  36385 th iteration => 0.00835855100171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  36386 th iteration => 0.00835830006454\n",
      "Loss for  36387 th iteration => 0.00835801144494\n",
      "Loss for  36388 th iteration => 0.0083578237566\n",
      "Loss for  36389 th iteration => 0.00835747232497\n",
      "Loss for  36390 th iteration => 0.00835734719681\n",
      "Loss for  36391 th iteration => 0.00835694016682\n",
      "Loss for  36392 th iteration => 0.0083567741167\n",
      "Loss for  36393 th iteration => 0.00835642301592\n",
      "Loss for  36394 th iteration => 0.00835617044659\n",
      "Loss for  36395 th iteration => 0.00835591789265\n",
      "Loss for  36396 th iteration => 0.00835566623697\n",
      "Loss for  36397 th iteration => 0.00835556962169\n",
      "Loss for  36398 th iteration => 0.00835527524742\n",
      "Loss for  36399 th iteration => 0.00835516620357\n",
      "Loss for  36400 th iteration => 0.00835468881209\n",
      "Loss for  36401 th iteration => 0.00835452720069\n",
      "Loss for  36402 th iteration => 0.00835419953026\n",
      "Loss for  36403 th iteration => 0.00835400405444\n",
      "Loss for  36404 th iteration => 0.00835371021666\n",
      "Loss for  36405 th iteration => 0.00835348107606\n",
      "Loss for  36406 th iteration => 0.00835322087233\n",
      "Loss for  36407 th iteration => 0.00835295826406\n",
      "Loss for  36408 th iteration => 0.00835273149829\n",
      "Loss for  36409 th iteration => 0.00835243561697\n",
      "Loss for  36410 th iteration => 0.00835224209555\n",
      "Loss for  36411 th iteration => 0.00835191313336\n",
      "Loss for  36412 th iteration => 0.00835175322138\n",
      "Loss for  36413 th iteration => 0.00835151765755\n",
      "Loss for  36414 th iteration => 0.00835129071471\n",
      "Loss for  36415 th iteration => 0.00835089749381\n",
      "Loss for  36416 th iteration => 0.0083507586647\n",
      "Loss for  36417 th iteration => 0.00835040603624\n",
      "Loss for  36418 th iteration => 0.00835024047816\n",
      "Loss for  36419 th iteration => 0.00835003721767\n",
      "Loss for  36420 th iteration => 0.00834972209631\n",
      "Loss for  36421 th iteration => 0.00834942520619\n",
      "Loss for  36422 th iteration => 0.00834920278708\n",
      "Loss for  36423 th iteration => 0.00834893368518\n",
      "Loss for  36424 th iteration => 0.0083486836101\n",
      "Loss for  36425 th iteration => 0.00834844216268\n",
      "Loss for  36426 th iteration => 0.0083481650855\n",
      "Loss for  36427 th iteration => 0.00834811158593\n",
      "Loss for  36428 th iteration => 0.0083476625934\n",
      "Loss for  36429 th iteration => 0.0083476007929\n",
      "Loss for  36430 th iteration => 0.00834716431855\n",
      "Loss for  36431 th iteration => 0.00834700102571\n",
      "Loss for  36432 th iteration => 0.00834667694878\n",
      "Loss for  36433 th iteration => 0.00834647767337\n",
      "Loss for  36434 th iteration => 0.00834633707125\n",
      "Loss for  36435 th iteration => 0.00834605184476\n",
      "Loss for  36436 th iteration => 0.00834583976503\n",
      "Loss for  36437 th iteration => 0.00834545417207\n",
      "Loss for  36438 th iteration => 0.0083453340572\n",
      "Loss for  36439 th iteration => 0.00834494673038\n",
      "Loss for  36440 th iteration => 0.00834482842814\n",
      "Loss for  36441 th iteration => 0.00834443939092\n",
      "Loss for  36442 th iteration => 0.00834432287715\n",
      "Loss for  36443 th iteration => 0.00834394366997\n",
      "Loss for  36444 th iteration => 0.0083436893293\n",
      "Loss for  36445 th iteration => 0.00834345546943\n",
      "Loss for  36446 th iteration => 0.00834316768968\n",
      "Loss for  36447 th iteration => 0.00834296723871\n",
      "Loss for  36448 th iteration => 0.00834264621521\n",
      "Loss for  36449 th iteration => 0.00834247897883\n",
      "Loss for  36450 th iteration => 0.00834213380322\n",
      "Loss for  36451 th iteration => 0.0083419951041\n",
      "Loss for  36452 th iteration => 0.00834176661286\n",
      "Loss for  36453 th iteration => 0.00834138196606\n",
      "Loss for  36454 th iteration => 0.00834113028947\n",
      "Loss for  36455 th iteration => 0.00834087862821\n",
      "Loss for  36456 th iteration => 0.00834062840044\n",
      "Loss for  36457 th iteration => 0.0083404899479\n",
      "Loss for  36458 th iteration => 0.00834012606764\n",
      "Loss for  36459 th iteration => 0.00833987446599\n",
      "Loss for  36460 th iteration => 0.00833962287966\n",
      "Loss for  36461 th iteration => 0.00833937130865\n",
      "Loss for  36462 th iteration => 0.00833912007045\n",
      "Loss for  36463 th iteration => 0.00833899323714\n",
      "Loss for  36464 th iteration => 0.00833863002603\n",
      "Loss for  36465 th iteration => 0.00833847497177\n",
      "Loss for  36466 th iteration => 0.00833814038157\n",
      "Loss for  36467 th iteration => 0.00833811745276\n",
      "Loss for  36468 th iteration => 0.00833766030129\n",
      "Loss for  36469 th iteration => 0.00833758456332\n",
      "Loss for  36470 th iteration => 0.00833718760312\n",
      "Loss for  36471 th iteration => 0.00833704664294\n",
      "Loss for  36472 th iteration => 0.00833685438244\n",
      "Loss for  36473 th iteration => 0.00833651163217\n",
      "Loss for  36474 th iteration => 0.00833624318894\n",
      "Loss for  36475 th iteration => 0.00833597454678\n",
      "Loss for  36476 th iteration => 0.00833576958422\n",
      "Loss for  36477 th iteration => 0.00833543789988\n",
      "Loss for  36478 th iteration => 0.00833529572612\n",
      "Loss for  36479 th iteration => 0.00833491237132\n",
      "Loss for  36480 th iteration => 0.00833486324082\n",
      "Loss for  36481 th iteration => 0.00833440623392\n",
      "Loss for  36482 th iteration => 0.00833435875251\n",
      "Loss for  36483 th iteration => 0.00833390874771\n",
      "Loss for  36484 th iteration => 0.00833373244379\n",
      "Loss for  36485 th iteration => 0.00833342198207\n",
      "Loss for  36486 th iteration => 0.00833321168907\n",
      "Loss for  36487 th iteration => 0.00833293518294\n",
      "Loss for  36488 th iteration => 0.00833269110379\n",
      "Loss for  36489 th iteration => 0.0083324491397\n",
      "Loss for  36490 th iteration => 0.00833228078575\n",
      "Loss for  36491 th iteration => 0.00833209270451\n",
      "Loss for  36492 th iteration => 0.00833165542234\n",
      "Loss for  36493 th iteration => 0.00833147840966\n",
      "Loss for  36494 th iteration => 0.00833113546936\n",
      "Loss for  36495 th iteration => 0.00833099137027\n",
      "Loss for  36496 th iteration => 0.00833063502518\n",
      "Loss for  36497 th iteration => 0.0083303839862\n",
      "Loss for  36498 th iteration => 0.00833013296248\n",
      "Loss for  36499 th iteration => 0.00832988195402\n",
      "Loss for  36500 th iteration => 0.00832963096081\n",
      "Loss for  36501 th iteration => 0.00832937998285\n",
      "Loss for  36502 th iteration => 0.00832912902015\n",
      "Loss for  36503 th iteration => 0.00832887807269\n",
      "Loss for  36504 th iteration => 0.00832862777677\n",
      "Loss for  36505 th iteration => 0.00832850534557\n",
      "Loss for  36506 th iteration => 0.00832829025102\n",
      "Loss for  36507 th iteration => 0.00832799216767\n",
      "Loss for  36508 th iteration => 0.00832779266129\n",
      "Loss for  36509 th iteration => 0.00832749191077\n",
      "Loss for  36510 th iteration => 0.00832728496916\n",
      "Loss for  36511 th iteration => 0.00832712484639\n",
      "Loss for  36512 th iteration => 0.00832677843471\n",
      "Loss for  36513 th iteration => 0.00832649381103\n",
      "Loss for  36514 th iteration => 0.00832627035208\n",
      "Loss for  36515 th iteration => 0.00832599367701\n",
      "Loss for  36516 th iteration => 0.00832576239724\n",
      "Loss for  36517 th iteration => 0.00832549358498\n",
      "Loss for  36518 th iteration => 0.00832525456892\n",
      "Loss for  36519 th iteration => 0.0083249935348\n",
      "Loss for  36520 th iteration => 0.00832474686589\n",
      "Loss for  36521 th iteration => 0.0083244994584\n",
      "Loss for  36522 th iteration => 0.00832414950306\n",
      "Loss for  36523 th iteration => 0.00832389884989\n",
      "Loss for  36524 th iteration => 0.00832364829798\n",
      "Loss for  36525 th iteration => 0.00832354907411\n",
      "Loss for  36526 th iteration => 0.00832316298913\n",
      "Loss for  36527 th iteration => 0.00832303004327\n",
      "Loss for  36528 th iteration => 0.00832278079117\n",
      "Loss for  36529 th iteration => 0.00832263329502\n",
      "Loss for  36530 th iteration => 0.00832219607405\n",
      "Loss for  36531 th iteration => 0.00832199509452\n",
      "Loss for  36532 th iteration => 0.00832171054213\n",
      "Loss for  36533 th iteration => 0.00832147590017\n",
      "Loss for  36534 th iteration => 0.00832122497834\n",
      "Loss for  36535 th iteration => 0.00832095687246\n",
      "Loss for  36536 th iteration => 0.00832073938369\n",
      "Loss for  36537 th iteration => 0.00832043800989\n",
      "Loss for  36538 th iteration => 0.0083202537592\n",
      "Loss for  36539 th iteration => 0.00831991931102\n",
      "Loss for  36540 th iteration => 0.00831976810587\n",
      "Loss for  36541 th iteration => 0.00831941213598\n",
      "Loss for  36542 th iteration => 0.00831916176401\n",
      "Loss for  36543 th iteration => 0.00831891194377\n",
      "Loss for  36544 th iteration => 0.00831878526622\n",
      "Loss for  36545 th iteration => 0.00831856568096\n",
      "Loss for  36546 th iteration => 0.00831827612198\n",
      "Loss for  36547 th iteration => 0.00831793192203\n",
      "Loss for  36548 th iteration => 0.00831776033394\n",
      "Loss for  36549 th iteration => 0.00831744429094\n",
      "Loss for  36550 th iteration => 0.00831724670807\n",
      "Loss for  36551 th iteration => 0.00831723330166\n",
      "Loss for  36552 th iteration => 0.00831674850455\n",
      "Loss for  36553 th iteration => 0.00831659984196\n",
      "Loss for  36554 th iteration => 0.00831624960585\n",
      "Loss for  36555 th iteration => 0.00831609285667\n",
      "Loss for  36556 th iteration => 0.00831575074906\n",
      "Loss for  36557 th iteration => 0.00831558599988\n",
      "Loss for  36558 th iteration => 0.00831525193401\n",
      "Loss for  36559 th iteration => 0.00831507927033\n",
      "Loss for  36560 th iteration => 0.00831475996814\n",
      "Loss for  36561 th iteration => 0.0083144793167\n",
      "Loss for  36562 th iteration => 0.00831427609241\n",
      "Loss for  36563 th iteration => 0.00831396010237\n",
      "Loss for  36564 th iteration => 0.00831379217526\n",
      "Loss for  36565 th iteration => 0.00831344380239\n",
      "Loss for  36566 th iteration => 0.00831319488852\n",
      "Loss for  36567 th iteration => 0.00831319660037\n",
      "Loss for  36568 th iteration => 0.00831282548599\n",
      "Loss for  36569 th iteration => 0.00831258473361\n",
      "Loss for  36570 th iteration => 0.00831222745577\n",
      "Loss for  36571 th iteration => 0.00831206637452\n",
      "Loss for  36572 th iteration => 0.00831174323877\n",
      "Loss for  36573 th iteration => 0.00831154818613\n",
      "Loss for  36574 th iteration => 0.00831125898785\n",
      "Loss for  36575 th iteration => 0.00831103016689\n",
      "Loss for  36576 th iteration => 0.00831077470404\n",
      "Loss for  36577 th iteration => 0.00831051231527\n",
      "Loss for  36578 th iteration => 0.00831029038837\n",
      "Loss for  36579 th iteration => 0.00830999462977\n",
      "Loss for  36580 th iteration => 0.00830980604188\n",
      "Loss for  36581 th iteration => 0.00830947710893\n",
      "Loss for  36582 th iteration => 0.00830932166557\n",
      "Loss for  36583 th iteration => 0.00830896926235\n",
      "Loss for  36584 th iteration => 0.00830883934368\n",
      "Loss for  36585 th iteration => 0.0083086164922\n",
      "Loss for  36586 th iteration => 0.00830833487644\n",
      "Loss for  36587 th iteration => 0.00830798786364\n",
      "Loss for  36588 th iteration => 0.00830782022154\n",
      "Loss for  36589 th iteration => 0.00830750143735\n",
      "Loss for  36590 th iteration => 0.00830730707008\n",
      "Loss for  36591 th iteration => 0.00830714068807\n",
      "Loss for  36592 th iteration => 0.00830679385129\n",
      "Loss for  36593 th iteration => 0.00830653069169\n",
      "Loss for  36594 th iteration => 0.00830643223301\n",
      "Loss for  36595 th iteration => 0.00830606154054\n",
      "Loss for  36596 th iteration => 0.0083058979038\n",
      "Loss for  36597 th iteration => 0.00830559212267\n",
      "Loss for  36598 th iteration => 0.00830536402569\n",
      "Loss for  36599 th iteration => 0.00830512244239\n",
      "Loss for  36600 th iteration => 0.00830483794535\n",
      "Loss for  36601 th iteration => 0.00830456089881\n",
      "Loss for  36602 th iteration => 0.008304319593\n",
      "Loss for  36603 th iteration => 0.00830407835472\n",
      "Loss for  36604 th iteration => 0.00830380142192\n",
      "Loss for  36605 th iteration => 0.0083035957673\n",
      "Loss for  36606 th iteration => 0.00830328343039\n",
      "Loss for  36607 th iteration => 0.0083031145525\n",
      "Loss for  36608 th iteration => 0.00830301502355\n",
      "Loss for  36609 th iteration => 0.00830264396348\n",
      "Loss for  36610 th iteration => 0.00830240755962\n",
      "Loss for  36611 th iteration => 0.00830204894687\n",
      "Loss for  36612 th iteration => 0.00830189036966\n",
      "Loss for  36613 th iteration => 0.00830156595255\n",
      "Loss for  36614 th iteration => 0.00830137335097\n",
      "Loss for  36615 th iteration => 0.00830108292362\n",
      "Loss for  36616 th iteration => 0.00830085650199\n",
      "Loss for  36617 th iteration => 0.00830059986112\n",
      "Loss for  36618 th iteration => 0.00830033982118\n",
      "Loss for  36619 th iteration => 0.00830011676609\n",
      "Loss for  36620 th iteration => 0.00829982330702\n",
      "Loss for  36621 th iteration => 0.00829963363957\n",
      "Loss for  36622 th iteration => 0.00829930695804\n",
      "Loss for  36623 th iteration => 0.00829915048256\n",
      "Loss for  36624 th iteration => 0.00829879991124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  36625 th iteration => 0.00829866969046\n",
      "Loss for  36626 th iteration => 0.00829844658455\n",
      "Loss for  36627 th iteration => 0.00829816700978\n",
      "Loss for  36628 th iteration => 0.00829782049274\n",
      "Loss for  36629 th iteration => 0.00829765353488\n",
      "Loss for  36630 th iteration => 0.0082973352784\n",
      "Loss for  36631 th iteration => 0.0082971414464\n",
      "Loss for  36632 th iteration => 0.00829697445757\n",
      "Loss for  36633 th iteration => 0.00829662951528\n",
      "Loss for  36634 th iteration => 0.00829636688651\n",
      "Loss for  36635 th iteration => 0.00829611664644\n",
      "Loss for  36636 th iteration => 0.00829603438095\n",
      "Loss for  36637 th iteration => 0.00829562023974\n",
      "Loss for  36638 th iteration => 0.00829552970731\n",
      "Loss for  36639 th iteration => 0.00829512387463\n",
      "Loss for  36640 th iteration => 0.00829502516285\n",
      "Loss for  36641 th iteration => 0.00829463489134\n",
      "Loss for  36642 th iteration => 0.0082944309375\n",
      "Loss for  36643 th iteration => 0.0082941536351\n",
      "Loss for  36644 th iteration => 0.00829391378718\n",
      "Loss for  36645 th iteration => 0.00829367233377\n",
      "Loss for  36646 th iteration => 0.00829339681855\n",
      "Loss for  36647 th iteration => 0.00829319098852\n",
      "Loss for  36648 th iteration => 0.00829288097797\n",
      "Loss for  36649 th iteration => 0.00829295798963\n",
      "Loss for  36650 th iteration => 0.00829237741542\n",
      "Loss for  36651 th iteration => 0.00829236481537\n",
      "Loss for  36652 th iteration => 0.00829189594868\n",
      "Loss for  36653 th iteration => 0.00829173753994\n",
      "Loss for  36654 th iteration => 0.00829141416186\n",
      "Loss for  36655 th iteration => 0.00829122169941\n",
      "Loss for  36656 th iteration => 0.00829093233988\n",
      "Loss for  36657 th iteration => 0.00829070602894\n",
      "Loss for  36658 th iteration => 0.00829045048379\n",
      "Loss for  36659 th iteration => 0.00829019052699\n",
      "Loss for  36660 th iteration => 0.00828996859464\n",
      "Loss for  36661 th iteration => 0.00828967519204\n",
      "Loss for  36662 th iteration => 0.00828948667345\n",
      "Loss for  36663 th iteration => 0.0082891600226\n",
      "Loss for  36664 th iteration => 0.00828900472125\n",
      "Loss for  36665 th iteration => 0.00828865468473\n",
      "Loss for  36666 th iteration => 0.00828852399768\n",
      "Loss for  36667 th iteration => 0.00828816098514\n",
      "Loss for  36668 th iteration => 0.00828816381601\n",
      "Loss for  36669 th iteration => 0.00828767773113\n",
      "Loss for  36670 th iteration => 0.00828751096241\n",
      "Loss for  36671 th iteration => 0.00828719371729\n",
      "Loss for  36672 th iteration => 0.00828699880826\n",
      "Loss for  36673 th iteration => 0.00828671072399\n",
      "Loss for  36674 th iteration => 0.00828661144575\n",
      "Loss for  36675 th iteration => 0.00828622772111\n",
      "Loss for  36676 th iteration => 0.00828597746851\n",
      "Loss for  36677 th iteration => 0.00828574474838\n",
      "Loss for  36678 th iteration => 0.00828561882212\n",
      "Loss for  36679 th iteration => 0.00828527793585\n",
      "Loss for  36680 th iteration => 0.00828508704844\n",
      "Loss for  36681 th iteration => 0.00828481085772\n",
      "Loss for  36682 th iteration => 0.00828456431636\n",
      "Loss for  36683 th iteration => 0.00828425309739\n",
      "Loss for  36684 th iteration => 0.00828404820111\n",
      "Loss for  36685 th iteration => 0.00828377306208\n",
      "Loss for  36686 th iteration => 0.00828353226939\n",
      "Loss for  36687 th iteration => 0.00828329298131\n",
      "Loss for  36688 th iteration => 0.00828301651945\n",
      "Loss for  36689 th iteration => 0.00828281285627\n",
      "Loss for  36690 th iteration => 0.00828250127908\n",
      "Loss for  36691 th iteration => 0.00828248641508\n",
      "Loss for  36692 th iteration => 0.0082820960262\n",
      "Loss for  36693 th iteration => 0.00828198848123\n",
      "Loss for  36694 th iteration => 0.00828152093007\n",
      "Loss for  36695 th iteration => 0.00828136129262\n",
      "Loss for  36696 th iteration => 0.0082810403603\n",
      "Loss for  36697 th iteration => 0.00828084667325\n",
      "Loss for  36698 th iteration => 0.00828055975506\n",
      "Loss for  36699 th iteration => 0.00828033222398\n",
      "Loss for  36700 th iteration => 0.0082800791154\n",
      "Loss for  36701 th iteration => 0.00827981794325\n",
      "Loss for  36702 th iteration => 0.00827959844236\n",
      "Loss for  36703 th iteration => 0.00827930382953\n",
      "Loss for  36704 th iteration => 0.00827911773697\n",
      "Loss for  36705 th iteration => 0.00827878988134\n",
      "Loss for  36706 th iteration => 0.00827863700024\n",
      "Loss for  36707 th iteration => 0.00827828600947\n",
      "Loss for  36708 th iteration => 0.00827803865403\n",
      "Loss for  36709 th iteration => 0.00827791083231\n",
      "Loss for  36710 th iteration => 0.00827769165355\n",
      "Loss for  36711 th iteration => 0.00827740880402\n",
      "Loss for  36712 th iteration => 0.00827706628799\n",
      "Loss for  36713 th iteration => 0.00827689776695\n",
      "Loss for  36714 th iteration => 0.00827658351616\n",
      "Loss for  36715 th iteration => 0.00827638736056\n",
      "Loss for  36716 th iteration => 0.00827622730208\n",
      "Loss for  36717 th iteration => 0.0082760260812\n",
      "Loss for  36718 th iteration => 0.00827563733257\n",
      "Loss for  36719 th iteration => 0.00827549512828\n",
      "Loss for  36720 th iteration => 0.00827517162747\n",
      "Loss for  36721 th iteration => 0.00827496462917\n",
      "Loss for  36722 th iteration => 0.00827470565768\n",
      "Loss for  36723 th iteration => 0.00827443457753\n",
      "Loss for  36724 th iteration => 0.00827423942747\n",
      "Loss for  36725 th iteration => 0.00827391403639\n",
      "Loss for  36726 th iteration => 0.00827367984118\n",
      "Loss for  36727 th iteration => 0.00827339942789\n",
      "Loss for  36728 th iteration => 0.00827320083247\n",
      "Loss for  36729 th iteration => 0.00827288499941\n",
      "Loss for  36730 th iteration => 0.00827272178042\n",
      "Loss for  36731 th iteration => 0.00827237549095\n",
      "Loss for  36732 th iteration => 0.00827212789705\n",
      "Loss for  36733 th iteration => 0.00827188162942\n",
      "Loss for  36734 th iteration => 0.0082716484883\n",
      "Loss for  36735 th iteration => 0.00827161833093\n",
      "Loss for  36736 th iteration => 0.00827129007067\n",
      "Loss for  36737 th iteration => 0.00827100908215\n",
      "Loss for  36738 th iteration => 0.00827069325852\n",
      "Loss for  36739 th iteration => 0.00827049570253\n",
      "Loss for  36740 th iteration => 0.00827021384631\n",
      "Loss for  36741 th iteration => 0.00826998249265\n",
      "Loss for  36742 th iteration => 0.00826973439962\n",
      "Loss for  36743 th iteration => 0.00826946945096\n",
      "Loss for  36744 th iteration => 0.00826925491949\n",
      "Loss for  36745 th iteration => 0.00826895657595\n",
      "Loss for  36746 th iteration => 0.00826877540695\n",
      "Loss for  36747 th iteration => 0.0082684438661\n",
      "Loss for  36748 th iteration => 0.008268295863\n",
      "Loss for  36749 th iteration => 0.00826794315321\n",
      "Loss for  36750 th iteration => 0.00826769582041\n",
      "Loss for  36751 th iteration => 0.00826744850252\n",
      "Loss for  36752 th iteration => 0.00826720191216\n",
      "Loss for  36753 th iteration => 0.00826707845299\n",
      "Loss for  36754 th iteration => 0.00826686522608\n",
      "Loss for  36755 th iteration => 0.00826657367197\n",
      "Loss for  36756 th iteration => 0.00826623717221\n",
      "Loss for  36757 th iteration => 0.0082662057054\n",
      "Loss for  36758 th iteration => 0.00826577322831\n",
      "Loss for  36759 th iteration => 0.00826567608133\n",
      "Loss for  36760 th iteration => 0.00826544062745\n",
      "Loss for  36761 th iteration => 0.00826514850889\n",
      "Loss for  36762 th iteration => 0.00826484624703\n",
      "Loss for  36763 th iteration => 0.00826461934194\n",
      "Loss for  36764 th iteration => 0.00826438138236\n",
      "Loss for  36765 th iteration => 0.00826409062029\n",
      "Loss for  36766 th iteration => 0.00826391625857\n",
      "Loss for  36767 th iteration => 0.00826356233773\n",
      "Loss for  36768 th iteration => 0.00826345087983\n",
      "Loss for  36769 th iteration => 0.008263043651\n",
      "Loss for  36770 th iteration => 0.00826288812704\n",
      "Loss for  36771 th iteration => 0.00826254114454\n",
      "Loss for  36772 th iteration => 0.00826229492644\n",
      "Loss for  36773 th iteration => 0.00826219275304\n",
      "Loss for  36774 th iteration => 0.00826181686051\n",
      "Loss for  36775 th iteration => 0.00826168008427\n",
      "Loss for  36776 th iteration => 0.00826133875535\n",
      "Loss for  36777 th iteration => 0.00826116759033\n",
      "Loss for  36778 th iteration => 0.00826086061204\n",
      "Loss for  36779 th iteration => 0.0082606555409\n",
      "Loss for  36780 th iteration => 0.00826048892088\n",
      "Loss for  36781 th iteration => 0.00826014658588\n",
      "Loss for  36782 th iteration => 0.00826003430269\n",
      "Loss for  36783 th iteration => 0.00825963599235\n",
      "Loss for  36784 th iteration => 0.00825942964924\n",
      "Loss for  36785 th iteration => 0.00825912431324\n",
      "Loss for  36786 th iteration => 0.00825895125062\n",
      "Loss for  36787 th iteration => 0.00825861279872\n",
      "Loss for  36788 th iteration => 0.00825847282072\n",
      "Loss for  36789 th iteration => 0.00825811915305\n",
      "Loss for  36790 th iteration => 0.00825787239721\n",
      "Loss for  36791 th iteration => 0.00825762565621\n",
      "Loss for  36792 th iteration => 0.00825737893007\n",
      "Loss for  36793 th iteration => 0.00825713221878\n",
      "Loss for  36794 th iteration => 0.00825688552234\n",
      "Loss for  36795 th iteration => 0.00825663941036\n",
      "Loss for  36796 th iteration => 0.00825651613496\n",
      "Loss for  36797 th iteration => 0.00825615915857\n",
      "Loss for  36798 th iteration => 0.00825600755558\n",
      "Loss for  36799 th iteration => 0.00825583695315\n",
      "Loss for  36800 th iteration => 0.00825550990243\n",
      "Loss for  36801 th iteration => 0.00825533942672\n",
      "Loss for  36802 th iteration => 0.00825501815307\n",
      "Loss for  36803 th iteration => 0.0082548395318\n",
      "Loss for  36804 th iteration => 0.00825452644547\n",
      "Loss for  36805 th iteration => 0.00825434084727\n",
      "Loss for  36806 th iteration => 0.00825417424266\n",
      "Loss for  36807 th iteration => 0.00825384246094\n",
      "Loss for  36808 th iteration => 0.00825354537855\n",
      "Loss for  36809 th iteration => 0.0082533434797\n",
      "Loss for  36810 th iteration => 0.00825318821773\n",
      "Loss for  36811 th iteration => 0.00825285987263\n",
      "Loss for  36812 th iteration => 0.00825268973313\n",
      "Loss for  36813 th iteration => 0.00825227483619\n",
      "Loss for  36814 th iteration => 0.00825219490987\n",
      "Loss for  36815 th iteration => 0.00825179095232\n",
      "Loss for  36816 th iteration => 0.00825157761936\n",
      "Loss for  36817 th iteration => 0.00825131380207\n",
      "Loss for  36818 th iteration => 0.00825106656802\n",
      "Loss for  36819 th iteration => 0.00825083661583\n",
      "Loss for  36820 th iteration => 0.00825055568633\n",
      "Loss for  36821 th iteration => 0.00825035939462\n",
      "Loss for  36822 th iteration => 0.00825004497272\n",
      "Loss for  36823 th iteration => 0.0082498821395\n",
      "Loss for  36824 th iteration => 0.00824954295082\n",
      "Loss for  36825 th iteration => 0.00824940819678\n",
      "Loss for  36826 th iteration => 0.00824918596295\n",
      "Loss for  36827 th iteration => 0.00824880819761\n",
      "Loss for  36828 th iteration => 0.00824856198836\n",
      "Loss for  36829 th iteration => 0.00824831579392\n",
      "Loss for  36830 th iteration => 0.00824806961427\n",
      "Loss for  36831 th iteration => 0.00824782398328\n",
      "Loss for  36832 th iteration => 0.00824769286868\n",
      "Loss for  36833 th iteration => 0.00824744856473\n",
      "Loss for  36834 th iteration => 0.00824710026787\n",
      "Loss for  36835 th iteration => 0.00824694100781\n",
      "Loss for  36836 th iteration => 0.00824662103451\n",
      "Loss for  36837 th iteration => 0.00824643358808\n",
      "Loss for  36838 th iteration => 0.00824614179307\n",
      "Loss for  36839 th iteration => 0.00824592630443\n",
      "Loss for  36840 th iteration => 0.00824566254427\n",
      "Loss for  36841 th iteration => 0.00824541954093\n",
      "Loss for  36842 th iteration => 0.00824533982061\n",
      "Loss for  36843 th iteration => 0.00824492905718\n",
      "Loss for  36844 th iteration => 0.00824484103487\n",
      "Loss for  36845 th iteration => 0.00824443861469\n",
      "Loss for  36846 th iteration => 0.00824434237854\n",
      "Loss for  36847 th iteration => 0.00824395210271\n",
      "Loss for  36848 th iteration => 0.00824375408393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  36849 th iteration => 0.00824347686445\n",
      "Loss for  36850 th iteration => 0.00824324278246\n",
      "Loss for  36851 th iteration => 0.00824300226487\n",
      "Loss for  36852 th iteration => 0.00824288068571\n",
      "Loss for  36853 th iteration => 0.00824263076875\n",
      "Loss for  36854 th iteration => 0.0082423573604\n",
      "Loss for  36855 th iteration => 0.0082420783921\n",
      "Loss for  36856 th iteration => 0.00824182763806\n",
      "Loss for  36857 th iteration => 0.00824161887153\n",
      "Loss for  36858 th iteration => 0.00824131555009\n",
      "Loss for  36859 th iteration => 0.0082410290019\n",
      "Loss for  36860 th iteration => 0.00824080566317\n",
      "Loss for  36861 th iteration => 0.00824055304764\n",
      "Loss for  36862 th iteration => 0.00824029594624\n",
      "Loss for  36863 th iteration => 0.00824007705791\n",
      "Loss for  36864 th iteration => 0.00823978639773\n",
      "Loss for  36865 th iteration => 0.00823960103373\n",
      "Loss for  36866 th iteration => 0.0082392770161\n",
      "Loss for  36867 th iteration => 0.00823912497614\n",
      "Loss for  36868 th iteration => 0.00823877890155\n",
      "Loss for  36869 th iteration => 0.00823853328226\n",
      "Loss for  36870 th iteration => 0.00823828769757\n",
      "Loss for  36871 th iteration => 0.0082381613277\n",
      "Loss for  36872 th iteration => 0.00823780176847\n",
      "Loss for  36873 th iteration => 0.00823780606361\n",
      "Loss for  36874 th iteration => 0.00823732536104\n",
      "Loss for  36875 th iteration => 0.0082371572091\n",
      "Loss for  36876 th iteration => 0.00823684727392\n",
      "Loss for  36877 th iteration => 0.00823665096042\n",
      "Loss for  36878 th iteration => 0.00823651181862\n",
      "Loss for  36879 th iteration => 0.00823628821191\n",
      "Loss for  36880 th iteration => 0.00823601592247\n",
      "Loss for  36881 th iteration => 0.00823567416574\n",
      "Loss for  36882 th iteration => 0.00823551833172\n",
      "Loss for  36883 th iteration => 0.00823518472484\n",
      "Loss for  36884 th iteration => 0.0082350208693\n",
      "Loss for  36885 th iteration => 0.00823469532503\n",
      "Loss for  36886 th iteration => 0.00823452353392\n",
      "Loss for  36887 th iteration => 0.00823421619368\n",
      "Loss for  36888 th iteration => 0.00823393410526\n",
      "Loss for  36889 th iteration => 0.0082337418308\n",
      "Loss for  36890 th iteration => 0.00823342426107\n",
      "Loss for  36891 th iteration => 0.00823326742283\n",
      "Loss for  36892 th iteration => 0.00823292363361\n",
      "Loss for  36893 th iteration => 0.00823267892009\n",
      "Loss for  36894 th iteration => 0.00823257872825\n",
      "Loss for  36895 th iteration => 0.00823220435723\n",
      "Loss for  36896 th iteration => 0.00823206947331\n",
      "Loss for  36897 th iteration => 0.00823172975365\n",
      "Loss for  36898 th iteration => 0.00823156039425\n",
      "Loss for  36899 th iteration => 0.0082312556492\n",
      "Loss for  36900 th iteration => 0.00823115516441\n",
      "Loss for  36901 th iteration => 0.00823078263696\n",
      "Loss for  36902 th iteration => 0.0082306707816\n",
      "Loss for  36903 th iteration => 0.00823030935123\n",
      "Loss for  36904 th iteration => 0.00823003909917\n",
      "Loss for  36905 th iteration => 0.00822983448449\n",
      "Loss for  36906 th iteration => 0.00822953083506\n",
      "Loss for  36907 th iteration => 0.00822935958393\n",
      "Loss for  36908 th iteration => 0.00822902273649\n",
      "Loss for  36909 th iteration => 0.00822888465057\n",
      "Loss for  36910 th iteration => 0.00822853271353\n",
      "Loss for  36911 th iteration => 0.00822828769441\n",
      "Loss for  36912 th iteration => 0.00822804268999\n",
      "Loss for  36913 th iteration => 0.00822779770026\n",
      "Loss for  36914 th iteration => 0.00822755272523\n",
      "Loss for  36915 th iteration => 0.00822730776488\n",
      "Loss for  36916 th iteration => 0.00822706340879\n",
      "Loss for  36917 th iteration => 0.00822694147276\n",
      "Loss for  36918 th iteration => 0.008226586636\n",
      "Loss for  36919 th iteration => 0.00822643625175\n",
      "Loss for  36920 th iteration => 0.00822611024654\n",
      "Loss for  36921 th iteration => 0.00822608994201\n",
      "Loss for  36922 th iteration => 0.0082256458028\n",
      "Loss for  36923 th iteration => 0.0082255676255\n",
      "Loss for  36924 th iteration => 0.00822518637661\n",
      "Loss for  36925 th iteration => 0.0082250423929\n",
      "Loss for  36926 th iteration => 0.00822472667937\n",
      "Loss for  36927 th iteration => 0.00822451848904\n",
      "Loss for  36928 th iteration => 0.00822440854028\n",
      "Loss for  36929 th iteration => 0.00822399752418\n",
      "Loss for  36930 th iteration => 0.00822393828218\n",
      "Loss for  36931 th iteration => 0.00822348888217\n",
      "Loss for  36932 th iteration => 0.00822345973387\n",
      "Loss for  36933 th iteration => 0.00822299356275\n",
      "Loss for  36934 th iteration => 0.00822287888308\n",
      "Loss for  36935 th iteration => 0.00822250044055\n",
      "Loss for  36936 th iteration => 0.00822238757682\n",
      "Loss for  36937 th iteration => 0.00822202109541\n",
      "Loss for  36938 th iteration => 0.00822177089741\n",
      "Loss for  36939 th iteration => 0.00822154733583\n",
      "Loss for  36940 th iteration => 0.00822126343039\n",
      "Loss for  36941 th iteration => 0.00822107354022\n",
      "Loss for  36942 th iteration => 0.00822075613181\n",
      "Loss for  36943 th iteration => 0.00822059970963\n",
      "Loss for  36944 th iteration => 0.00822026100169\n",
      "Loss for  36945 th iteration => 0.00822001646657\n",
      "Loss for  36946 th iteration => 0.00821977194611\n",
      "Loss for  36947 th iteration => 0.00821952756138\n",
      "Loss for  36948 th iteration => 0.00821939787526\n",
      "Loss for  36949 th iteration => 0.00821904103494\n",
      "Loss for  36950 th iteration => 0.00821893562795\n",
      "Loss for  36951 th iteration => 0.00821866265725\n",
      "Loss for  36952 th iteration => 0.00821832227036\n",
      "Loss for  36953 th iteration => 0.00821815840857\n",
      "Loss for  36954 th iteration => 0.00821784644273\n",
      "Loss for  36955 th iteration => 0.00821765435295\n",
      "Loss for  36956 th iteration => 0.0082174927173\n",
      "Loss for  36957 th iteration => 0.00821715276859\n",
      "Loss for  36958 th iteration => 0.00821689687301\n",
      "Loss for  36959 th iteration => 0.00821679568493\n",
      "Loss for  36960 th iteration => 0.00821643832134\n",
      "Loss for  36961 th iteration => 0.00821627164567\n",
      "Loss for  36962 th iteration => 0.00821597950039\n",
      "Loss for  36963 th iteration => 0.00821574805948\n",
      "Loss for  36964 th iteration => 0.00821552041444\n",
      "Loss for  36965 th iteration => 0.00821523554982\n",
      "Loss for  36966 th iteration => 0.00821497024946\n",
      "Loss for  36967 th iteration => 0.00821472760211\n",
      "Loss for  36968 th iteration => 0.00821449826502\n",
      "Loss for  36969 th iteration => 0.00821421983782\n",
      "Loss for  36970 th iteration => 0.00821402623329\n",
      "Loss for  36971 th iteration => 0.00821371225518\n",
      "Loss for  36972 th iteration => 0.00821355415547\n",
      "Loss for  36973 th iteration => 0.0082132112723\n",
      "Loss for  36974 th iteration => 0.00821311359385\n",
      "Loss for  36975 th iteration => 0.00821273900438\n",
      "Loss for  36976 th iteration => 0.00821260657768\n",
      "Loss for  36977 th iteration => 0.00821226669481\n",
      "Loss for  36978 th iteration => 0.00821210030203\n",
      "Loss for  36979 th iteration => 0.00821189787438\n",
      "Loss for  36980 th iteration => 0.00821159638341\n",
      "Loss for  36981 th iteration => 0.00821144869545\n",
      "Loss for  36982 th iteration => 0.00821109133014\n",
      "Loss for  36983 th iteration => 0.00821085313004\n",
      "Loss for  36984 th iteration => 0.00821058514063\n",
      "Loss for  36985 th iteration => 0.00821038055365\n",
      "Loss for  36986 th iteration => 0.00821007911872\n",
      "Loss for  36987 th iteration => 0.00820990794253\n",
      "Loss for  36988 th iteration => 0.00820957326289\n",
      "Loss for  36989 th iteration => 0.0082094352977\n",
      "Loss for  36990 th iteration => 0.00820908431511\n",
      "Loss for  36991 th iteration => 0.00820884043446\n",
      "Loss for  36992 th iteration => 0.0082085965684\n",
      "Loss for  36993 th iteration => 0.00820835271693\n",
      "Loss for  36994 th iteration => 0.00820810918953\n",
      "Loss for  36995 th iteration => 0.00820798608818\n",
      "Loss for  36996 th iteration => 0.00820763463974\n",
      "Loss for  36997 th iteration => 0.00820748306099\n",
      "Loss for  36998 th iteration => 0.00820716007903\n",
      "Loss for  36999 th iteration => 0.00820698017241\n",
      "Loss for  37000 th iteration => 0.00820668608801\n",
      "Loss for  37001 th iteration => 0.00820663711253\n",
      "Loss for  37002 th iteration => 0.00820622525632\n",
      "Loss for  37003 th iteration => 0.00820611569356\n",
      "Loss for  37004 th iteration => 0.00820576795865\n",
      "Loss for  37005 th iteration => 0.00820559287134\n",
      "Loss for  37006 th iteration => 0.00820531039147\n",
      "Loss for  37007 th iteration => 0.00820507167867\n",
      "Loss for  37008 th iteration => 0.00820499510789\n",
      "Loss for  37009 th iteration => 0.00820455416814\n",
      "Loss for  37010 th iteration => 0.00820452598298\n",
      "Loss for  37011 th iteration => 0.00820406330985\n",
      "Loss for  37012 th iteration => 0.00820394771903\n",
      "Loss for  37013 th iteration => 0.00820357233591\n",
      "Loss for  37014 th iteration => 0.0082034585867\n",
      "Loss for  37015 th iteration => 0.00820308146377\n",
      "Loss for  37016 th iteration => 0.00820296953176\n",
      "Loss for  37017 th iteration => 0.00820260469116\n",
      "Loss for  37018 th iteration => 0.00820235359432\n",
      "Loss for  37019 th iteration => 0.00820213319474\n",
      "Loss for  37020 th iteration => 0.00820184838659\n",
      "Loss for  37021 th iteration => 0.00820166166166\n",
      "Loss for  37022 th iteration => 0.00820134334746\n",
      "Loss for  37023 th iteration => 0.00820119009294\n",
      "Loss for  37024 th iteration => 0.00820085094109\n",
      "Loss for  37025 th iteration => 0.00820060754084\n",
      "Loss for  37026 th iteration => 0.00820036415514\n",
      "Loss for  37027 th iteration => 0.00820012078399\n",
      "Loss for  37028 th iteration => 0.00819987742739\n",
      "Loss for  37029 th iteration => 0.00819963452831\n",
      "Loss for  37030 th iteration => 0.00819951034654\n",
      "Loss for  37031 th iteration => 0.00819929577069\n",
      "Loss for  37032 th iteration => 0.00819901607973\n",
      "Loss for  37033 th iteration => 0.00819868157007\n",
      "Loss for  37034 th iteration => 0.00819851412939\n",
      "Loss for  37035 th iteration => 0.00819820802258\n",
      "Loss for  37036 th iteration => 0.00819801302867\n",
      "Loss for  37037 th iteration => 0.0081980011498\n",
      "Loss for  37038 th iteration => 0.00819752997745\n",
      "Loss for  37039 th iteration => 0.00819738534733\n",
      "Loss for  37040 th iteration => 0.00819704498046\n",
      "Loss for  37041 th iteration => 0.00819689222578\n",
      "Loss for  37042 th iteration => 0.00819656002445\n",
      "Loss for  37043 th iteration => 0.00819639923251\n",
      "Loss for  37044 th iteration => 0.00819607510923\n",
      "Loss for  37045 th iteration => 0.00819590636619\n",
      "Loss for  37046 th iteration => 0.00819560135359\n",
      "Loss for  37047 th iteration => 0.00819532034603\n",
      "Loss for  37048 th iteration => 0.00819513151863\n",
      "Loss for  37049 th iteration => 0.00819481496352\n",
      "Loss for  37050 th iteration => 0.00819466184441\n",
      "Loss for  37051 th iteration => 0.00819446099428\n",
      "Loss for  37052 th iteration => 0.00819409222843\n",
      "Loss for  37053 th iteration => 0.00819395615131\n",
      "Loss for  37054 th iteration => 0.00819362208335\n",
      "Loss for  37055 th iteration => 0.00819345148499\n",
      "Loss for  37056 th iteration => 0.00819315189712\n",
      "Loss for  37057 th iteration => 0.00819294699364\n",
      "Loss for  37058 th iteration => 0.00819268167082\n",
      "Loss for  37059 th iteration => 0.00819244267558\n",
      "Loss for  37060 th iteration => 0.00819221152641\n",
      "Loss for  37061 th iteration => 0.00819204607244\n",
      "Loss for  37062 th iteration => 0.00819174387006\n",
      "Loss for  37063 th iteration => 0.00819156654074\n",
      "Loss for  37064 th iteration => 0.00819127436714\n",
      "Loss for  37065 th iteration => 0.00819093776673\n",
      "Loss for  37066 th iteration => 0.00819069494493\n",
      "Loss for  37067 th iteration => 0.00819045213763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  37068 th iteration => 0.00819021021345\n",
      "Loss for  37069 th iteration => 0.00819007902151\n",
      "Loss for  37070 th iteration => 0.00818972599413\n",
      "Loss for  37071 th iteration => 0.00818948392227\n",
      "Loss for  37072 th iteration => 0.00818935611351\n",
      "Loss for  37073 th iteration => 0.00818901152683\n",
      "Loss for  37074 th iteration => 0.00818885526703\n",
      "Loss for  37075 th iteration => 0.00818853911995\n",
      "Loss for  37076 th iteration => 0.00818835455912\n",
      "Loss for  37077 th iteration => 0.00818806670239\n",
      "Loss for  37078 th iteration => 0.00818785402335\n",
      "Loss for  37079 th iteration => 0.00818774763611\n",
      "Loss for  37080 th iteration => 0.00818737019168\n",
      "Loss for  37081 th iteration => 0.00818725551998\n",
      "Loss for  37082 th iteration => 0.00818688640105\n",
      "Loss for  37083 th iteration => 0.00818676353358\n",
      "Loss for  37084 th iteration => 0.00818640265124\n",
      "Loss for  37085 th iteration => 0.00818627167558\n",
      "Loss for  37086 th iteration => 0.00818592279922\n",
      "Loss for  37087 th iteration => 0.00818568897837\n",
      "Loss for  37088 th iteration => 0.00818545426183\n",
      "Loss for  37089 th iteration => 0.00818518450813\n",
      "Loss for  37090 th iteration => 0.00818498583425\n",
      "Loss for  37091 th iteration => 0.00818483013482\n",
      "Loss for  37092 th iteration => 0.00818462630877\n",
      "Loss for  37093 th iteration => 0.00818431014215\n",
      "Loss for  37094 th iteration => 0.00818407905707\n",
      "Loss for  37095 th iteration => 0.00818380755427\n",
      "Loss for  37096 th iteration => 0.00818349632727\n",
      "Loss for  37097 th iteration => 0.00818330422376\n",
      "Loss for  37098 th iteration => 0.00818302718778\n",
      "Loss for  37099 th iteration => 0.00818280106571\n",
      "Loss for  37100 th iteration => 0.00818255800959\n",
      "Loss for  37101 th iteration => 0.0081822980785\n",
      "Loss for  37102 th iteration => 0.00818208879376\n",
      "Loss for  37103 th iteration => 0.00818179526051\n",
      "Loss for  37104 th iteration => 0.00818161954134\n",
      "Loss for  37105 th iteration => 0.00818129261019\n",
      "Loss for  37106 th iteration => 0.00818115025334\n",
      "Loss for  37107 th iteration => 0.00818080391207\n",
      "Loss for  37108 th iteration => 0.00818056168198\n",
      "Loss for  37109 th iteration => 0.00818031946635\n",
      "Loss for  37110 th iteration => 0.00818007726517\n",
      "Loss for  37111 th iteration => 0.00817983507843\n",
      "Loss for  37112 th iteration => 0.00817959298444\n",
      "Loss for  37113 th iteration => 0.00817947569853\n",
      "Loss for  37114 th iteration => 0.00817912119037\n",
      "Loss for  37115 th iteration => 0.00817912744176\n",
      "Loss for  37116 th iteration => 0.00817865779492\n",
      "Loss for  37117 th iteration => 0.00817861496327\n",
      "Loss for  37118 th iteration => 0.00817820407734\n",
      "Loss for  37119 th iteration => 0.00817809484822\n",
      "Loss for  37120 th iteration => 0.00817775008491\n",
      "Loss for  37121 th iteration => 0.00817757519395\n",
      "Loss for  37122 th iteration => 0.00817729635791\n",
      "Loss for  37123 th iteration => 0.00817719310203\n",
      "Loss for  37124 th iteration => 0.00817684299622\n",
      "Loss for  37125 th iteration => 0.00817654032585\n",
      "Loss for  37126 th iteration => 0.00817651793905\n",
      "Loss for  37127 th iteration => 0.00817603724162\n",
      "Loss for  37128 th iteration => 0.00817604479848\n",
      "Loss for  37129 th iteration => 0.00817554747702\n",
      "Loss for  37130 th iteration => 0.00817547050933\n",
      "Loss for  37131 th iteration => 0.00817507603129\n",
      "Loss for  37132 th iteration => 0.0081748628759\n",
      "Loss for  37133 th iteration => 0.00817460786894\n",
      "Loss for  37134 th iteration => 0.00817436069528\n",
      "Loss for  37135 th iteration => 0.00817413966737\n",
      "Loss for  37136 th iteration => 0.00817385868595\n",
      "Loss for  37137 th iteration => 0.00817367142763\n",
      "Loss for  37138 th iteration => 0.00817335684629\n",
      "Loss for  37139 th iteration => 0.00817320315078\n",
      "Loss for  37140 th iteration => 0.00817286790313\n",
      "Loss for  37141 th iteration => 0.00817262613379\n",
      "Loss for  37142 th iteration => 0.00817238437888\n",
      "Loss for  37143 th iteration => 0.00817214263837\n",
      "Loss for  37144 th iteration => 0.00817190091228\n",
      "Loss for  37145 th iteration => 0.00817165920061\n",
      "Loss for  37146 th iteration => 0.00817141822261\n",
      "Loss for  37147 th iteration => 0.00817129412153\n",
      "Loss for  37148 th iteration => 0.00817108213984\n",
      "Loss for  37149 th iteration => 0.00817080385304\n",
      "Loss for  37150 th iteration => 0.00817047152408\n",
      "Loss for  37151 th iteration => 0.00817030515447\n",
      "Loss for  37152 th iteration => 0.00817000164379\n",
      "Loss for  37153 th iteration => 0.00816994610906\n",
      "Loss for  37154 th iteration => 0.00816954909757\n",
      "Loss for  37155 th iteration => 0.00816955266434\n",
      "Loss for  37156 th iteration => 0.00816909758074\n",
      "Loss for  37157 th iteration => 0.00816891114225\n",
      "Loss for  37158 th iteration => 0.00816864420235\n",
      "Loss for  37159 th iteration => 0.00816839308383\n",
      "Loss for  37160 th iteration => 0.00816819055915\n",
      "Loss for  37161 th iteration => 0.00816787547092\n",
      "Loss for  37162 th iteration => 0.00816773665533\n",
      "Loss for  37163 th iteration => 0.00816737091081\n",
      "Loss for  37164 th iteration => 0.00816718776859\n",
      "Loss for  37165 th iteration => 0.00816686875692\n",
      "Loss for  37166 th iteration => 0.00816672131565\n",
      "Loss for  37167 th iteration => 0.00816651859337\n",
      "Loss for  37168 th iteration => 0.00816615447768\n",
      "Loss for  37169 th iteration => 0.00816601698573\n",
      "Loss for  37170 th iteration => 0.00816568757683\n",
      "Loss for  37171 th iteration => 0.00816551555508\n",
      "Loss for  37172 th iteration => 0.0081652206339\n",
      "Loss for  37173 th iteration => 0.0081650142997\n",
      "Loss for  37174 th iteration => 0.00816475364996\n",
      "Loss for  37175 th iteration => 0.00816451321793\n",
      "Loss for  37176 th iteration => 0.0081642866261\n",
      "Loss for  37177 th iteration => 0.00816401230809\n",
      "Loss for  37178 th iteration => 0.00816381956339\n",
      "Loss for  37179 th iteration => 0.00816351156857\n",
      "Loss for  37180 th iteration => 0.0081633525349\n",
      "Loss for  37181 th iteration => 0.008163129046\n",
      "Loss for  37182 th iteration => 0.00816277856504\n",
      "Loss for  37183 th iteration => 0.00816267244971\n",
      "Loss for  37184 th iteration => 0.00816229748266\n",
      "Loss for  37185 th iteration => 0.00816205718526\n",
      "Loss for  37186 th iteration => 0.0081619241195\n",
      "Loss for  37187 th iteration => 0.00816158801547\n",
      "Loss for  37188 th iteration => 0.00816142733913\n",
      "Loss for  37189 th iteration => 0.00816123713686\n",
      "Loss for  37190 th iteration => 0.00816093120798\n",
      "Loss for  37191 th iteration => 0.00816065157329\n",
      "Loss for  37192 th iteration => 0.00816043440318\n",
      "Loss for  37193 th iteration => 0.00816032670711\n",
      "Loss for  37194 th iteration => 0.00815995373335\n",
      "Loss for  37195 th iteration => 0.00815983776904\n",
      "Loss for  37196 th iteration => 0.00815947310432\n",
      "Loss for  37197 th iteration => 0.00815934896033\n",
      "Loss for  37198 th iteration => 0.00815899251589\n",
      "Loss for  37199 th iteration => 0.00815886027961\n",
      "Loss for  37200 th iteration => 0.00815852230942\n",
      "Loss for  37201 th iteration => 0.00815828070696\n",
      "Loss for  37202 th iteration => 0.0081580569282\n",
      "Loss for  37203 th iteration => 0.00815777943403\n",
      "Loss for  37204 th iteration => 0.00815759149724\n",
      "Loss for  37205 th iteration => 0.00815727834418\n",
      "Loss for  37206 th iteration => 0.0081571267856\n",
      "Loss for  37207 th iteration => 0.00815692848471\n",
      "Loss for  37208 th iteration => 0.00815656086375\n",
      "Loss for  37209 th iteration => 0.0081564279479\n",
      "Loss for  37210 th iteration => 0.00815609510567\n",
      "Loss for  37211 th iteration => 0.00815592758874\n",
      "Loss for  37212 th iteration => 0.00815562930483\n",
      "Loss for  37213 th iteration => 0.00815542740549\n",
      "Loss for  37214 th iteration => 0.00815516346234\n",
      "Loss for  37215 th iteration => 0.00815492799967\n",
      "Loss for  37216 th iteration => 0.00815480511743\n",
      "Loss for  37217 th iteration => 0.00815443054306\n",
      "Loss for  37218 th iteration => 0.00815436383674\n",
      "Loss for  37219 th iteration => 0.00815393255063\n",
      "Loss for  37220 th iteration => 0.00815376922398\n",
      "Loss for  37221 th iteration => 0.00815343398403\n",
      "Loss for  37222 th iteration => 0.00815319346135\n",
      "Loss for  37223 th iteration => 0.00815306373173\n",
      "Loss for  37224 th iteration => 0.00815271426826\n",
      "Loss for  37225 th iteration => 0.00815247457502\n",
      "Loss for  37226 th iteration => 0.00815234635964\n",
      "Loss for  37227 th iteration => 0.00815200649175\n",
      "Loss for  37228 th iteration => 0.00815184977912\n",
      "Loss for  37229 th iteration => 0.00815153839556\n",
      "Loss for  37230 th iteration => 0.00815135333785\n",
      "Loss for  37231 th iteration => 0.00815107045558\n",
      "Loss for  37232 th iteration => 0.00815100563633\n",
      "Loss for  37233 th iteration => 0.00815061980875\n",
      "Loss for  37234 th iteration => 0.0081504888428\n",
      "Loss for  37235 th iteration => 0.00815016888845\n",
      "Loss for  37236 th iteration => 0.00814997250749\n",
      "Loss for  37237 th iteration => 0.00814971769894\n",
      "Loss for  37238 th iteration => 0.00814945662397\n",
      "Loss for  37239 th iteration => 0.00814926624444\n",
      "Loss for  37240 th iteration => 0.00814894825145\n",
      "Loss for  37241 th iteration => 0.00814872191233\n",
      "Loss for  37242 th iteration => 0.00814844802539\n",
      "Loss for  37243 th iteration => 0.00814825758743\n",
      "Loss for  37244 th iteration => 0.00814794798296\n",
      "Loss for  37245 th iteration => 0.00814779393311\n",
      "Loss for  37246 th iteration => 0.00814759911762\n",
      "Loss for  37247 th iteration => 0.00814722901642\n",
      "Loss for  37248 th iteration => 0.00814709963286\n",
      "Loss for  37249 th iteration => 0.00814676435886\n",
      "Loss for  37250 th iteration => 0.0081466003262\n",
      "Loss for  37251 th iteration => 0.00814630010127\n",
      "Loss for  37252 th iteration => 0.00814620535372\n",
      "Loss for  37253 th iteration => 0.00814583728065\n",
      "Loss for  37254 th iteration => 0.00814573059543\n",
      "Loss for  37255 th iteration => 0.00814537364717\n",
      "Loss for  37256 th iteration => 0.00814510819103\n",
      "Loss for  37257 th iteration => 0.00814490871571\n",
      "Loss for  37258 th iteration => 0.00814460970527\n",
      "Loss for  37259 th iteration => 0.00814444374675\n",
      "Loss for  37260 th iteration => 0.00814411138702\n",
      "Loss for  37261 th iteration => 0.00814397874132\n",
      "Loss for  37262 th iteration => 0.00814363111597\n",
      "Loss for  37263 th iteration => 0.0081433914186\n",
      "Loss for  37264 th iteration => 0.00814326535621\n",
      "Loss for  37265 th iteration => 0.00814292440534\n",
      "Loss for  37266 th iteration => 0.00814276980221\n",
      "Loss for  37267 th iteration => 0.00814245737861\n",
      "Loss for  37268 th iteration => 0.00814227438789\n",
      "Loss for  37269 th iteration => 0.00814199033917\n",
      "Loss for  37270 th iteration => 0.00814177982531\n",
      "Loss for  37271 th iteration => 0.00814167374982\n",
      "Loss for  37272 th iteration => 0.0081413013052\n",
      "Loss for  37273 th iteration => 0.00814118693652\n",
      "Loss for  37274 th iteration => 0.00814082282584\n",
      "Loss for  37275 th iteration => 0.00814070025286\n",
      "Loss for  37276 th iteration => 0.00814034438701\n",
      "Loss for  37277 th iteration => 0.00814021369748\n",
      "Loss for  37278 th iteration => 0.00813987134882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  37279 th iteration => 0.0081396363021\n",
      "Loss for  37280 th iteration => 0.00813940817201\n",
      "Loss for  37281 th iteration => 0.00813913712557\n",
      "Loss for  37282 th iteration => 0.00813894494431\n",
      "Loss for  37283 th iteration => 0.00813863825958\n",
      "Loss for  37284 th iteration => 0.00813862856907\n",
      "Loss for  37285 th iteration => 0.00813815730766\n",
      "Loss for  37286 th iteration => 0.00813803068317\n",
      "Loss for  37287 th iteration => 0.00813779381561\n",
      "Loss for  37288 th iteration => 0.00813753421086\n",
      "Loss for  37289 th iteration => 0.00813735320137\n",
      "Loss for  37290 th iteration => 0.00813703845093\n",
      "Loss for  37291 th iteration => 0.00813676989557\n",
      "Loss for  37292 th iteration => 0.00813654064535\n",
      "Loss for  37293 th iteration => 0.00813630607211\n",
      "Loss for  37294 th iteration => 0.00813604301096\n",
      "Loss for  37295 th iteration => 0.00813584220954\n",
      "Loss for  37296 th iteration => 0.00813554554614\n",
      "Loss for  37297 th iteration => 0.00813537830891\n",
      "Loss for  37298 th iteration => 0.00813504824929\n",
      "Loss for  37299 th iteration => 0.00813491437125\n",
      "Loss for  37300 th iteration => 0.00813456782151\n",
      "Loss for  37301 th iteration => 0.00813432827745\n",
      "Loss for  37302 th iteration => 0.00813408962816\n",
      "Loss for  37303 th iteration => 0.00813396536475\n",
      "Loss for  37304 th iteration => 0.00813362370103\n",
      "Loss for  37305 th iteration => 0.00813347087163\n",
      "Loss for  37306 th iteration => 0.00813315776002\n",
      "Loss for  37307 th iteration => 0.00813297651845\n",
      "Loss for  37308 th iteration => 0.00813269234684\n",
      "Loss for  37309 th iteration => 0.00813263408792\n",
      "Loss for  37310 th iteration => 0.00813224387981\n",
      "Loss for  37311 th iteration => 0.00813211932631\n",
      "Loss for  37312 th iteration => 0.00813179513839\n",
      "Loss for  37313 th iteration => 0.00813160502401\n",
      "Loss for  37314 th iteration => 0.00813134612683\n",
      "Loss for  37315 th iteration => 0.00813109117458\n",
      "Loss for  37316 th iteration => 0.00813089684937\n",
      "Loss for  37317 th iteration => 0.00813058221401\n",
      "Loss for  37318 th iteration => 0.0081303544776\n",
      "Loss for  37319 th iteration => 0.00813008407084\n",
      "Loss for  37320 th iteration => 0.00812989230344\n",
      "Loss for  37321 th iteration => 0.00812958700815\n",
      "Loss for  37322 th iteration => 0.0081294445485\n",
      "Loss for  37323 th iteration => 0.00812921877297\n",
      "Loss for  37324 th iteration => 0.00812898017225\n",
      "Loss for  37325 th iteration => 0.00812872345122\n",
      "Loss for  37326 th iteration => 0.00812842043116\n",
      "Loss for  37327 th iteration => 0.00812834747717\n",
      "Loss for  37328 th iteration => 0.00812795926464\n",
      "Loss for  37329 th iteration => 0.00812773212576\n",
      "Loss for  37330 th iteration => 0.00812749647352\n",
      "Loss for  37331 th iteration => 0.0081272354879\n",
      "Loss for  37332 th iteration => 0.00812703364279\n",
      "Loss for  37333 th iteration => 0.00812673902001\n",
      "Loss for  37334 th iteration => 0.00812657077351\n",
      "Loss for  37335 th iteration => 0.00812624272049\n",
      "Loss for  37336 th iteration => 0.00812610786671\n",
      "Loss for  37337 th iteration => 0.00812576336142\n",
      "Loss for  37338 th iteration => 0.00812552432692\n",
      "Loss for  37339 th iteration => 0.0081252857685\n",
      "Loss for  37340 th iteration => 0.00812516137696\n",
      "Loss for  37341 th iteration => 0.00812482086636\n",
      "Loss for  37342 th iteration => 0.00812466788798\n",
      "Loss for  37343 th iteration => 0.00812435594993\n",
      "Loss for  37344 th iteration => 0.00812417453921\n",
      "Loss for  37345 th iteration => 0.00812389101995\n",
      "Loss for  37346 th iteration => 0.00812368232399\n",
      "Loss for  37347 th iteration => 0.00812357722412\n",
      "Loss for  37348 th iteration => 0.00812320588323\n",
      "Loss for  37349 th iteration => 0.00812309248166\n",
      "Loss for  37350 th iteration => 0.00812272948311\n",
      "Loss for  37351 th iteration => 0.00812260786887\n",
      "Loss for  37352 th iteration => 0.00812225312341\n",
      "Loss for  37353 th iteration => 0.00812212338438\n",
      "Loss for  37354 th iteration => 0.00812178159829\n",
      "Loss for  37355 th iteration => 0.00812154781368\n",
      "Loss for  37356 th iteration => 0.00812132052572\n",
      "Loss for  37357 th iteration => 0.00812105070071\n",
      "Loss for  37358 th iteration => 0.00812085940147\n",
      "Loss for  37359 th iteration => 0.00812055533422\n",
      "Loss for  37360 th iteration => 0.00812041314355\n",
      "Loss for  37361 th iteration => 0.00812018747346\n",
      "Loss for  37362 th iteration => 0.00811994961623\n",
      "Loss for  37363 th iteration => 0.00811969291251\n",
      "Loss for  37364 th iteration => 0.008119510906\n",
      "Loss for  37365 th iteration => 0.00811919935667\n",
      "Loss for  37366 th iteration => 0.00811893076931\n",
      "Loss for  37367 th iteration => 0.00811870358161\n",
      "Loss for  37368 th iteration => 0.00811846902349\n",
      "Loss for  37369 th iteration => 0.00811820797831\n",
      "Loss for  37370 th iteration => 0.00811800723772\n",
      "Loss for  37371 th iteration => 0.00811771254513\n",
      "Loss for  37372 th iteration => 0.00811754541306\n",
      "Loss for  37373 th iteration => 0.00811721728047\n",
      "Loss for  37374 th iteration => 0.00811708355055\n",
      "Loss for  37375 th iteration => 0.00811673906711\n",
      "Loss for  37376 th iteration => 0.00811650055465\n",
      "Loss for  37377 th iteration => 0.00811626293792\n",
      "Loss for  37378 th iteration => 0.00811613849658\n",
      "Loss for  37379 th iteration => 0.00811579907746\n",
      "Loss for  37380 th iteration => 0.00811564604561\n",
      "Loss for  37381 th iteration => 0.00811533520241\n",
      "Loss for  37382 th iteration => 0.00811515373493\n",
      "Loss for  37383 th iteration => 0.00811487208867\n",
      "Loss for  37384 th iteration => 0.00811481151895\n",
      "Loss for  37385 th iteration => 0.00811442568025\n",
      "Loss for  37386 th iteration => 0.00811429878718\n",
      "Loss for  37387 th iteration => 0.00811397899719\n",
      "Loss for  37388 th iteration => 0.00811378651482\n",
      "Loss for  37389 th iteration => 0.00811353204375\n",
      "Loss for  37390 th iteration => 0.00811327469541\n",
      "Loss for  37391 th iteration => 0.00811308482413\n",
      "Loss for  37392 th iteration => 0.00811276902558\n",
      "Loss for  37393 th iteration => 0.00811254444051\n",
      "Loss for  37394 th iteration => 0.00811227292546\n",
      "Loss for  37395 th iteration => 0.00811208432427\n",
      "Loss for  37396 th iteration => 0.00811177748863\n",
      "Loss for  37397 th iteration => 0.00811176964506\n",
      "Loss for  37398 th iteration => 0.00811130117622\n",
      "Loss for  37399 th iteration => 0.00811117387149\n",
      "Loss for  37400 th iteration => 0.00811084069119\n",
      "Loss for  37401 th iteration => 0.00811067866915\n",
      "Loss for  37402 th iteration => 0.00811038042639\n",
      "Loss for  37403 th iteration => 0.00811028757178\n",
      "Loss for  37404 th iteration => 0.00810992158383\n",
      "Loss for  37405 th iteration => 0.00810981698067\n",
      "Loss for  37406 th iteration => 0.00810946243526\n",
      "Loss for  37407 th iteration => 0.00810919878192\n",
      "Loss for  37408 th iteration => 0.00810900167208\n",
      "Loss for  37409 th iteration => 0.00810870440289\n",
      "Loss for  37410 th iteration => 0.00810854086991\n",
      "Loss for  37411 th iteration => 0.00810821019215\n",
      "Loss for  37412 th iteration => 0.0081080800298\n",
      "Loss for  37413 th iteration => 0.00810773509847\n",
      "Loss for  37414 th iteration => 0.00810760813449\n",
      "Loss for  37415 th iteration => 0.0081072722144\n",
      "Loss for  37416 th iteration => 0.00810711667114\n",
      "Loss for  37417 th iteration => 0.0081068093155\n",
      "Loss for  37418 th iteration => 0.00810662534809\n",
      "Loss for  37419 th iteration => 0.00810634668269\n",
      "Loss for  37420 th iteration => 0.00810628017911\n",
      "Loss for  37421 th iteration => 0.00810590122934\n",
      "Loss for  37422 th iteration => 0.00810576845296\n",
      "Loss for  37423 th iteration => 0.00810545550159\n",
      "Loss for  37424 th iteration => 0.00810525718574\n",
      "Loss for  37425 th iteration => 0.00810500950369\n",
      "Loss for  37426 th iteration => 0.00810474637099\n",
      "Loss for  37427 th iteration => 0.00810456323982\n",
      "Loss for  37428 th iteration => 0.00810424487534\n",
      "Loss for  37429 th iteration => 0.00810402410003\n",
      "Loss for  37430 th iteration => 0.00810374976607\n",
      "Loss for  37431 th iteration => 0.0081035649542\n",
      "Loss for  37432 th iteration => 0.00810325486654\n",
      "Loss for  37433 th iteration => 0.00810324750992\n",
      "Loss for  37434 th iteration => 0.00810278335814\n",
      "Loss for  37435 th iteration => 0.00810265328447\n",
      "Loss for  37436 th iteration => 0.00810232384201\n",
      "Loss for  37437 th iteration => 0.00810215907374\n",
      "Loss for  37438 th iteration => 0.00810186428098\n",
      "Loss for  37439 th iteration => 0.00810166504019\n",
      "Loss for  37440 th iteration => 0.00810140467613\n",
      "Loss for  37441 th iteration => 0.00810117118208\n",
      "Loss for  37442 th iteration => 0.00810094502855\n",
      "Loss for  37443 th iteration => 0.00810067755809\n",
      "Loss for  37444 th iteration => 0.0081005952379\n",
      "Loss for  37445 th iteration => 0.00810018667168\n",
      "Loss for  37446 th iteration => 0.00810016045686\n",
      "Loss for  37447 th iteration => 0.00809969734455\n",
      "Loss for  37448 th iteration => 0.00809946007868\n",
      "Loss for  37449 th iteration => 0.00809932901435\n",
      "Loss for  37450 th iteration => 0.00809899818291\n",
      "Loss for  37451 th iteration => 0.00809883917156\n",
      "Loss for  37452 th iteration => 0.00809865170831\n",
      "Loss for  37453 th iteration => 0.00809835036431\n",
      "Loss for  37454 th iteration => 0.00809807622842\n",
      "Loss for  37455 th iteration => 0.00809786084107\n",
      "Loss for  37456 th iteration => 0.00809775469029\n",
      "Loss for  37457 th iteration => 0.0080973873554\n",
      "Loss for  37458 th iteration => 0.00809727292371\n",
      "Loss for  37459 th iteration => 0.00809691391015\n",
      "Loss for  37460 th iteration => 0.00809679128634\n",
      "Loss for  37461 th iteration => 0.0080964405051\n",
      "Loss for  37462 th iteration => 0.0080963097768\n",
      "Loss for  37463 th iteration => 0.00809598002955\n",
      "Loss for  37464 th iteration => 0.00809573757232\n",
      "Loss for  37465 th iteration => 0.00809552189524\n",
      "Loss for  37466 th iteration => 0.00809524346079\n",
      "Loss for  37467 th iteration => 0.00809506370876\n",
      "Loss for  37468 th iteration => 0.0080947500312\n",
      "Loss for  37469 th iteration => 0.0080947424229\n",
      "Loss for  37470 th iteration => 0.00809428399421\n",
      "Loss for  37471 th iteration => 0.00809414917349\n",
      "Loss for  37472 th iteration => 0.00809382543626\n",
      "Loss for  37473 th iteration => 0.00809365596086\n",
      "Loss for  37474 th iteration => 0.00809336683334\n",
      "Loss for  37475 th iteration => 0.00809316292518\n",
      "Loss for  37476 th iteration => 0.00809290818654\n",
      "Loss for  37477 th iteration => 0.00809267006471\n",
      "Loss for  37478 th iteration => 0.00809244949694\n",
      "Loss for  37479 th iteration => 0.00809217737775\n",
      "Loss for  37480 th iteration => 0.00809199076561\n",
      "Loss for  37481 th iteration => 0.00809168486262\n",
      "Loss for  37482 th iteration => 0.00809153199363\n",
      "Loss for  37483 th iteration => 0.00809119987254\n",
      "Loss for  37484 th iteration => 0.00809096283375\n",
      "Loss for  37485 th iteration => 0.00809072627825\n",
      "Loss for  37486 th iteration => 0.0080906049169\n",
      "Loss for  37487 th iteration => 0.00809025846759\n",
      "Loss for  37488 th iteration => 0.00809026250422\n",
      "Loss for  37489 th iteration => 0.00808979883865\n",
      "Loss for  37490 th iteration => 0.00808963401645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  37491 th iteration => 0.00808933954651\n",
      "Loss for  37492 th iteration => 0.00808928018073\n",
      "Loss for  37493 th iteration => 0.00808889617502\n",
      "Loss for  37494 th iteration => 0.00808877069931\n",
      "Loss for  37495 th iteration => 0.00808857896101\n",
      "Loss for  37496 th iteration => 0.00808826333959\n",
      "Loss for  37497 th iteration => 0.00808801020031\n",
      "Loss for  37498 th iteration => 0.00808775443936\n",
      "Loss for  37499 th iteration => 0.00808756590713\n",
      "Loss for  37500 th iteration => 0.00808724598557\n",
      "Loss for  37501 th iteration => 0.00808712135174\n",
      "Loss for  37502 th iteration => 0.00808675351136\n",
      "Loss for  37503 th iteration => 0.00808658142304\n",
      "Loss for  37504 th iteration => 0.00808639158604\n",
      "Loss for  37505 th iteration => 0.00808614043696\n",
      "Loss for  37506 th iteration => 0.00808589772822\n",
      "Loss for  37507 th iteration => 0.00808558057326\n",
      "Loss for  37508 th iteration => 0.00808540549348\n",
      "Loss for  37509 th iteration => 0.00808512289014\n",
      "Loss for  37510 th iteration => 0.00808491343533\n",
      "Loss for  37511 th iteration => 0.00808466516317\n",
      "Loss for  37512 th iteration => 0.00808442155202\n",
      "Loss for  37513 th iteration => 0.00808420739342\n",
      "Loss for  37514 th iteration => 0.00808392984187\n",
      "Loss for  37515 th iteration => 0.00808374958197\n",
      "Loss for  37516 th iteration => 0.00808343830319\n",
      "Loss for  37517 th iteration => 0.00808329172988\n",
      "Loss for  37518 th iteration => 0.00808296042364\n",
      "Loss for  37519 th iteration => 0.00808272385841\n",
      "Loss for  37520 th iteration => 0.00808248730716\n",
      "Loss for  37521 th iteration => 0.00808225076989\n",
      "Loss for  37522 th iteration => 0.00808201424661\n",
      "Loss for  37523 th iteration => 0.0080817777373\n",
      "Loss for  37524 th iteration => 0.00808154124197\n",
      "Loss for  37525 th iteration => 0.00808130476061\n",
      "Loss for  37526 th iteration => 0.00808106850904\n",
      "Loss for  37527 th iteration => 0.00808095106917\n",
      "Loss for  37528 th iteration => 0.00808060875541\n",
      "Loss for  37529 th iteration => 0.00808046258933\n",
      "Loss for  37530 th iteration => 0.00808014898537\n",
      "Loss for  37531 th iteration => 0.00807997444554\n",
      "Loss for  37532 th iteration => 0.00807984479273\n",
      "Loss for  37533 th iteration => 0.0080795009886\n",
      "Loss for  37534 th iteration => 0.00807936384691\n",
      "Loss for  37535 th iteration => 0.00807902956772\n",
      "Loss for  37536 th iteration => 0.00807888420679\n",
      "Loss for  37537 th iteration => 0.00807855818727\n",
      "Loss for  37538 th iteration => 0.0080784047625\n",
      "Loss for  37539 th iteration => 0.0080782252583\n",
      "Loss for  37540 th iteration => 0.00807793117293\n",
      "Loss for  37541 th iteration => 0.00807774292231\n",
      "Loss for  37542 th iteration => 0.00807737652766\n",
      "Loss for  37543 th iteration => 0.00807736069177\n",
      "Loss for  37544 th iteration => 0.00807690179521\n",
      "Loss for  37545 th iteration => 0.00807679543278\n",
      "Loss for  37546 th iteration => 0.00807642546076\n",
      "Loss for  37547 th iteration => 0.00807632092709\n",
      "Loss for  37548 th iteration => 0.00807596900961\n",
      "Loss for  37549 th iteration => 0.00807571853185\n",
      "Loss for  37550 th iteration => 0.00807551220187\n",
      "Loss for  37551 th iteration => 0.00807522786136\n",
      "Loss for  37552 th iteration => 0.0080750553526\n",
      "Loss for  37553 th iteration => 0.00807473736175\n",
      "Loss for  37554 th iteration => 0.00807459846284\n",
      "Loss for  37555 th iteration => 0.00807426526341\n",
      "Loss for  37556 th iteration => 0.00807402919819\n",
      "Loss for  37557 th iteration => 0.00807379314692\n",
      "Loss for  37558 th iteration => 0.00807355710958\n",
      "Loss for  37559 th iteration => 0.00807332108618\n",
      "Loss for  37560 th iteration => 0.00807308507671\n",
      "Loss for  37561 th iteration => 0.00807284908117\n",
      "Loss for  37562 th iteration => 0.00807261415249\n",
      "Loss for  37563 th iteration => 0.00807249174748\n",
      "Loss for  37564 th iteration => 0.00807215534473\n",
      "Loss for  37565 th iteration => 0.00807200426498\n",
      "Loss for  37566 th iteration => 0.00807169652056\n",
      "Loss for  37567 th iteration => 0.00807151692381\n",
      "Loss for  37568 th iteration => 0.00807123875968\n",
      "Loss for  37569 th iteration => 0.00807117909269\n",
      "Loss for  37570 th iteration => 0.00807079740454\n",
      "Loss for  37571 th iteration => 0.00807067127261\n",
      "Loss for  37572 th iteration => 0.00807035577381\n",
      "Loss for  37573 th iteration => 0.00807016391275\n",
      "Loss for  37574 th iteration => 0.00806991387174\n",
      "Loss for  37575 th iteration => 0.00806965700663\n",
      "Loss for  37576 th iteration => 0.00806947170249\n",
      "Loss for  37577 th iteration => 0.00806915679565\n",
      "Loss for  37578 th iteration => 0.00806893593031\n",
      "Loss for  37579 th iteration => 0.00806866566219\n",
      "Loss for  37580 th iteration => 0.0080684814529\n",
      "Loss for  37581 th iteration => 0.00806831806455\n",
      "Loss for  37582 th iteration => 0.00806804271913\n",
      "Loss for  37583 th iteration => 0.00806781292349\n",
      "Loss for  37584 th iteration => 0.00806748486441\n",
      "Loss for  37585 th iteration => 0.00806732266917\n",
      "Loss for  37586 th iteration => 0.00806702932975\n",
      "Loss for  37587 th iteration => 0.00806683259291\n",
      "Loss for  37588 th iteration => 0.00806657374987\n",
      "Loss for  37589 th iteration => 0.00806634269294\n",
      "Loss for  37590 th iteration => 0.00806611838793\n",
      "Loss for  37591 th iteration => 0.00806596164172\n",
      "Loss for  37592 th iteration => 0.00806566466824\n",
      "Loss for  37593 th iteration => 0.00806549745631\n",
      "Loss for  37594 th iteration => 0.00806521017274\n",
      "Loss for  37595 th iteration => 0.00806488020718\n",
      "Loss for  37596 th iteration => 0.00806475020109\n",
      "Loss for  37597 th iteration => 0.00806442230102\n",
      "Loss for  37598 th iteration => 0.00806426424504\n",
      "Loss for  37599 th iteration => 0.00806407929397\n",
      "Loss for  37600 th iteration => 0.00806377939626\n",
      "Loss for  37601 th iteration => 0.00806350851621\n",
      "Loss for  37602 th iteration => 0.00806343154057\n",
      "Loss for  37603 th iteration => 0.00806306795092\n",
      "Loss for  37604 th iteration => 0.00806292471908\n",
      "Loss for  37605 th iteration => 0.00806262711116\n",
      "Loss for  37606 th iteration => 0.008062418356\n",
      "Loss for  37607 th iteration => 0.00806218600113\n",
      "Loss for  37608 th iteration => 0.00806191244487\n",
      "Loss for  37609 th iteration => 0.00806174462502\n",
      "Loss for  37610 th iteration => 0.00806142144735\n",
      "Loss for  37611 th iteration => 0.00806121060484\n",
      "Loss for  37612 th iteration => 0.00806093124761\n",
      "Loss for  37613 th iteration => 0.00806075636552\n",
      "Loss for  37614 th iteration => 0.00806044192726\n",
      "Loss for  37615 th iteration => 0.00806043576689\n",
      "Loss for  37616 th iteration => 0.00805998279962\n",
      "Loss for  37617 th iteration => 0.00805984646876\n",
      "Loss for  37618 th iteration => 0.00805952817756\n",
      "Loss for  37619 th iteration => 0.00805935717801\n",
      "Loss for  37620 th iteration => 0.00805907350943\n",
      "Loss for  37621 th iteration => 0.00805886806462\n",
      "Loss for  37622 th iteration => 0.00805861879631\n",
      "Loss for  37623 th iteration => 0.00805837912685\n",
      "Loss for  37624 th iteration => 0.0080581640393\n",
      "Loss for  37625 th iteration => 0.00805789036298\n",
      "Loss for  37626 th iteration => 0.00805770923947\n",
      "Loss for  37627 th iteration => 0.00805740177131\n",
      "Loss for  37628 th iteration => 0.00805725439789\n",
      "Loss for  37629 th iteration => 0.00805692397139\n",
      "Loss for  37630 th iteration => 0.00805668890317\n",
      "Loss for  37631 th iteration => 0.00805645384881\n",
      "Loss for  37632 th iteration => 0.00805621880831\n",
      "Loss for  37633 th iteration => 0.00805598378168\n",
      "Loss for  37634 th iteration => 0.0080557487689\n",
      "Loss for  37635 th iteration => 0.00805551376999\n",
      "Loss for  37636 th iteration => 0.00805527978802\n",
      "Loss for  37637 th iteration => 0.00805516307128\n",
      "Loss for  37638 th iteration => 0.00805482300654\n",
      "Loss for  37639 th iteration => 0.00805467752528\n",
      "Loss for  37640 th iteration => 0.00805451865499\n",
      "Loss for  37641 th iteration => 0.00805420488366\n",
      "Loss for  37642 th iteration => 0.0080540426117\n",
      "Loss for  37643 th iteration => 0.00805373632074\n",
      "Loss for  37644 th iteration => 0.00805356584765\n",
      "Loss for  37645 th iteration => 0.00805326779834\n",
      "Loss for  37646 th iteration => 0.00805308921323\n",
      "Loss for  37647 th iteration => 0.00805279936288\n",
      "Loss for  37648 th iteration => 0.00805274979569\n",
      "Loss for  37649 th iteration => 0.0080523362859\n",
      "Loss for  37650 th iteration => 0.00805226282732\n",
      "Loss for  37651 th iteration => 0.00805188459335\n",
      "Loss for  37652 th iteration => 0.00805167590843\n",
      "Loss for  37653 th iteration => 0.00805154137381\n",
      "Loss for  37654 th iteration => 0.00805117003114\n",
      "Loss for  37655 th iteration => 0.00805089849408\n",
      "Loss for  37656 th iteration => 0.0080507961302\n",
      "Loss for  37657 th iteration => 0.00805044640254\n",
      "Loss for  37658 th iteration => 0.00805019637461\n",
      "Loss for  37659 th iteration => 0.00804999252588\n",
      "Loss for  37660 th iteration => 0.0080497086099\n",
      "Loss for  37661 th iteration => 0.00804953860678\n",
      "Loss for  37662 th iteration => 0.0080492210165\n",
      "Loss for  37663 th iteration => 0.00804908464628\n",
      "Loss for  37664 th iteration => 0.00804875289729\n",
      "Loss for  37665 th iteration => 0.00804851829681\n",
      "Loss for  37666 th iteration => 0.00804828371015\n",
      "Loss for  37667 th iteration => 0.00804804913731\n",
      "Loss for  37668 th iteration => 0.00804781457828\n",
      "Loss for  37669 th iteration => 0.00804758003308\n",
      "Loss for  37670 th iteration => 0.0080473466309\n",
      "Loss for  37671 th iteration => 0.0080472225937\n",
      "Loss for  37672 th iteration => 0.00804689071944\n",
      "Loss for  37673 th iteration => 0.00804673799476\n",
      "Loss for  37674 th iteration => 0.0080464347907\n",
      "Loss for  37675 th iteration => 0.00804625353749\n",
      "Loss for  37676 th iteration => 0.00804598045626\n",
      "Loss for  37677 th iteration => 0.00804591420895\n",
      "Loss for  37678 th iteration => 0.00804554197163\n",
      "Loss for  37679 th iteration => 0.00804540927443\n",
      "Loss for  37680 th iteration => 0.00804510321143\n",
      "Loss for  37681 th iteration => 0.00804490479977\n",
      "Loss for  37682 th iteration => 0.00804466417988\n",
      "Loss for  37683 th iteration => 0.00804440077849\n",
      "Loss for  37684 th iteration => 0.00804422488113\n",
      "Loss for  37685 th iteration => 0.00804390644224\n",
      "Loss for  37686 th iteration => 0.008043691887\n",
      "Loss for  37687 th iteration => 0.00804341819635\n",
      "Loss for  37688 th iteration => 0.00804324072055\n",
      "Loss for  37689 th iteration => 0.00804306939138\n",
      "Loss for  37690 th iteration => 0.00804280483873\n",
      "Loss for  37691 th iteration => 0.00804257145712\n",
      "Loss for  37692 th iteration => 0.00804224932632\n",
      "Loss for  37693 th iteration => 0.00804208409513\n",
      "Loss for  37694 th iteration => 0.00804179666762\n",
      "Loss for  37695 th iteration => 0.00804159691128\n",
      "Loss for  37696 th iteration => 0.00804134396305\n",
      "Loss for  37697 th iteration => 0.00804110990382\n",
      "Loss for  37698 th iteration => 0.00804089121369\n",
      "Loss for  37699 th iteration => 0.00804062307101\n",
      "Loss for  37700 th iteration => 0.00804043842062\n",
      "Loss for  37701 th iteration => 0.00804013641112\n",
      "Loss for  37702 th iteration => 0.00803998558491\n",
      "Loss for  37703 th iteration => 0.00803965569707\n",
      "Loss for  37704 th iteration => 0.00803942161951\n",
      "Loss for  37705 th iteration => 0.00803918802163\n",
      "Loss for  37706 th iteration => 0.00803907103893\n",
      "Loss for  37707 th iteration => 0.008038728816\n",
      "Loss for  37708 th iteration => 0.00803873367346\n",
      "Loss for  37709 th iteration => 0.0080382777189\n",
      "Loss for  37710 th iteration => 0.00803824106749\n",
      "Loss for  37711 th iteration => 0.00803784033385\n",
      "Loss for  37712 th iteration => 0.00803773672856\n",
      "Loss for  37713 th iteration => 0.00803740267018\n",
      "Loss for  37714 th iteration => 0.00803723319732\n",
      "Loss for  37715 th iteration => 0.00803709302034\n",
      "Loss for  37716 th iteration => 0.00803673197951\n",
      "Loss for  37717 th iteration => 0.00803652808323\n",
      "Loss for  37718 th iteration => 0.00803622914203\n",
      "Loss for  37719 th iteration => 0.00803608950578\n",
      "Loss for  37720 th iteration => 0.00803572731303\n",
      "Loss for  37721 th iteration => 0.00803568901393\n",
      "Loss for  37722 th iteration => 0.00803534514166\n",
      "Loss for  37723 th iteration => 0.00803511607221\n",
      "Loss for  37724 th iteration => 0.00803491322259\n",
      "Loss for  37725 th iteration => 0.00803463164688\n",
      "Loss for  37726 th iteration => 0.00803435602383\n",
      "Loss for  37727 th iteration => 0.00803414537701\n",
      "Loss for  37728 th iteration => 0.00803390411396\n",
      "Loss for  37729 th iteration => 0.0080336592827\n",
      "Loss for  37730 th iteration => 0.00803345215964\n",
      "Loss for  37731 th iteration => 0.00803317336222\n",
      "Loss for  37732 th iteration => 0.00803300016195\n",
      "Loss for  37733 th iteration => 0.00803268761387\n",
      "Loss for  37734 th iteration => 0.00803254812193\n",
      "Loss for  37735 th iteration => 0.00803221987409\n",
      "Loss for  37736 th iteration => 0.00803198622018\n",
      "Loss for  37737 th iteration => 0.00803175258002\n",
      "Loss for  37738 th iteration => 0.00803151895361\n",
      "Loss for  37739 th iteration => 0.00803128534095\n",
      "Loss for  37740 th iteration => 0.00803105174203\n",
      "Loss for  37741 th iteration => 0.00803081855696\n",
      "Loss for  37742 th iteration => 0.00803069577179\n",
      "Loss for  37743 th iteration => 0.00803036455183\n",
      "Loss for  37744 th iteration => 0.00803021303361\n",
      "Loss for  37745 th iteration => 0.00802991052866\n",
      "Loss for  37746 th iteration => 0.00802973043765\n",
      "Loss for  37747 th iteration => 0.00802945710596\n",
      "Loss for  37748 th iteration => 0.00802939245726\n",
      "Loss for  37749 th iteration => 0.0080290205352\n",
      "Loss for  37750 th iteration => 0.00802888935443\n",
      "Loss for  37751 th iteration => 0.00802858368833\n",
      "Loss for  37752 th iteration => 0.00802838671205\n",
      "Loss for  37753 th iteration => 0.00802814656958\n",
      "Loss for  37754 th iteration => 0.00802788452365\n",
      "Loss for  37755 th iteration => 0.00802770918309\n",
      "Loss for  37756 th iteration => 0.00802739258259\n",
      "Loss for  37757 th iteration => 0.00802717868285\n",
      "Loss for  37758 th iteration => 0.00802690619579\n",
      "Loss for  37759 th iteration => 0.00802672838407\n",
      "Loss for  37760 th iteration => 0.00802642119534\n",
      "Loss for  37761 th iteration => 0.00802641717391\n",
      "Loss for  37762 th iteration => 0.00802596037822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  37763 th iteration => 0.00802583082561\n",
      "Loss for  37764 th iteration => 0.00802550968214\n",
      "Loss for  37765 th iteration => 0.0080253453624\n",
      "Loss for  37766 th iteration => 0.00802505893852\n",
      "Loss for  37767 th iteration => 0.0080248600776\n",
      "Loss for  37768 th iteration => 0.00802460814848\n",
      "Loss for  37769 th iteration => 0.00802437496944\n",
      "Loss for  37770 th iteration => 0.0080241573131\n",
      "Loss for  37771 th iteration => 0.00802389003616\n",
      "Loss for  37772 th iteration => 0.00802370643346\n",
      "Loss for  37773 th iteration => 0.00802340527605\n",
      "Loss for  37774 th iteration => 0.00802325551062\n",
      "Loss for  37775 th iteration => 0.0080229261989\n",
      "Loss for  37776 th iteration => 0.00802269307845\n",
      "Loss for  37777 th iteration => 0.00802245997173\n",
      "Loss for  37778 th iteration => 0.00802222698752\n",
      "Loss for  37779 th iteration => 0.0080221136812\n",
      "Loss for  37780 th iteration => 0.00802177304672\n",
      "Loss for  37781 th iteration => 0.00802177755046\n",
      "Loss for  37782 th iteration => 0.00802132679721\n",
      "Loss for  37783 th iteration => 0.00802128394979\n",
      "Loss for  37784 th iteration => 0.00802089131501\n",
      "Loss for  37785 th iteration => 0.00802078156024\n",
      "Loss for  37786 th iteration => 0.00802045555449\n",
      "Loss for  37787 th iteration => 0.0080202804102\n",
      "Loss for  37788 th iteration => 0.00802014988581\n",
      "Loss for  37789 th iteration => 0.00801978069023\n",
      "Loss for  37790 th iteration => 0.0080195847638\n",
      "Loss for  37791 th iteration => 0.0080192800486\n",
      "Loss for  37792 th iteration => 0.008019272269\n",
      "Loss for  37793 th iteration => 0.00801879600992\n",
      "Loss for  37794 th iteration => 0.00801872662527\n",
      "Loss for  37795 th iteration => 0.00801832732789\n",
      "Loss for  37796 th iteration => 0.00801815030136\n",
      "Loss for  37797 th iteration => 0.00801787760934\n",
      "Loss for  37798 th iteration => 0.00801776169639\n",
      "Loss for  37799 th iteration => 0.00801754286712\n",
      "Loss for  37800 th iteration => 0.00801718580623\n",
      "Loss for  37801 th iteration => 0.00801698086032\n",
      "Loss for  37802 th iteration => 0.00801670175885\n",
      "Loss for  37803 th iteration => 0.00801653074013\n",
      "Loss for  37804 th iteration => 0.00801621788378\n",
      "Loss for  37805 th iteration => 0.00801608057712\n",
      "Loss for  37806 th iteration => 0.00801575342459\n",
      "Loss for  37807 th iteration => 0.0080155207116\n",
      "Loss for  37808 th iteration => 0.00801528801228\n",
      "Loss for  37809 th iteration => 0.00801505532664\n",
      "Loss for  37810 th iteration => 0.00801482265468\n",
      "Loss for  37811 th iteration => 0.0080145901979\n",
      "Loss for  37812 th iteration => 0.00801446550751\n",
      "Loss for  37813 th iteration => 0.00801413804329\n",
      "Loss for  37814 th iteration => 0.008013984614\n",
      "Loss for  37815 th iteration => 0.0080136858701\n",
      "Loss for  37816 th iteration => 0.00801350386293\n",
      "Loss for  37817 th iteration => 0.00801323382992\n",
      "Loss for  37818 th iteration => 0.00801316404997\n",
      "Loss for  37819 th iteration => 0.0080127990908\n",
      "Loss for  37820 th iteration => 0.00801266279738\n",
      "Loss for  37821 th iteration => 0.00801236407564\n",
      "Loss for  37822 th iteration => 0.00801216200495\n",
      "Loss for  37823 th iteration => 0.00801192878864\n",
      "Loss for  37824 th iteration => 0.00801166166621\n",
      "Loss for  37825 th iteration => 0.00801149323392\n",
      "Loss for  37826 th iteration => 0.00801117458199\n",
      "Loss for  37827 th iteration => 0.00801096476472\n",
      "Loss for  37828 th iteration => 0.00801069004246\n",
      "Loss for  37829 th iteration => 0.00801051630783\n",
      "Loss for  37830 th iteration => 0.00801020642479\n",
      "Loss for  37831 th iteration => 0.00801020346724\n",
      "Loss for  37832 th iteration => 0.00800975106173\n",
      "Loss for  37833 th iteration => 0.00800961943553\n",
      "Loss for  37834 th iteration => 0.00800930220346\n",
      "Loss for  37835 th iteration => 0.0080091358228\n",
      "Loss for  37836 th iteration => 0.00800885329725\n",
      "Loss for  37837 th iteration => 0.00800865238854\n",
      "Loss for  37838 th iteration => 0.0080084043442\n",
      "Loss for  37839 th iteration => 0.00800816913098\n",
      "Loss for  37840 th iteration => 0.0080079553454\n",
      "Loss for  37841 th iteration => 0.00800768604834\n",
      "Loss for  37842 th iteration => 0.00800750630193\n",
      "Loss for  37843 th iteration => 0.00800720313892\n",
      "Loss for  37844 th iteration => 0.00800705721485\n",
      "Loss for  37845 th iteration => 0.00800672931072\n",
      "Loss for  37846 th iteration => 0.00800649711438\n",
      "Loss for  37847 th iteration => 0.00800626493169\n",
      "Loss for  37848 th iteration => 0.00800603276263\n",
      "Loss for  37849 th iteration => 0.00800580137872\n",
      "Loss for  37850 th iteration => 0.00800568389553\n",
      "Loss for  37851 th iteration => 0.00800535026588\n",
      "Loss for  37852 th iteration => 0.00800520395075\n",
      "Loss for  37853 th iteration => 0.00800489913376\n",
      "Loss for  37854 th iteration => 0.0080047241492\n",
      "Loss for  37855 th iteration => 0.00800444878527\n",
      "Loss for  37856 th iteration => 0.00800439466159\n",
      "Loss for  37857 th iteration => 0.00800401514525\n",
      "Loss for  37858 th iteration => 0.0080038942698\n",
      "Loss for  37859 th iteration => 0.00800358122787\n",
      "Loss for  37860 th iteration => 0.00800339434011\n",
      "Loss for  37861 th iteration => 0.00800314703735\n",
      "Loss for  37862 th iteration => 0.00800289486601\n",
      "Loss for  37863 th iteration => 0.00800271257782\n",
      "Loss for  37864 th iteration => 0.00800240174859\n",
      "Loss for  37865 th iteration => 0.00800218433803\n",
      "Loss for  37866 th iteration => 0.00800206022283\n",
      "Loss for  37867 th iteration => 0.00800174901964\n",
      "Loss for  37868 th iteration => 0.00800155992168\n",
      "Loss for  37869 th iteration => 0.00800131780444\n",
      "Loss for  37870 th iteration => 0.00800106169767\n",
      "Loss for  37871 th iteration => 0.0080007675901\n",
      "Loss for  37872 th iteration => 0.0080005790903\n",
      "Loss for  37873 th iteration => 0.00800031962729\n",
      "Loss for  37874 th iteration => 0.00800009666111\n",
      "Loss for  37875 th iteration => 0.00799987161763\n",
      "Loss for  37876 th iteration => 0.00799961440831\n",
      "Loss for  37877 th iteration => 0.0079994235622\n",
      "Loss for  37878 th iteration => 0.00799913233015\n",
      "Loss for  37879 th iteration => 0.00799897553019\n",
      "Loss for  37880 th iteration => 0.0079987648266\n",
      "Loss for  37881 th iteration => 0.00799855935926\n",
      "Loss for  37882 th iteration => 0.00799829433313\n",
      "Loss for  37883 th iteration => 0.00799797656184\n",
      "Loss for  37884 th iteration => 0.0079979264609\n",
      "Loss for  37885 th iteration => 0.00799752798448\n",
      "Loss for  37886 th iteration => 0.00799733976141\n",
      "Loss for  37887 th iteration => 0.00799720998302\n",
      "Loss for  37888 th iteration => 0.00799687767752\n",
      "Loss for  37889 th iteration => 0.00799673963701\n",
      "Loss for  37890 th iteration => 0.0079964156337\n",
      "Loss for  37891 th iteration => 0.00799626942031\n",
      "Loss for  37892 th iteration => 0.00799595362968\n",
      "Loss for  37893 th iteration => 0.0079957993315\n",
      "Loss for  37894 th iteration => 0.00799549277319\n",
      "Loss for  37895 th iteration => 0.00799524667717\n",
      "Loss for  37896 th iteration => 0.00799504619883\n",
      "Loss for  37897 th iteration => 0.00799476414484\n",
      "Loss for  37898 th iteration => 0.00799468665309\n",
      "Loss for  37899 th iteration => 0.00799441177001\n",
      "Loss for  37900 th iteration => 0.00799417062293\n",
      "Loss for  37901 th iteration => 0.00799393106789\n",
      "Loss for  37902 th iteration => 0.00799362129491\n",
      "Loss for  37903 th iteration => 0.00799344926363\n",
      "Loss for  37904 th iteration => 0.00799317414537\n",
      "Loss for  37905 th iteration => 0.00799296763763\n",
      "Loss for  37906 th iteration => 0.00799272694877\n",
      "Loss for  37907 th iteration => 0.00799248618809\n",
      "Loss for  37908 th iteration => 0.0079922797062\n",
      "Loss for  37909 th iteration => 0.00799200491326\n",
      "Loss for  37910 th iteration => 0.00799183241872\n",
      "Loss for  37911 th iteration => 0.00799152381141\n",
      "Loss for  37912 th iteration => 0.0079913850874\n",
      "Loss for  37913 th iteration => 0.0079910591131\n",
      "Loss for  37914 th iteration => 0.00799082780841\n",
      "Loss for  37915 th iteration => 0.0079905965173\n",
      "Loss for  37916 th iteration => 0.00799036523976\n",
      "Loss for  37917 th iteration => 0.0079901344717\n",
      "Loss for  37918 th iteration => 0.00799001130129\n",
      "Loss for  37919 th iteration => 0.00798968511564\n",
      "Loss for  37920 th iteration => 0.00798953316134\n",
      "Loss for  37921 th iteration => 0.00798923574004\n",
      "Loss for  37922 th iteration => 0.00798905516442\n",
      "Loss for  37923 th iteration => 0.00798878731238\n",
      "Loss for  37924 th iteration => 0.00798871810541\n",
      "Loss for  37925 th iteration => 0.0079883553721\n",
      "Loss for  37926 th iteration => 0.00798821957505\n",
      "Loss for  37927 th iteration => 0.00798792315532\n",
      "Loss for  37928 th iteration => 0.00798772150526\n",
      "Loss for  37929 th iteration => 0.00798749066623\n",
      "Loss for  37930 th iteration => 0.00798722388956\n",
      "Loss for  37931 th iteration => 0.00798705790895\n",
      "Loss for  37932 th iteration => 0.00798673923474\n",
      "Loss for  37933 th iteration => 0.00798653179771\n",
      "Loss for  37934 th iteration => 0.00798625744836\n",
      "Loss for  37935 th iteration => 0.00798608612922\n",
      "Loss for  37936 th iteration => 0.00798577718\n",
      "Loss for  37937 th iteration => 0.00798577620873\n",
      "Loss for  37938 th iteration => 0.00798532469321\n",
      "Loss for  37939 th iteration => 0.00798519396226\n",
      "Loss for  37940 th iteration => 0.00798487861566\n",
      "Loss for  37941 th iteration => 0.0079847131097\n",
      "Loss for  37942 th iteration => 0.00798443248939\n",
      "Loss for  37943 th iteration => 0.00798423243598\n",
      "Loss for  37944 th iteration => 0.00798398631552\n",
      "Loss for  37945 th iteration => 0.0079837519393\n",
      "Loss for  37946 th iteration => 0.00798354009512\n",
      "Loss for  37947 th iteration => 0.00798327161787\n",
      "Loss for  37948 th iteration => 0.00798309382928\n",
      "Loss for  37949 th iteration => 0.00798279146996\n",
      "Loss for  37950 th iteration => 0.00798264751905\n",
      "Loss for  37951 th iteration => 0.00798232032185\n",
      "Loss for  37952 th iteration => 0.00798208951571\n",
      "Loss for  37953 th iteration => 0.00798185874957\n",
      "Loss for  37954 th iteration => 0.00798174113562\n",
      "Loss for  37955 th iteration => 0.00798141036699\n",
      "Loss for  37956 th iteration => 0.00798126389072\n",
      "Loss for  37957 th iteration => 0.00798096196425\n",
      "Loss for  37958 th iteration => 0.00798078678954\n",
      "Loss for  37959 th iteration => 0.00798051382226\n",
      "Loss for  37960 th iteration => 0.007980456615\n",
      "Loss for  37961 th iteration => 0.00798008289995\n",
      "Loss for  37962 th iteration => 0.00797995891141\n",
      "Loss for  37963 th iteration => 0.00797965170007\n",
      "Loss for  37964 th iteration => 0.00797946166997\n",
      "Loss for  37965 th iteration => 0.00797922022681\n",
      "Loss for  37966 th iteration => 0.00797896488417\n",
      "Loss for  37967 th iteration => 0.0079787884843\n",
      "Loss for  37968 th iteration => 0.00797847675018\n",
      "Loss for  37969 th iteration => 0.00797826356806\n",
      "Loss for  37970 th iteration => 0.00797799585093\n",
      "Loss for  37971 th iteration => 0.00797781887545\n",
      "Loss for  37972 th iteration => 0.00797751604949\n",
      "Loss for  37973 th iteration => 0.00797751528255\n",
      "Loss for  37974 th iteration => 0.00797705887324\n",
      "Loss for  37975 th iteration => 0.00797693430788\n",
      "Loss for  37976 th iteration => 0.00797661376548\n",
      "Loss for  37977 th iteration => 0.00797645434937\n",
      "Loss for  37978 th iteration => 0.00797616860841\n",
      "Loss for  37979 th iteration => 0.00797597457036\n",
      "Loss for  37980 th iteration => 0.00797572340314\n",
      "Loss for  37981 th iteration => 0.00797549496905\n",
      "Loss for  37982 th iteration => 0.00797527815076\n",
      "Loss for  37983 th iteration => 0.00797501554364\n",
      "Loss for  37984 th iteration => 0.00797483285235\n",
      "Loss for  37985 th iteration => 0.00797453629239\n",
      "Loss for  37986 th iteration => 0.00797438750897\n",
      "Loss for  37987 th iteration => 0.00797406097044\n",
      "Loss for  37988 th iteration => 0.00797383063432\n",
      "Loss for  37989 th iteration => 0.00797360082389\n",
      "Loss for  37990 th iteration => 0.00797348666991\n",
      "Loss for  37991 th iteration => 0.00797315339918\n",
      "Loss for  37992 th iteration => 0.00797301033307\n",
      "Loss for  37993 th iteration => 0.00797270595384\n",
      "Loss for  37994 th iteration => 0.00797253457105\n",
      "Loss for  37995 th iteration => 0.00797241071756\n",
      "Loss for  37996 th iteration => 0.00797207356882\n",
      "Loss for  37997 th iteration => 0.00797194182131\n",
      "Loss for  37998 th iteration => 0.00797174217947\n",
      "Loss for  37999 th iteration => 0.00797147632115\n",
      "Loss for  38000 th iteration => 0.00797115709764\n",
      "Loss for  38001 th iteration => 0.00797100926328\n",
      "Loss for  38002 th iteration => 0.00797081974905\n",
      "Loss for  38003 th iteration => 0.00797055800529\n",
      "Loss for  38004 th iteration => 0.007970345355\n",
      "Loss for  38005 th iteration => 0.0079700142306\n",
      "Loss for  38006 th iteration => 0.00796988289643\n",
      "Loss for  38007 th iteration => 0.00796954992072\n",
      "Loss for  38008 th iteration => 0.00796942051473\n",
      "Loss for  38009 th iteration => 0.00796908836296\n",
      "Loss for  38010 th iteration => 0.00796884274203\n",
      "Loss for  38011 th iteration => 0.00796864388517\n",
      "Loss for  38012 th iteration => 0.00796836403641\n",
      "Loss for  38013 th iteration => 0.00796819936058\n",
      "Loss for  38014 th iteration => 0.00796788779967\n",
      "Loss for  38015 th iteration => 0.00796765781062\n",
      "Loss for  38016 th iteration => 0.00796742812635\n",
      "Loss for  38017 th iteration => 0.00796743400557\n",
      "Loss for  38018 th iteration => 0.00796697168173\n",
      "Loss for  38019 th iteration => 0.00796674273192\n",
      "Loss for  38020 th iteration => 0.00796661331322\n",
      "Loss for  38021 th iteration => 0.00796640273615\n",
      "Loss for  38022 th iteration => 0.00796613962537\n",
      "Loss for  38023 th iteration => 0.00796585112678\n",
      "Loss for  38024 th iteration => 0.00796566429801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  38025 th iteration => 0.00796540512018\n",
      "Loss for  38026 th iteration => 0.00796532127179\n",
      "Loss for  38027 th iteration => 0.00796497575517\n",
      "Loss for  38028 th iteration => 0.00796482545805\n",
      "Loss for  38029 th iteration => 0.0079645461144\n",
      "Loss for  38030 th iteration => 0.00796433010346\n",
      "Loss for  38031 th iteration => 0.00796411620204\n",
      "Loss for  38032 th iteration => 0.00796383781662\n",
      "Loss for  38033 th iteration => 0.00796360382418\n",
      "Loss for  38034 th iteration => 0.00796335846088\n",
      "Loss for  38035 th iteration => 0.00796316087645\n",
      "Loss for  38036 th iteration => 0.00796287929481\n",
      "Loss for  38037 th iteration => 0.00796271835613\n",
      "Loss for  38038 th iteration => 0.00796252741419\n",
      "Loss for  38039 th iteration => 0.00796218729338\n",
      "Loss for  38040 th iteration => 0.00796204890095\n",
      "Loss for  38041 th iteration => 0.00796174390914\n",
      "Loss for  38042 th iteration => 0.00796157101439\n",
      "Loss for  38043 th iteration => 0.00796150856103\n",
      "Loss for  38044 th iteration => 0.00796109664361\n",
      "Loss for  38045 th iteration => 0.00796086025569\n",
      "Loss for  38046 th iteration => 0.00796061879762\n",
      "Loss for  38047 th iteration => 0.0079604166232\n",
      "Loss for  38048 th iteration => 0.00796014112665\n",
      "Loss for  38049 th iteration => 0.00795997294487\n",
      "Loss for  38050 th iteration => 0.00795966362896\n",
      "Loss for  38051 th iteration => 0.00795952922177\n",
      "Loss for  38052 th iteration => 0.00795920440625\n",
      "Loss for  38053 th iteration => 0.00795897491124\n",
      "Loss for  38054 th iteration => 0.00795874577858\n",
      "Loss for  38055 th iteration => 0.00795861997114\n",
      "Loss for  38056 th iteration => 0.00795829998393\n",
      "Loss for  38057 th iteration => 0.00795814537441\n",
      "Loss for  38058 th iteration => 0.0079578541687\n",
      "Loss for  38059 th iteration => 0.00795767092117\n",
      "Loss for  38060 th iteration => 0.00795740848512\n",
      "Loss for  38061 th iteration => 0.00795733220778\n",
      "Loss for  38062 th iteration => 0.00795698007732\n",
      "Loss for  38063 th iteration => 0.00795683722152\n",
      "Loss for  38064 th iteration => 0.00795655139304\n",
      "Loss for  38065 th iteration => 0.00795634269544\n",
      "Loss for  38066 th iteration => 0.00795612243644\n",
      "Loss for  38067 th iteration => 0.00795584868729\n",
      "Loss for  38068 th iteration => 0.00795561139173\n",
      "Loss for  38069 th iteration => 0.00795545300561\n",
      "Loss for  38070 th iteration => 0.00795517074199\n",
      "Loss for  38071 th iteration => 0.00795489383343\n",
      "Loss for  38072 th iteration => 0.00795472861997\n",
      "Loss for  38073 th iteration => 0.00795441680952\n",
      "Loss for  38074 th iteration => 0.00795441674767\n",
      "Loss for  38075 th iteration => 0.00795397297824\n",
      "Loss for  38076 th iteration => 0.00795383883852\n",
      "Loss for  38077 th iteration => 0.00795353043832\n",
      "Loss for  38078 th iteration => 0.00795336153983\n",
      "Loss for  38079 th iteration => 0.00795308784889\n",
      "Loss for  38080 th iteration => 0.00795288442014\n",
      "Loss for  38081 th iteration => 0.00795264521104\n",
      "Loss for  38082 th iteration => 0.00795240747764\n",
      "Loss for  38083 th iteration => 0.00795220252587\n",
      "Loss for  38084 th iteration => 0.00795193071056\n",
      "Loss for  38085 th iteration => 0.00795175979444\n",
      "Loss for  38086 th iteration => 0.00795145411714\n",
      "Loss for  38087 th iteration => 0.00795131701781\n",
      "Loss for  38088 th iteration => 0.00795099191254\n",
      "Loss for  38089 th iteration => 0.00795076299842\n",
      "Loss for  38090 th iteration => 0.00795064024614\n",
      "Loss for  38091 th iteration => 0.00795031812594\n",
      "Loss for  38092 th iteration => 0.00795016652556\n",
      "Loss for  38093 th iteration => 0.00794987323244\n",
      "Loss for  38094 th iteration => 0.00794969294893\n",
      "Loss for  38095 th iteration => 0.00794942831867\n",
      "Loss for  38096 th iteration => 0.00794922093857\n",
      "Loss for  38097 th iteration => 0.00794912198786\n",
      "Loss for  38098 th iteration => 0.00794876430518\n",
      "Loss for  38099 th iteration => 0.00794865702852\n",
      "Loss for  38100 th iteration => 0.00794830771153\n",
      "Loss for  38101 th iteration => 0.00794819219831\n",
      "Loss for  38102 th iteration => 0.00794785115731\n",
      "Loss for  38103 th iteration => 0.0079477274958\n",
      "Loss for  38104 th iteration => 0.00794740882804\n",
      "Loss for  38105 th iteration => 0.00794717115014\n",
      "Loss for  38106 th iteration => 0.00794696770645\n",
      "Loss for  38107 th iteration => 0.00794669381443\n",
      "Loss for  38108 th iteration => 0.00794652758871\n",
      "Loss for  38109 th iteration => 0.00794634999264\n",
      "Loss for  38110 th iteration => 0.00794610268416\n",
      "Loss for  38111 th iteration => 0.00794586877413\n",
      "Loss for  38112 th iteration => 0.00794555814715\n",
      "Loss for  38113 th iteration => 0.00794539234817\n",
      "Loss for  38114 th iteration => 0.00794511647804\n",
      "Loss for  38115 th iteration => 0.00794491610169\n",
      "Loss for  38116 th iteration => 0.00794467476008\n",
      "Loss for  38117 th iteration => 0.00794444003286\n",
      "Loss for  38118 th iteration => 0.00794423299433\n",
      "Loss for  38119 th iteration => 0.00794396413989\n",
      "Loss for  38120 th iteration => 0.00794379118188\n",
      "Loss for  38121 th iteration => 0.00794348842101\n",
      "Loss for  38122 th iteration => 0.00794334932379\n",
      "Loss for  38123 th iteration => 0.00794302463585\n",
      "Loss for  38124 th iteration => 0.00794279605828\n",
      "Loss for  38125 th iteration => 0.00794256877408\n",
      "Loss for  38126 th iteration => 0.00794244785507\n",
      "Loss for  38127 th iteration => 0.00794212483294\n",
      "Loss for  38128 th iteration => 0.00794197505191\n",
      "Loss for  38129 th iteration => 0.00794168087051\n",
      "Loss for  38130 th iteration => 0.00794150243572\n",
      "Loss for  38131 th iteration => 0.00794137733396\n",
      "Loss for  38132 th iteration => 0.00794104664313\n",
      "Loss for  38133 th iteration => 0.00794091323495\n",
      "Loss for  38134 th iteration => 0.00794059089049\n",
      "Loss for  38135 th iteration => 0.00794044926568\n",
      "Loss for  38136 th iteration => 0.0079401351775\n",
      "Loss for  38137 th iteration => 0.00793998542472\n",
      "Loss for  38138 th iteration => 0.00793967950388\n",
      "Loss for  38139 th iteration => 0.00793952171064\n",
      "Loss for  38140 th iteration => 0.00793923557788\n",
      "Loss for  38141 th iteration => 0.00793896512154\n",
      "Loss for  38142 th iteration => 0.00793879538345\n",
      "Loss for  38143 th iteration => 0.00793862350704\n",
      "Loss for  38144 th iteration => 0.00793837139544\n",
      "Loss for  38145 th iteration => 0.0079381414315\n",
      "Loss for  38146 th iteration => 0.00793782897415\n",
      "Loss for  38147 th iteration => 0.00793766585001\n",
      "Loss for  38148 th iteration => 0.00793738819667\n",
      "Loss for  38149 th iteration => 0.00793719044848\n",
      "Loss for  38150 th iteration => 0.00793694736986\n",
      "Loss for  38151 th iteration => 0.00793671522508\n",
      "Loss for  38152 th iteration => 0.00793650649481\n",
      "Loss for  38153 th iteration => 0.00793624017801\n",
      "Loss for  38154 th iteration => 0.00793606557261\n",
      "Loss for  38155 th iteration => 0.00793576530549\n",
      "Loss for  38156 th iteration => 0.00793562460431\n",
      "Loss for  38157 th iteration => 0.00793530117392\n",
      "Loss for  38158 th iteration => 0.0079350730331\n",
      "Loss for  38159 th iteration => 0.00793484528372\n",
      "Loss for  38160 th iteration => 0.00793472691429\n",
      "Loss for  38161 th iteration => 0.00793440222708\n",
      "Loss for  38162 th iteration => 0.00793425496397\n",
      "Loss for  38163 th iteration => 0.00793395914876\n",
      "Loss for  38164 th iteration => 0.00793378315805\n",
      "Loss for  38165 th iteration => 0.00793351750201\n",
      "Loss for  38166 th iteration => 0.00793345345221\n",
      "Loss for  38167 th iteration => 0.0079330918692\n",
      "Loss for  38168 th iteration => 0.00793296103269\n",
      "Loss for  38169 th iteration => 0.0079326659588\n",
      "Loss for  38170 th iteration => 0.00793246907486\n",
      "Loss for  38171 th iteration => 0.00793223977495\n",
      "Loss for  38172 th iteration => 0.00793197757222\n",
      "Loss for  38173 th iteration => 0.00793181332176\n",
      "Loss for  38174 th iteration => 0.00793149780475\n",
      "Loss for  38175 th iteration => 0.00793129326972\n",
      "Loss for  38176 th iteration => 0.00793102220563\n",
      "Loss for  38177 th iteration => 0.00793085534519\n",
      "Loss for  38178 th iteration => 0.00793068231899\n",
      "Loss for  38179 th iteration => 0.00793043224303\n",
      "Loss for  38180 th iteration => 0.00793020048454\n",
      "Loss for  38181 th iteration => 0.00792988901262\n",
      "Loss for  38182 th iteration => 0.00792972580128\n",
      "Loss for  38183 th iteration => 0.00792944912618\n",
      "Loss for  38184 th iteration => 0.00792925129794\n",
      "Loss for  38185 th iteration => 0.00792900919028\n",
      "Loss for  38186 th iteration => 0.00792877697269\n",
      "Loss for  38187 th iteration => 0.00792856920598\n",
      "Loss for  38188 th iteration => 0.00792830282373\n",
      "Loss for  38189 th iteration => 0.00792812917438\n",
      "Loss for  38190 th iteration => 0.00792782884927\n",
      "Loss for  38191 th iteration => 0.00792768909653\n",
      "Loss for  38192 th iteration => 0.00792736475059\n",
      "Loss for  38193 th iteration => 0.00792713770646\n",
      "Loss for  38194 th iteration => 0.00792701935772\n",
      "Loss for  38195 th iteration => 0.00792669552155\n",
      "Loss for  38196 th iteration => 0.00792654827077\n",
      "Loss for  38197 th iteration => 0.00792625331471\n",
      "Loss for  38198 th iteration => 0.00792607732834\n",
      "Loss for  38199 th iteration => 0.00792581295859\n",
      "Loss for  38200 th iteration => 0.00792574820485\n",
      "Loss for  38201 th iteration => 0.00792538819416\n",
      "Loss for  38202 th iteration => 0.00792525664336\n",
      "Loss for  38203 th iteration => 0.00792496315208\n",
      "Loss for  38204 th iteration => 0.00792476554358\n",
      "Loss for  38205 th iteration => 0.0079245378365\n",
      "Loss for  38206 th iteration => 0.00792427489898\n",
      "Loss for  38207 th iteration => 0.00792411225152\n",
      "Loss for  38208 th iteration => 0.00792379615172\n",
      "Loss for  38209 th iteration => 0.00792359277299\n",
      "Loss for  38210 th iteration => 0.00792332141634\n",
      "Loss for  38211 th iteration => 0.00792315593083\n",
      "Loss for  38212 th iteration => 0.00792298231759\n",
      "Loss for  38213 th iteration => 0.00792273369265\n",
      "Loss for  38214 th iteration => 0.00792250163363\n",
      "Loss for  38215 th iteration => 0.0079221908856\n",
      "Loss for  38216 th iteration => 0.00792202781564\n",
      "Loss for  38217 th iteration => 0.00792175186552\n",
      "Loss for  38218 th iteration => 0.00792155417765\n",
      "Loss for  38219 th iteration => 0.00792131279577\n",
      "Loss for  38220 th iteration => 0.00792108071781\n",
      "Loss for  38221 th iteration => 0.00792087367742\n",
      "Loss for  38222 th iteration => 0.00792060743431\n",
      "Loss for  38223 th iteration => 0.00792043451156\n",
      "Loss for  38224 th iteration => 0.00792013432537\n",
      "Loss for  38225 th iteration => 0.00791999529925\n",
      "Loss for  38226 th iteration => 0.00791967116163\n",
      "Loss for  38227 th iteration => 0.00791944522674\n",
      "Loss for  38228 th iteration => 0.00791932599614\n",
      "Loss for  38229 th iteration => 0.00791900390567\n",
      "Loss for  38230 th iteration => 0.00791885577696\n",
      "Loss for  38231 th iteration => 0.00791856256248\n",
      "Loss for  38232 th iteration => 0.007918386545\n",
      "Loss for  38233 th iteration => 0.00791826133543\n",
      "Loss for  38234 th iteration => 0.00791793334176\n",
      "Loss for  38235 th iteration => 0.00791779982338\n",
      "Loss for  38236 th iteration => 0.0079174801784\n",
      "Loss for  38237 th iteration => 0.00791733844103\n",
      "Loss for  38238 th iteration => 0.00791702705459\n",
      "Loss for  38239 th iteration => 0.00791687718695\n",
      "Loss for  38240 th iteration => 0.00791657397004\n",
      "Loss for  38241 th iteration => 0.00791641605969\n",
      "Loss for  38242 th iteration => 0.00791613231129\n",
      "Loss for  38243 th iteration => 0.00791586188634\n",
      "Loss for  38244 th iteration => 0.00791569537458\n",
      "Loss for  38245 th iteration => 0.00791552264505\n",
      "Loss for  38246 th iteration => 0.00791527398965\n",
      "Loss for  38247 th iteration => 0.00791504353603\n",
      "Loss for  38248 th iteration => 0.00791473325576\n",
      "Loss for  38249 th iteration => 0.00791457054468\n",
      "Loss for  38250 th iteration => 0.00791429508214\n",
      "Loss for  38251 th iteration => 0.0079140977336\n",
      "Loss for  38252 th iteration => 0.00791385685852\n",
      "Loss for  38253 th iteration => 0.00791362510094\n",
      "Loss for  38254 th iteration => 0.007913418586\n",
      "Loss for  38255 th iteration => 0.00791315264488\n",
      "Loss for  38256 th iteration => 0.00791298026564\n",
      "Loss for  38257 th iteration => 0.00791268036364\n",
      "Loss for  38258 th iteration => 0.00791254189852\n",
      "Loss for  38259 th iteration => 0.0079122190564\n",
      "Loss for  38260 th iteration => 0.00791199255649\n",
      "Loss for  38261 th iteration => 0.00791187413308\n",
      "Loss for  38262 th iteration => 0.00791155207814\n",
      "Loss for  38263 th iteration => 0.00791140474452\n",
      "Loss for  38264 th iteration => 0.00791111157741\n",
      "Loss for  38265 th iteration => 0.00791093568798\n",
      "Loss for  38266 th iteration => 0.00791080974475\n",
      "Loss for  38267 th iteration => 0.00791048331719\n",
      "Loss for  38268 th iteration => 0.00791034906644\n",
      "Loss for  38269 th iteration => 0.00791003098624\n",
      "Loss for  38270 th iteration => 0.00790988851783\n",
      "Loss for  38271 th iteration => 0.00790957869482\n",
      "Loss for  38272 th iteration => 0.00790942809745\n",
      "Loss for  38273 th iteration => 0.00790912644263\n",
      "Loss for  38274 th iteration => 0.00790896780388\n",
      "Loss for  38275 th iteration => 0.00790868624328\n",
      "Loss for  38276 th iteration => 0.00790841540911\n",
      "Loss for  38277 th iteration => 0.00790824977181\n",
      "Loss for  38278 th iteration => 0.00790807536007\n",
      "Loss for  38279 th iteration => 0.00790782922047\n",
      "Loss for  38280 th iteration => 0.00790759854557\n",
      "Loss for  38281 th iteration => 0.00790728994861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  38282 th iteration => 0.00790712638804\n",
      "Loss for  38283 th iteration => 0.00790685261173\n",
      "Loss for  38284 th iteration => 0.0079066544109\n",
      "Loss for  38285 th iteration => 0.00790641522461\n",
      "Loss for  38286 th iteration => 0.0079061826123\n",
      "Loss for  38287 th iteration => 0.00790597778836\n",
      "Loss for  38288 th iteration => 0.00790571099042\n",
      "Loss for  38289 th iteration => 0.00790554030405\n",
      "Loss for  38290 th iteration => 0.00790523954346\n",
      "Loss for  38291 th iteration => 0.00790510277275\n",
      "Loss for  38292 th iteration => 0.00790478070616\n",
      "Loss for  38293 th iteration => 0.00790455428731\n",
      "Loss for  38294 th iteration => 0.00790432905892\n",
      "Loss for  38295 th iteration => 0.00790420933009\n",
      "Loss for  38296 th iteration => 0.00790388943234\n",
      "Loss for  38297 th iteration => 0.00790374081373\n",
      "Loss for  38298 th iteration => 0.00790344987869\n",
      "Loss for  38299 th iteration => 0.00790340958651\n",
      "Loss for  38300 th iteration => 0.00790302775012\n",
      "Loss for  38301 th iteration => 0.00790292035832\n",
      "Loss for  38302 th iteration => 0.00790260534221\n",
      "Loss for  38303 th iteration => 0.00790243159428\n",
      "Loss for  38304 th iteration => 0.00790218265913\n",
      "Loss for  38305 th iteration => 0.00790194328785\n",
      "Loss for  38306 th iteration => 0.00790175970496\n",
      "Loss for  38307 th iteration => 0.00790145543259\n",
      "Loss for  38308 th iteration => 0.00790133648376\n",
      "Loss for  38309 th iteration => 0.00790098159996\n",
      "Loss for  38310 th iteration => 0.00790081919839\n",
      "Loss for  38311 th iteration => 0.00790064030253\n",
      "Loss for  38312 th iteration => 0.00790039945444\n",
      "Loss for  38313 th iteration => 0.00790016685106\n",
      "Loss for  38314 th iteration => 0.0078998607221\n",
      "Loss for  38315 th iteration => 0.00789969553803\n",
      "Loss for  38316 th iteration => 0.00789942420958\n",
      "Loss for  38317 th iteration => 0.00789922440529\n",
      "Loss for  38318 th iteration => 0.00789898764674\n",
      "Loss for  38319 th iteration => 0.00789875345099\n",
      "Loss for  38320 th iteration => 0.00789855103466\n",
      "Loss for  38321 th iteration => 0.00789828267331\n",
      "Loss for  38322 th iteration => 0.00789811437441\n",
      "Loss for  38323 th iteration => 0.00789781207046\n",
      "Loss for  38324 th iteration => 0.00789767766707\n",
      "Loss for  38325 th iteration => 0.0078973560757\n",
      "Loss for  38326 th iteration => 0.00789713035064\n",
      "Loss for  38327 th iteration => 0.00789700940984\n",
      "Loss for  38328 th iteration => 0.00789669153135\n",
      "Loss for  38329 th iteration => 0.00789654170045\n",
      "Loss for  38330 th iteration => 0.00789625268934\n",
      "Loss for  38331 th iteration => 0.00789607484375\n",
      "Loss for  38332 th iteration => 0.00789594746482\n",
      "Loss for  38333 th iteration => 0.00789562413069\n",
      "Loss for  38334 th iteration => 0.00789548845768\n",
      "Loss for  38335 th iteration => 0.00789517345737\n",
      "Loss for  38336 th iteration => 0.00789502957999\n",
      "Loss for  38337 th iteration => 0.00789472282348\n",
      "Loss for  38338 th iteration => 0.00789457083028\n",
      "Loss for  38339 th iteration => 0.00789427222872\n",
      "Loss for  38340 th iteration => 0.00789411220712\n",
      "Loss for  38341 th iteration => 0.00789383700202\n",
      "Loss for  38342 th iteration => 0.0078935616046\n",
      "Loss for  38343 th iteration => 0.00789340253521\n",
      "Loss for  38344 th iteration => 0.00789321861538\n",
      "Loss for  38345 th iteration => 0.00789298359769\n",
      "Loss for  38346 th iteration => 0.00789274840255\n",
      "Loss for  38347 th iteration => 0.00789244554685\n",
      "Loss for  38348 th iteration => 0.00789227793114\n",
      "Loss for  38349 th iteration => 0.00789200985684\n",
      "Loss for  38350 th iteration => 0.00789180763993\n",
      "Loss for  38351 th iteration => 0.00789157411639\n",
      "Loss for  38352 th iteration => 0.00789133752708\n",
      "Loss for  38353 th iteration => 0.0078911383266\n",
      "Loss for  38354 th iteration => 0.00789086759078\n",
      "Loss for  38355 th iteration => 0.00789070248853\n",
      "Loss for  38356 th iteration => 0.00789039782923\n",
      "Loss for  38357 th iteration => 0.00789026660325\n",
      "Loss for  38358 th iteration => 0.00788994511804\n",
      "Loss for  38359 th iteration => 0.00788971980785\n",
      "Loss for  38360 th iteration => 0.00788959749994\n",
      "Loss for  38361 th iteration => 0.00788928181164\n",
      "Loss for  38362 th iteration => 0.00788913063056\n",
      "Loss for  38363 th iteration => 0.00788884379259\n",
      "Loss for  38364 th iteration => 0.00788866513801\n",
      "Loss for  38365 th iteration => 0.00788853651397\n",
      "Loss for  38366 th iteration => 0.0078882152498\n",
      "Loss for  38367 th iteration => 0.00788807834137\n",
      "Loss for  38368 th iteration => 0.00788776540128\n",
      "Loss for  38369 th iteration => 0.00788762029804\n",
      "Loss for  38370 th iteration => 0.00788731559213\n",
      "Loss for  38371 th iteration => 0.00788716238252\n",
      "Loss for  38372 th iteration => 0.00788686602223\n",
      "Loss for  38373 th iteration => 0.00788662369082\n",
      "Loss for  38374 th iteration => 0.00788651459612\n",
      "Loss for  38375 th iteration => 0.00788615481871\n",
      "Loss for  38376 th iteration => 0.00788600021562\n",
      "Loss for  38377 th iteration => 0.00788581081231\n",
      "Loss for  38378 th iteration => 0.00788547733954\n",
      "Loss for  38379 th iteration => 0.00788534093416\n",
      "Loss for  38380 th iteration => 0.00788504257155\n",
      "Loss for  38381 th iteration => 0.00788487123918\n",
      "Loss for  38382 th iteration => 0.00788460775128\n",
      "Loss for  38383 th iteration => 0.00788440172544\n",
      "Loss for  38384 th iteration => 0.00788417287982\n",
      "Loss for  38385 th iteration => 0.00788393239108\n",
      "Loss for  38386 th iteration => 0.00788373795826\n",
      "Loss for  38387 th iteration => 0.00788346323425\n",
      "Loss for  38388 th iteration => 0.00788330305397\n",
      "Loss for  38389 th iteration => 0.00788311936438\n",
      "Loss for  38390 th iteration => 0.00788286976165\n",
      "Loss for  38391 th iteration => 0.00788254780232\n",
      "Loss for  38392 th iteration => 0.00788232355724\n",
      "Loss for  38393 th iteration => 0.00788219847435\n",
      "Loss for  38394 th iteration => 0.00788188637514\n",
      "Loss for  38395 th iteration => 0.0078817324513\n",
      "Loss for  38396 th iteration => 0.00788144973656\n",
      "Loss for  38397 th iteration => 0.00788139406794\n",
      "Loss for  38398 th iteration => 0.00788102998984\n",
      "Loss for  38399 th iteration => 0.00788090739034\n",
      "Loss for  38400 th iteration => 0.00788060996472\n",
      "Loss for  38401 th iteration => 0.00788042117521\n",
      "Loss for  38402 th iteration => 0.00788018966532\n",
      "Loss for  38403 th iteration => 0.00787993541605\n",
      "Loss for  38404 th iteration => 0.00787976909573\n",
      "Loss for  38405 th iteration => 0.00787945336039\n",
      "Loss for  38406 th iteration => 0.00787926563553\n",
      "Loss for  38407 th iteration => 0.00787898357153\n",
      "Loss for  38408 th iteration => 0.00787883320247\n",
      "Loss for  38409 th iteration => 0.00787864071551\n",
      "Loss for  38410 th iteration => 0.00787831134879\n",
      "Loss for  38411 th iteration => 0.00787817160238\n",
      "Loss for  38412 th iteration => 0.00787787741016\n",
      "Loss for  38413 th iteration => 0.00787770267317\n",
      "Loss for  38414 th iteration => 0.00787744341866\n",
      "Loss for  38415 th iteration => 0.00787723392598\n",
      "Loss for  38416 th iteration => 0.0078770093754\n",
      "Loss for  38417 th iteration => 0.00787676535891\n",
      "Loss for  38418 th iteration => 0.00787657528146\n",
      "Loss for  38419 th iteration => 0.00787629697009\n",
      "Loss for  38420 th iteration => 0.00787614113792\n",
      "Loss for  38421 th iteration => 0.00787583291129\n",
      "Loss for  38422 th iteration => 0.00787560811719\n",
      "Loss for  38423 th iteration => 0.00787538333623\n",
      "Loss for  38424 th iteration => 0.00787515856843\n",
      "Loss for  38425 th iteration => 0.00787493381376\n",
      "Loss for  38426 th iteration => 0.00787470907222\n",
      "Loss for  38427 th iteration => 0.00787448434381\n",
      "Loss for  38428 th iteration => 0.00787425962852\n",
      "Loss for  38429 th iteration => 0.00787403586978\n",
      "Loss for  38430 th iteration => 0.00787392437482\n",
      "Loss for  38431 th iteration => 0.00787359968697\n",
      "Loss for  38432 th iteration => 0.00787345918985\n",
      "Loss for  38433 th iteration => 0.00787316415225\n",
      "Loss for  38434 th iteration => 0.00787313835967\n",
      "Loss for  38435 th iteration => 0.007872745514\n",
      "Loss for  38436 th iteration => 0.00787265236495\n",
      "Loss for  38437 th iteration => 0.00787232659508\n",
      "Loss for  38438 th iteration => 0.00787216683625\n",
      "Loss for  38439 th iteration => 0.00787190739965\n",
      "Loss for  38440 th iteration => 0.007871681767\n",
      "Loss for  38441 th iteration => 0.00787148793179\n",
      "Loss for  38442 th iteration => 0.00787119715074\n",
      "Loss for  38443 th iteration => 0.00787106819554\n",
      "Loss for  38444 th iteration => 0.00787072066817\n",
      "Loss for  38445 th iteration => 0.00787055502298\n",
      "Loss for  38446 th iteration => 0.00787038899071\n",
      "Loss for  38447 th iteration => 0.00787013875438\n",
      "Loss for  38448 th iteration => 0.00786991220515\n",
      "Loss for  38449 th iteration => 0.00786960243125\n",
      "Loss for  38450 th iteration => 0.0078694442271\n",
      "Loss for  38451 th iteration => 0.00786916934505\n",
      "Loss for  38452 th iteration => 0.00786897643031\n",
      "Loss for  38453 th iteration => 0.00786873620734\n",
      "Loss for  38454 th iteration => 0.00786850881291\n",
      "Loss for  38455 th iteration => 0.00786830301921\n",
      "Loss for  38456 th iteration => 0.00786804137304\n",
      "Loss for  38457 th iteration => 0.00786786978172\n",
      "Loss for  38458 th iteration => 0.00786757410891\n",
      "Loss for  38459 th iteration => 0.00786743649595\n",
      "Loss for  38460 th iteration => 0.0078671161212\n",
      "Loss for  38461 th iteration => 0.00786700024459\n",
      "Loss for  38462 th iteration => 0.00786668069434\n",
      "Loss for  38463 th iteration => 0.00786653585409\n",
      "Loss for  38464 th iteration => 0.007866245532\n",
      "Loss for  38465 th iteration => 0.00786620930416\n",
      "Loss for  38466 th iteration => 0.00786582761165\n",
      "Loss for  38467 th iteration => 0.00786572414733\n",
      "Loss for  38468 th iteration => 0.00786540941128\n",
      "Loss for  38469 th iteration => 0.00786523945545\n",
      "Loss for  38470 th iteration => 0.00786499093501\n",
      "Loss for  38471 th iteration => 0.00786475522196\n",
      "Loss for  38472 th iteration => 0.00786457218693\n",
      "Loss for  38473 th iteration => 0.00786427144042\n",
      "Loss for  38474 th iteration => 0.00786415317107\n",
      "Loss for  38475 th iteration => 0.00786380067851\n",
      "Loss for  38476 th iteration => 0.00786364043763\n",
      "Loss for  38477 th iteration => 0.00786346411056\n",
      "Loss for  38478 th iteration => 0.00786322488102\n",
      "Loss for  38479 th iteration => 0.00786299441755\n",
      "Loss for  38480 th iteration => 0.00786268987408\n",
      "Loss for  38481 th iteration => 0.00786252723845\n",
      "Loss for  38482 th iteration => 0.00786225753779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  38483 th iteration => 0.00786206024031\n",
      "Loss for  38484 th iteration => 0.00786182515001\n",
      "Loss for  38485 th iteration => 0.00786159342126\n",
      "Loss for  38486 th iteration => 0.00786139271185\n",
      "Loss for  38487 th iteration => 0.00786112677945\n",
      "Loss for  38488 th iteration => 0.00786096022436\n",
      "Loss for  38489 th iteration => 0.00786066031308\n",
      "Loss for  38490 th iteration => 0.00786052768862\n",
      "Loss for  38491 th iteration => 0.00786020834919\n",
      "Loss for  38492 th iteration => 0.00786008795421\n",
      "Loss for  38493 th iteration => 0.00785977367544\n",
      "Loss for  38494 th iteration => 0.00785962435733\n",
      "Loss for  38495 th iteration => 0.0078593389778\n",
      "Loss for  38496 th iteration => 0.00785916266148\n",
      "Loss for  38497 th iteration => 0.00785903503864\n",
      "Loss for  38498 th iteration => 0.00785871605647\n",
      "Loss for  38499 th iteration => 0.00785858014253\n",
      "Loss for  38500 th iteration => 0.00785826949105\n",
      "Loss for  38501 th iteration => 0.0078581253757\n",
      "Loss for  38502 th iteration => 0.0078578229649\n",
      "Loss for  38503 th iteration => 0.00785767073666\n",
      "Loss for  38504 th iteration => 0.00785737647771\n",
      "Loss for  38505 th iteration => 0.00785721622399\n",
      "Loss for  38506 th iteration => 0.00785694684446\n",
      "Loss for  38507 th iteration => 0.00785666949878\n",
      "Loss for  38508 th iteration => 0.00785651724845\n",
      "Loss for  38509 th iteration => 0.00785632926151\n",
      "Loss for  38510 th iteration => 0.00785599706409\n",
      "Loss for  38511 th iteration => 0.00785586266148\n",
      "Loss for  38512 th iteration => 0.00785556560038\n",
      "Loss for  38513 th iteration => 0.00785539624507\n",
      "Loss for  38514 th iteration => 0.00785513408352\n",
      "Loss for  38515 th iteration => 0.00785493001035\n",
      "Loss for  38516 th iteration => 0.00785470251462\n",
      "Loss for  38517 th iteration => 0.00785446395544\n",
      "Loss for  38518 th iteration => 0.00785427089477\n",
      "Loss for  38519 th iteration => 0.00785399807848\n",
      "Loss for  38520 th iteration => 0.00785383922505\n",
      "Loss for  38521 th iteration => 0.00785353237764\n",
      "Loss for  38522 th iteration => 0.00785340759562\n",
      "Loss for  38523 th iteration => 0.00785318580406\n",
      "Loss for  38524 th iteration => 0.00785300851137\n",
      "Loss for  38525 th iteration => 0.00785272513416\n",
      "Loss for  38526 th iteration => 0.00785244424011\n",
      "Loss for  38527 th iteration => 0.0078522638765\n",
      "Loss for  38528 th iteration => 0.0078521344954\n",
      "Loss for  38529 th iteration => 0.0078518180355\n",
      "Loss for  38530 th iteration => 0.00785168038073\n",
      "Loss for  38531 th iteration => 0.00785137223403\n",
      "Loss for  38532 th iteration => 0.00785122639504\n",
      "Loss for  38533 th iteration => 0.00785092647174\n",
      "Loss for  38534 th iteration => 0.00785077253687\n",
      "Loss for  38535 th iteration => 0.00785048533263\n",
      "Loss for  38536 th iteration => 0.00785023828267\n",
      "Loss for  38537 th iteration => 0.00785005512632\n",
      "Loss for  38538 th iteration => 0.00784977174646\n",
      "Loss for  38539 th iteration => 0.0078496263233\n",
      "Loss for  38540 th iteration => 0.00784943239524\n",
      "Loss for  38541 th iteration => 0.00784910715322\n",
      "Loss for  38542 th iteration => 0.00784896654139\n",
      "Loss for  38543 th iteration => 0.0078486764734\n",
      "Loss for  38544 th iteration => 0.00784850087173\n",
      "Loss for  38545 th iteration => 0.00784824574\n",
      "Loss for  38546 th iteration => 0.00784803538433\n",
      "Loss for  38547 th iteration => 0.0078478149541\n",
      "Loss for  38548 th iteration => 0.00784757007728\n",
      "Loss for  38549 th iteration => 0.00784738411679\n",
      "Loss for  38550 th iteration => 0.00784710494871\n",
      "Loss for  38551 th iteration => 0.00784695322916\n",
      "Loss for  38552 th iteration => 0.00784664625317\n",
      "Loss for  38553 th iteration => 0.00784642309364\n",
      "Loss for  38554 th iteration => 0.00784619994715\n",
      "Loss for  38555 th iteration => 0.00784597681368\n",
      "Loss for  38556 th iteration => 0.00784575369324\n",
      "Loss for  38557 th iteration => 0.00784553058582\n",
      "Loss for  38558 th iteration => 0.00784530749141\n",
      "Loss for  38559 th iteration => 0.0078450852327\n",
      "Loss for  38560 th iteration => 0.00784497223269\n",
      "Loss for  38561 th iteration => 0.00784465228119\n",
      "Loss for  38562 th iteration => 0.00784451027855\n",
      "Loss for  38563 th iteration => 0.00784422066745\n",
      "Loss for  38564 th iteration => 0.00784418756997\n",
      "Loss for  38565 th iteration => 0.00784380522663\n",
      "Loss for  38566 th iteration => 0.0078437048171\n",
      "Loss for  38567 th iteration => 0.00784338950546\n",
      "Loss for  38568 th iteration => 0.00784322252955\n",
      "Loss for  38569 th iteration => 0.00784297350805\n",
      "Loss for  38570 th iteration => 0.00784274070077\n",
      "Loss for  38571 th iteration => 0.00784255723845\n",
      "Loss for  38572 th iteration => 0.00784225932432\n",
      "Loss for  38573 th iteration => 0.00784214070071\n",
      "Loss for  38574 th iteration => 0.00784178925137\n",
      "Loss for  38575 th iteration => 0.00784163135383\n",
      "Loss for  38576 th iteration => 0.00784145615424\n",
      "Loss for  38577 th iteration => 0.00784121826369\n",
      "Loss for  38578 th iteration => 0.00784098759058\n",
      "Loss for  38579 th iteration => 0.00784068450309\n",
      "Loss for  38580 th iteration => 0.00784052285325\n",
      "Loss for  38581 th iteration => 0.00784025462779\n",
      "Loss for  38582 th iteration => 0.00784005829719\n",
      "Loss for  38583 th iteration => 0.00783982470039\n",
      "Loss for  38584 th iteration => 0.00783959392054\n",
      "Loss for  38585 th iteration => 0.00783939472196\n",
      "Loss for  38586 th iteration => 0.00783912972143\n",
      "Loss for  38587 th iteration => 0.00783896469358\n",
      "Loss for  38588 th iteration => 0.00783866569805\n",
      "Loss for  38589 th iteration => 0.00783853507711\n",
      "Loss for  38590 th iteration => 0.00783831904414\n",
      "Loss for  38591 th iteration => 0.00783800555766\n",
      "Loss for  38592 th iteration => 0.00783785785914\n",
      "Loss for  38593 th iteration => 0.00783757393954\n",
      "Loss for  38594 th iteration => 0.00783752745041\n",
      "Loss for  38595 th iteration => 0.00783715916663\n",
      "Loss for  38596 th iteration => 0.00783704552507\n",
      "Loss for  38597 th iteration => 0.00783674411421\n",
      "Loss for  38598 th iteration => 0.00783656406366\n",
      "Loss for  38599 th iteration => 0.00783632878637\n",
      "Loss for  38600 th iteration => 0.00783608305964\n",
      "Loss for  38601 th iteration => 0.00783591318717\n",
      "Loss for  38602 th iteration => 0.00783560283224\n",
      "Loss for  38603 th iteration => 0.00783541501489\n",
      "Loss for  38604 th iteration => 0.00783513787595\n",
      "Loss for  38605 th iteration => 0.00783498722356\n",
      "Loss for  38606 th iteration => 0.00783480035235\n",
      "Loss for  38607 th iteration => 0.00783446988359\n",
      "Loss for  38608 th iteration => 0.00783433608494\n",
      "Loss for  38609 th iteration => 0.00783404086569\n",
      "Loss for  38610 th iteration => 0.00783387200246\n",
      "Loss for  38611 th iteration => 0.0078336117935\n",
      "Loss for  38612 th iteration => 0.00783340810296\n",
      "Loss for  38613 th iteration => 0.00783318266812\n",
      "Loss for  38614 th iteration => 0.0078329443845\n",
      "Loss for  38615 th iteration => 0.00783275349064\n",
      "Loss for  38616 th iteration => 0.00783248084522\n",
      "Loss for  38617 th iteration => 0.00783232426214\n",
      "Loss for  38618 th iteration => 0.00783201856125\n",
      "Loss for  38619 th iteration => 0.00783179621914\n",
      "Loss for  38620 th iteration => 0.00783157389001\n",
      "Loss for  38621 th iteration => 0.00783135157387\n",
      "Loss for  38622 th iteration => 0.0078311292707\n",
      "Loss for  38623 th iteration => 0.0078309069805\n",
      "Loss for  38624 th iteration => 0.00783068494088\n",
      "Loss for  38625 th iteration => 0.00783057604517\n",
      "Loss for  38626 th iteration => 0.00783025362109\n",
      "Loss for  38627 th iteration => 0.00783011565706\n",
      "Loss for  38628 th iteration => 0.00782982289209\n",
      "Loss for  38629 th iteration => 0.00782979797988\n",
      "Loss for  38630 th iteration => 0.00782940911248\n",
      "Loss for  38631 th iteration => 0.00782931673844\n",
      "Loss for  38632 th iteration => 0.00782899505174\n",
      "Loss for  38633 th iteration => 0.00782883596351\n",
      "Loss for  38634 th iteration => 0.00782858071397\n",
      "Loss for  38635 th iteration => 0.00782835564848\n",
      "Loss for  38636 th iteration => 0.00782816610323\n",
      "Loss for  38637 th iteration => 0.0078278757869\n",
      "Loss for  38638 th iteration => 0.00782775122355\n",
      "Loss for  38639 th iteration => 0.00782740475363\n",
      "Loss for  38640 th iteration => 0.00782724289191\n",
      "Loss for  38641 th iteration => 0.00782707677327\n",
      "Loss for  38642 th iteration => 0.00782683145799\n",
      "Loss for  38643 th iteration => 0.00782660654933\n",
      "Loss for  38644 th iteration => 0.00782629981104\n",
      "Loss for  38645 th iteration => 0.00782614337826\n",
      "Loss for  38646 th iteration => 0.00782587156072\n",
      "Loss for  38647 th iteration => 0.0078256803891\n",
      "Loss for  38648 th iteration => 0.00782544325766\n",
      "Loss for  38649 th iteration => 0.00782521757992\n",
      "Loss for  38650 th iteration => 0.00782501490295\n",
      "Loss for  38651 th iteration => 0.00782475494887\n",
      "Loss for  38652 th iteration => 0.00782458649766\n",
      "Loss for  38653 th iteration => 0.0078242924941\n",
      "Loss for  38654 th iteration => 0.00782415893081\n",
      "Loss for  38655 th iteration => 0.00782394559986\n",
      "Loss for  38656 th iteration => 0.00782363182237\n",
      "Loss for  38657 th iteration => 0.00782348599371\n",
      "Loss for  38658 th iteration => 0.00782320233864\n",
      "Loss for  38659 th iteration => 0.00782315914555\n",
      "Loss for  38660 th iteration => 0.00782278919118\n",
      "Loss for  38661 th iteration => 0.00782267876794\n",
      "Loss for  38662 th iteration => 0.00782237576383\n",
      "Loss for  38663 th iteration => 0.00782219885478\n",
      "Loss for  38664 th iteration => 0.00782196206068\n",
      "Loss for  38665 th iteration => 0.0078217193995\n",
      "Loss for  38666 th iteration => 0.00782154808579\n",
      "Loss for  38667 th iteration => 0.0078212403957\n",
      "Loss for  38668 th iteration => 0.00782113384315\n",
      "Loss for  38669 th iteration => 0.00782077768157\n",
      "Loss for  38670 th iteration => 0.00782062602057\n",
      "Loss for  38671 th iteration => 0.00782044219274\n",
      "Loss for  38672 th iteration => 0.00782010981732\n",
      "Loss for  38673 th iteration => 0.00781997955797\n",
      "Loss for  38674 th iteration => 0.0078196823663\n",
      "Loss for  38675 th iteration => 0.00781951710753\n",
      "Loss for  38676 th iteration => 0.00781925486105\n",
      "Loss for  38677 th iteration => 0.00781905483947\n",
      "Loss for  38678 th iteration => 0.00781882730266\n",
      "Loss for  38679 th iteration => 0.00781859275189\n",
      "Loss for  38680 th iteration => 0.00781839969222\n",
      "Loss for  38681 th iteration => 0.00781813084289\n",
      "Loss for  38682 th iteration => 0.00781797203081\n",
      "Loss for  38683 th iteration => 0.00781766921242\n",
      "Loss for  38684 th iteration => 0.00781764155438\n",
      "Loss for  38685 th iteration => 0.00781723931873\n",
      "Loss for  38686 th iteration => 0.00781708646807\n",
      "Loss for  38687 th iteration => 0.0078168094004\n",
      "Loss for  38688 th iteration => 0.00781662769432\n",
      "Loss for  38689 th iteration => 0.00781637945722\n",
      "Loss for  38690 th iteration => 0.00781616906592\n",
      "Loss for  38691 th iteration => 0.00781594948998\n",
      "Loss for  38692 th iteration => 0.00781571058151\n",
      "Loss for  38693 th iteration => 0.00781552084969\n",
      "Loss for  38694 th iteration => 0.00781539630369\n",
      "Loss for  38695 th iteration => 0.00781510811801\n",
      "Loss for  38696 th iteration => 0.00781492374532\n",
      "Loss for  38697 th iteration => 0.00781461352899\n",
      "Loss for  38698 th iteration => 0.00781446068586\n",
      "Loss for  38699 th iteration => 0.00781418755135\n",
      "Loss for  38700 th iteration => 0.00781399916483\n",
      "Loss for  38701 th iteration => 0.00781387856387\n",
      "Loss for  38702 th iteration => 0.00781355214581\n",
      "Loss for  38703 th iteration => 0.00781343351536\n",
      "Loss for  38704 th iteration => 0.00781310522916\n",
      "Loss for  38705 th iteration => 0.00781298854391\n",
      "Loss for  38706 th iteration => 0.00781265841365\n",
      "Loss for  38707 th iteration => 0.00781254364866\n",
      "Loss for  38708 th iteration => 0.00781222278909\n",
      "Loss for  38709 th iteration => 0.0078119792675\n",
      "Loss for  38710 th iteration => 0.00781179590911\n",
      "Loss for  38711 th iteration => 0.00781151809375\n",
      "Loss for  38712 th iteration => 0.00781136897792\n",
      "Loss for  38713 th iteration => 0.00781106447829\n",
      "Loss for  38714 th iteration => 0.00781084330346\n",
      "Loss for  38715 th iteration => 0.00781062214154\n",
      "Loss for  38716 th iteration => 0.00781040099252\n",
      "Loss for  38717 th iteration => 0.00781017985639\n",
      "Loss for  38718 th iteration => 0.00780995902424\n",
      "Loss for  38719 th iteration => 0.00780984401771\n",
      "Loss for  38720 th iteration => 0.00780952998752\n",
      "Loss for  38721 th iteration => 0.00780938595453\n",
      "Loss for  38722 th iteration => 0.00780910127649\n",
      "Loss for  38723 th iteration => 0.0078090617445\n",
      "Loss for  38724 th iteration => 0.00780868972443\n",
      "Loss for  38725 th iteration => 0.00780858287887\n",
      "Loss for  38726 th iteration => 0.00780827789206\n",
      "Loss for  38727 th iteration => 0.00780810447827\n",
      "Loss for  38728 th iteration => 0.00780786578347\n",
      "Loss for  38729 th iteration => 0.00780762653614\n",
      "Loss for  38730 th iteration => 0.00780745340269\n",
      "Loss for  38731 th iteration => 0.00780714904605\n",
      "Loss for  38732 th iteration => 0.00780704075374\n",
      "Loss for  38733 th iteration => 0.00780668735314\n",
      "Loss for  38734 th iteration => 0.00780653414324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  38735 th iteration => 0.00780622723644\n",
      "Loss for  38736 th iteration => 0.00780623653363\n",
      "Loss for  38737 th iteration => 0.00780580267549\n",
      "Loss for  38738 th iteration => 0.00780567381889\n",
      "Loss for  38739 th iteration => 0.00780537676052\n",
      "Loss for  38740 th iteration => 0.00780521302062\n",
      "Loss for  38741 th iteration => 0.00780495079164\n",
      "Loss for  38742 th iteration => 0.00780475240367\n",
      "Loss for  38743 th iteration => 0.00780452476995\n",
      "Loss for  38744 th iteration => 0.00780429196613\n",
      "Loss for  38745 th iteration => 0.00780409869652\n",
      "Loss for  38746 th iteration => 0.00780383170615\n",
      "Loss for  38747 th iteration => 0.00780367269263\n",
      "Loss for  38748 th iteration => 0.00780346935396\n",
      "Loss for  38749 th iteration => 0.00780325991904\n",
      "Loss for  38750 th iteration => 0.00780301064188\n",
      "Loss for  38751 th iteration => 0.00780273392974\n",
      "Loss for  38752 th iteration => 0.00780255476179\n",
      "Loss for  38753 th iteration => 0.00780242824778\n",
      "Loss for  38754 th iteration => 0.00780211442202\n",
      "Loss for  38755 th iteration => 0.00780197962457\n",
      "Loss for  38756 th iteration => 0.00780167412162\n",
      "Loss for  38757 th iteration => 0.00780153113028\n",
      "Loss for  38758 th iteration => 0.00780123386025\n",
      "Loss for  38759 th iteration => 0.00780108276344\n",
      "Loss for  38760 th iteration => 0.00780079967417\n",
      "Loss for  38761 th iteration => 0.00780055375105\n",
      "Loss for  38762 th iteration => 0.00780037500934\n",
      "Loss for  38763 th iteration => 0.00780009270441\n",
      "Loss for  38764 th iteration => 0.00779995118657\n",
      "Loss for  38765 th iteration => 0.00779975949281\n",
      "Loss for  38766 th iteration => 0.00779943745969\n",
      "Loss for  38767 th iteration => 0.00779929914051\n",
      "Loss for  38768 th iteration => 0.00779901230629\n",
      "Loss for  38769 th iteration => 0.00779883897305\n",
      "Loss for  38770 th iteration => 0.00779858709799\n",
      "Loss for  38771 th iteration => 0.00779837898846\n",
      "Loss for  38772 th iteration => 0.00779816183588\n",
      "Loss for  38773 th iteration => 0.00779791918482\n",
      "Loss for  38774 th iteration => 0.00779773652106\n",
      "Loss for  38775 th iteration => 0.00779745956024\n",
      "Loss for  38776 th iteration => 0.00779731115459\n",
      "Loss for  38777 th iteration => 0.00779700740548\n",
      "Loss for  38778 th iteration => 0.00779678701231\n",
      "Loss for  38779 th iteration => 0.007796566632\n",
      "Loss for  38780 th iteration => 0.00779634626454\n",
      "Loss for  38781 th iteration => 0.00779612590991\n",
      "Loss for  38782 th iteration => 0.00779590632423\n",
      "Loss for  38783 th iteration => 0.00779579112846\n",
      "Loss for  38784 th iteration => 0.00779547884697\n",
      "Loss for  38785 th iteration => 0.00779533462036\n",
      "Loss for  38786 th iteration => 0.0077950523167\n",
      "Loss for  38787 th iteration => 0.00779501128048\n",
      "Loss for  38788 th iteration => 0.00779464231317\n",
      "Loss for  38789 th iteration => 0.00779453396741\n",
      "Loss for  38790 th iteration => 0.00779423202943\n",
      "Loss for  38791 th iteration => 0.00779405711916\n",
      "Loss for  38792 th iteration => 0.00779382146953\n",
      "Loss for  38793 th iteration => 0.00779358072918\n",
      "Loss for  38794 th iteration => 0.00779341063752\n",
      "Loss for  38795 th iteration => 0.00779310479104\n",
      "Loss for  38796 th iteration => 0.00779299953736\n",
      "Loss for  38797 th iteration => 0.00779264514423\n",
      "Loss for  38798 th iteration => 0.00779249474565\n",
      "Loss for  38799 th iteration => 0.00779231249424\n",
      "Loss for  38800 th iteration => 0.00779198172484\n",
      "Loss for  38801 th iteration => 0.00779185296368\n",
      "Loss for  38802 th iteration => 0.00779155739974\n",
      "Loss for  38803 th iteration => 0.00779139361787\n",
      "Loss for  38804 th iteration => 0.00779113301963\n",
      "Loss for  38805 th iteration => 0.00779093445485\n",
      "Loss for  38806 th iteration => 0.00779070858562\n",
      "Loss for  38807 th iteration => 0.0077904754727\n",
      "Loss for  38808 th iteration => 0.00779028409878\n",
      "Loss for  38809 th iteration => 0.00779001666953\n",
      "Loss for  38810 th iteration => 0.00778985971288\n",
      "Loss for  38811 th iteration => 0.00778965401988\n",
      "Loss for  38812 th iteration => 0.00778944855959\n",
      "Loss for  38813 th iteration => 0.00778919846712\n",
      "Loss for  38814 th iteration => 0.00778892444741\n",
      "Loss for  38815 th iteration => 0.00778874277028\n",
      "Loss for  38816 th iteration => 0.00778849760517\n",
      "Loss for  38817 th iteration => 0.00778828721928\n",
      "Loss for  38818 th iteration => 0.00778807073802\n",
      "Loss for  38819 th iteration => 0.00778783181273\n",
      "Loss for  38820 th iteration => 0.00778764514696\n",
      "Loss for  38821 th iteration => 0.00778751841516\n",
      "Loss for  38822 th iteration => 0.00778723549997\n",
      "Loss for  38823 th iteration => 0.0077870505135\n",
      "Loss for  38824 th iteration => 0.00778674399503\n",
      "Loss for  38825 th iteration => 0.00778659053425\n",
      "Loss for  38826 th iteration => 0.00778632110646\n",
      "Loss for  38827 th iteration => 0.00778613215797\n",
      "Loss for  38828 th iteration => 0.00778601311123\n",
      "Loss for  38829 th iteration => 0.0077856882116\n",
      "Loss for  38830 th iteration => 0.0077855711353\n",
      "Loss for  38831 th iteration => 0.00778524436741\n",
      "Loss for  38832 th iteration => 0.0077851292363\n",
      "Loss for  38833 th iteration => 0.00778480062421\n",
      "Loss for  38834 th iteration => 0.00778468741336\n",
      "Loss for  38835 th iteration => 0.00778437025297\n",
      "Loss for  38836 th iteration => 0.0077841261041\n",
      "Loss for  38837 th iteration => 0.00778394645513\n",
      "Loss for  38838 th iteration => 0.00778366801601\n",
      "Loss for  38839 th iteration => 0.0077835226054\n",
      "Loss for  38840 th iteration => 0.00778321956284\n",
      "Loss for  38841 th iteration => 0.00778299993492\n",
      "Loss for  38842 th iteration => 0.0077827803198\n",
      "Loss for  38843 th iteration => 0.00778256071747\n",
      "Loss for  38844 th iteration => 0.00778234131316\n",
      "Loss for  38845 th iteration => 0.00778222454723\n",
      "Loss for  38846 th iteration => 0.00778191533697\n",
      "Loss for  38847 th iteration => 0.00778176953846\n",
      "Loss for  38848 th iteration => 0.00778148933436\n",
      "Loss for  38849 th iteration => 0.00778131648705\n",
      "Loss for  38850 th iteration => 0.00778119250719\n",
      "Loss for  38851 th iteration => 0.00778087848826\n",
      "Loss for  38852 th iteration => 0.00778074620386\n",
      "Loss for  38853 th iteration => 0.00778044052885\n",
      "Loss for  38854 th iteration => 0.00778030002971\n",
      "Loss for  38855 th iteration => 0.00778000260848\n",
      "Loss for  38856 th iteration => 0.00777985398323\n",
      "Loss for  38857 th iteration => 0.00777956587083\n",
      "Loss for  38858 th iteration => 0.00777932670212\n",
      "Loss for  38859 th iteration => 0.00777914359144\n",
      "Loss for  38860 th iteration => 0.0077788679563\n",
      "Loss for  38861 th iteration => 0.00777872310701\n",
      "Loss for  38862 th iteration => 0.00777853637514\n",
      "Loss for  38863 th iteration => 0.00777821098928\n",
      "Loss for  38864 th iteration => 0.00777807833264\n",
      "Loss for  38865 th iteration => 0.00777778821146\n",
      "Loss for  38866 th iteration => 0.00777762047566\n",
      "Loss for  38867 th iteration => 0.00777736537797\n",
      "Loss for  38868 th iteration => 0.00777716280223\n",
      "Loss for  38869 th iteration => 0.00777694248989\n",
      "Loss for  38870 th iteration => 0.00777670531039\n",
      "Loss for  38871 th iteration => 0.00777651954833\n",
      "Loss for  38872 th iteration => 0.00777624799824\n",
      "Loss for  38873 th iteration => 0.00777609655435\n",
      "Loss for  38874 th iteration => 0.00777579285013\n",
      "Loss for  38875 th iteration => 0.0077755736348\n",
      "Loss for  38876 th iteration => 0.00777535443226\n",
      "Loss for  38877 th iteration => 0.00777513524249\n",
      "Loss for  38878 th iteration => 0.00777491711308\n",
      "Loss for  38879 th iteration => 0.00777480612003\n",
      "Loss for  38880 th iteration => 0.00777449198326\n",
      "Loss for  38881 th iteration => 0.00777435189959\n",
      "Loss for  38882 th iteration => 0.00777406817316\n",
      "Loss for  38883 th iteration => 0.0077740350975\n",
      "Loss for  38884 th iteration => 0.00777366053894\n",
      "Loss for  38885 th iteration => 0.00777356001837\n",
      "Loss for  38886 th iteration => 0.00777325262387\n",
      "Loss for  38887 th iteration => 0.00777308540497\n",
      "Loss for  38888 th iteration => 0.00777284443203\n",
      "Loss for  38889 th iteration => 0.00777261125074\n",
      "Loss for  38890 th iteration => 0.00777243596742\n",
      "Loss for  38891 th iteration => 0.00777213754922\n",
      "Loss for  38892 th iteration => 0.00777202723403\n",
      "Loss for  38893 th iteration => 0.00777167702107\n",
      "Loss for  38894 th iteration => 0.00777152506974\n",
      "Loss for  38895 th iteration => 0.00777134990403\n",
      "Loss for  38896 th iteration => 0.00777111974978\n",
      "Loss for  38897 th iteration => 0.00777089102846\n",
      "Loss for  38898 th iteration => 0.00777059349501\n",
      "Loss for  38899 th iteration => 0.00777043404091\n",
      "Loss for  38900 th iteration => 0.0077701713986\n",
      "Loss for  38901 th iteration => 0.00776997723549\n",
      "Loss for  38902 th iteration => 0.00776974924826\n",
      "Loss for  38903 th iteration => 0.00776952061029\n",
      "Loss for  38904 th iteration => 0.00776932704506\n",
      "Loss for  38905 th iteration => 0.00776906416342\n",
      "Loss for  38906 th iteration => 0.00776890594054\n",
      "Loss for  38907 th iteration => 0.00776870635135\n",
      "Loss for  38908 th iteration => 0.00776849703691\n",
      "Loss for  38909 th iteration => 0.00776824922993\n",
      "Loss for  38910 th iteration => 0.00776797578648\n",
      "Loss for  38911 th iteration => 0.00776791968819\n",
      "Loss for  38912 th iteration => 0.00776756869455\n",
      "Loss for  38913 th iteration => 0.00776744544573\n",
      "Loss for  38914 th iteration => 0.00776716132332\n",
      "Loss for  38915 th iteration => 0.00776697166644\n",
      "Loss for  38916 th iteration => 0.00776675367685\n",
      "Loss for  38917 th iteration => 0.0077664983438\n",
      "Loss for  38918 th iteration => 0.00776634575914\n",
      "Loss for  38919 th iteration => 0.00776603145693\n",
      "Loss for  38920 th iteration => 0.00776585464226\n",
      "Loss for  38921 th iteration => 0.00776557420172\n",
      "Loss for  38922 th iteration => 0.00776543476757\n",
      "Loss for  38923 th iteration => 0.00776524469175\n",
      "Loss for  38924 th iteration => 0.00776492470743\n",
      "Loss for  38925 th iteration => 0.0077647881401\n",
      "Loss for  38926 th iteration => 0.00776450337545\n",
      "Loss for  38927 th iteration => 0.00776433177376\n",
      "Loss for  38928 th iteration => 0.00776408198768\n",
      "Loss for  38929 th iteration => 0.00776387559074\n",
      "Loss for  38930 th iteration => 0.00776366054521\n",
      "Loss for  38931 th iteration => 0.00776341958911\n",
      "Loss for  38932 th iteration => 0.00776323904912\n",
      "Loss for  38933 th iteration => 0.00776296376696\n",
      "Loss for  38934 th iteration => 0.00776281750049\n",
      "Loss for  38935 th iteration => 0.00776251546939\n",
      "Loss for  38936 th iteration => 0.00776229698848\n",
      "Loss for  38937 th iteration => 0.0077620785203\n",
      "Loss for  38938 th iteration => 0.00776186006484\n",
      "Loss for  38939 th iteration => 0.00776164282055\n",
      "Loss for  38940 th iteration => 0.00776152682112\n",
      "Loss for  38941 th iteration => 0.00776121913675\n",
      "Loss for  38942 th iteration => 0.00776107408878\n",
      "Loss for  38943 th iteration => 0.0077607968283\n",
      "Loss for  38944 th iteration => 0.00776075177396\n",
      "Loss for  38945 th iteration => 0.00776039059416\n",
      "Loss for  38946 th iteration => 0.00776027822932\n",
      "Loss for  38947 th iteration => 0.00775998407993\n",
      "Loss for  38948 th iteration => 0.00775980514912\n",
      "Loss for  38949 th iteration => 0.00775957728965\n",
      "Loss for  38950 th iteration => 0.00775933252683\n",
      "Loss for  38951 th iteration => 0.00775917022733\n",
      "Loss for  38952 th iteration => 0.00775886151399\n",
      "Loss for  38953 th iteration => 0.00775867942416\n",
      "Loss for  38954 th iteration => 0.00775840500921\n",
      "Loss for  38955 th iteration => 0.00775826076986\n",
      "Loss for  38956 th iteration => 0.00775807611223\n",
      "Loss for  38957 th iteration => 0.00775775089843\n",
      "Loss for  38958 th iteration => 0.00775762031573\n",
      "Loss for  38959 th iteration => 0.00775733038417\n",
      "Loss for  38960 th iteration => 0.00775716470508\n",
      "Loss for  38961 th iteration => 0.00775690981369\n",
      "Loss for  38962 th iteration => 0.00775670927828\n",
      "Loss for  38963 th iteration => 0.00775648918809\n",
      "Loss for  38964 th iteration => 0.00775625403338\n",
      "Loss for  38965 th iteration => 0.00775606850845\n",
      "Loss for  38966 th iteration => 0.00775579896846\n",
      "Loss for  38967 th iteration => 0.00775564777584\n",
      "Loss for  38968 th iteration => 0.00775534558282\n",
      "Loss for  38969 th iteration => 0.007755127499\n",
      "Loss for  38970 th iteration => 0.00775490942789\n",
      "Loss for  38971 th iteration => 0.00775469199927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  38972 th iteration => 0.00775458125797\n",
      "Loss for  38973 th iteration => 0.00775426911093\n",
      "Loss for  38974 th iteration => 0.00775412925353\n",
      "Loss for  38975 th iteration => 0.00775384687126\n",
      "Loss for  38976 th iteration => 0.00775381317138\n",
      "Loss for  38977 th iteration => 0.00775344147306\n",
      "Loss for  38978 th iteration => 0.00775334029393\n",
      "Loss for  38979 th iteration => 0.0077530357939\n",
      "Loss for  38980 th iteration => 0.00775286788232\n",
      "Loss for  38981 th iteration => 0.00775262983782\n",
      "Loss for  38982 th iteration => 0.00775239592997\n",
      "Loss for  38983 th iteration => 0.00775222360884\n",
      "Loss for  38984 th iteration => 0.00775192443045\n",
      "Loss for  38985 th iteration => 0.00775181711091\n",
      "Loss for  38986 th iteration => 0.00775146744062\n",
      "Loss for  38987 th iteration => 0.00775131652998\n",
      "Loss for  38988 th iteration => 0.00775114150011\n",
      "Loss for  38989 th iteration => 0.00775091343626\n",
      "Loss for  38990 th iteration => 0.0077506863673\n",
      "Loss for  38991 th iteration => 0.00775039000222\n",
      "Loss for  38992 th iteration => 0.00775023160015\n",
      "Loss for  38993 th iteration => 0.0077499701356\n",
      "Loss for  38994 th iteration => 0.00774977701542\n",
      "Loss for  38995 th iteration => 0.00774955021452\n",
      "Loss for  38996 th iteration => 0.00774932261116\n",
      "Loss for  38997 th iteration => 0.00774913024006\n",
      "Loss for  38998 th iteration => 0.00774886841341\n",
      "Loss for  38999 th iteration => 0.00774880605046\n",
      "Loss for  39000 th iteration => 0.00774842718633\n",
      "Loss for  39001 th iteration => 0.00774837040714\n",
      "Loss for  39002 th iteration => 0.00774800216367\n",
      "Loss for  39003 th iteration => 0.00774782472903\n",
      "Loss for  39004 th iteration => 0.0077477002749\n",
      "Loss for  39005 th iteration => 0.00774739040543\n",
      "Loss for  39006 th iteration => 0.00774725766366\n",
      "Loss for  39007 th iteration => 0.00774695612105\n",
      "Loss for  39008 th iteration => 0.00774681518121\n",
      "Loss for  39009 th iteration => 0.00774652187555\n",
      "Loss for  39010 th iteration => 0.00774637282605\n",
      "Loss for  39011 th iteration => 0.00774609531227\n",
      "Loss for  39012 th iteration => 0.00774584975232\n",
      "Loss for  39013 th iteration => 0.00774567670054\n",
      "Loss for  39014 th iteration => 0.00774539472068\n",
      "Loss for  39015 th iteration => 0.00774525894182\n",
      "Loss for  39016 th iteration => 0.00774506782163\n",
      "Loss for  39017 th iteration => 0.00774475090627\n",
      "Loss for  39018 th iteration => 0.00774461349722\n",
      "Loss for  39019 th iteration => 0.00774433179006\n",
      "Loss for  39020 th iteration => 0.00774415935821\n",
      "Loss for  39021 th iteration => 0.00774391261765\n",
      "Loss for  39022 th iteration => 0.0077437054026\n",
      "Loss for  39023 th iteration => 0.00774349339013\n",
      "Loss for  39024 th iteration => 0.00774325162846\n",
      "Loss for  39025 th iteration => 0.00774307410858\n",
      "Loss for  39026 th iteration => 0.00774279803386\n",
      "Loss for  39027 th iteration => 0.00774265477407\n",
      "Loss for  39028 th iteration => 0.00774235397415\n",
      "Loss for  39029 th iteration => 0.00774213660694\n",
      "Loss for  39030 th iteration => 0.0077419192524\n",
      "Loss for  39031 th iteration => 0.00774170236267\n",
      "Loss for  39032 th iteration => 0.00774158545706\n",
      "Loss for  39033 th iteration => 0.00774128087668\n",
      "Loss for  39034 th iteration => 0.00774113491667\n",
      "Loss for  39035 th iteration => 0.00774086012699\n",
      "Loss for  39036 th iteration => 0.00774081091921\n",
      "Loss for  39037 th iteration => 0.0077404560708\n",
      "Loss for  39038 th iteration => 0.00774033957189\n",
      "Loss for  39039 th iteration => 0.00774005173469\n",
      "Loss for  39040 th iteration => 0.00773986868866\n",
      "Loss for  39041 th iteration => 0.00773964712268\n",
      "Loss for  39042 th iteration => 0.00773939826299\n",
      "Loss for  39043 th iteration => 0.00773924223876\n",
      "Loss for  39044 th iteration => 0.00773893217711\n",
      "Loss for  39045 th iteration => 0.00773875436618\n",
      "Loss for  39046 th iteration => 0.00773847786777\n",
      "Loss for  39047 th iteration => 0.00773833704193\n",
      "Loss for  39048 th iteration => 0.00773815159627\n",
      "Loss for  39049 th iteration => 0.00773782999955\n",
      "Loss for  39050 th iteration => 0.00773769799862\n",
      "Loss for  39051 th iteration => 0.00773741166974\n",
      "Loss for  39052 th iteration => 0.0077372445869\n",
      "Loss for  39053 th iteration => 0.00773699328331\n",
      "Loss for  39054 th iteration => 0.00773679135913\n",
      "Loss for  39055 th iteration => 0.00773657484135\n",
      "Loss for  39056 th iteration => 0.00773633831334\n",
      "Loss for  39057 th iteration => 0.00773615634495\n",
      "Loss for  39058 th iteration => 0.00773588544759\n",
      "Loss for  39059 th iteration => 0.00773573779518\n",
      "Loss for  39060 th iteration => 0.00773543757921\n",
      "Loss for  39061 th iteration => 0.00773522059374\n",
      "Loss for  39062 th iteration => 0.00773500362092\n",
      "Loss for  39063 th iteration => 0.00773478749271\n",
      "Loss for  39064 th iteration => 0.00773467373104\n",
      "Loss for  39065 th iteration => 0.00773436678415\n",
      "Loss for  39066 th iteration => 0.00773422392894\n",
      "Loss for  39067 th iteration => 0.00773394726148\n",
      "Loss for  39068 th iteration => 0.00773390472545\n",
      "Loss for  39069 th iteration => 0.007733544005\n",
      "Loss for  39070 th iteration => 0.00773343407831\n",
      "Loss for  39071 th iteration => 0.0077331404681\n",
      "Loss for  39072 th iteration => 0.00773296389601\n",
      "Loss for  39073 th iteration => 0.00773273665484\n",
      "Loss for  39074 th iteration => 0.00773249417203\n",
      "Loss for  39075 th iteration => 0.00773233256918\n",
      "Loss for  39076 th iteration => 0.00773202578261\n",
      "Loss for  39077 th iteration => 0.00773184498506\n",
      "Loss for  39078 th iteration => 0.00773157220718\n",
      "Loss for  39079 th iteration => 0.00773142881794\n",
      "Loss for  39080 th iteration => 0.00773124663169\n",
      "Loss for  39081 th iteration => 0.00773092196425\n",
      "Loss for  39082 th iteration => 0.00773079377133\n",
      "Loss for  39083 th iteration => 0.0077305044094\n",
      "Loss for  39084 th iteration => 0.00773034109724\n",
      "Loss for  39085 th iteration => 0.00773008679762\n",
      "Loss for  39086 th iteration => 0.00772988860743\n",
      "Loss for  39087 th iteration => 0.00772966913\n",
      "Loss for  39088 th iteration => 0.00772943629992\n",
      "Loss for  39089 th iteration => 0.00772925140765\n",
      "Loss for  39090 th iteration => 0.00772898417279\n",
      "Loss for  39091 th iteration => 0.00772883363161\n",
      "Loss for  39092 th iteration => 0.00772853331901\n",
      "Loss for  39093 th iteration => 0.00772831671444\n",
      "Loss for  39094 th iteration => 0.00772810022224\n",
      "Loss for  39095 th iteration => 0.00772799022179\n",
      "Loss for  39096 th iteration => 0.00772768027023\n",
      "Loss for  39097 th iteration => 0.00772754112505\n",
      "Loss for  39098 th iteration => 0.00772726055536\n",
      "Loss for  39099 th iteration => 0.00772722604422\n",
      "Loss for  39100 th iteration => 0.00772685808378\n",
      "Loss for  39101 th iteration => 0.00772675605858\n",
      "Loss for  39102 th iteration => 0.00772645533117\n",
      "Loss for  39103 th iteration => 0.0077262865388\n",
      "Loss for  39104 th iteration => 0.00772605230155\n",
      "Loss for  39105 th iteration => 0.00772581747832\n",
      "Loss for  39106 th iteration => 0.00772564899891\n",
      "Loss for  39107 th iteration => 0.00772534887069\n",
      "Loss for  39108 th iteration => 0.0077252454272\n",
      "Loss for  39109 th iteration => 0.00772489607882\n",
      "Loss for  39110 th iteration => 0.00772474769689\n",
      "Loss for  39111 th iteration => 0.00772444503793\n",
      "Loss for  39112 th iteration => 0.00772445887544\n",
      "Loss for  39113 th iteration => 0.00772402957478\n",
      "Loss for  39114 th iteration => 0.00772390478386\n",
      "Loss for  39115 th iteration => 0.00772361269538\n",
      "Loss for  39116 th iteration => 0.00772345297223\n",
      "Loss for  39117 th iteration => 0.00772319575997\n",
      "Loss for  39118 th iteration => 0.00772300134301\n",
      "Loss for  39119 th iteration => 0.00772277876961\n",
      "Loss for  39120 th iteration => 0.00772254989426\n",
      "Loss for  39121 th iteration => 0.00772236251179\n",
      "Loss for  39122 th iteration => 0.00772219216157\n",
      "Loss for  39123 th iteration => 0.00772195885941\n",
      "Loss for  39124 th iteration => 0.00772172484238\n",
      "Loss for  39125 th iteration => 0.00772155543925\n",
      "Loss for  39126 th iteration => 0.00772139364497\n",
      "Loss for  39127 th iteration => 0.00772105499271\n",
      "Loss for  39128 th iteration => 0.00772092450767\n",
      "Loss for  39129 th iteration => 0.00772065271325\n",
      "Loss for  39130 th iteration => 0.00772045583296\n",
      "Loss for  39131 th iteration => 0.00772025015875\n",
      "Loss for  39132 th iteration => 0.00771998761433\n",
      "Loss for  39133 th iteration => 0.00771984733317\n",
      "Loss for  39134 th iteration => 0.0077195306906\n",
      "Loss for  39135 th iteration => 0.00771936170674\n",
      "Loss for  39136 th iteration => 0.00771907854723\n",
      "Loss for  39137 th iteration => 0.00771894591997\n",
      "Loss for  39138 th iteration => 0.00771864358667\n",
      "Loss for  39139 th iteration => 0.00771853862991\n",
      "Loss for  39140 th iteration => 0.00771822742011\n",
      "Loss for  39141 th iteration => 0.00771808724246\n",
      "Loss for  39142 th iteration => 0.00771781119588\n",
      "Loss for  39143 th iteration => 0.00771763604025\n",
      "Loss for  39144 th iteration => 0.00771739491509\n",
      "Loss for  39145 th iteration => 0.00771718502127\n",
      "Loss for  39146 th iteration => 0.00771697857881\n",
      "Loss for  39147 th iteration => 0.00771673418358\n",
      "Loss for  39148 th iteration => 0.00771656218811\n",
      "Loss for  39149 th iteration => 0.00771628352526\n",
      "Loss for  39150 th iteration => 0.00771614574407\n",
      "Loss for  39151 th iteration => 0.00771584557832\n",
      "Loss for  39152 th iteration => 0.00771562967148\n",
      "Loss for  39153 th iteration => 0.00771541511302\n",
      "Loss for  39154 th iteration => 0.00771529408428\n",
      "Loss for  39155 th iteration => 0.00771499650326\n",
      "Loss for  39156 th iteration => 0.00771484644267\n",
      "Loss for  39157 th iteration => 0.00771457922495\n",
      "Loss for  39158 th iteration => 0.00771451994219\n",
      "Loss for  39159 th iteration => 0.00771417800168\n",
      "Loss for  39160 th iteration => 0.00771405152121\n",
      "Loss for  39161 th iteration => 0.00771377649909\n",
      "Loss for  39162 th iteration => 0.00771358356324\n",
      "Loss for  39163 th iteration => 0.00771337472121\n",
      "Loss for  39164 th iteration => 0.00771311606174\n",
      "Loss for  39165 th iteration => 0.00771297267197\n",
      "Loss for  39166 th iteration => 0.00771265745691\n",
      "Loss for  39167 th iteration => 0.0077124868503\n",
      "Loss for  39168 th iteration => 0.0077122060526\n",
      "Loss for  39169 th iteration => 0.00771207262194\n",
      "Loss for  39170 th iteration => 0.00771188312911\n",
      "Loss for  39171 th iteration => 0.00771156794445\n",
      "Loss for  39172 th iteration => 0.00771143243973\n",
      "Loss for  39173 th iteration => 0.00771115247313\n",
      "Loss for  39174 th iteration => 0.00771098193612\n",
      "Loss for  39175 th iteration => 0.00771073694485\n",
      "Loss for  39176 th iteration => 0.00771053161626\n",
      "Loss for  39177 th iteration => 0.00771032136068\n",
      "Loss for  39178 th iteration => 0.00771008147819\n",
      "Loss for  39179 th iteration => 0.0077099057217\n",
      "Loss for  39180 th iteration => 0.00770963151998\n",
      "Loss for  39181 th iteration => 0.00770949002898\n",
      "Loss for  39182 th iteration => 0.00770919112865\n",
      "Loss for  39183 th iteration => 0.00770897558753\n",
      "Loss for  39184 th iteration => 0.00770876113676\n",
      "Loss for  39185 th iteration => 0.00770864278587\n",
      "Loss for  39186 th iteration => 0.00770834327182\n",
      "Loss for  39187 th iteration => 0.00770819585266\n",
      "Loss for  39188 th iteration => 0.0077079263763\n",
      "Loss for  39189 th iteration => 0.00770787272283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  39190 th iteration => 0.00770752591677\n",
      "Loss for  39191 th iteration => 0.00770740497818\n",
      "Loss for  39192 th iteration => 0.00770712517752\n",
      "Loss for  39193 th iteration => 0.00770693769721\n",
      "Loss for  39194 th iteration => 0.00770672416253\n",
      "Loss for  39195 th iteration => 0.00770647087339\n",
      "Loss for  39196 th iteration => 0.00770632287576\n",
      "Loss for  39197 th iteration => 0.00770601101424\n",
      "Loss for  39198 th iteration => 0.00770583795781\n",
      "Loss for  39199 th iteration => 0.00770556031477\n",
      "Loss for  39200 th iteration => 0.0077054241273\n",
      "Loss for  39201 th iteration => 0.00770523808059\n",
      "Loss for  39202 th iteration => 0.00770492043118\n",
      "Loss for  39203 th iteration => 0.00770478809917\n",
      "Loss for  39204 th iteration => 0.00770450570211\n",
      "Loss for  39205 th iteration => 0.00770433830385\n",
      "Loss for  39206 th iteration => 0.00770409091578\n",
      "Loss for  39207 th iteration => 0.00770388869261\n",
      "Loss for  39208 th iteration => 0.00770367607326\n",
      "Loss for  39209 th iteration => 0.00770343926349\n",
      "Loss for  39210 th iteration => 0.00770326117564\n",
      "Loss for  39211 th iteration => 0.00770299001455\n",
      "Loss for  39212 th iteration => 0.00770284622398\n",
      "Loss for  39213 th iteration => 0.0077025479454\n",
      "Loss for  39214 th iteration => 0.00770233276922\n",
      "Loss for  39215 th iteration => 0.0077021189467\n",
      "Loss for  39216 th iteration => 0.00770200212812\n",
      "Loss for  39217 th iteration => 0.0077017018182\n",
      "Loss for  39218 th iteration => 0.00770155591017\n",
      "Loss for  39219 th iteration => 0.00770128594668\n",
      "Loss for  39220 th iteration => 0.00770123537836\n",
      "Loss for  39221 th iteration => 0.00770088623275\n",
      "Loss for  39222 th iteration => 0.00770076832897\n",
      "Loss for  39223 th iteration => 0.00770048623887\n",
      "Loss for  39224 th iteration => 0.0077003017436\n",
      "Loss for  39225 th iteration => 0.00770008596902\n",
      "Loss for  39226 th iteration => 0.00769983561572\n",
      "Loss for  39227 th iteration => 0.00769968542718\n",
      "Loss for  39228 th iteration => 0.00769937509097\n",
      "Loss for  39229 th iteration => 0.00769920092283\n",
      "Loss for  39230 th iteration => 0.00769892510479\n",
      "Loss for  39231 th iteration => 0.0076987880396\n",
      "Loss for  39232 th iteration => 0.00769860360627\n",
      "Loss for  39233 th iteration => 0.00769828471747\n",
      "Loss for  39234 th iteration => 0.00769815434045\n",
      "Loss for  39235 th iteration => 0.00769787072171\n",
      "Loss for  39236 th iteration => 0.00769770526092\n",
      "Loss for  39237 th iteration => 0.00769745666846\n",
      "Loss for  39238 th iteration => 0.00769725636565\n",
      "Loss for  39239 th iteration => 0.00769704255882\n",
      "Loss for  39240 th iteration => 0.00769680765268\n",
      "Loss for  39241 th iteration => 0.00769662839387\n",
      "Loss for  39242 th iteration => 0.00769635912006\n",
      "Loss for  39243 th iteration => 0.00769621417467\n",
      "Loss for  39244 th iteration => 0.00769591599971\n",
      "Loss for  39245 th iteration => 0.00769570151189\n",
      "Loss for  39246 th iteration => 0.00769558722911\n",
      "Loss for  39247 th iteration => 0.00769528510153\n",
      "Loss for  39248 th iteration => 0.00769514169103\n",
      "Loss for  39249 th iteration => 0.00769486866219\n",
      "Loss for  39250 th iteration => 0.00769469835507\n",
      "Loss for  39251 th iteration => 0.00769457850697\n",
      "Loss for  39252 th iteration => 0.00769426983368\n",
      "Loss for  39253 th iteration => 0.00769414167275\n",
      "Loss for  39254 th iteration => 0.0076938413515\n",
      "Loss for  39255 th iteration => 0.00769370496752\n",
      "Loss for  39256 th iteration => 0.00769341290812\n",
      "Loss for  39257 th iteration => 0.00769326838975\n",
      "Loss for  39258 th iteration => 0.0076929880158\n",
      "Loss for  39259 th iteration => 0.00769275015728\n",
      "Loss for  39260 th iteration => 0.00769257527125\n",
      "Loss for  39261 th iteration => 0.00769230088612\n",
      "Loss for  39262 th iteration => 0.00769216387798\n",
      "Loss for  39263 th iteration => 0.00769197969387\n",
      "Loss for  39264 th iteration => 0.00769166076707\n",
      "Loss for  39265 th iteration => 0.0076915311457\n",
      "Loss for  39266 th iteration => 0.00769124749982\n",
      "Loss for  39267 th iteration => 0.00769108278395\n",
      "Loss for  39268 th iteration => 0.00769083417491\n",
      "Loss for  39269 th iteration => 0.0076906346066\n",
      "Loss for  39270 th iteration => 0.00769042079342\n",
      "Loss for  39271 th iteration => 0.00769018661166\n",
      "Loss for  39272 th iteration => 0.00769000735643\n",
      "Loss for  39273 th iteration => 0.0076897387972\n",
      "Loss for  39274 th iteration => 0.00768959386501\n",
      "Loss for  39275 th iteration => 0.00768929526284\n",
      "Loss for  39276 th iteration => 0.0076890816172\n",
      "Loss for  39277 th iteration => 0.00768896832967\n",
      "Loss for  39278 th iteration => 0.007688665933\n",
      "Loss for  39279 th iteration => 0.00768852351217\n",
      "Loss for  39280 th iteration => 0.00768825111337\n",
      "Loss for  39281 th iteration => 0.00768820696318\n",
      "Loss for  39282 th iteration => 0.00768785286134\n",
      "Loss for  39283 th iteration => 0.00768774127693\n",
      "Loss for  39284 th iteration => 0.00768745432891\n",
      "Loss for  39285 th iteration => 0.0076872760554\n",
      "Loss for  39286 th iteration => 0.00768705552007\n",
      "Loss for  39287 th iteration => 0.00768681129206\n",
      "Loss for  39288 th iteration => 0.00768665643878\n",
      "Loss for  39289 th iteration => 0.00768634999157\n",
      "Loss for  39290 th iteration => 0.00768617379643\n",
      "Loss for  39291 th iteration => 0.00768590140272\n",
      "Loss for  39292 th iteration => 0.0076857622715\n",
      "Loss for  39293 th iteration => 0.0076855809695\n",
      "Loss for  39294 th iteration => 0.00768526063614\n",
      "Loss for  39295 th iteration => 0.0076851331061\n",
      "Loss for  39296 th iteration => 0.00768484807767\n",
      "Loss for  39297 th iteration => 0.00768468542939\n",
      "Loss for  39298 th iteration => 0.00768443546128\n",
      "Loss for  39299 th iteration => 0.00768423793735\n",
      "Loss for  39300 th iteration => 0.00768402278807\n",
      "Loss for  39301 th iteration => 0.00768379062798\n",
      "Loss for  39302 th iteration => 0.00768361005911\n",
      "Loss for  39303 th iteration => 0.00768334349934\n",
      "Loss for  39304 th iteration => 0.00768319727546\n",
      "Loss for  39305 th iteration => 0.00768289980106\n",
      "Loss for  39306 th iteration => 0.00768268631619\n",
      "Loss for  39307 th iteration => 0.0076825737442\n",
      "Loss for  39308 th iteration => 0.00768227133633\n",
      "Loss for  39309 th iteration => 0.0076821296169\n",
      "Loss for  39310 th iteration => 0.00768185692954\n",
      "Loss for  39311 th iteration => 0.00768181403866\n",
      "Loss for  39312 th iteration => 0.00768145938489\n",
      "Loss for  39313 th iteration => 0.00768134903199\n",
      "Loss for  39314 th iteration => 0.00768106155972\n",
      "Loss for  39315 th iteration => 0.00768088449023\n",
      "Loss for  39316 th iteration => 0.00768066345803\n",
      "Loss for  39317 th iteration => 0.00768042040682\n",
      "Loss for  39318 th iteration => 0.00768026508375\n",
      "Loss for  39319 th iteration => 0.00767995964956\n",
      "Loss for  39320 th iteration => 0.00767978329687\n",
      "Loss for  39321 th iteration => 0.00767951175021\n",
      "Loss for  39322 th iteration => 0.00767937211953\n",
      "Loss for  39323 th iteration => 0.00767919211203\n",
      "Loss for  39324 th iteration => 0.00767887143312\n",
      "Loss for  39325 th iteration => 0.00767874493989\n",
      "Loss for  39326 th iteration => 0.00767845957562\n",
      "Loss for  39327 th iteration => 0.00767829795457\n",
      "Loss for  39328 th iteration => 0.00767804766003\n",
      "Loss for  39329 th iteration => 0.00767785115407\n",
      "Loss for  39330 th iteration => 0.00767763568743\n",
      "Loss for  39331 th iteration => 0.00767740453637\n",
      "Loss for  39332 th iteration => 0.0076772236589\n",
      "Loss for  39333 th iteration => 0.00767695809953\n",
      "Loss for  39334 th iteration => 0.00767681157551\n",
      "Loss for  39335 th iteration => 0.0076765147765\n",
      "Loss for  39336 th iteration => 0.00767630181565\n",
      "Loss for  39337 th iteration => 0.00767618915834\n",
      "Loss for  39338 th iteration => 0.00767588753389\n",
      "Loss for  39339 th iteration => 0.00767574572565\n",
      "Loss for  39340 th iteration => 0.00767547398681\n",
      "Loss for  39341 th iteration => 0.00767543057773\n",
      "Loss for  39342 th iteration => 0.00767507713645\n",
      "Loss for  39343 th iteration => 0.0076749662635\n",
      "Loss for  39344 th iteration => 0.00767468000561\n",
      "Loss for  39345 th iteration => 0.00767450241412\n",
      "Loss for  39346 th iteration => 0.00767428259827\n",
      "Loss for  39347 th iteration => 0.00767403902304\n",
      "Loss for  39348 th iteration => 0.00767388491837\n",
      "Loss for  39349 th iteration => 0.00767357922056\n",
      "Loss for  39350 th iteration => 0.00767340364438\n",
      "Loss for  39351 th iteration => 0.00767313201623\n",
      "Loss for  39352 th iteration => 0.00767299320371\n",
      "Loss for  39353 th iteration => 0.00767281319782\n",
      "Loss for  39354 th iteration => 0.00767249304227\n",
      "Loss for  39355 th iteration => 0.0076723667219\n",
      "Loss for  39356 th iteration => 0.0076720818791\n",
      "Loss for  39357 th iteration => 0.00767192043285\n",
      "Loss for  39358 th iteration => 0.00767167065771\n",
      "Loss for  39359 th iteration => 0.00767147432864\n",
      "Loss for  39360 th iteration => 0.0076712593792\n",
      "Loss for  39361 th iteration => 0.00767102840728\n",
      "Loss for  39362 th iteration => 0.00767084804462\n",
      "Loss for  39363 th iteration => 0.00767058266679\n",
      "Loss for  39364 th iteration => 0.00767043665505\n",
      "Loss for  39365 th iteration => 0.00767014016373\n",
      "Loss for  39366 th iteration => 0.00766992801903\n",
      "Loss for  39367 th iteration => 0.00766981462585\n",
      "Loss for  39368 th iteration => 0.00766951442999\n",
      "Loss for  39369 th iteration => 0.00766937189104\n",
      "Loss for  39370 th iteration => 0.00766910210113\n",
      "Loss for  39371 th iteration => 0.00766905673142\n",
      "Loss for  39372 th iteration => 0.00766870593414\n",
      "Loss for  39373 th iteration => 0.00766859311979\n",
      "Loss for  39374 th iteration => 0.00766830948681\n",
      "Loss for  39375 th iteration => 0.00766812997276\n",
      "Loss for  39376 th iteration => 0.00766791276313\n",
      "Loss for  39377 th iteration => 0.00766766728379\n",
      "Loss for  39378 th iteration => 0.00766751576702\n",
      "Loss for  39379 th iteration => 0.0076672087727\n",
      "Loss for  39380 th iteration => 0.00766703473015\n",
      "Loss for  39381 th iteration => 0.00766676226756\n",
      "Loss for  39382 th iteration => 0.00766662534187\n",
      "Loss for  39383 th iteration => 0.00766644428453\n",
      "Loss for  39384 th iteration => 0.00766612536485\n",
      "Loss for  39385 th iteration => 0.00766599850858\n",
      "Loss for  39386 th iteration => 0.00766571489029\n",
      "Loss for  39387 th iteration => 0.00766555291945\n",
      "Loss for  39388 th iteration => 0.00766530435743\n",
      "Loss for  39389 th iteration => 0.0076651075151\n",
      "Loss for  39390 th iteration => 0.00766489376737\n",
      "Loss for  39391 th iteration => 0.00766466229355\n",
      "Loss for  39392 th iteration => 0.00766448312118\n",
      "Loss for  39393 th iteration => 0.00766421725282\n",
      "Loss for  39394 th iteration => 0.00766407241991\n",
      "Loss for  39395 th iteration => 0.00766377615251\n",
      "Loss for  39396 th iteration => 0.00766366353859\n",
      "Loss for  39397 th iteration => 0.00766336324113\n",
      "Loss for  39398 th iteration => 0.00766322146287\n",
      "Loss for  39399 th iteration => 0.00766295029985\n",
      "Loss for  39400 th iteration => 0.00766278160539\n",
      "Loss for  39401 th iteration => 0.00766266321185\n",
      "Loss for  39402 th iteration => 0.00766235654989\n",
      "Loss for  39403 th iteration => 0.00766222984017\n",
      "Loss for  39404 th iteration => 0.00766193153355\n",
      "Loss for  39405 th iteration => 0.00766179659743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  39406 th iteration => 0.00766150655595\n",
      "Loss for  39407 th iteration => 0.0076613634821\n",
      "Loss for  39408 th iteration => 0.00766108540063\n",
      "Loss for  39409 th iteration => 0.00766084871291\n",
      "Loss for  39410 th iteration => 0.00766067614418\n",
      "Loss for  39411 th iteration => 0.00766040290538\n",
      "Loss for  39412 th iteration => 0.00766026816387\n",
      "Loss for  39413 th iteration => 0.00766008531097\n",
      "Loss for  39414 th iteration => 0.00765976841521\n",
      "Loss for  39415 th iteration => 0.00765964023398\n",
      "Loss for  39416 th iteration => 0.00765935862701\n",
      "Loss for  39417 th iteration => 0.00765919534375\n",
      "Loss for  39418 th iteration => 0.00765894878045\n",
      "Loss for  39419 th iteration => 0.00765875063826\n",
      "Loss for  39420 th iteration => 0.00765853887661\n",
      "Loss for  39421 th iteration => 0.0076583061155\n",
      "Loss for  39422 th iteration => 0.00765812891656\n",
      "Loss for  39423 th iteration => 0.00765786177352\n",
      "Loss for  39424 th iteration => 0.00765771890136\n",
      "Loss for  39425 th iteration => 0.00765742265246\n",
      "Loss for  39426 th iteration => 0.00765730946275\n",
      "Loss for  39427 th iteration => 0.00765701042781\n",
      "Loss for  39428 th iteration => 0.00765686808525\n",
      "Loss for  39429 th iteration => 0.0076565988624\n",
      "Loss for  39430 th iteration => 0.00765655251314\n",
      "Loss for  39431 th iteration => 0.00765620405069\n",
      "Loss for  39432 th iteration => 0.00765609025823\n",
      "Loss for  39433 th iteration => 0.00765580895872\n",
      "Loss for  39434 th iteration => 0.00765562846778\n",
      "Loss for  39435 th iteration => 0.00765541359045\n",
      "Loss for  39436 th iteration => 0.00765516713525\n",
      "Loss for  39437 th iteration => 0.00765501794982\n",
      "Loss for  39438 th iteration => 0.00765471113535\n",
      "Loss for  39439 th iteration => 0.00765453893446\n",
      "Loss for  39440 th iteration => 0.00765426598928\n",
      "Loss for  39441 th iteration => 0.00765413054762\n",
      "Loss for  39442 th iteration => 0.00765394920465\n",
      "Loss for  39443 th iteration => 0.00765363251709\n",
      "Loss for  39444 th iteration => 0.00765350479062\n",
      "Loss for  39445 th iteration => 0.00765322339857\n",
      "Loss for  39446 th iteration => 0.00765306056349\n",
      "Loss for  39447 th iteration => 0.00765281422151\n",
      "Loss for  39448 th iteration => 0.00765261652123\n",
      "Loss for  39449 th iteration => 0.007652404987\n",
      "Loss for  39450 th iteration => 0.00765217266184\n",
      "Loss for  39451 th iteration => 0.0076519956961\n",
      "Loss for  39452 th iteration => 0.00765172898335\n",
      "Loss for  39453 th iteration => 0.00765158634987\n",
      "Loss for  39454 th iteration => 0.00765129102412\n",
      "Loss for  39455 th iteration => 0.0076511773522\n",
      "Loss for  39456 th iteration => 0.00765087946682\n",
      "Loss for  39457 th iteration => 0.0076507366402\n",
      "Loss for  39458 th iteration => 0.00765046791475\n",
      "Loss for  39459 th iteration => 0.00765042015974\n",
      "Loss for  39460 th iteration => 0.00765007376401\n",
      "Loss for  39461 th iteration => 0.00764995857322\n",
      "Loss for  39462 th iteration => 0.00764967933307\n",
      "Loss for  39463 th iteration => 0.00764949745104\n",
      "Loss for  39464 th iteration => 0.00764928462589\n",
      "Loss for  39465 th iteration => 0.00764903678669\n",
      "Loss for  39466 th iteration => 0.0076488896464\n",
      "Loss for  39467 th iteration => 0.00764858272912\n",
      "Loss for  39468 th iteration => 0.00764841176146\n",
      "Loss for  39469 th iteration => 0.00764813824964\n",
      "Loss for  39470 th iteration => 0.00764800377473\n",
      "Loss for  39471 th iteration => 0.00764770494654\n",
      "Loss for  39472 th iteration => 0.00764760971142\n",
      "Loss for  39473 th iteration => 0.00764729655808\n",
      "Loss for  39474 th iteration => 0.00764716601014\n",
      "Loss for  39475 th iteration => 0.00764688811007\n",
      "Loss for  39476 th iteration => 0.00764672249544\n",
      "Loss for  39477 th iteration => 0.0076464796036\n",
      "Loss for  39478 th iteration => 0.00764627916528\n",
      "Loss for  39479 th iteration => 0.00764607103975\n",
      "Loss for  39480 th iteration => 0.00764583601765\n",
      "Loss for  39481 th iteration => 0.00764566241959\n",
      "Loss for  39482 th iteration => 0.00764539305062\n",
      "Loss for  39483 th iteration => 0.00764525377484\n",
      "Loss for  39484 th iteration => 0.00764505460171\n",
      "Loss for  39485 th iteration => 0.00764475835433\n",
      "Loss for  39486 th iteration => 0.0076446145571\n",
      "Loss for  39487 th iteration => 0.00764434743\n",
      "Loss for  39488 th iteration => 0.00764417645722\n",
      "Loss for  39489 th iteration => 0.00764405782914\n",
      "Loss for  39490 th iteration => 0.00764375341492\n",
      "Loss for  39491 th iteration => 0.00764362648089\n",
      "Loss for  39492 th iteration => 0.00764333041171\n",
      "Loss for  39493 th iteration => 0.00764319526135\n",
      "Loss for  39494 th iteration => 0.00764290744716\n",
      "Loss for  39495 th iteration => 0.00764276416901\n",
      "Loss for  39496 th iteration => 0.00764249162787\n",
      "Loss for  39497 th iteration => 0.00764225180694\n",
      "Loss for  39498 th iteration => 0.00764208437868\n",
      "Loss for  39499 th iteration => 0.00764180803618\n",
      "Loss for  39500 th iteration => 0.00764167819427\n",
      "Loss for  39501 th iteration => 0.0076414925483\n",
      "Loss for  39502 th iteration => 0.0076411807788\n",
      "Loss for  39503 th iteration => 0.00764104951066\n",
      "Loss for  39504 th iteration => 0.00764077299438\n",
      "Loss for  39505 th iteration => 0.00764060665968\n",
      "Loss for  39506 th iteration => 0.00764036515136\n",
      "Loss for  39507 th iteration => 0.00764016399332\n",
      "Loss for  39508 th iteration => 0.00763995725082\n",
      "Loss for  39509 th iteration => 0.00763972150959\n",
      "Loss for  39510 th iteration => 0.00763954929381\n",
      "Loss for  39511 th iteration => 0.00763927920653\n",
      "Loss for  39512 th iteration => 0.0076391412814\n",
      "Loss for  39513 th iteration => 0.00763884672067\n",
      "Loss for  39514 th iteration => 0.00763872986448\n",
      "Loss for  39515 th iteration => 0.00763843649933\n",
      "Loss for  39516 th iteration => 0.00763829052557\n",
      "Loss for  39517 th iteration => 0.00763802754487\n",
      "Loss for  39518 th iteration => 0.00763797138767\n",
      "Loss for  39519 th iteration => 0.00763763469274\n",
      "Loss for  39520 th iteration => 0.00763751120866\n",
      "Loss for  39521 th iteration => 0.00763724156106\n",
      "Loss for  39522 th iteration => 0.00763705149289\n",
      "Loss for  39523 th iteration => 0.0076368481538\n",
      "Loss for  39524 th iteration => 0.00763659223384\n",
      "Loss for  39525 th iteration => 0.00763645447485\n",
      "Loss for  39526 th iteration => 0.00763614243293\n",
      "Loss for  39527 th iteration => 0.00763597716448\n",
      "Loss for  39528 th iteration => 0.00763569933285\n",
      "Loss for  39529 th iteration => 0.00763557132933\n",
      "Loss for  39530 th iteration => 0.00763538468472\n",
      "Loss for  39531 th iteration => 0.00763507482056\n",
      "Loss for  39532 th iteration => 0.00763494231819\n",
      "Loss for  39533 th iteration => 0.00763466769157\n",
      "Loss for  39534 th iteration => 0.00763450013826\n",
      "Loss for  39535 th iteration => 0.00763426050391\n",
      "Loss for  39536 th iteration => 0.0076340581429\n",
      "Loss for  39537 th iteration => 0.00763385325866\n",
      "Loss for  39538 th iteration => 0.0076336163301\n",
      "Loss for  39539 th iteration => 0.00763344595688\n",
      "Loss for  39540 th iteration => 0.0076331746979\n",
      "Loss for  39541 th iteration => 0.00763303859964\n",
      "Loss for  39542 th iteration => 0.00763274481097\n",
      "Loss for  39543 th iteration => 0.00763262620971\n",
      "Loss for  39544 th iteration => 0.00763233524514\n",
      "Loss for  39545 th iteration => 0.00763218754126\n",
      "Loss for  39546 th iteration => 0.00763192699128\n",
      "Loss for  39547 th iteration => 0.00763186663703\n",
      "Loss for  39548 th iteration => 0.00763153477753\n",
      "Loss for  39549 th iteration => 0.00763140714469\n",
      "Loss for  39550 th iteration => 0.00763114228454\n",
      "Loss for  39551 th iteration => 0.00763094811511\n",
      "Loss for  39552 th iteration => 0.00763074951624\n",
      "Loss for  39553 th iteration => 0.00763048954178\n",
      "Loss for  39554 th iteration => 0.00763035647652\n",
      "Loss for  39555 th iteration => 0.00763004233735\n",
      "Loss for  39556 th iteration => 0.00762987976273\n",
      "Loss for  39557 th iteration => 0.00762959991057\n",
      "Loss for  39558 th iteration => 0.00762947450641\n",
      "Loss for  39559 th iteration => 0.00762928610611\n",
      "Loss for  39560 th iteration => 0.00762897866213\n",
      "Loss for  39561 th iteration => 0.00762884441303\n",
      "Loss for  39562 th iteration => 0.00762857218416\n",
      "Loss for  39563 th iteration => 0.00762840290641\n",
      "Loss for  39564 th iteration => 0.00762816564751\n",
      "Loss for  39565 th iteration => 0.00762796158424\n",
      "Loss for  39566 th iteration => 0.00762775905322\n",
      "Loss for  39567 th iteration => 0.00762752044451\n",
      "Loss for  39568 th iteration => 0.00762735240238\n",
      "Loss for  39569 th iteration => 0.00762707948526\n",
      "Loss for  39570 th iteration => 0.00762694569604\n",
      "Loss for  39571 th iteration => 0.0076266526297\n",
      "Loss for  39572 th iteration => 0.00762653190786\n",
      "Loss for  39573 th iteration => 0.00762624371571\n",
      "Loss for  39574 th iteration => 0.00762609391122\n",
      "Loss for  39575 th iteration => 0.00762583636719\n",
      "Loss for  39576 th iteration => 0.00762577098531\n",
      "Loss for  39577 th iteration => 0.00762544478497\n",
      "Loss for  39578 th iteration => 0.00762531218496\n",
      "Loss for  39579 th iteration => 0.00762505292385\n",
      "Loss for  39580 th iteration => 0.00762485384679\n",
      "Loss for  39581 th iteration => 0.00762466078776\n",
      "Loss for  39582 th iteration => 0.00762439596428\n",
      "Loss for  39583 th iteration => 0.0076242683806\n",
      "Loss for  39584 th iteration => 0.00762395155043\n",
      "Loss for  39585 th iteration => 0.00762379210444\n",
      "Loss for  39586 th iteration => 0.00762350979882\n",
      "Loss for  39587 th iteration => 0.00762338761532\n",
      "Loss for  39588 th iteration => 0.00762319683696\n",
      "Loss for  39589 th iteration => 0.00762289224006\n",
      "Loss for  39590 th iteration => 0.00762275581894\n",
      "Loss for  39591 th iteration => 0.00762248640925\n",
      "Loss for  39592 th iteration => 0.00762231498722\n",
      "Loss for  39593 th iteration => 0.00762208051975\n",
      "Loss for  39594 th iteration => 0.00762187433977\n",
      "Loss for  39595 th iteration => 0.00762167457262\n",
      "Loss for  39596 th iteration => 0.00762143387458\n",
      "Loss for  39597 th iteration => 0.00762126856892\n",
      "Loss for  39598 th iteration => 0.0076209935897\n",
      "Loss for  39599 th iteration => 0.00762086265512\n",
      "Loss for  39600 th iteration => 0.00762065801981\n",
      "Loss for  39601 th iteration => 0.00762037080435\n",
      "Loss for  39602 th iteration => 0.00762022065445\n",
      "Loss for  39603 th iteration => 0.00761996249868\n",
      "Loss for  39604 th iteration => 0.00761978541761\n",
      "Loss for  39605 th iteration => 0.00761966573303\n",
      "Loss for  39606 th iteration => 0.00761936500714\n",
      "Loss for  39607 th iteration => 0.00761923704278\n",
      "Loss for  39608 th iteration => 0.00761894463558\n",
      "Loss for  39609 th iteration => 0.00761880848073\n",
      "Loss for  39610 th iteration => 0.00761852430253\n",
      "Loss for  39611 th iteration => 0.00761838004537\n",
      "Loss for  39612 th iteration => 0.00761811850797\n",
      "Loss for  39613 th iteration => 0.00761787039463\n",
      "Loss for  39614 th iteration => 0.00761771386056\n",
      "Loss for  39615 th iteration => 0.00761742931552\n",
      "Loss for  39616 th iteration => 0.00761731038835\n",
      "Loss for  39617 th iteration => 0.00761711679459\n",
      "Loss for  39618 th iteration => 0.00761681559402\n",
      "Loss for  39619 th iteration => 0.00761667644957\n",
      "Loss for  39620 th iteration => 0.00761641040917\n",
      "Loss for  39621 th iteration => 0.0076162362907\n",
      "Loss for  39622 th iteration => 0.00761600516562\n",
      "Loss for  39623 th iteration => 0.00761579631593\n",
      "Loss for  39624 th iteration => 0.00761559986442\n",
      "Loss for  39625 th iteration => 0.00761535652328\n",
      "Loss for  39626 th iteration => 0.00761519450665\n",
      "Loss for  39627 th iteration => 0.00761491691078\n",
      "Loss for  39628 th iteration => 0.00761478929797\n",
      "Loss for  39629 th iteration => 0.00761458263902\n",
      "Loss for  39630 th iteration => 0.00761429778808\n",
      "Loss for  39631 th iteration => 0.00761414594366\n",
      "Loss for  39632 th iteration => 0.00761389022993\n",
      "Loss for  39633 th iteration => 0.00761381904487\n",
      "Loss for  39634 th iteration => 0.00761349990883\n",
      "Loss for  39635 th iteration => 0.00761336156959\n",
      "Loss for  39636 th iteration => 0.00761310930922\n",
      "Loss for  39637 th iteration => 0.00761290455583\n",
      "Loss for  39638 th iteration => 0.00761271843501\n",
      "Loss for  39639 th iteration => 0.00761244799712\n",
      "Loss for  39640 th iteration => 0.00761232729007\n",
      "Loss for  39641 th iteration => 0.00761200881473\n",
      "Loss for  39642 th iteration => 0.00761185363467\n",
      "Loss for  39643 th iteration => 0.00761157014249\n",
      "Loss for  39644 th iteration => 0.00761135992642\n",
      "Loss for  39645 th iteration => 0.00761115162112\n",
      "Loss for  39646 th iteration => 0.00761104389577\n",
      "Loss for  39647 th iteration => 0.00761074718143\n",
      "Loss for  39648 th iteration => 0.00761060416763\n",
      "Loss for  39649 th iteration => 0.00761034268144\n",
      "Loss for  39650 th iteration => 0.00761016462643\n",
      "Loss for  39651 th iteration => 0.00760993812225\n",
      "Loss for  39652 th iteration => 0.00760972527012\n",
      "Loss for  39653 th iteration => 0.00760953350491\n",
      "Loss for  39654 th iteration => 0.00760928609668\n",
      "Loss for  39655 th iteration => 0.00760912883051\n",
      "Loss for  39656 th iteration => 0.00760884772135\n",
      "Loss for  39657 th iteration => 0.00760863916062\n",
      "Loss for  39658 th iteration => 0.00760851169655\n",
      "Loss for  39659 th iteration => 0.00760823224748\n",
      "Loss for  39660 th iteration => 0.00760807557828\n",
      "Loss for  39661 th iteration => 0.0076078253029\n",
      "Loss for  39662 th iteration => 0.00760763961039\n",
      "Loss for  39663 th iteration => 0.00760741832762\n",
      "Loss for  39664 th iteration => 0.00760720379136\n",
      "Loss for  39665 th iteration => 0.00760701272314\n",
      "Loss for  39666 th iteration => 0.00760689159808\n",
      "Loss for  39667 th iteration => 0.00760662295258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  39668 th iteration => 0.00760643845805\n",
      "Loss for  39669 th iteration => 0.00760616060897\n",
      "Loss for  39670 th iteration => 0.00760599808862\n",
      "Loss for  39671 th iteration => 0.0076057575383\n",
      "Loss for  39672 th iteration => 0.00760555791961\n",
      "Loss for  39673 th iteration => 0.00760535439755\n",
      "Loss for  39674 th iteration => 0.00760511794877\n",
      "Loss for  39675 th iteration => 0.00760495118792\n",
      "Loss for  39676 th iteration => 0.00760467817391\n",
      "Loss for  39677 th iteration => 0.00760454917335\n",
      "Loss for  39678 th iteration => 0.00760436635481\n",
      "Loss for  39679 th iteration => 0.00760405577766\n",
      "Loss for  39680 th iteration => 0.00760392732304\n",
      "Loss for  39681 th iteration => 0.00760365202136\n",
      "Loss for  39682 th iteration => 0.00760348847845\n",
      "Loss for  39683 th iteration => 0.00760324820562\n",
      "Loss for  39684 th iteration => 0.00760304981897\n",
      "Loss for  39685 th iteration => 0.0076028443315\n",
      "Loss for  39686 th iteration => 0.0076026113426\n",
      "Loss for  39687 th iteration => 0.00760244040008\n",
      "Loss for  39688 th iteration => 0.00760217304734\n",
      "Loss for  39689 th iteration => 0.00760203738836\n",
      "Loss for  39690 th iteration => 0.00760183787535\n",
      "Loss for  39691 th iteration => 0.00760154697565\n",
      "Loss for  39692 th iteration => 0.00760140251326\n",
      "Loss for  39693 th iteration => 0.00760114192389\n",
      "Loss for  39694 th iteration => 0.00760108647345\n",
      "Loss for  39695 th iteration => 0.00760075306607\n",
      "Loss for  39696 th iteration => 0.00760063025003\n",
      "Loss for  39697 th iteration => 0.00760036392877\n",
      "Loss for  39698 th iteration => 0.00760017448975\n",
      "Loss for  39699 th iteration => 0.00759997451591\n",
      "Loss for  39700 th iteration => 0.00759971918609\n",
      "Loss for  39701 th iteration => 0.00759958483137\n",
      "Loss for  39702 th iteration => 0.00759927400637\n",
      "Loss for  39703 th iteration => 0.00759911159009\n",
      "Loss for  39704 th iteration => 0.00759883488486\n",
      "Loss for  39705 th iteration => 0.00759870987322\n",
      "Loss for  39706 th iteration => 0.00759852398197\n",
      "Loss for  39707 th iteration => 0.00759821737277\n",
      "Loss for  39708 th iteration => 0.00759808560273\n",
      "Loss for  39709 th iteration => 0.00759781422982\n",
      "Loss for  39710 th iteration => 0.00759764741041\n",
      "Loss for  39711 th iteration => 0.00759741102746\n",
      "Loss for  39712 th iteration => 0.00759720940296\n",
      "Loss for  39713 th iteration => 0.00759700776678\n",
      "Loss for  39714 th iteration => 0.00759677157836\n",
      "Loss for  39715 th iteration => 0.00759660444883\n",
      "Loss for  39716 th iteration => 0.00759633393465\n",
      "Loss for  39717 th iteration => 0.00759620208795\n",
      "Loss for  39718 th iteration => 0.00759599960758\n",
      "Loss for  39719 th iteration => 0.00759571269965\n",
      "Loss for  39720 th iteration => 0.00759556489382\n",
      "Loss for  39721 th iteration => 0.00759530818252\n",
      "Loss for  39722 th iteration => 0.00759524497695\n",
      "Loss for  39723 th iteration => 0.00759491991102\n",
      "Loss for  39724 th iteration => 0.00759478943545\n",
      "Loss for  39725 th iteration => 0.00759453136057\n",
      "Loss for  39726 th iteration => 0.00759433435622\n",
      "Loss for  39727 th iteration => 0.00759414253508\n",
      "Loss for  39728 th iteration => 0.00759387973275\n",
      "Loss for  39729 th iteration => 0.00759375343841\n",
      "Loss for  39730 th iteration => 0.00759343875429\n",
      "Loss for  39731 th iteration => 0.00759328086348\n",
      "Loss for  39732 th iteration => 0.00759300028621\n",
      "Loss for  39733 th iteration => 0.00759287953407\n",
      "Loss for  39734 th iteration => 0.00759269028405\n",
      "Loss for  39735 th iteration => 0.00759238784693\n",
      "Loss for  39736 th iteration => 0.00759225255733\n",
      "Loss for  39737 th iteration => 0.00759198531489\n",
      "Loss for  39738 th iteration => 0.00759181501725\n",
      "Loss for  39739 th iteration => 0.0075915827235\n",
      "Loss for  39740 th iteration => 0.00759137766178\n",
      "Loss for  39741 th iteration => 0.00759118007384\n",
      "Loss for  39742 th iteration => 0.00759094048889\n",
      "Loss for  39743 th iteration => 0.00759077736697\n",
      "Loss for  39744 th iteration => 0.00759050349662\n",
      "Loss for  39745 th iteration => 0.00759037571128\n",
      "Loss for  39746 th iteration => 0.00759017004822\n",
      "Loss for  39747 th iteration => 0.00758988725909\n",
      "Loss for  39748 th iteration => 0.00758973598243\n",
      "Loss for  39749 th iteration => 0.00758948334361\n",
      "Loss for  39750 th iteration => 0.00758941210045\n",
      "Loss for  39751 th iteration => 0.00758909565515\n",
      "Loss for  39752 th iteration => 0.00758895724191\n",
      "Loss for  39753 th iteration => 0.00758870768828\n",
      "Loss for  39754 th iteration => 0.00758850284473\n",
      "Loss for  39755 th iteration => 0.00758831944691\n",
      "Loss for  39756 th iteration => 0.00758804890242\n",
      "Loss for  39757 th iteration => 0.00758793093489\n",
      "Loss for  39758 th iteration => 0.00758761219148\n",
      "Loss for  39759 th iteration => 0.00758745897204\n",
      "Loss for  39760 th iteration => 0.00758717577377\n",
      "Loss for  39761 th iteration => 0.00758696745843\n",
      "Loss for  39762 th iteration => 0.007586863266\n",
      "Loss for  39763 th iteration => 0.00758656564265\n",
      "Loss for  39764 th iteration => 0.00758642612799\n",
      "Loss for  39765 th iteration => 0.00758616376597\n",
      "Loss for  39766 th iteration => 0.00758598917743\n",
      "Loss for  39767 th iteration => 0.00758576182944\n",
      "Loss for  39768 th iteration => 0.00758555241226\n",
      "Loss for  39769 th iteration => 0.00758535983414\n",
      "Loss for  39770 th iteration => 0.00758511583047\n",
      "Loss for  39771 th iteration => 0.00758495778113\n",
      "Loss for  39772 th iteration => 0.00758467943004\n",
      "Loss for  39773 th iteration => 0.00758455740641\n",
      "Loss for  39774 th iteration => 0.00758434687689\n",
      "Loss for  39775 th iteration => 0.00758406937339\n",
      "Loss for  39776 th iteration => 0.00758391340994\n",
      "Loss for  39777 th iteration => 0.0075836650061\n",
      "Loss for  39778 th iteration => 0.00758348009283\n",
      "Loss for  39779 th iteration => 0.00758326060797\n",
      "Loss for  39780 th iteration => 0.00758304692405\n",
      "Loss for  39781 th iteration => 0.00758285745797\n",
      "Loss for  39782 th iteration => 0.00758273672837\n",
      "Loss for  39783 th iteration => 0.00758247020478\n",
      "Loss for  39784 th iteration => 0.00758228667018\n",
      "Loss for  39785 th iteration => 0.0075820105005\n",
      "Loss for  39786 th iteration => 0.00758184896418\n",
      "Loss for  39787 th iteration => 0.0075816099901\n",
      "Loss for  39788 th iteration => 0.00758141145778\n",
      "Loss for  39789 th iteration => 0.00758121096007\n",
      "Loss for  39790 th iteration => 0.00758107421766\n",
      "Loss for  39791 th iteration => 0.00758082663408\n",
      "Loss for  39792 th iteration => 0.00758061727913\n",
      "Loss for  39793 th iteration => 0.00758044197906\n",
      "Loss for  39794 th iteration => 0.00758017192679\n",
      "Loss for  39795 th iteration => 0.00757995054545\n",
      "Loss for  39796 th iteration => 0.007579735772\n",
      "Loss for  39797 th iteration => 0.00757954918712\n",
      "Loss for  39798 th iteration => 0.00757929980099\n",
      "Loss for  39799 th iteration => 0.00757914777077\n",
      "Loss for  39800 th iteration => 0.00757886889406\n",
      "Loss for  39801 th iteration => 0.00757866044898\n",
      "Loss for  39802 th iteration => 0.0075784520161\n",
      "Loss for  39803 th iteration => 0.00757824359541\n",
      "Loss for  39804 th iteration => 0.00757803538887\n",
      "Loss for  39805 th iteration => 0.00757792039709\n",
      "Loss for  39806 th iteration => 0.00757763176254\n",
      "Loss for  39807 th iteration => 0.00757748760416\n",
      "Loss for  39808 th iteration => 0.00757722854814\n",
      "Loss for  39809 th iteration => 0.00757717206372\n",
      "Loss for  39810 th iteration => 0.00757684226464\n",
      "Loss for  39811 th iteration => 0.00757671840192\n",
      "Loss for  39812 th iteration => 0.00757645570177\n",
      "Loss for  39813 th iteration => 0.00757626520311\n",
      "Loss for  39814 th iteration => 0.00757606886341\n",
      "Loss for  39815 th iteration => 0.0075758124608\n",
      "Loss for  39816 th iteration => 0.00757568175344\n",
      "Loss for  39817 th iteration => 0.00757537144808\n",
      "Loss for  39818 th iteration => 0.007575211665\n",
      "Loss for  39819 th iteration => 0.00757493489773\n",
      "Loss for  39820 th iteration => 0.00757481157891\n",
      "Loss for  39821 th iteration => 0.00757451678999\n",
      "Loss for  39822 th iteration => 0.00757441821894\n",
      "Loss for  39823 th iteration => 0.00757411628596\n",
      "Loss for  39824 th iteration => 0.00757398246187\n",
      "Loss for  39825 th iteration => 0.00757371572121\n",
      "Loss for  39826 th iteration => 0.00757354689153\n",
      "Loss for  39827 th iteration => 0.00757331509679\n",
      "Loss for  39828 th iteration => 0.00757311150587\n",
      "Loss for  39829 th iteration => 0.00757291441379\n",
      "Loss for  39830 th iteration => 0.00757267630287\n",
      "Loss for  39831 th iteration => 0.00757251367325\n",
      "Loss for  39832 th iteration => 0.0075722426582\n",
      "Loss for  39833 th iteration => 0.00757220128051\n",
      "Loss for  39834 th iteration => 0.00757183441209\n",
      "Loss for  39835 th iteration => 0.00757168479962\n",
      "Loss for  39836 th iteration => 0.0075714329665\n",
      "Loss for  39837 th iteration => 0.00757136389452\n",
      "Loss for  39838 th iteration => 0.00757104723375\n",
      "Loss for  39839 th iteration => 0.00757091094747\n",
      "Loss for  39840 th iteration => 0.00757066122253\n",
      "Loss for  39841 th iteration => 0.00757045846192\n",
      "Loss for  39842 th iteration => 0.0075702749367\n",
      "Loss for  39843 th iteration => 0.0075700064314\n",
      "Loss for  39844 th iteration => 0.00756988838012\n",
      "Loss for  39845 th iteration => 0.0075695707177\n",
      "Loss for  39846 th iteration => 0.00756941804112\n",
      "Loss for  39847 th iteration => 0.0075691352448\n",
      "Loss for  39848 th iteration => 0.00756892804442\n",
      "Loss for  39849 th iteration => 0.00756882600755\n",
      "Loss for  39850 th iteration => 0.00756852817434\n",
      "Loss for  39851 th iteration => 0.00756839080528\n",
      "Loss for  39852 th iteration => 0.00756812824296\n",
      "Loss for  39853 th iteration => 0.00756795579068\n",
      "Loss for  39854 th iteration => 0.00756772825138\n",
      "Loss for  39855 th iteration => 0.00756752096169\n",
      "Loss for  39856 th iteration => 0.00756732820064\n",
      "Loss for  39857 th iteration => 0.00756708631626\n",
      "Loss for  39858 th iteration => 0.00756692809182\n",
      "Loss for  39859 th iteration => 0.00756665281897\n",
      "Loss for  39860 th iteration => 0.00756661151466\n",
      "Loss for  39861 th iteration => 0.00756625039116\n",
      "Loss for  39862 th iteration => 0.00756609661644\n",
      "Loss for  39863 th iteration => 0.00756584795595\n",
      "Loss for  39864 th iteration => 0.0075656651986\n",
      "Loss for  39865 th iteration => 0.00756544548936\n",
      "Loss for  39866 th iteration => 0.00756523392955\n",
      "Loss for  39867 th iteration => 0.0075650437014\n",
      "Loss for  39868 th iteration => 0.00756492569336\n",
      "Loss for  39869 th iteration => 0.0075646583833\n",
      "Loss for  39870 th iteration => 0.00756447711114\n",
      "Loss for  39871 th iteration => 0.00756420131229\n",
      "Loss for  39872 th iteration => 0.00756404130411\n",
      "Loss for  39873 th iteration => 0.00756380272875\n",
      "Loss for  39874 th iteration => 0.00756360569711\n",
      "Loss for  39875 th iteration => 0.00756340453637\n",
      "Loss for  39876 th iteration => 0.00756327106714\n",
      "Loss for  39877 th iteration => 0.00756302213726\n",
      "Loss for  39878 th iteration => 0.00756281601088\n",
      "Loss for  39879 th iteration => 0.00756263940918\n",
      "Loss for  39880 th iteration => 0.00756237255594\n",
      "Loss for  39881 th iteration => 0.00756215038237\n",
      "Loss for  39882 th iteration => 0.00756193830442\n",
      "Loss for  39883 th iteration => 0.00756175094619\n",
      "Loss for  39884 th iteration => 0.00756150423701\n",
      "Loss for  39885 th iteration => 0.00756135145156\n",
      "Loss for  39886 th iteration => 0.00756107457226\n",
      "Loss for  39887 th iteration => 0.00756086708714\n",
      "Loss for  39888 th iteration => 0.00756065961418\n",
      "Loss for  39889 th iteration => 0.00756045215336\n",
      "Loss for  39890 th iteration => 0.00756024515497\n",
      "Loss for  39891 th iteration => 0.00756013009358\n",
      "Loss for  39892 th iteration => 0.00755984344402\n",
      "Loss for  39893 th iteration => 0.00755969921186\n",
      "Loss for  39894 th iteration => 0.00755944266816\n",
      "Loss for  39895 th iteration => 0.00755938435644\n",
      "Loss for  39896 th iteration => 0.00755905828614\n",
      "Loss for  39897 th iteration => 0.00755893260501\n",
      "Loss for  39898 th iteration => 0.00755867362493\n",
      "Loss for  39899 th iteration => 0.00755848131628\n",
      "Loss for  39900 th iteration => 0.00755828868842\n",
      "Loss for  39901 th iteration => 0.00755803048376\n",
      "Loss for  39902 th iteration => 0.00755790348045\n",
      "Loss for  39903 th iteration => 0.00755759214809\n",
      "Loss for  39904 th iteration => 0.00755743504358\n",
      "Loss for  39905 th iteration => 0.00755715751126\n",
      "Loss for  39906 th iteration => 0.00755703686569\n",
      "Loss for  39907 th iteration => 0.0075567425605\n",
      "Loss for  39908 th iteration => 0.00755664417741\n",
      "Loss for  39909 th iteration => 0.00755634396128\n",
      "Loss for  39910 th iteration => 0.00755621033695\n",
      "Loss for  39911 th iteration => 0.00755594530106\n",
      "Loss for  39912 th iteration => 0.00755577668325\n",
      "Loss for  39913 th iteration => 0.00755554658091\n",
      "Loss for  39914 th iteration => 0.00755534321428\n",
      "Loss for  39915 th iteration => 0.00755514780187\n",
      "Loss for  39916 th iteration => 0.00755490992799\n",
      "Loss for  39917 th iteration => 0.00755474963386\n",
      "Loss for  39918 th iteration => 0.00755456426175\n",
      "Loss for  39919 th iteration => 0.00755436410442\n",
      "Loss for  39920 th iteration => 0.00755412972717\n",
      "Loss for  39921 th iteration => 0.00755387839265\n",
      "Loss for  39922 th iteration => 0.00755380896385\n",
      "Loss for  39923 th iteration => 0.00755349454529\n",
      "Loss for  39924 th iteration => 0.00755335788838\n",
      "Loss for  39925 th iteration => 0.0075531104195\n",
      "Loss for  39926 th iteration => 0.00755290727435\n",
      "Loss for  39927 th iteration => 0.00755272601914\n",
      "Loss for  39928 th iteration => 0.00755245721945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  39929 th iteration => 0.00755226999732\n",
      "Loss for  39930 th iteration => 0.00755202296685\n",
      "Loss for  39931 th iteration => 0.00755187249641\n",
      "Loss for  39932 th iteration => 0.00755159132116\n",
      "Loss for  39933 th iteration => 0.00755138434549\n",
      "Loss for  39934 th iteration => 0.00755117961903\n",
      "Loss for  39935 th iteration => 0.00755107461607\n",
      "Loss for  39936 th iteration => 0.00755078167143\n",
      "Loss for  39937 th iteration => 0.00755064129656\n",
      "Loss for  39938 th iteration => 0.00755038366201\n",
      "Loss for  39939 th iteration => 0.00755020816532\n",
      "Loss for  39940 th iteration => 0.00754998559182\n",
      "Loss for  39941 th iteration => 0.00754977522025\n",
      "Loss for  39942 th iteration => 0.00754958746194\n",
      "Loss for  39943 th iteration => 0.0075493424593\n",
      "Loss for  39944 th iteration => 0.00754918927341\n",
      "Loss for  39945 th iteration => 0.00754891087367\n",
      "Loss for  39946 th iteration => 0.00754870404475\n",
      "Loss for  39947 th iteration => 0.00754849722795\n",
      "Loss for  39948 th iteration => 0.00754829152464\n",
      "Loss for  39949 th iteration => 0.00754817878497\n",
      "Loss for  39950 th iteration => 0.00754789110907\n",
      "Loss for  39951 th iteration => 0.00754774917099\n",
      "Loss for  39952 th iteration => 0.00754749230208\n",
      "Loss for  39953 th iteration => 0.00754743873195\n",
      "Loss for  39954 th iteration => 0.00754710922613\n",
      "Loss for  39955 th iteration => 0.00754698822009\n",
      "Loss for  39956 th iteration => 0.00754672587078\n",
      "Loss for  39957 th iteration => 0.00754653817134\n",
      "Loss for  39958 th iteration => 0.0075463422399\n",
      "Loss for  39959 th iteration => 0.00754608857919\n",
      "Loss for  39960 th iteration => 0.00754595833733\n",
      "Loss for  39961 th iteration => 0.00754564924444\n",
      "Loss for  39962 th iteration => 0.00754549046378\n",
      "Loss for  39963 th iteration => 0.00754521587284\n",
      "Loss for  39964 th iteration => 0.00754509420405\n",
      "Loss for  39965 th iteration => 0.00754491150383\n",
      "Loss for  39966 th iteration => 0.00754460688517\n",
      "Loss for  39967 th iteration => 0.00754447888585\n",
      "Loss for  39968 th iteration => 0.00754420950987\n",
      "Loss for  39969 th iteration => 0.0075440464553\n",
      "Loss for  39970 th iteration => 0.00754381207413\n",
      "Loss for  39971 th iteration => 0.0075436142101\n",
      "Loss for  39972 th iteration => 0.00754341457902\n",
      "Loss for  39973 th iteration => 0.00754318214822\n",
      "Loss for  39974 th iteration => 0.00754301715622\n",
      "Loss for  39975 th iteration => 0.00754283975276\n",
      "Loss for  39976 th iteration => 0.00754263292847\n",
      "Loss for  39977 th iteration => 0.00754240311235\n",
      "Loss for  39978 th iteration => 0.00754214916751\n",
      "Loss for  39979 th iteration => 0.00754197647316\n",
      "Loss for  39980 th iteration => 0.0075418602722\n",
      "Loss for  39981 th iteration => 0.00754156440695\n",
      "Loss for  39982 th iteration => 0.00754143992181\n",
      "Loss for  39983 th iteration => 0.0075411523796\n",
      "Loss for  39984 th iteration => 0.00754101969947\n",
      "Loss for  39985 th iteration => 0.00754074039067\n",
      "Loss for  39986 th iteration => 0.00754059960366\n",
      "Loss for  39987 th iteration => 0.00754034314134\n",
      "Loss for  39988 th iteration => 0.0075400981865\n",
      "Loss for  39989 th iteration => 0.00753994688497\n",
      "Loss for  39990 th iteration => 0.0075396654548\n",
      "Loss for  39991 th iteration => 0.00753955173014\n",
      "Loss for  39992 th iteration => 0.00753936159309\n",
      "Loss for  39993 th iteration => 0.00753906484811\n",
      "Loss for  39994 th iteration => 0.00753892961297\n",
      "Loss for  39995 th iteration => 0.00753866803373\n",
      "Loss for  39996 th iteration => 0.0075384978197\n",
      "Loss for  39997 th iteration => 0.00753827115914\n",
      "Loss for  39998 th iteration => 0.00753806621124\n",
      "Loss for  39999 th iteration => 0.00753787422537\n",
      "Loss for  40000 th iteration => 0.00753763478555\n",
      "Loss for  40001 th iteration => 0.00753747739184\n",
      "Loss for  40002 th iteration => 0.0075372876243\n",
      "Loss for  40003 th iteration => 0.00753709368751\n",
      "Loss for  40004 th iteration => 0.00753685846339\n",
      "Loss for  40005 th iteration => 0.0075366102409\n",
      "Loss for  40006 th iteration => 0.00753653524742\n",
      "Loss for  40007 th iteration => 0.00753622820908\n",
      "Loss for  40008 th iteration => 0.00753608605507\n",
      "Loss for  40009 th iteration => 0.00753584589932\n",
      "Loss for  40010 th iteration => 0.0075356373234\n",
      "Loss for  40011 th iteration => 0.00753546331544\n",
      "Loss for  40012 th iteration => 0.00753519242839\n",
      "Loss for  40013 th iteration => 0.00753500968656\n",
      "Loss for  40014 th iteration => 0.00753476004261\n",
      "Loss for  40015 th iteration => 0.00753461402153\n",
      "Loss for  40016 th iteration => 0.0075343344645\n",
      "Loss for  40017 th iteration => 0.00753412841635\n",
      "Loss for  40018 th iteration => 0.00753392438082\n",
      "Loss for  40019 th iteration => 0.00753381605116\n",
      "Loss for  40020 th iteration => 0.00753352826525\n",
      "Loss for  40021 th iteration => 0.00753338460155\n",
      "Loss for  40022 th iteration => 0.00753313208768\n",
      "Loss for  40023 th iteration => 0.00753295334008\n",
      "Loss for  40024 th iteration => 0.00753273584918\n",
      "Loss for  40025 th iteration => 0.00753252226465\n",
      "Loss for  40026 th iteration => 0.00753233955081\n",
      "Loss for  40027 th iteration => 0.00753209137322\n",
      "Loss for  40028 th iteration => 0.00753194319361\n",
      "Loss for  40029 th iteration => 0.00753166602909\n",
      "Loss for  40030 th iteration => 0.00753146012689\n",
      "Loss for  40031 th iteration => 0.00753125423675\n",
      "Loss for  40032 th iteration => 0.00753104963542\n",
      "Loss for  40033 th iteration => 0.00753093342568\n",
      "Loss for  40034 th iteration => 0.00753065105153\n",
      "Loss for  40035 th iteration => 0.0075305056803\n",
      "Loss for  40036 th iteration => 0.00753025430126\n",
      "Loss for  40037 th iteration => 0.00753019171064\n",
      "Loss for  40038 th iteration => 0.00752987301449\n",
      "Loss for  40039 th iteration => 0.00752974310553\n",
      "Loss for  40040 th iteration => 0.00752949144906\n",
      "Loss for  40041 th iteration => 0.00752929496231\n",
      "Loss for  40042 th iteration => 0.00752910960883\n",
      "Loss for  40043 th iteration => 0.00752884727448\n",
      "Loss for  40044 th iteration => 0.00752872749761\n",
      "Loss for  40045 th iteration => 0.00752841389943\n",
      "Loss for  40046 th iteration => 0.00752826137612\n",
      "Loss for  40047 th iteration => 0.00752798240369\n",
      "Loss for  40048 th iteration => 0.00752786743328\n",
      "Loss for  40049 th iteration => 0.00752767965408\n",
      "Loss for  40050 th iteration => 0.00752738185045\n",
      "Loss for  40051 th iteration => 0.00752724891356\n",
      "Loss for  40052 th iteration => 0.00752698629402\n",
      "Loss for  40053 th iteration => 0.00752681836014\n",
      "Loss for  40054 th iteration => 0.00752659067708\n",
      "Loss for  40055 th iteration => 0.00752638799177\n",
      "Loss for  40056 th iteration => 0.00752619500068\n",
      "Loss for  40057 th iteration => 0.0075259578064\n",
      "Loss for  40058 th iteration => 0.00752580003769\n",
      "Loss for  40059 th iteration => 0.00752561251264\n",
      "Loss for  40060 th iteration => 0.00752541759106\n",
      "Loss for  40061 th iteration => 0.00752518298909\n",
      "Loss for  40062 th iteration => 0.00752493600681\n",
      "Loss for  40063 th iteration => 0.00752486200702\n",
      "Loss for  40064 th iteration => 0.00752455522636\n",
      "Loss for  40065 th iteration => 0.00752441404991\n",
      "Loss for  40066 th iteration => 0.00752417416798\n",
      "Loss for  40067 th iteration => 0.00752396655346\n",
      "Loss for  40068 th iteration => 0.00752379283553\n",
      "Loss for  40069 th iteration => 0.00752352228886\n",
      "Loss for  40070 th iteration => 0.00752333982199\n",
      "Loss for  40071 th iteration => 0.00752309114802\n",
      "Loss for  40072 th iteration => 0.0075229454075\n",
      "Loss for  40073 th iteration => 0.00752266584297\n",
      "Loss for  40074 th iteration => 0.00752246067859\n",
      "Loss for  40075 th iteration => 0.00752235631556\n",
      "Loss for  40076 th iteration => 0.00752206580696\n",
      "Loss for  40077 th iteration => 0.00752192606609\n",
      "Loss for  40078 th iteration => 0.00752167087293\n",
      "Loss for  40079 th iteration => 0.00752149600526\n",
      "Loss for  40080 th iteration => 0.00752127587755\n",
      "Loss for  40081 th iteration => 0.00752106613097\n",
      "Loss for  40082 th iteration => 0.00752088082189\n",
      "Loss for  40083 th iteration => 0.00752063644116\n",
      "Loss for  40084 th iteration => 0.00752048570699\n",
      "Loss for  40085 th iteration => 0.007520210805\n",
      "Loss for  40086 th iteration => 0.00752000551679\n",
      "Loss for  40087 th iteration => 0.00751980024063\n",
      "Loss for  40088 th iteration => 0.00751959561373\n",
      "Loss for  40089 th iteration => 0.00751948104364\n",
      "Loss for  40090 th iteration => 0.00751919826551\n",
      "Loss for  40091 th iteration => 0.00751905450815\n",
      "Loss for  40092 th iteration => 0.00751880178731\n",
      "Loss for  40093 th iteration => 0.00751874240483\n",
      "Loss for  40094 th iteration => 0.00751842174281\n",
      "Loss for  40095 th iteration => 0.00751829499021\n",
      "Loss for  40096 th iteration => 0.00751804141947\n",
      "Loss for  40097 th iteration => 0.0075178480378\n",
      "Loss for  40098 th iteration => 0.00751766082115\n",
      "Loss for  40099 th iteration => 0.00751740154112\n",
      "Loss for  40100 th iteration => 0.00751727995166\n",
      "Loss for  40101 th iteration => 0.00751696904356\n",
      "Loss for  40102 th iteration => 0.0075168156348\n",
      "Loss for  40103 th iteration => 0.0075165387564\n",
      "Loss for  40104 th iteration => 0.00751642207787\n",
      "Loss for  40105 th iteration => 0.00751623716677\n",
      "Loss for  40106 th iteration => 0.00751593860737\n",
      "Loss for  40107 th iteration => 0.00751580763827\n",
      "Loss for  40108 th iteration => 0.007515544281\n",
      "Loss for  40109 th iteration => 0.00751537829715\n",
      "Loss for  40110 th iteration => 0.00751514989382\n",
      "Loss for  40111 th iteration => 0.00751494914133\n",
      "Loss for  40112 th iteration => 0.00751475544689\n",
      "Loss for  40113 th iteration => 0.00751452016877\n",
      "Loss for  40114 th iteration => 0.00751436166401\n",
      "Loss for  40115 th iteration => 0.00751417578969\n",
      "Loss for  40116 th iteration => 0.00751398044593\n",
      "Loss for  40117 th iteration => 0.00751374699091\n",
      "Loss for  40118 th iteration => 0.00751350065635\n",
      "Loss for  40119 th iteration => 0.00751342702662\n",
      "Loss for  40120 th iteration => 0.00751312109805\n",
      "Loss for  40121 th iteration => 0.00751298028038\n",
      "Loss for  40122 th iteration => 0.00751274126188\n",
      "Loss for  40123 th iteration => 0.00751253399476\n",
      "Loss for  40124 th iteration => 0.00751236115166\n",
      "Loss for  40125 th iteration => 0.00751209125638\n",
      "Loss for  40126 th iteration => 0.00751190939118\n",
      "Loss for  40127 th iteration => 0.00751166133305\n",
      "Loss for  40128 th iteration => 0.0075115161994\n",
      "Loss for  40129 th iteration => 0.00751123745549\n",
      "Loss for  40130 th iteration => 0.00751103269635\n",
      "Loss for  40131 th iteration => 0.0075109289334\n",
      "Loss for  40132 th iteration => 0.00751063904443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  40133 th iteration => 0.00751049990434\n",
      "Loss for  40134 th iteration => 0.0075102453299\n",
      "Loss for  40135 th iteration => 0.00751007106402\n",
      "Loss for  40136 th iteration => 0.00750985155382\n",
      "Loss for  40137 th iteration => 0.00750964241033\n",
      "Loss for  40138 th iteration => 0.00750945771726\n",
      "Loss for  40139 th iteration => 0.0075092139412\n",
      "Loss for  40140 th iteration => 0.00750906382126\n",
      "Loss for  40141 th iteration => 0.00750878973584\n",
      "Loss for  40142 th iteration => 0.00750858505947\n",
      "Loss for  40143 th iteration => 0.00750838039512\n",
      "Loss for  40144 th iteration => 0.00750817689078\n",
      "Loss for  40145 th iteration => 0.00750806142889\n",
      "Loss for  40146 th iteration => 0.00750778075902\n",
      "Loss for  40147 th iteration => 0.00750763611713\n",
      "Loss for  40148 th iteration => 0.00750738596479\n",
      "Loss for  40149 th iteration => 0.00750732417252\n",
      "Loss for  40150 th iteration => 0.00750700712209\n",
      "Loss for  40151 th iteration => 0.00750687798844\n",
      "Loss for  40152 th iteration => 0.00750662800081\n",
      "Loss for  40153 th iteration => 0.00750643226617\n",
      "Loss for  40154 th iteration => 0.0075062486048\n",
      "Loss for  40155 th iteration => 0.00750598699924\n",
      "Loss for  40156 th iteration => 0.00750586893785\n",
      "Loss for  40157 th iteration => 0.00750555668166\n",
      "Loss for  40158 th iteration => 0.00750540534193\n",
      "Loss for  40159 th iteration => 0.00750512762033\n",
      "Loss for  40160 th iteration => 0.00750501339865\n",
      "Loss for  40161 th iteration => 0.00750482727292\n",
      "Loss for  40162 th iteration => 0.00750453069988\n",
      "Loss for  40163 th iteration => 0.00750439897197\n",
      "Loss for  40164 th iteration => 0.00750413758298\n",
      "Loss for  40165 th iteration => 0.00750397085834\n",
      "Loss for  40166 th iteration => 0.00750374440515\n",
      "Loss for  40167 th iteration => 0.00750354292995\n",
      "Loss for  40168 th iteration => 0.00750335116744\n",
      "Loss for  40169 th iteration => 0.00750311518475\n",
      "Loss for  40170 th iteration => 0.00750295950085\n",
      "Loss for  40171 th iteration => 0.00750277057062\n",
      "Loss for  40172 th iteration => 0.00750257947677\n",
      "Loss for  40173 th iteration => 0.00750234391962\n",
      "Loss for  40174 th iteration => 0.00750210149741\n",
      "Loss for  40175 th iteration => 0.00750202353185\n",
      "Loss for  40176 th iteration => 0.0075017231265\n",
      "Loss for  40177 th iteration => 0.00750157802921\n",
      "Loss for  40178 th iteration => 0.00750134447812\n",
      "Loss for  40179 th iteration => 0.00750113298654\n",
      "Loss for  40180 th iteration => 0.00750096555607\n",
      "Loss for  40181 th iteration => 0.00750069288662\n",
      "Loss for  40182 th iteration => 0.00750051413466\n",
      "Loss for  40183 th iteration => 0.00750026419475\n",
      "Loss for  40184 th iteration => 0.00750012214597\n",
      "Loss for  40185 th iteration => 0.00749984310656\n",
      "Loss for  40186 th iteration => 0.00749963979711\n",
      "Loss for  40187 th iteration => 0.00749953430199\n",
      "Loss for  40188 th iteration => 0.00749924734666\n",
      "Loss for  40189 th iteration => 0.00749910650554\n",
      "Loss for  40190 th iteration => 0.00749885483355\n",
      "Loss for  40191 th iteration => 0.00749867889765\n",
      "Loss for  40192 th iteration => 0.00749846225883\n",
      "Loss for  40193 th iteration => 0.00749825147621\n",
      "Loss for  40194 th iteration => 0.00749806962357\n",
      "Loss for  40195 th iteration => 0.00749782423915\n",
      "Loss for  40196 th iteration => 0.0074976769288\n",
      "Loss for  40197 th iteration => 0.00749740267408\n",
      "Loss for  40198 th iteration => 0.00749719860726\n",
      "Loss for  40199 th iteration => 0.00749699503252\n",
      "Loss for  40200 th iteration => 0.00749687951771\n",
      "Loss for  40201 th iteration => 0.00749660009572\n",
      "Loss for  40202 th iteration => 0.00749645539385\n",
      "Loss for  40203 th iteration => 0.00749620548494\n",
      "Loss for  40204 th iteration => 0.00749614312933\n",
      "Loss for  40205 th iteration => 0.00749582783033\n",
      "Loss for  40206 th iteration => 0.00749569813222\n",
      "Loss for  40207 th iteration => 0.00749544989723\n",
      "Loss for  40208 th iteration => 0.00749525359683\n",
      "Loss for  40209 th iteration => 0.00749507168946\n",
      "Loss for  40210 th iteration => 0.00749480951669\n",
      "Loss for  40211 th iteration => 0.00749469321081\n",
      "Loss for  40212 th iteration => 0.0074943817063\n",
      "Loss for  40213 th iteration => 0.00749423145857\n",
      "Loss for  40214 th iteration => 0.007493953834\n",
      "Loss for  40215 th iteration => 0.00749384003146\n",
      "Loss for  40216 th iteration => 0.00749354958627\n",
      "Loss for  40217 th iteration => 0.00749345011029\n",
      "Loss for  40218 th iteration => 0.00749315772333\n",
      "Loss for  40219 th iteration => 0.0074930230483\n",
      "Loss for  40220 th iteration => 0.00749276579841\n",
      "Loss for  40221 th iteration => 0.00749259617329\n",
      "Loss for  40222 th iteration => 0.00749237381258\n",
      "Loss for  40223 th iteration => 0.0074921694832\n",
      "Loss for  40224 th iteration => 0.00749198176688\n",
      "Loss for  40225 th iteration => 0.00749174428987\n",
      "Loss for  40226 th iteration => 0.00749167050374\n",
      "Loss for  40227 th iteration => 0.00749133076092\n",
      "Loss for  40228 th iteration => 0.00749126262276\n",
      "Loss for  40229 th iteration => 0.0074909372651\n",
      "Loss for  40230 th iteration => 0.00749085695497\n",
      "Loss for  40231 th iteration => 0.00749056007032\n",
      "Loss for  40232 th iteration => 0.00749041264933\n",
      "Loss for  40233 th iteration => 0.00749018259826\n",
      "Loss for  40234 th iteration => 0.00748996880337\n",
      "Loss for  40235 th iteration => 0.00748980485273\n",
      "Loss for  40236 th iteration => 0.00748953158365\n",
      "Loss for  40237 th iteration => 0.00748935496805\n",
      "Loss for  40238 th iteration => 0.00748910408474\n",
      "Loss for  40239 th iteration => 0.0074889641635\n",
      "Loss for  40240 th iteration => 0.00748868624597\n",
      "Loss for  40241 th iteration => 0.00748848281948\n",
      "Loss for  40242 th iteration => 0.00748837689882\n",
      "Loss for  40243 th iteration => 0.00748809155107\n",
      "Loss for  40244 th iteration => 0.00748795029707\n",
      "Loss for  40245 th iteration => 0.00748770021986\n",
      "Loss for  40246 th iteration => 0.00748752388389\n",
      "Loss for  40247 th iteration => 0.00748730882688\n",
      "Loss for  40248 th iteration => 0.00748709765717\n",
      "Loss for  40249 th iteration => 0.00748691737321\n",
      "Loss for  40250 th iteration => 0.00748667161483\n",
      "Loss for  40251 th iteration => 0.00748652585988\n",
      "Loss for  40252 th iteration => 0.00748625293519\n",
      "Loss for  40253 th iteration => 0.0074860494637\n",
      "Loss for  40254 th iteration => 0.00748584661998\n",
      "Loss for  40255 th iteration => 0.00748572927019\n",
      "Loss for  40256 th iteration => 0.00748545286344\n",
      "Loss for  40257 th iteration => 0.00748530634241\n",
      "Loss for  40258 th iteration => 0.00748505951749\n",
      "Loss for  40259 th iteration => 0.00748499244303\n",
      "Loss for  40260 th iteration => 0.00748468302136\n",
      "Loss for  40261 th iteration => 0.00748454865952\n",
      "Loss for  40262 th iteration => 0.00748430624712\n",
      "Loss for  40263 th iteration => 0.00748410533714\n",
      "Loss for  40264 th iteration => 0.00748392919858\n",
      "Loss for  40265 th iteration => 0.00748366349599\n",
      "Loss for  40266 th iteration => 0.00748348022573\n",
      "Loss for  40267 th iteration => 0.00748323657488\n",
      "Loss for  40268 th iteration => 0.00748309008067\n",
      "Loss for  40269 th iteration => 0.00748281277405\n",
      "Loss for  40270 th iteration => 0.00748260948391\n",
      "Loss for  40271 th iteration => 0.00748240845277\n",
      "Loss for  40272 th iteration => 0.00748230619351\n",
      "Loss for  40273 th iteration => 0.00748201784145\n",
      "Loss for  40274 th iteration => 0.00748188022399\n",
      "Loss for  40275 th iteration => 0.00748162716715\n",
      "Loss for  40276 th iteration => 0.00748145444321\n",
      "Loss for  40277 th iteration => 0.00748123643093\n",
      "Loss for  40278 th iteration => 0.00748102884905\n",
      "Loss for  40279 th iteration => 0.00748084563384\n",
      "Loss for  40280 th iteration => 0.00748060343945\n",
      "Loss for  40281 th iteration => 0.00748045477692\n",
      "Loss for  40282 th iteration => 0.00748018005595\n",
      "Loss for  40283 th iteration => 0.00747997766303\n",
      "Loss for  40284 th iteration => 0.00747986518069\n",
      "Loss for  40285 th iteration => 0.00747958455312\n",
      "Loss for  40286 th iteration => 0.00747944284705\n",
      "Loss for  40287 th iteration => 0.00747919204145\n",
      "Loss for  40288 th iteration => 0.00747913552162\n",
      "Loss for  40289 th iteration => 0.00747881622577\n",
      "Loss for  40290 th iteration => 0.0074786922805\n",
      "Loss for  40291 th iteration => 0.00747844013136\n",
      "Loss for  40292 th iteration => 0.00747824950158\n",
      "Loss for  40293 th iteration => 0.00747806376204\n",
      "Loss for  40294 th iteration => 0.0074778071784\n",
      "Loss for  40295 th iteration => 0.00747768712157\n",
      "Loss for  40296 th iteration => 0.00747737884021\n",
      "Loss for  40297 th iteration => 0.00747722681703\n",
      "Loss for  40298 th iteration => 0.00747695275557\n",
      "Loss for  40299 th iteration => 0.00747683721387\n",
      "Loss for  40300 th iteration => 0.00747654738553\n",
      "Loss for  40301 th iteration => 0.00747645178949\n",
      "Loss for  40302 th iteration => 0.00747615734162\n",
      "Loss for  40303 th iteration => 0.0074760265204\n",
      "Loss for  40304 th iteration => 0.00747576723535\n",
      "Loss for  40305 th iteration => 0.00747560143863\n",
      "Loss for  40306 th iteration => 0.00747537706777\n",
      "Loss for  40307 th iteration => 0.0074751765421\n",
      "Loss for  40308 th iteration => 0.00747498745683\n",
      "Loss for  40309 th iteration => 0.00747483496312\n",
      "Loss for  40310 th iteration => 0.00747461056502\n",
      "Loss for  40311 th iteration => 0.00747439407533\n",
      "Loss for  40312 th iteration => 0.00747423345096\n",
      "Loss for  40313 th iteration => 0.00747407462205\n",
      "Loss for  40314 th iteration => 0.00747377235444\n",
      "Loss for  40315 th iteration => 0.00747363203633\n",
      "Loss for  40316 th iteration => 0.00747339670491\n",
      "Loss for  40317 th iteration => 0.007473189911\n",
      "Loss for  40318 th iteration => 0.00747302078152\n",
      "Loss for  40319 th iteration => 0.00747275261249\n",
      "Loss for  40320 th iteration => 0.00747257331735\n",
      "Loss for  40321 th iteration => 0.00747232686882\n",
      "Loss for  40322 th iteration => 0.00747218431665\n",
      "Loss for  40323 th iteration => 0.00747190827054\n",
      "Loss for  40324 th iteration => 0.00747170556139\n",
      "Loss for  40325 th iteration => 0.00747150455288\n",
      "Loss for  40326 th iteration => 0.00747139949733\n",
      "Loss for  40327 th iteration => 0.00747111508433\n",
      "Loss for  40328 th iteration => 0.00747097470614\n",
      "Loss for  40329 th iteration => 0.00747072555275\n",
      "Loss for  40330 th iteration => 0.00747055010351\n",
      "Loss for  40331 th iteration => 0.00747033595919\n",
      "Loss for  40332 th iteration => 0.00747012568733\n",
      "Loss for  40333 th iteration => 0.00746994630472\n",
      "Loss for  40334 th iteration => 0.00746970145554\n",
      "Loss for  40335 th iteration => 0.00746955659035\n",
      "Loss for  40336 th iteration => 0.00746928307711\n",
      "Loss for  40337 th iteration => 0.0074690813104\n",
      "Loss for  40338 th iteration => 0.00746896548987\n",
      "Loss for  40339 th iteration => 0.00746868934436\n",
      "Loss for  40340 th iteration => 0.00746854433224\n",
      "Loss for  40341 th iteration => 0.00746829781112\n",
      "Loss for  40342 th iteration => 0.00746823348422\n",
      "Loss for  40343 th iteration => 0.00746792310591\n",
      "Loss for  40344 th iteration => 0.00746779145273\n",
      "Loss for  40345 th iteration => 0.00746754812256\n",
      "Loss for  40346 th iteration => 0.00746734988249\n",
      "Loss for  40347 th iteration => 0.00746717286487\n",
      "Loss for  40348 th iteration => 0.00746690902182\n",
      "Loss for  40349 th iteration => 0.00746672545182\n",
      "Loss for  40350 th iteration => 0.0074664838702\n",
      "Loss for  40351 th iteration => 0.00746633709302\n",
      "Loss for  40352 th iteration => 0.00746606060672\n",
      "Loss for  40353 th iteration => 0.0074658582093\n",
      "Loss for  40354 th iteration => 0.00746565818235\n",
      "Loss for  40355 th iteration => 0.00746555776406\n",
      "Loss for  40356 th iteration => 0.00746526935215\n",
      "Loss for  40357 th iteration => 0.00746513356914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  40358 th iteration => 0.00746488045867\n",
      "Loss for  40359 th iteration => 0.00746470956314\n",
      "Loss for  40360 th iteration => 0.00746449150298\n",
      "Loss for  40361 th iteration => 0.00746428574395\n",
      "Loss for  40362 th iteration => 0.0074641024861\n",
      "Loss for  40363 th iteration => 0.00746386210948\n",
      "Loss for  40364 th iteration => 0.00746371340908\n",
      "Loss for  40365 th iteration => 0.00746343944988\n",
      "Loss for  40366 th iteration => 0.00746323896418\n",
      "Loss for  40367 th iteration => 0.0074631261515\n",
      "Loss for  40368 th iteration => 0.00746284763062\n",
      "Loss for  40369 th iteration => 0.00746270559805\n",
      "Loss for  40370 th iteration => 0.00746245795225\n",
      "Loss for  40371 th iteration => 0.00746240005223\n",
      "Loss for  40372 th iteration => 0.00746208389778\n",
      "Loss for  40373 th iteration => 0.0074619585924\n",
      "Loss for  40374 th iteration => 0.00746170956486\n",
      "Loss for  40375 th iteration => 0.00746151759438\n",
      "Loss for  40376 th iteration => 0.0074613349573\n",
      "Loss for  40377 th iteration => 0.00746107705168\n",
      "Loss for  40378 th iteration => 0.00746096007885\n",
      "Loss for  40379 th iteration => 0.00746065066804\n",
      "Loss for  40380 th iteration => 0.00746050053721\n",
      "Loss for  40381 th iteration => 0.00746022636553\n",
      "Loss for  40382 th iteration => 0.00746011353098\n",
      "Loss for  40383 th iteration => 0.00745993081601\n",
      "Loss for  40384 th iteration => 0.00745963516714\n",
      "Loss for  40385 th iteration => 0.0074595072841\n",
      "Loss for  40386 th iteration => 0.00745924682462\n",
      "Loss for  40387 th iteration => 0.00745908393991\n",
      "Loss for  40388 th iteration => 0.00745885842037\n",
      "Loss for  40389 th iteration => 0.00745866078136\n",
      "Loss for  40390 th iteration => 0.00745847027649\n",
      "Loss for  40391 th iteration => 0.00745832064042\n",
      "Loss for  40392 th iteration => 0.00745809514763\n",
      "Loss for  40393 th iteration => 0.00745788147861\n",
      "Loss for  40394 th iteration => 0.00745771976465\n",
      "Loss for  40395 th iteration => 0.00745746002354\n",
      "Loss for  40396 th iteration => 0.00745734795819\n",
      "Loss for  40397 th iteration => 0.0074570569276\n",
      "Loss for  40398 th iteration => 0.00745693659414\n",
      "Loss for  40399 th iteration => 0.00745665387036\n",
      "Loss for  40400 th iteration => 0.00745652535762\n",
      "Loss for  40401 th iteration => 0.00745625514608\n",
      "Loss for  40402 th iteration => 0.00745604406499\n",
      "Loss for  40403 th iteration => 0.00745586797554\n",
      "Loss for  40404 th iteration => 0.00745562011087\n",
      "Loss for  40405 th iteration => 0.00745548073347\n",
      "Loss for  40406 th iteration => 0.00745520394822\n",
      "Loss for  40407 th iteration => 0.00745500325228\n",
      "Loss for  40408 th iteration => 0.00745489946808\n",
      "Loss for  40409 th iteration => 0.00745461553716\n",
      "Loss for  40410 th iteration => 0.00745447642031\n",
      "Loss for  40411 th iteration => 0.00745422775868\n",
      "Loss for  40412 th iteration => 0.00745405356136\n",
      "Loss for  40413 th iteration => 0.0074538399179\n",
      "Loss for  40414 th iteration => 0.00745363088912\n",
      "Loss for  40415 th iteration => 0.00745345201587\n",
      "Loss for  40416 th iteration => 0.00745320840148\n",
      "Loss for  40417 th iteration => 0.0074530640536\n",
      "Loss for  40418 th iteration => 0.00745279195614\n",
      "Loss for  40419 th iteration => 0.00745259108131\n",
      "Loss for  40420 th iteration => 0.0074524753834\n",
      "Loss for  40421 th iteration => 0.00745220086406\n",
      "Loss for  40422 th iteration => 0.00745205597447\n",
      "Loss for  40423 th iteration => 0.00745181110374\n",
      "Loss for  40424 th iteration => 0.00745174592197\n",
      "Loss for  40425 th iteration => 0.00745143813333\n",
      "Loss for  40426 th iteration => 0.00745130564068\n",
      "Loss for  40427 th iteration => 0.00745106488504\n",
      "Loss for  40428 th iteration => 0.00745086582028\n",
      "Loss for  40429 th iteration => 0.00745069136264\n",
      "Loss for  40430 th iteration => 0.00745042743323\n",
      "Loss for  40431 th iteration => 0.00745024568057\n",
      "Loss for  40432 th iteration => 0.00745000403275\n",
      "Loss for  40433 th iteration => 0.00744985906416\n",
      "Loss for  40434 th iteration => 0.00744958343935\n",
      "Loss for  40435 th iteration => 0.00744938218637\n",
      "Loss for  40436 th iteration => 0.00744928431502\n",
      "Loss for  40437 th iteration => 0.00744899509269\n",
      "Loss for  40438 th iteration => 0.00744886182593\n",
      "Loss for  40439 th iteration => 0.00744860793532\n",
      "Loss for  40440 th iteration => 0.00744843952624\n",
      "Loss for  40441 th iteration => 0.00744822071529\n",
      "Loss for  40442 th iteration => 0.00744801741381\n",
      "Loss for  40443 th iteration => 0.00744783343365\n",
      "Loss for  40444 th iteration => 0.00744759548655\n",
      "Loss for  40445 th iteration => 0.00744744609144\n",
      "Loss for  40446 th iteration => 0.0074471750162\n",
      "Loss for  40447 th iteration => 0.00744697419536\n",
      "Loss for  40448 th iteration => 0.00744686245331\n",
      "Loss for  40449 th iteration => 0.00744658459153\n",
      "Loss for  40450 th iteration => 0.0074464436136\n",
      "Loss for  40451 th iteration => 0.00744619514281\n",
      "Loss for  40452 th iteration => 0.0074461384798\n",
      "Loss for  40453 th iteration => 0.00744582281282\n",
      "Loss for  40454 th iteration => 0.00744569872619\n",
      "Loss for  40455 th iteration => 0.00744545020445\n",
      "Loss for  40456 th iteration => 0.00744525943438\n",
      "Loss for  40457 th iteration => 0.00744507732147\n",
      "Loss for  40458 th iteration => 0.00744482059788\n",
      "Loss for  40459 th iteration => 0.00744470416764\n",
      "Loss for  40460 th iteration => 0.00744439693786\n",
      "Loss for  40461 th iteration => 0.00744424750988\n",
      "Loss for  40462 th iteration => 0.00744397435023\n",
      "Loss for  40463 th iteration => 0.00744386140136\n",
      "Loss for  40464 th iteration => 0.0074435737279\n",
      "Loss for  40465 th iteration => 0.00744347814145\n",
      "Loss for  40466 th iteration => 0.00744318717085\n",
      "Loss for  40467 th iteration => 0.00744305637703\n",
      "Loss for  40468 th iteration => 0.00744280055095\n",
      "Loss for  40469 th iteration => 0.00744263480009\n",
      "Loss for  40470 th iteration => 0.00744241386924\n",
      "Loss for  40471 th iteration => 0.00744221340853\n",
      "Loss for  40472 th iteration => 0.00744202886309\n",
      "Loss for  40473 th iteration => 0.00744187262473\n",
      "Loss for  40474 th iteration => 0.00744165542816\n",
      "Loss for  40475 th iteration => 0.00744143525275\n",
      "Loss for  40476 th iteration => 0.00744128295479\n",
      "Loss for  40477 th iteration => 0.0074411177757\n",
      "Loss for  40478 th iteration => 0.00744082534984\n",
      "Loss for  40479 th iteration => 0.00744067872083\n",
      "Loss for  40480 th iteration => 0.00744045314176\n",
      "Loss for  40481 th iteration => 0.00744024012513\n",
      "Loss for  40482 th iteration => 0.00744008066061\n",
      "Loss for  40483 th iteration => 0.00743980896008\n",
      "Loss for  40484 th iteration => 0.00743963560567\n",
      "Loss for  40485 th iteration => 0.00743938673238\n",
      "Loss for  40486 th iteration => 0.0074392500743\n",
      "Loss for  40487 th iteration => 0.00743897467953\n",
      "Loss for  40488 th iteration => 0.00743877435388\n",
      "Loss for  40489 th iteration => 0.00743866961271\n",
      "Loss for  40490 th iteration => 0.00743838834618\n",
      "Loss for  40491 th iteration => 0.00743824829421\n",
      "Loss for  40492 th iteration => 0.00743800227494\n",
      "Loss for  40493 th iteration => 0.00743782716451\n",
      "Loss for  40494 th iteration => 0.00743761614122\n",
      "Loss for  40495 th iteration => 0.00743740622148\n",
      "Loss for  40496 th iteration => 0.00743722994604\n",
      "Loss for  40497 th iteration => 0.00743698546304\n",
      "Loss for  40498 th iteration => 0.00743684369044\n",
      "Loss for  40499 th iteration => 0.00743657298408\n",
      "Loss for  40500 th iteration => 0.00743637340837\n",
      "Loss for  40501 th iteration => 0.00743625521793\n",
      "Loss for  40502 th iteration => 0.00743598489692\n",
      "Loss for  40503 th iteration => 0.00743583753947\n",
      "Loss for  40504 th iteration => 0.00743559715681\n",
      "Loss for  40505 th iteration => 0.00743552576605\n",
      "Loss for  40506 th iteration => 0.00743522586086\n",
      "Loss for  40507 th iteration => 0.00743508723995\n",
      "Loss for  40508 th iteration => 0.0074348542876\n",
      "Loss for  40509 th iteration => 0.00743464917385\n",
      "Loss for  40510 th iteration => 0.00743448244078\n",
      "Loss for  40511 th iteration => 0.00743421536394\n",
      "Loss for  40512 th iteration => 0.00743403807775\n",
      "Loss for  40513 th iteration => 0.00743379369945\n",
      "Loss for  40514 th iteration => 0.00743365315851\n",
      "Loss for  40515 th iteration => 0.00743337815993\n",
      "Loss for  40516 th iteration => 0.00743317799503\n",
      "Loss for  40517 th iteration => 0.00743307755301\n",
      "Loss for  40518 th iteration => 0.00743279259621\n",
      "Loss for  40519 th iteration => 0.00743265680156\n",
      "Loss for  40520 th iteration => 0.00743240713359\n",
      "Loss for  40521 th iteration => 0.0074322362393\n",
      "Loss for  40522 th iteration => 0.00743202160823\n",
      "Loss for  40523 th iteration => 0.00743181586409\n",
      "Loss for  40524 th iteration => 0.00743163602116\n",
      "Loss for  40525 th iteration => 0.00743139567383\n",
      "Loss for  40526 th iteration => 0.00743125037342\n",
      "Loss for  40527 th iteration => 0.00743098001181\n",
      "Loss for  40528 th iteration => 0.00743078102459\n",
      "Loss for  40529 th iteration => 0.00743066557942\n",
      "Loss for  40530 th iteration => 0.00743039311623\n",
      "Loss for  40531 th iteration => 0.00743024847563\n",
      "Loss for  40532 th iteration => 0.00743000634266\n",
      "Loss for  40533 th iteration => 0.00742994078727\n",
      "Loss for  40534 th iteration => 0.00742963566668\n",
      "Loss for  40535 th iteration => 0.00742950280688\n",
      "Loss for  40536 th iteration => 0.00742926471309\n",
      "Loss for  40537 th iteration => 0.00742906528703\n",
      "Loss for  40538 th iteration => 0.00742889348566\n",
      "Loss for  40539 th iteration => 0.00742862936829\n",
      "Loss for  40540 th iteration => 0.00742844936905\n",
      "Loss for  40541 th iteration => 0.00742820827499\n",
      "Loss for  40542 th iteration => 0.00742806505388\n",
      "Loss for  40543 th iteration => 0.0074277899057\n",
      "Loss for  40544 th iteration => 0.00742759039969\n",
      "Loss for  40545 th iteration => 0.00742749315609\n",
      "Loss for  40546 th iteration => 0.00742720560236\n",
      "Loss for  40547 th iteration => 0.00742707297884\n",
      "Loss for  40548 th iteration => 0.00742682074105\n",
      "Loss for  40549 th iteration => 0.00742665299101\n",
      "Loss for  40550 th iteration => 0.00742643581681\n",
      "Loss for  40551 th iteration => 0.00742623319048\n",
      "Loss for  40552 th iteration => 0.00742605083067\n",
      "Loss for  40553 th iteration => 0.00742581357514\n",
      "Loss for  40554 th iteration => 0.00742566578367\n",
      "Loss for  40555 th iteration => 0.00742539547201\n",
      "Loss for  40556 th iteration => 0.00742528473133\n",
      "Loss for  40557 th iteration => 0.00742500815583\n",
      "Loss for  40558 th iteration => 0.00742486816253\n",
      "Loss for  40559 th iteration => 0.00742462080449\n",
      "Loss for  40560 th iteration => 0.00742445372359\n",
      "Loss for  40561 th iteration => 0.00742434518924\n",
      "Loss for  40562 th iteration => 0.00742405410373\n",
      "Loss for  40563 th iteration => 0.00742393727926\n",
      "Loss for  40564 th iteration => 0.00742365452269\n",
      "Loss for  40565 th iteration => 0.0074235294971\n",
      "Loss for  40566 th iteration => 0.00742325498\n",
      "Loss for  40567 th iteration => 0.00742312184119\n",
      "Loss for  40568 th iteration => 0.0074228702591\n",
      "Loss for  40569 th iteration => 0.00742263228915\n",
      "Loss for  40570 th iteration => 0.00742248650686\n",
      "Loss for  40571 th iteration => 0.00742221201815\n",
      "Loss for  40572 th iteration => 0.00742210414272\n",
      "Loss for  40573 th iteration => 0.0074219198181\n",
      "Loss for  40574 th iteration => 0.00742162973811\n",
      "Loss for  40575 th iteration => 0.00742150032538\n",
      "Loss for  40576 th iteration => 0.00742124539766\n",
      "Loss for  40577 th iteration => 0.00742108102041\n",
      "Loss for  40578 th iteration => 0.00742086099501\n",
      "Loss for  40579 th iteration => 0.00742066190111\n",
      "Loss for  40580 th iteration => 0.00742047819348\n",
      "Loss for  40581 th iteration => 0.00742032245501\n",
      "Loss for  40582 th iteration => 0.00742010702711\n",
      "Loss for  40583 th iteration => 0.00741988735223\n",
      "Loss for  40584 th iteration => 0.00741973683835\n",
      "Loss for  40585 th iteration => 0.00741957127679\n",
      "Loss for  40586 th iteration => 0.00741928216477\n",
      "Loss for  40587 th iteration => 0.0074191345004\n",
      "Loss for  40588 th iteration => 0.00741891221541\n",
      "Loss for  40589 th iteration => 0.00741869818274\n",
      "Loss for  40590 th iteration => 0.00741854199327\n",
      "Loss for  40591 th iteration => 0.00741827020989\n",
      "Loss for  40592 th iteration => 0.00741809928305\n",
      "Loss for  40593 th iteration => 0.00741785026187\n",
      "Loss for  40594 th iteration => 0.00741771601983\n",
      "Loss for  40595 th iteration => 0.00741744176092\n",
      "Loss for  40596 th iteration => 0.00741724287632\n",
      "Loss for  40597 th iteration => 0.00741713737742\n",
      "Loss for  40598 th iteration => 0.00741685913174\n",
      "Loss for  40599 th iteration => 0.00741671834335\n",
      "Loss for  40600 th iteration => 0.00741647532338\n",
      "Loss for  40601 th iteration => 0.00741629949809\n",
      "Loss for  40602 th iteration => 0.00741609145225\n",
      "Loss for  40603 th iteration => 0.00741588083951\n",
      "Loss for  40604 th iteration => 0.00741570751941\n",
      "Loss for  40605 th iteration => 0.00741546236552\n",
      "Loss for  40606 th iteration => 0.00741532352588\n",
      "Loss for  40607 th iteration => 0.00741505372946\n",
      "Loss for  40608 th iteration => 0.00741493662048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  40609 th iteration => 0.00741466747378\n",
      "Loss for  40610 th iteration => 0.00741452118389\n",
      "Loss for  40611 th iteration => 0.00741428118301\n",
      "Loss for  40612 th iteration => 0.00741410780355\n",
      "Loss for  40613 th iteration => 0.00741399750388\n",
      "Loss for  40614 th iteration => 0.00741370926976\n",
      "Loss for  40615 th iteration => 0.00741359070649\n",
      "Loss for  40616 th iteration => 0.00741331077467\n",
      "Loss for  40617 th iteration => 0.00741318403644\n",
      "Loss for  40618 th iteration => 0.00741291699744\n",
      "Loss for  40619 th iteration => 0.00741270725441\n",
      "Loss for  40620 th iteration => 0.00741253440212\n",
      "Loss for  40621 th iteration => 0.00741228788035\n",
      "Loss for  40622 th iteration => 0.00741215173459\n",
      "Loss for  40623 th iteration => 0.00741187699803\n",
      "Loss for  40624 th iteration => 0.00741167935519\n",
      "Loss for  40625 th iteration => 0.00741157573722\n",
      "Loss for  40626 th iteration => 0.00741129620386\n",
      "Loss for  40627 th iteration => 0.00741115727992\n",
      "Loss for  40628 th iteration => 0.0074109129886\n",
      "Loss for  40629 th iteration => 0.00741073901158\n",
      "Loss for  40630 th iteration => 0.00741052971044\n",
      "Loss for  40631 th iteration => 0.00741032093008\n",
      "Loss for  40632 th iteration => 0.00741014637042\n",
      "Loss for  40633 th iteration => 0.00740990303332\n",
      "Loss for  40634 th iteration => 0.00740976296956\n",
      "Loss for  40635 th iteration => 0.00740949317702\n",
      "Loss for  40636 th iteration => 0.00740937786274\n",
      "Loss for  40637 th iteration => 0.00740910751212\n",
      "Loss for  40638 th iteration => 0.0074089630064\n",
      "Loss for  40639 th iteration => 0.00740872230827\n",
      "Loss for  40640 th iteration => 0.007408654835\n",
      "Loss for  40641 th iteration => 0.00740835385343\n",
      "Loss for  40642 th iteration => 0.00740821911114\n",
      "Loss for  40643 th iteration => 0.00740798512138\n",
      "Loss for  40644 th iteration => 0.00740778384728\n",
      "Loss for  40645 th iteration => 0.00740761611586\n",
      "Loss for  40646 th iteration => 0.00740735196098\n",
      "Loss for  40647 th iteration => 0.00740717506747\n",
      "Loss for  40648 th iteration => 0.00740693311974\n",
      "Loss for  40649 th iteration => 0.00740679298669\n",
      "Loss for  40650 th iteration => 0.00740651966484\n",
      "Loss for  40651 th iteration => 0.00740632121028\n",
      "Loss for  40652 th iteration => 0.0074062219349\n",
      "Loss for  40653 th iteration => 0.00740593864228\n",
      "Loss for  40654 th iteration => 0.00740580401443\n",
      "Loss for  40655 th iteration => 0.00740555601007\n",
      "Loss for  40656 th iteration => 0.00740538628336\n",
      "Loss for  40657 th iteration => 0.00740517331469\n",
      "Loss for  40658 th iteration => 0.00740496873958\n",
      "Loss for  40659 th iteration => 0.00740479055716\n",
      "Loss for  40660 th iteration => 0.00740455138098\n",
      "Loss for  40661 th iteration => 0.00740440773852\n",
      "Loss for  40662 th iteration => 0.00740413870978\n",
      "Loss for  40663 th iteration => 0.00740402620122\n",
      "Loss for  40664 th iteration => 0.00740375362217\n",
      "Loss for  40665 th iteration => 0.00740361188954\n",
      "Loss for  40666 th iteration => 0.00740336849911\n",
      "Loss for  40667 th iteration => 0.00740320011823\n",
      "Loss for  40668 th iteration => 0.00740309105282\n",
      "Loss for  40669 th iteration => 0.00740280272794\n",
      "Loss for  40670 th iteration => 0.00740268538745\n",
      "Loss for  40671 th iteration => 0.00740240537641\n",
      "Loss for  40672 th iteration => 0.0074022798496\n",
      "Loss for  40673 th iteration => 0.00740200865842\n",
      "Loss for  40674 th iteration => 0.00740180391718\n",
      "Loss for  40675 th iteration => 0.00740162723161\n",
      "Loss for  40676 th iteration => 0.00740138566365\n",
      "Loss for  40677 th iteration => 0.00740124573223\n",
      "Loss for  40678 th iteration => 0.00740097107356\n",
      "Loss for  40679 th iteration => 0.00740087525081\n",
      "Loss for  40680 th iteration => 0.00740058908528\n",
      "Loss for  40681 th iteration => 0.0074004578692\n",
      "Loss for  40682 th iteration => 0.00740020703253\n",
      "Loss for  40683 th iteration => 0.00740004067742\n",
      "Loss for  40684 th iteration => 0.00739982491636\n",
      "Loss for  40685 th iteration => 0.00739962367332\n",
      "Loss for  40686 th iteration => 0.00739944273779\n",
      "Loss for  40687 th iteration => 0.00739920685478\n",
      "Loss for  40688 th iteration => 0.00739906049784\n",
      "Loss for  40689 th iteration => 0.00739879214419\n",
      "Loss for  40690 th iteration => 0.0073985949912\n",
      "Loss for  40691 th iteration => 0.00739848257291\n",
      "Loss for  40692 th iteration => 0.0073982104829\n",
      "Loss for  40693 th iteration => 0.00739806885183\n",
      "Loss for  40694 th iteration => 0.00739782785792\n",
      "Loss for  40695 th iteration => 0.00739776575838\n",
      "Loss for  40696 th iteration => 0.00739746056911\n",
      "Loss for  40697 th iteration => 0.00739733114366\n",
      "Loss for  40698 th iteration => 0.00739709300297\n",
      "Loss for  40699 th iteration => 0.00739689698922\n",
      "Loss for  40700 th iteration => 0.00739672516323\n",
      "Loss for  40701 th iteration => 0.00739646328862\n",
      "Loss for  40702 th iteration => 0.0073963570536\n",
      "Loss for  40703 th iteration => 0.00739604674586\n",
      "Loss for  40704 th iteration => 0.00739590442442\n",
      "Loss for  40705 th iteration => 0.00739563037106\n",
      "Loss for  40706 th iteration => 0.00739543376575\n",
      "Loss for  40707 th iteration => 0.0073953375476\n",
      "Loss for  40708 th iteration => 0.00739505231605\n",
      "Loss for  40709 th iteration => 0.00739492081056\n",
      "Loss for  40710 th iteration => 0.00739467080232\n",
      "Loss for  40711 th iteration => 0.0073945042623\n",
      "Loss for  40712 th iteration => 0.00739428922558\n",
      "Loss for  40713 th iteration => 0.00739408865925\n",
      "Loss for  40714 th iteration => 0.00739398476409\n",
      "Loss for  40715 th iteration => 0.0073936853405\n",
      "Loss for  40716 th iteration => 0.00739358720438\n",
      "Loss for  40717 th iteration => 0.00739328277861\n",
      "Loss for  40718 th iteration => 0.00739310302457\n",
      "Loss for  40719 th iteration => 0.00739289871188\n",
      "Loss for  40720 th iteration => 0.00739269003084\n",
      "Loss for  40721 th iteration => 0.00739251625436\n",
      "Loss for  40722 th iteration => 0.00739238967516\n",
      "Loss for  40723 th iteration => 0.0073921492548\n",
      "Loss for  40724 th iteration => 0.00739196583461\n",
      "Loss for  40725 th iteration => 0.00739171057438\n",
      "Loss for  40726 th iteration => 0.00739154831943\n",
      "Loss for  40727 th iteration => 0.00739133033044\n",
      "Loss for  40728 th iteration => 0.00739113100648\n",
      "Loss for  40729 th iteration => 0.00739095001282\n",
      "Loss for  40730 th iteration => 0.00739071389343\n",
      "Loss for  40731 th iteration => 0.00739056962265\n",
      "Loss for  40732 th iteration => 0.007390296978\n",
      "Loss for  40733 th iteration => 0.00739019097658\n",
      "Loss for  40734 th iteration => 0.00739000753312\n",
      "Loss for  40735 th iteration => 0.0073897199975\n",
      "Loss for  40736 th iteration => 0.0073895914034\n",
      "Loss for  40737 th iteration => 0.00738933901097\n",
      "Loss for  40738 th iteration => 0.00738917546167\n",
      "Loss for  40739 th iteration => 0.00738895796175\n",
      "Loss for  40740 th iteration => 0.00738876006513\n",
      "Loss for  40741 th iteration => 0.0073886558354\n",
      "Loss for  40742 th iteration => 0.00738835732081\n",
      "Loss for  40743 th iteration => 0.00738825883771\n",
      "Loss for  40744 th iteration => 0.00738795474946\n",
      "Loss for  40745 th iteration => 0.00738796097353\n",
      "Loss for  40746 th iteration => 0.00738758789154\n",
      "Loss for  40747 th iteration => 0.00738744360604\n",
      "Loss for  40748 th iteration => 0.00738722128332\n",
      "Loss for  40749 th iteration => 0.00738701058862\n",
      "Loss for  40750 th iteration => 0.00738685440244\n",
      "Loss for  40751 th iteration => 0.00738658602042\n",
      "Loss for  40752 th iteration => 0.00738641634208\n",
      "Loss for  40753 th iteration => 0.00738616939398\n",
      "Loss for  40754 th iteration => 0.00738603641824\n",
      "Loss for  40755 th iteration => 0.00738576494487\n",
      "Loss for  40756 th iteration => 0.00738556679854\n",
      "Loss for  40757 th iteration => 0.00738537110394\n",
      "Loss for  40758 th iteration => 0.00738526393776\n",
      "Loss for  40759 th iteration => 0.00738499069076\n",
      "Loss for  40760 th iteration => 0.00738484828352\n",
      "Loss for  40761 th iteration => 0.00738461021351\n",
      "Loss for  40762 th iteration => 0.00738443281793\n",
      "Loss for  40763 th iteration => 0.00738422967322\n",
      "Loss for  40764 th iteration => 0.00738401753886\n",
      "Loss for  40765 th iteration => 0.00738384907091\n",
      "Loss for  40766 th iteration => 0.00738360244421\n",
      "Loss for  40767 th iteration => 0.0073834686202\n",
      "Loss for  40768 th iteration => 0.0073832797058\n",
      "Loss for  40769 th iteration => 0.00738301399501\n",
      "Loss for  40770 th iteration => 0.00738286760497\n",
      "Loss for  40771 th iteration => 0.00738263102944\n",
      "Loss for  40772 th iteration => 0.00738245812896\n",
      "Loss for  40773 th iteration => 0.00738234784895\n",
      "Loss for  40774 th iteration => 0.00738206290867\n",
      "Loss for  40775 th iteration => 0.00738194437682\n",
      "Loss for  40776 th iteration => 0.00738166772704\n",
      "Loss for  40777 th iteration => 0.0073815410318\n",
      "Loss for  40778 th iteration => 0.00738127896269\n",
      "Loss for  40779 th iteration => 0.00738106756973\n",
      "Loss for  40780 th iteration => 0.00738089967917\n",
      "Loss for  40781 th iteration => 0.00738065153864\n",
      "Loss for  40782 th iteration => 0.00738052032306\n",
      "Loss for  40783 th iteration => 0.00738024673045\n",
      "Loss for  40784 th iteration => 0.00738005128464\n",
      "Loss for  40785 th iteration => 0.00737994608064\n",
      "Loss for  40786 th iteration => 0.00737967143851\n",
      "Loss for  40787 th iteration => 0.00737953097191\n",
      "Loss for  40788 th iteration => 0.00737929152814\n",
      "Loss for  40789 th iteration => 0.00737911605205\n",
      "Loss for  40790 th iteration => 0.00737891155456\n",
      "Loss for  40791 th iteration => 0.00737870131893\n",
      "Loss for  40792 th iteration => 0.00737853151878\n",
      "Loss for  40793 th iteration => 0.00737828677045\n",
      "Loss for  40794 th iteration => 0.0073781515351\n",
      "Loss for  40795 th iteration => 0.0073779644242\n",
      "Loss for  40796 th iteration => 0.00737769745943\n",
      "Loss for  40797 th iteration => 0.0073775528727\n",
      "Loss for  40798 th iteration => 0.00737731505792\n",
      "Loss for  40799 th iteration => 0.00737714402577\n",
      "Loss for  40800 th iteration => 0.00737703404319\n",
      "Loss for  40801 th iteration => 0.00737674936032\n",
      "Loss for  40802 th iteration => 0.00737663112275\n",
      "Loss for  40803 th iteration => 0.00737635473356\n",
      "Loss for  40804 th iteration => 0.00737622832947\n",
      "Loss for  40805 th iteration => 0.00737596512352\n",
      "Loss for  40806 th iteration => 0.00737575548219\n",
      "Loss for  40807 th iteration => 0.00737558640365\n",
      "Loss for  40808 th iteration => 0.00737533999934\n",
      "Loss for  40809 th iteration => 0.00737520761107\n",
      "Loss for  40810 th iteration => 0.00737493439395\n",
      "Loss for  40811 th iteration => 0.00737483390649\n",
      "Loss for  40812 th iteration => 0.00737455510923\n",
      "Loss for  40813 th iteration => 0.00737441929798\n",
      "Loss for  40814 th iteration => 0.00737417575992\n",
      "Loss for  40815 th iteration => 0.00737400487896\n",
      "Loss for  40816 th iteration => 0.00737379634705\n",
      "Loss for  40817 th iteration => 0.00737359064729\n",
      "Loss for  40818 th iteration => 0.00737341687164\n",
      "Loss for  40819 th iteration => 0.00737317660086\n",
      "Loss for  40820 th iteration => 0.0073730373347\n",
      "Loss for  40821 th iteration => 0.00737277059583\n",
      "Loss for  40822 th iteration => 0.00737265622631\n",
      "Loss for  40823 th iteration => 0.00737238878885\n",
      "Loss for  40824 th iteration => 0.00737224522915\n",
      "Loss for  40825 th iteration => 0.00737200747546\n",
      "Loss for  40826 th iteration => 0.00737193827543\n",
      "Loss for  40827 th iteration => 0.00737164284641\n",
      "Loss for  40828 th iteration => 0.00737150641668\n",
      "Loss for  40829 th iteration => 0.00737127794077\n",
      "Loss for  40830 th iteration => 0.00737107501711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  40831 th iteration => 0.00737091276225\n",
      "Loss for  40832 th iteration => 0.00737064844337\n",
      "Loss for  40833 th iteration => 0.00737047606712\n",
      "Loss for  40834 th iteration => 0.00737023346788\n",
      "Loss for  40835 th iteration => 0.00737009782956\n",
      "Loss for  40836 th iteration => 0.00736982643502\n",
      "Loss for  40837 th iteration => 0.00736963018558\n",
      "Loss for  40838 th iteration => 0.00736952974315\n",
      "Loss for  40839 th iteration => 0.00736925145277\n",
      "Loss for  40840 th iteration => 0.00736911569595\n",
      "Loss for  40841 th iteration => 0.00736887265535\n",
      "Loss for  40842 th iteration => 0.00736870183819\n",
      "Loss for  40843 th iteration => 0.00736849379433\n",
      "Loss for  40844 th iteration => 0.0073682881677\n",
      "Loss for  40845 th iteration => 0.00736811487074\n",
      "Loss for  40846 th iteration => 0.0073678746824\n",
      "Loss for  40847 th iteration => 0.0073677358856\n",
      "Loss for  40848 th iteration => 0.00736746956933\n",
      "Loss for  40849 th iteration => 0.00736735470061\n",
      "Loss for  40850 th iteration => 0.00736708831409\n",
      "Loss for  40851 th iteration => 0.00736694426504\n",
      "Loss for  40852 th iteration => 0.00736670874037\n",
      "Loss for  40853 th iteration => 0.0073666381763\n",
      "Loss for  40854 th iteration => 0.00736634465391\n",
      "Loss for  40855 th iteration => 0.00736620688414\n",
      "Loss for  40856 th iteration => 0.00736598029107\n",
      "Loss for  40857 th iteration => 0.00736577605086\n",
      "Loss for  40858 th iteration => 0.00736561565554\n",
      "Loss for  40859 th iteration => 0.0073653498197\n",
      "Loss for  40860 th iteration => 0.00736517855861\n",
      "Loss for  40861 th iteration => 0.00736493540721\n",
      "Loss for  40862 th iteration => 0.00736480087004\n",
      "Loss for  40863 th iteration => 0.00736452859286\n",
      "Loss for  40864 th iteration => 0.00736433366213\n",
      "Loss for  40865 th iteration => 0.00736423282071\n",
      "Loss for  40866 th iteration => 0.00736395547772\n",
      "Loss for  40867 th iteration => 0.00736381933697\n",
      "Loss for  40868 th iteration => 0.00736357722869\n",
      "Loss for  40869 th iteration => 0.00736340604255\n",
      "Loss for  40870 th iteration => 0.00736319891606\n",
      "Loss for  40871 th iteration => 0.00736299293529\n",
      "Loss for  40872 th iteration => 0.00736282054086\n",
      "Loss for  40873 th iteration => 0.00736258001311\n",
      "Loss for  40874 th iteration => 0.00736244248547\n",
      "Loss for  40875 th iteration => 0.00736225821631\n",
      "Loss for  40876 th iteration => 0.00736199029595\n",
      "Loss for  40877 th iteration => 0.00736184829838\n",
      "Loss for  40878 th iteration => 0.00736160999909\n",
      "Loss for  40879 th iteration => 0.00736154385537\n",
      "Loss for  40880 th iteration => 0.00736124647064\n",
      "Loss for  40881 th iteration => 0.00736111305994\n",
      "Loss for  40882 th iteration => 0.00736088266555\n",
      "Loss for  40883 th iteration => 0.00736068272388\n",
      "Loss for  40884 th iteration => 0.00736051858752\n",
      "Loss for  40885 th iteration => 0.00736025621579\n",
      "Loss for  40886 th iteration => 0.00736008293996\n",
      "Loss for  40887 th iteration => 0.00735984231852\n",
      "Loss for  40888 th iteration => 0.00735970579696\n",
      "Loss for  40889 th iteration => 0.00735943498262\n",
      "Loss for  40890 th iteration => 0.00735923917459\n",
      "Loss for  40891 th iteration => 0.00735914074121\n",
      "Loss for  40892 th iteration => 0.00735886153336\n",
      "Loss for  40893 th iteration => 0.00735872777536\n",
      "Loss for  40894 th iteration => 0.00735848382732\n",
      "Loss for  40895 th iteration => 0.00735831499911\n",
      "Loss for  40896 th iteration => 0.00735810605749\n",
      "Loss for  40897 th iteration => 0.00735790241031\n",
      "Loss for  40898 th iteration => 0.00735772822491\n",
      "Loss for  40899 th iteration => 0.00735749000685\n",
      "Loss for  40900 th iteration => 0.00735735033057\n",
      "Loss for  40901 th iteration => 0.00735708516382\n",
      "Loss for  40902 th iteration => 0.00735697073382\n",
      "Loss for  40903 th iteration => 0.00735670499682\n",
      "Loss for  40904 th iteration => 0.00735656138387\n",
      "Loss for  40905 th iteration => 0.00735632707308\n",
      "Loss for  40906 th iteration => 0.00735625703241\n",
      "Loss for  40907 th iteration => 0.00735596406959\n",
      "Loss for  40908 th iteration => 0.00735582682155\n",
      "Loss for  40909 th iteration => 0.00735560078984\n",
      "Loss for  40910 th iteration => 0.00735539706944\n",
      "Loss for  40911 th iteration => 0.00735523723751\n",
      "Loss for  40912 th iteration => 0.00735497150283\n",
      "Loss for  40913 th iteration => 0.00735480069518\n",
      "Loss for  40914 th iteration => 0.00735455817662\n",
      "Loss for  40915 th iteration => 0.0073544240915\n",
      "Loss for  40916 th iteration => 0.00735415179217\n",
      "Loss for  40917 th iteration => 0.00735395786205\n",
      "Loss for  40918 th iteration => 0.00735385778481\n",
      "Loss for  40919 th iteration => 0.00735358076028\n",
      "Loss for  40920 th iteration => 0.00735344538955\n",
      "Loss for  40921 th iteration => 0.00735320359378\n",
      "Loss for  40922 th iteration => 0.00735303318363\n",
      "Loss for  40923 th iteration => 0.00735282636355\n",
      "Loss for  40924 th iteration => 0.0073526211649\n",
      "Loss for  40925 th iteration => 0.00735244907063\n",
      "Loss for  40926 th iteration => 0.00735220933125\n",
      "Loss for  40927 th iteration => 0.00735207303884\n",
      "Loss for  40928 th iteration => 0.0073518878337\n",
      "Loss for  40929 th iteration => 0.00735162148435\n",
      "Loss for  40930 th iteration => 0.00735147900624\n",
      "Loss for  40931 th iteration => 0.00735124335081\n",
      "Loss for  40932 th iteration => 0.00735117566798\n",
      "Loss for  40933 th iteration => 0.00735088089056\n",
      "Loss for  40934 th iteration => 0.00735074596822\n",
      "Loss for  40935 th iteration => 0.00735051815395\n",
      "Loss for  40936 th iteration => 0.00735031672743\n",
      "Loss for  40937 th iteration => 0.00735015514466\n",
      "Loss for  40938 th iteration => 0.0073498913727\n",
      "Loss for  40939 th iteration => 0.00734971965197\n",
      "Loss for  40940 th iteration => 0.00734947856795\n",
      "Loss for  40941 th iteration => 0.00734934358563\n",
      "Loss for  40942 th iteration => 0.00734907226072\n",
      "Loss for  40943 th iteration => 0.0073488779094\n",
      "Loss for  40944 th iteration => 0.00734877921987\n",
      "Loss for  40945 th iteration => 0.00734850134317\n",
      "Loss for  40946 th iteration => 0.007348367348\n",
      "Loss for  40947 th iteration => 0.00734812471206\n",
      "Loss for  40948 th iteration => 0.00734795566563\n",
      "Loss for  40949 th iteration => 0.00734774801712\n",
      "Loss for  40950 th iteration => 0.0073475441706\n",
      "Loss for  40951 th iteration => 0.00734737125934\n",
      "Loss for  40952 th iteration => 0.00734713286081\n",
      "Loss for  40953 th iteration => 0.00734699550847\n",
      "Loss for  40954 th iteration => 0.0073468114793\n",
      "Loss for  40955 th iteration => 0.00734654514737\n",
      "Loss for  40956 th iteration => 0.00734640317809\n",
      "Loss for  40957 th iteration => 0.00734616716151\n",
      "Loss for  40958 th iteration => 0.00734610042888\n",
      "Loss for  40959 th iteration => 0.0073458052349\n",
      "Loss for  40960 th iteration => 0.00734567125039\n",
      "Loss for  40961 th iteration => 0.00734544303192\n",
      "Loss for  40962 th iteration => 0.00734524253092\n",
      "Loss for  40963 th iteration => 0.00734508055625\n",
      "Loss for  40964 th iteration => 0.00734481771709\n",
      "Loss for  40965 th iteration => 0.00734464583508\n",
      "Loss for  40966 th iteration => 0.00734440543847\n",
      "Loss for  40967 th iteration => 0.00734427030089\n",
      "Loss for  40968 th iteration => 0.0073439996105\n",
      "Loss for  40969 th iteration => 0.00734380515232\n",
      "Loss for  40970 th iteration => 0.00734370716201\n",
      "Loss for  40971 th iteration => 0.00734342911686\n",
      "Loss for  40972 th iteration => 0.00734329581768\n",
      "Loss for  40973 th iteration => 0.00734305301643\n",
      "Loss for  40974 th iteration => 0.00734288466294\n",
      "Loss for  40975 th iteration => 0.00734267685207\n",
      "Loss for  40976 th iteration => 0.00734247369561\n",
      "Loss for  40977 th iteration => 0.00734230062478\n",
      "Loss for  40978 th iteration => 0.0073420629136\n",
      "Loss for  40979 th iteration => 0.00734192539327\n",
      "Loss for  40980 th iteration => 0.00734174171069\n",
      "Loss for  40981 th iteration => 0.00734147592564\n",
      "Loss for  40982 th iteration => 0.0073413339387\n",
      "Loss for  40983 th iteration => 0.00734109838607\n",
      "Loss for  40984 th iteration => 0.00734103141987\n",
      "Loss for  40985 th iteration => 0.00734073698485\n",
      "Loss for  40986 th iteration => 0.00734060277106\n",
      "Loss for  40987 th iteration => 0.00734037530733\n",
      "Loss for  40988 th iteration => 0.00734017458118\n",
      "Loss for  40989 th iteration => 0.00734001335721\n",
      "Loss for  40990 th iteration => 0.00733975058617\n",
      "Loss for  40991 th iteration => 0.00733957917499\n",
      "Loss for  40992 th iteration => 0.00733933883746\n",
      "Loss for  40993 th iteration => 0.00733920416843\n",
      "Loss for  40994 th iteration => 0.0073389338285\n",
      "Loss for  40995 th iteration => 0.00733873952772\n",
      "Loss for  40996 th iteration => 0.00733864165388\n",
      "Loss for  40997 th iteration => 0.00733836401884\n",
      "Loss for  40998 th iteration => 0.00733823084043\n",
      "Loss for  40999 th iteration => 0.00733798844494\n",
      "Loss for  41000 th iteration => 0.00733782021658\n",
      "Loss for  41001 th iteration => 0.00733761280705\n",
      "Loss for  41002 th iteration => 0.00733740978016\n",
      "Loss for  41003 th iteration => 0.00733723710618\n",
      "Loss for  41004 th iteration => 0.00733699952906\n",
      "Loss for  41005 th iteration => 0.00733686259285\n",
      "Loss for  41006 th iteration => 0.0073366785595\n",
      "Loss for  41007 th iteration => 0.00733641376632\n",
      "Loss for  41008 th iteration => 0.00733627131909\n",
      "Loss for  41009 th iteration => 0.00733603692208\n",
      "Loss for  41010 th iteration => 0.00733596872709\n",
      "Loss for  41011 th iteration => 0.00733567603914\n",
      "Loss for  41012 th iteration => 0.00733554061488\n",
      "Loss for  41013 th iteration => 0.00733531488006\n",
      "Loss for  41014 th iteration => 0.00733511296136\n",
      "Loss for  41015 th iteration => 0.00733495344853\n",
      "Loss for  41016 th iteration => 0.00733469002009\n",
      "Loss for  41017 th iteration => 0.00733451961104\n",
      "Loss for  41018 th iteration => 0.00733427880429\n",
      "Loss for  41019 th iteration => 0.00733414512814\n",
      "Loss for  41020 th iteration => 0.00733387490163\n",
      "Loss for  41021 th iteration => 0.00733368098031\n",
      "Loss for  41022 th iteration => 0.00733358272931\n",
      "Loss for  41023 th iteration => 0.00733330599434\n",
      "Loss for  41024 th iteration => 0.0073331724494\n",
      "Loss for  41025 th iteration => 0.00733293094332\n",
      "Loss for  41026 th iteration => 0.00733276235903\n",
      "Loss for  41027 th iteration => 0.00733255582828\n",
      "Loss for  41028 th iteration => 0.00733235245604\n",
      "Loss for  41029 th iteration => 0.00733218065023\n",
      "Loss for  41030 th iteration => 0.00733194273833\n",
      "Loss for  41031 th iteration => 0.00733180702054\n",
      "Loss for  41032 th iteration => 0.00733162205029\n",
      "Loss for  41033 th iteration => 0.0073313586228\n",
      "Loss for  41034 th iteration => 0.00733121534325\n",
      "Loss for  41035 th iteration => 0.00733098268118\n",
      "Loss for  41036 th iteration => 0.00733091242102\n",
      "Loss for  41037 th iteration => 0.00733062231039\n",
      "Loss for  41038 th iteration => 0.00733048485105\n",
      "Loss for  41039 th iteration => 0.00733026166368\n",
      "Loss for  41040 th iteration => 0.00733005773944\n",
      "Loss for  41041 th iteration => 0.00732990074473\n",
      "Loss for  41042 th iteration => 0.00732963605055\n",
      "Loss for  41043 th iteration => 0.00732946709004\n",
      "Loss for  41044 th iteration => 0.00732922537004\n",
      "Loss for  41045 th iteration => 0.00732909312732\n",
      "Loss for  41046 th iteration => 0.00732882281677\n",
      "Loss for  41047 th iteration => 0.00732862946142\n",
      "Loss for  41048 th iteration => 0.00732853041473\n",
      "Loss for  41049 th iteration => 0.0073282549951\n",
      "Loss for  41050 th iteration => 0.00732812067042\n",
      "Loss for  41051 th iteration => 0.00732788046372\n",
      "Loss for  41052 th iteration => 0.00732771111555\n",
      "Loss for  41053 th iteration => 0.00732750586832\n",
      "Loss for  41054 th iteration => 0.00732730174796\n",
      "Loss for  41055 th iteration => 0.00732713120989\n",
      "Loss for  41056 th iteration => 0.00732689256555\n",
      "Loss for  41057 th iteration => 0.00732675860121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  41058 th iteration => 0.00732657220159\n",
      "Loss for  41059 th iteration => 0.00732631045372\n",
      "Loss for  41060 th iteration => 0.0073261660293\n",
      "Loss for  41061 th iteration => 0.00732593558688\n",
      "Loss for  41062 th iteration => 0.0073258625589\n",
      "Loss for  41063 th iteration => 0.00732557572292\n",
      "Loss for  41064 th iteration => 0.00732543553575\n",
      "Loss for  41065 th iteration => 0.00732521558331\n",
      "Loss for  41066 th iteration => 0.00732500897056\n",
      "Loss for  41067 th iteration => 0.0073248551717\n",
      "Loss for  41068 th iteration => 0.00732458870216\n",
      "Loss for  41069 th iteration => 0.00732442156509\n",
      "Loss for  41070 th iteration => 0.00732417855875\n",
      "Loss for  41071 th iteration => 0.00732404811943\n",
      "Loss for  41072 th iteration => 0.00732377756072\n",
      "Loss for  41073 th iteration => 0.0073235849279\n",
      "Loss for  41074 th iteration => 0.00732348473032\n",
      "Loss for  41075 th iteration => 0.00732321097832\n",
      "Loss for  41076 th iteration => 0.00732307552317\n",
      "Loss for  41077 th iteration => 0.0073228369637\n",
      "Loss for  41078 th iteration => 0.00732266650533\n",
      "Loss for  41079 th iteration => 0.00732246288506\n",
      "Loss for  41080 th iteration => 0.00732225767464\n",
      "Loss for  41081 th iteration => 0.00732208874341\n",
      "Loss for  41082 th iteration => 0.00732184958353\n",
      "Loss for  41083 th iteration => 0.00732179585495\n",
      "Loss for  41084 th iteration => 0.00732146182244\n",
      "Loss for  41085 th iteration => 0.00732131911247\n",
      "Loss for  41086 th iteration => 0.00732108547892\n",
      "Loss for  41087 th iteration => 0.00732101601266\n",
      "Loss for  41088 th iteration => 0.00732072614097\n",
      "Loss for  41089 th iteration => 0.00732058946232\n",
      "Loss for  41090 th iteration => 0.00732036652714\n",
      "Loss for  41091 th iteration => 0.00732016337039\n",
      "Loss for  41092 th iteration => 0.00732000664109\n",
      "Loss for  41093 th iteration => 0.00731974372579\n",
      "Loss for  41094 th iteration => 0.00731957506738\n",
      "Loss for  41095 th iteration => 0.00731933406979\n",
      "Loss for  41096 th iteration => 0.00731920213733\n",
      "Loss for  41097 th iteration => 0.0073189337575\n",
      "Loss for  41098 th iteration => 0.0073187394984\n",
      "Loss for  41099 th iteration => 0.00731864131391\n",
      "Loss for  41100 th iteration => 0.00731836606224\n",
      "Loss for  41101 th iteration => 0.00731823259656\n",
      "Loss for  41102 th iteration => 0.00731799256085\n",
      "Loss for  41103 th iteration => 0.0073178240688\n",
      "Loss for  41104 th iteration => 0.00731761899527\n",
      "Loss for  41105 th iteration => 0.00731741572848\n",
      "Loss for  41106 th iteration => 0.00731724536649\n",
      "Loss for  41107 th iteration => 0.00731700757348\n",
      "Loss for  41108 th iteration => 0.00731687318842\n",
      "Loss for  41109 th iteration => 0.00731668775102\n",
      "Loss for  41110 th iteration => 0.00731642747754\n",
      "Loss for  41111 th iteration => 0.00731628260835\n",
      "Loss for  41112 th iteration => 0.0073160527862\n",
      "Loss for  41113 th iteration => 0.00731597827382\n",
      "Loss for  41114 th iteration => 0.00731569393927\n",
      "Loss for  41115 th iteration => 0.00731555228644\n",
      "Loss for  41116 th iteration => 0.00731533481689\n",
      "Loss for  41117 th iteration => 0.00731512675676\n",
      "Loss for  41118 th iteration => 0.00731497542271\n",
      "Loss for  41119 th iteration => 0.00731470906569\n",
      "Loss for  41120 th iteration => 0.00731454344644\n",
      "Loss for  41121 th iteration => 0.00731429995404\n",
      "Loss for  41122 th iteration => 0.00731417102479\n",
      "Loss for  41123 th iteration => 0.00731390184733\n",
      "Loss for  41124 th iteration => 0.0073137088239\n",
      "Loss for  41125 th iteration => 0.00731360836345\n",
      "Loss for  41126 th iteration => 0.00731333589646\n",
      "Loss for  41127 th iteration => 0.00731320018968\n",
      "Loss for  41128 th iteration => 0.00731296290388\n",
      "Loss for  41129 th iteration => 0.00731279220525\n",
      "Loss for  41130 th iteration => 0.00731258984718\n",
      "Loss for  41131 th iteration => 0.00731238440797\n",
      "Loss for  41132 th iteration => 0.00731221672735\n",
      "Loss for  41133 th iteration => 0.00731197713544\n",
      "Loss for  41134 th iteration => 0.00731192284589\n",
      "Loss for  41135 th iteration => 0.00731159204452\n",
      "Loss for  41136 th iteration => 0.00731144806202\n",
      "Loss for  41137 th iteration => 0.00731121654668\n",
      "Loss for  41138 th iteration => 0.00731104603788\n",
      "Loss for  41139 th iteration => 0.00731093942224\n",
      "Loss for  41140 th iteration => 0.00731065824546\n",
      "Loss for  41141 th iteration => 0.00731054338599\n",
      "Loss for  41142 th iteration => 0.00731027049169\n",
      "Loss for  41143 th iteration => 0.00731014747656\n",
      "Loss for  41144 th iteration => 0.00730989035441\n",
      "Loss for  41145 th iteration => 0.00730968129768\n",
      "Loss for  41146 th iteration => 0.00730951851499\n",
      "Loss for  41147 th iteration => 0.00730927272746\n",
      "Loss for  41148 th iteration => 0.007309146602\n",
      "Loss for  41149 th iteration => 0.00730887673208\n",
      "Loss for  41150 th iteration => 0.00730868502118\n",
      "Loss for  41151 th iteration => 0.00730858209738\n",
      "Loss for  41152 th iteration => 0.0073083126024\n",
      "Loss for  41153 th iteration => 0.00730817446469\n",
      "Loss for  41154 th iteration => 0.00730794011855\n",
      "Loss for  41155 th iteration => 0.00730776702108\n",
      "Loss for  41156 th iteration => 0.00730756757063\n",
      "Loss for  41157 th iteration => 0.00730735976439\n",
      "Loss for  41158 th iteration => 0.00730719495965\n",
      "Loss for  41159 th iteration => 0.00730695373437\n",
      "Loss for  41160 th iteration => 0.00730690037623\n",
      "Loss for  41161 th iteration => 0.00730657075346\n",
      "Loss for  41162 th iteration => 0.0073064251425\n",
      "Loss for  41163 th iteration => 0.00730619666382\n",
      "Loss for  41164 th iteration => 0.00730611938196\n",
      "Loss for  41165 th iteration => 0.00730583881993\n",
      "Loss for  41166 th iteration => 0.0073056944395\n",
      "Loss for  41167 th iteration => 0.00730548070091\n",
      "Loss for  41168 th iteration => 0.00730526995427\n",
      "Loss for  41169 th iteration => 0.00730512231041\n",
      "Loss for  41170 th iteration => 0.00730485501185\n",
      "Loss for  41171 th iteration => 0.00730469186198\n",
      "Loss for  41172 th iteration => 0.00730444693495\n",
      "Loss for  41173 th iteration => 0.00730432045528\n",
      "Loss for  41174 th iteration => 0.00730405224974\n",
      "Loss for  41175 th iteration => 0.00730385940084\n",
      "Loss for  41176 th iteration => 0.00730375740233\n",
      "Loss for  41177 th iteration => 0.00730348748677\n",
      "Loss for  41178 th iteration => 0.00730335026456\n",
      "Loss for  41179 th iteration => 0.0073031155075\n",
      "Loss for  41180 th iteration => 0.00730294331604\n",
      "Loss for  41181 th iteration => 0.00730274346405\n",
      "Loss for  41182 th iteration => 0.0073025365546\n",
      "Loss for  41183 th iteration => 0.00730237135741\n",
      "Loss for  41184 th iteration => 0.00730213016247\n",
      "Loss for  41185 th iteration => 0.0073020763456\n",
      "Loss for  41186 th iteration => 0.00730174841542\n",
      "Loss for  41187 th iteration => 0.00730160297689\n",
      "Loss for  41188 th iteration => 0.00730137393176\n",
      "Loss for  41189 th iteration => 0.0073012019861\n",
      "Loss for  41190 th iteration => 0.00730109486344\n",
      "Loss for  41191 th iteration => 0.00730081521178\n",
      "Loss for  41192 th iteration => 0.00730069985485\n",
      "Loss for  41193 th iteration => 0.00730042847609\n",
      "Loss for  41194 th iteration => 0.0073003049729\n",
      "Loss for  41195 th iteration => 0.00730005143627\n",
      "Loss for  41196 th iteration => 0.00729984002727\n",
      "Loss for  41197 th iteration => 0.00729968060496\n",
      "Loss for  41198 th iteration => 0.00729943249655\n",
      "Loss for  41199 th iteration => 0.00729930970008\n",
      "Loss for  41200 th iteration => 0.00729904040401\n",
      "Loss for  41201 th iteration => 0.00729884923542\n",
      "Loss for  41202 th iteration => 0.0072987439558\n",
      "Loss for  41203 th iteration => 0.00729847782363\n",
      "Loss for  41204 th iteration => 0.00729833736325\n",
      "Loss for  41205 th iteration => 0.00729810634676\n",
      "Loss for  41206 th iteration => 0.0072979309596\n",
      "Loss for  41207 th iteration => 0.0072977348058\n",
      "Loss for  41208 th iteration => 0.0072975247427\n",
      "Loss for  41209 th iteration => 0.00729736320177\n",
      "Loss for  41210 th iteration => 0.00729711988731\n",
      "Loss for  41211 th iteration => 0.00729706712747\n",
      "Loss for  41212 th iteration => 0.00729674066959\n",
      "Loss for  41213 th iteration => 0.00729659296418\n",
      "Loss for  41214 th iteration => 0.00729636793439\n",
      "Loss for  41215 th iteration => 0.00729628485843\n",
      "Loss for  41216 th iteration => 0.00729601107134\n",
      "Loss for  41217 th iteration => 0.00729586098101\n",
      "Loss for  41218 th iteration => 0.00729565393369\n",
      "Loss for  41219 th iteration => 0.00729543755996\n",
      "Loss for  41220 th iteration => 0.00729529652507\n",
      "Loss for  41221 th iteration => 0.00729502608307\n",
      "Loss for  41222 th iteration => 0.00729486701007\n",
      "Loss for  41223 th iteration => 0.00729461904923\n",
      "Loss for  41224 th iteration => 0.00729449660557\n",
      "Loss for  41225 th iteration => 0.00729422865429\n",
      "Loss for  41226 th iteration => 0.00729403665144\n",
      "Loss for  41227 th iteration => 0.00729393162052\n",
      "Loss for  41228 th iteration => 0.00729366573885\n",
      "Loss for  41229 th iteration => 0.00729352552597\n",
      "Loss for  41230 th iteration => 0.00729329476109\n",
      "Loss for  41231 th iteration => 0.00729311962041\n",
      "Loss for  41232 th iteration => 0.00729292371917\n",
      "Loss for  41233 th iteration => 0.00729271390168\n",
      "Loss for  41234 th iteration => 0.00729255261408\n",
      "Loss for  41235 th iteration => 0.00729230891796\n",
      "Loss for  41236 th iteration => 0.00729225582285\n",
      "Loss for  41237 th iteration => 0.00729193128136\n",
      "Loss for  41238 th iteration => 0.00729178323916\n",
      "Loss for  41239 th iteration => 0.00729155822253\n",
      "Loss for  41240 th iteration => 0.00729147410801\n",
      "Loss for  41241 th iteration => 0.00729120185168\n",
      "Loss for  41242 th iteration => 0.00729105073439\n",
      "Loss for  41243 th iteration => 0.00729084520634\n",
      "Loss for  41244 th iteration => 0.00729062781701\n",
      "Loss for  41245 th iteration => 0.00729048829013\n",
      "Loss for  41246 th iteration => 0.00729021804704\n",
      "Loss for  41247 th iteration => 0.00729005990312\n",
      "Loss for  41248 th iteration => 0.00728981151357\n",
      "Loss for  41249 th iteration => 0.00728968999491\n",
      "Loss for  41250 th iteration => 0.00728942312348\n",
      "Loss for  41251 th iteration => 0.00728923053405\n",
      "Loss for  41252 th iteration => 0.00728912521474\n",
      "Loss for  41253 th iteration => 0.00728886011687\n",
      "Loss for  41254 th iteration => 0.00728871962129\n",
      "Loss for  41255 th iteration => 0.00728848963446\n",
      "Loss for  41256 th iteration => 0.00728831421684\n",
      "Loss for  41257 th iteration => 0.00728811908783\n",
      "Loss for  41258 th iteration => 0.00728790899925\n",
      "Loss for  41259 th iteration => 0.00728774847798\n",
      "Loss for  41260 th iteration => 0.00728750408126\n",
      "Loss for  41261 th iteration => 0.00728745074335\n",
      "Loss for  41262 th iteration => 0.00728712829589\n",
      "Loss for  41263 th iteration => 0.00728697950725\n",
      "Loss for  41264 th iteration => 0.00728675531158\n",
      "Loss for  41265 th iteration => 0.00728658011433\n",
      "Loss for  41266 th iteration => 0.00728647219196\n",
      "Loss for  41267 th iteration => 0.00728619485011\n",
      "Loss for  41268 th iteration => 0.00728607871063\n",
      "Loss for  41269 th iteration => 0.00728580962443\n",
      "Loss for  41270 th iteration => 0.00728568535563\n",
      "Loss for  41271 th iteration => 0.00728543818824\n",
      "Loss for  41272 th iteration => 0.00728522210802\n",
      "Loss for  41273 th iteration => 0.00728506884615\n",
      "Loss for  41274 th iteration => 0.00728481612651\n",
      "Loss for  41275 th iteration => 0.00728469943052\n",
      "Loss for  41276 th iteration => 0.00728443095443\n",
      "Loss for  41277 th iteration => 0.00728424052164\n",
      "Loss for  41278 th iteration => 0.00728413085763\n",
      "Loss for  41279 th iteration => 0.00728387059812\n",
      "Loss for  41280 th iteration => 0.00728372581439\n",
      "Loss for  41281 th iteration => 0.00728350060954\n",
      "Loss for  41282 th iteration => 0.00728332095969\n",
      "Loss for  41283 th iteration => 0.00728313055691\n",
      "Loss for  41284 th iteration => 0.00728291629137\n",
      "Loss for  41285 th iteration => 0.00728276044122\n",
      "Loss for  41286 th iteration => 0.00728251329026\n",
      "Loss for  41287 th iteration => 0.00728246117409\n",
      "Loss for  41288 th iteration => 0.0072821405811\n",
      "Loss for  41289 th iteration => 0.00728198869638\n",
      "Loss for  41290 th iteration => 0.00728176984369\n",
      "Loss for  41291 th iteration => 0.00728167603002\n",
      "Loss for  41292 th iteration => 0.00728141442428\n",
      "Loss for  41293 th iteration => 0.00728125374663\n",
      "Loss for  41294 th iteration => 0.0072810587312\n",
      "Loss for  41295 th iteration => 0.00728083191815\n",
      "Loss for  41296 th iteration => 0.00728070276804\n",
      "Loss for  41297 th iteration => 0.0072804265418\n",
      "Loss for  41298 th iteration => 0.00728027454841\n",
      "Loss for  41299 th iteration => 0.00728002179343\n",
      "Loss for  41300 th iteration => 0.00727982917315\n",
      "Loss for  41301 th iteration => 0.00727963656443\n",
      "Loss for  41302 th iteration => 0.00727944596098\n",
      "Loss for  41303 th iteration => 0.00727933509544\n",
      "Loss for  41304 th iteration => 0.00727907656915\n",
      "Loss for  41305 th iteration => 0.00727893049632\n",
      "Loss for  41306 th iteration => 0.00727870711177\n",
      "Loss for  41307 th iteration => 0.00727852608671\n",
      "Loss for  41308 th iteration => 0.00727833758985\n",
      "Loss for  41309 th iteration => 0.00727812186442\n",
      "Loss for  41310 th iteration => 0.0072779680044\n",
      "Loss for  41311 th iteration => 0.00727771992454\n",
      "Loss for  41312 th iteration => 0.00727759491004\n",
      "Loss for  41313 th iteration => 0.00727734802986\n",
      "Loss for  41314 th iteration => 0.00727719378948\n",
      "Loss for  41315 th iteration => 0.00727697609751\n",
      "Loss for  41316 th iteration => 0.00727679282278\n",
      "Loss for  41317 th iteration => 0.00727660412821\n",
      "Loss for  41318 th iteration => 0.007276393334\n",
      "Loss for  41319 th iteration => 0.00727633274314\n",
      "Loss for  41320 th iteration => 0.00727601030051\n",
      "Loss for  41321 th iteration => 0.0072758801422\n",
      "Loss for  41322 th iteration => 0.00727564221276\n",
      "Loss for  41323 th iteration => 0.00727547473745\n",
      "Loss for  41324 th iteration => 0.00727527404859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  41325 th iteration => 0.00727506953593\n",
      "Loss for  41326 th iteration => 0.00727490580914\n",
      "Loss for  41327 th iteration => 0.00727466453528\n",
      "Loss for  41328 th iteration => 0.00727453749552\n",
      "Loss for  41329 th iteration => 0.00727427024422\n",
      "Loss for  41330 th iteration => 0.00727407982778\n",
      "Loss for  41331 th iteration => 0.0072739805757\n",
      "Loss for  41332 th iteration => 0.00727371099899\n",
      "Loss for  41333 th iteration => 0.00727357652232\n",
      "Loss for  41334 th iteration => 0.0072733421046\n",
      "Loss for  41335 th iteration => 0.00727317265842\n",
      "Loss for  41336 th iteration => 0.00727297314564\n",
      "Loss for  41337 th iteration => 0.00727276898184\n",
      "Loss for  41338 th iteration => 0.00727260412308\n",
      "Loss for  41339 th iteration => 0.0072723668953\n",
      "Loss for  41340 th iteration => 0.0072723133326\n",
      "Loss for  41341 th iteration => 0.00727198612563\n",
      "Loss for  41342 th iteration => 0.00727184215148\n",
      "Loss for  41343 th iteration => 0.00727161662392\n",
      "Loss for  41344 th iteration => 0.00727153963982\n",
      "Loss for  41345 th iteration => 0.00727126233641\n",
      "Loss for  41346 th iteration => 0.00727111828639\n",
      "Loss for  41347 th iteration => 0.0072709077744\n",
      "Loss for  41348 th iteration => 0.00727069738943\n",
      "Loss for  41349 th iteration => 0.00727055294151\n",
      "Loss for  41350 th iteration => 0.0072702857965\n",
      "Loss for  41351 th iteration => 0.00727012570932\n",
      "Loss for  41352 th iteration => 0.00726988131284\n",
      "Loss for  41353 th iteration => 0.00726975787351\n",
      "Loss for  41354 th iteration => 0.00726949048422\n",
      "Loss for  41355 th iteration => 0.0072693005989\n",
      "Loss for  41356 th iteration => 0.00726919859421\n",
      "Loss for  41357 th iteration => 0.00726893224849\n",
      "Loss for  41358 th iteration => 0.00726879505668\n",
      "Loss for  41359 th iteration => 0.00726856383258\n",
      "Loss for  41360 th iteration => 0.00726839170837\n",
      "Loss for  41361 th iteration => 0.00726819535216\n",
      "Loss for  41362 th iteration => 0.0072679885471\n",
      "Loss for  41363 th iteration => 0.00726782680822\n",
      "Loss for  41364 th iteration => 0.00726758741852\n",
      "Loss for  41365 th iteration => 0.00726753399974\n",
      "Loss for  41366 th iteration => 0.00726720975851\n",
      "Loss for  41367 th iteration => 0.00726706312154\n",
      "Loss for  41368 th iteration => 0.00726684124646\n",
      "Loss for  41369 th iteration => 0.00726675799666\n",
      "Loss for  41370 th iteration => 0.00726648741498\n",
      "Loss for  41371 th iteration => 0.00726633718365\n",
      "Loss for  41372 th iteration => 0.00726613330948\n",
      "Loss for  41373 th iteration => 0.00726591682631\n",
      "Loss for  41374 th iteration => 0.00726577893357\n",
      "Loss for  41375 th iteration => 0.00726550817204\n",
      "Loss for  41376 th iteration => 0.00726535180499\n",
      "Loss for  41377 th iteration => 0.00726510420508\n",
      "Loss for  41378 th iteration => 0.00726498444557\n",
      "Loss for  41379 th iteration => 0.00726471688627\n",
      "Loss for  41380 th iteration => 0.00726452757208\n",
      "Loss for  41381 th iteration => 0.00726442271625\n",
      "Loss for  41382 th iteration => 0.00726415969868\n",
      "Loss for  41383 th iteration => 0.00726401969425\n",
      "Loss for  41384 th iteration => 0.00726379175985\n",
      "Loss for  41385 th iteration => 0.00726361686119\n",
      "Loss for  41386 th iteration => 0.00726342375661\n",
      "Loss for  41387 th iteration => 0.0072632142149\n",
      "Loss for  41388 th iteration => 0.00726305595022\n",
      "Loss for  41389 th iteration => 0.00726288623222\n",
      "Loss for  41390 th iteration => 0.00726270107048\n",
      "Loss for  41391 th iteration => 0.00726248284903\n",
      "Loss for  41392 th iteration => 0.00726225889679\n",
      "Loss for  41393 th iteration => 0.00726217628643\n",
      "Loss for  41394 th iteration => 0.00726190554297\n",
      "Loss for  41395 th iteration => 0.00726175593764\n",
      "Loss for  41396 th iteration => 0.0072615519151\n",
      "Loss for  41397 th iteration => 0.00726133604463\n",
      "Loss for  41398 th iteration => 0.00726119801677\n",
      "Loss for  41399 th iteration => 0.00726092944538\n",
      "Loss for  41400 th iteration => 0.00726077301821\n",
      "Loss for  41401 th iteration => 0.00726052594458\n",
      "Loss for  41402 th iteration => 0.00726040613517\n",
      "Loss for  41403 th iteration => 0.00726014101513\n",
      "Loss for  41404 th iteration => 0.00725994998055\n",
      "Loss for  41405 th iteration => 0.00725984536506\n",
      "Loss for  41406 th iteration => 0.00725958258192\n",
      "Loss for  41407 th iteration => 0.00725944281081\n",
      "Loss for  41408 th iteration => 0.00725921511775\n",
      "Loss for  41409 th iteration => 0.00725904044566\n",
      "Loss for  41410 th iteration => 0.00725884758905\n",
      "Loss for  41411 th iteration => 0.00725863826743\n",
      "Loss for  41412 th iteration => 0.00725847999679\n",
      "Loss for  41413 th iteration => 0.0072582371737\n",
      "Loss for  41414 th iteration => 0.00725818374192\n",
      "Loss for  41415 th iteration => 0.00725786511288\n",
      "Loss for  41416 th iteration => 0.00725771553919\n",
      "Loss for  41417 th iteration => 0.00725749618701\n",
      "Loss for  41418 th iteration => 0.00725740595833\n",
      "Loss for  41419 th iteration => 0.00725714327927\n",
      "Loss for  41420 th iteration => 0.0072569861589\n",
      "Loss for  41421 th iteration => 0.00725679009804\n",
      "Loss for  41422 th iteration => 0.00725656681431\n",
      "Loss for  41423 th iteration => 0.00725643664692\n",
      "Loss for  41424 th iteration => 0.00725616346768\n",
      "Loss for  41425 th iteration => 0.00725601148161\n",
      "Loss for  41426 th iteration => 0.00725576066355\n",
      "Loss for  41427 th iteration => 0.00725556930049\n",
      "Loss for  41428 th iteration => 0.00725537794894\n",
      "Loss for  41429 th iteration => 0.00725518806692\n",
      "Loss for  41430 th iteration => 0.00725507935144\n",
      "Loss for  41431 th iteration => 0.00725482117939\n",
      "Loss for  41432 th iteration => 0.00725467725884\n",
      "Loss for  41433 th iteration => 0.00725445422602\n",
      "Loss for  41434 th iteration => 0.00725427535594\n",
      "Loss for  41435 th iteration => 0.00725408720778\n",
      "Loss for  41436 th iteration => 0.00725387364058\n",
      "Loss for  41437 th iteration => 0.00725372012569\n",
      "Loss for  41438 th iteration => 0.00725347428936\n",
      "Loss for  41439 th iteration => 0.00725334934404\n",
      "Loss for  41440 th iteration => 0.007253104895\n",
      "Loss for  41441 th iteration => 0.00725295073526\n",
      "Loss for  41442 th iteration => 0.00725273546265\n",
      "Loss for  41443 th iteration => 0.00725255228046\n",
      "Loss for  41444 th iteration => 0.00725236599302\n",
      "Loss for  41445 th iteration => 0.00725215602431\n",
      "Loss for  41446 th iteration => 0.00725209590564\n",
      "Loss for  41447 th iteration => 0.00725177619214\n",
      "Loss for  41448 th iteration => 0.00725164538829\n",
      "Loss for  41449 th iteration => 0.00725141059374\n",
      "Loss for  41450 th iteration => 0.0072512425016\n",
      "Loss for  41451 th iteration => 0.00725104491875\n",
      "Loss for  41452 th iteration => 0.00725083981811\n",
      "Loss for  41453 th iteration => 0.00725067916829\n",
      "Loss for  41454 th iteration => 0.00725043733545\n",
      "Loss for  41455 th iteration => 0.00725031334346\n",
      "Loss for  41456 th iteration => 0.00725004667405\n",
      "Loss for  41457 th iteration => 0.00724985821752\n",
      "Loss for  41458 th iteration => 0.00724975829976\n",
      "Loss for  41459 th iteration => 0.00724949187316\n",
      "Loss for  41460 th iteration => 0.00724935676838\n",
      "Loss for  41461 th iteration => 0.00724912546304\n",
      "Loss for  41462 th iteration => 0.00724895542636\n",
      "Loss for  41463 th iteration => 0.00724875898817\n",
      "Loss for  41464 th iteration => 0.00724855427155\n",
      "Loss for  41465 th iteration => 0.00724839319525\n",
      "Loss for  41466 th iteration => 0.0072482308692\n",
      "Loss for  41467 th iteration => 0.00724803985418\n",
      "Loss for  41468 th iteration => 0.00724782477508\n",
      "Loss for  41469 th iteration => 0.00724759973049\n",
      "Loss for  41470 th iteration => 0.00724752355302\n",
      "Loss for  41471 th iteration => 0.007247247909\n",
      "Loss for  41472 th iteration => 0.00724710467527\n",
      "Loss for  41473 th iteration => 0.00724689581339\n",
      "Loss for  41474 th iteration => 0.00724668625361\n",
      "Loss for  41475 th iteration => 0.00724654344723\n",
      "Loss for  41476 th iteration => 0.00724627811365\n",
      "Loss for  41477 th iteration => 0.00724611942131\n",
      "Loss for  41478 th iteration => 0.00724587610968\n",
      "Loss for  41479 th iteration => 0.00724575405931\n",
      "Loss for  41480 th iteration => 0.00724548891719\n",
      "Loss for  41481 th iteration => 0.00724529957729\n",
      "Loss for  41482 th iteration => 0.00724519806616\n",
      "Loss for  41483 th iteration => 0.00724493369523\n",
      "Loss for  41484 th iteration => 0.00724479701369\n",
      "Loss for  41485 th iteration => 0.0072445677474\n",
      "Loss for  41486 th iteration => 0.00724439615054\n",
      "Loss for  41487 th iteration => 0.0072442017348\n",
      "Loss for  41488 th iteration => 0.00724399547452\n",
      "Loss for  41489 th iteration => 0.00724383565841\n",
      "Loss for  41490 th iteration => 0.00724359704959\n",
      "Loss for  41491 th iteration => 0.00724354319099\n",
      "Loss for  41492 th iteration => 0.00724322324202\n",
      "Loss for  41493 th iteration => 0.00724307551934\n",
      "Loss for  41494 th iteration => 0.00724285738256\n",
      "Loss for  41495 th iteration => 0.00724277027041\n",
      "Loss for  41496 th iteration => 0.00724250598463\n",
      "Loss for  41497 th iteration => 0.00724235196536\n",
      "Loss for  41498 th iteration => 0.00724215431338\n",
      "Loss for  41499 th iteration => 0.00724193411503\n",
      "Loss for  41500 th iteration => 0.00724180237237\n",
      "Loss for  41501 th iteration => 0.00724153000964\n",
      "Loss for  41502 th iteration => 0.00724137753102\n",
      "Loss for  41503 th iteration => 0.00724112853664\n",
      "Loss for  41504 th iteration => 0.00724101262741\n",
      "Loss for  41505 th iteration => 0.00724074636545\n",
      "Loss for  41506 th iteration => 0.00724064366869\n",
      "Loss for  41507 th iteration => 0.00724038094622\n",
      "Loss for  41508 th iteration => 0.00724024309225\n",
      "Loss for  41509 th iteration => 0.00724001546119\n",
      "Loss for  41510 th iteration => 0.00723984270511\n",
      "Loss for  41511 th iteration => 0.00723964991137\n",
      "Loss for  41512 th iteration => 0.00723944250507\n",
      "Loss for  41513 th iteration => 0.00723928429773\n",
      "Loss for  41514 th iteration => 0.00723904369967\n",
      "Loss for  41515 th iteration => 0.00723898974982\n",
      "Loss for  41516 th iteration => 0.00723867305524\n",
      "Loss for  41517 th iteration => 0.00723852389453\n",
      "Loss for  41518 th iteration => 0.0072383065357\n",
      "Loss for  41519 th iteration => 0.00723821591235\n",
      "Loss for  41520 th iteration => 0.00723795558696\n",
      "Loss for  41521 th iteration => 0.00723779809884\n",
      "Loss for  41522 th iteration => 0.00723760436513\n",
      "Loss for  41523 th iteration => 0.00723738073967\n",
      "Loss for  41524 th iteration => 0.00723725287379\n",
      "Loss for  41525 th iteration => 0.00723697962014\n",
      "Loss for  41526 th iteration => 0.00723682938012\n",
      "Loss for  41527 th iteration => 0.00723657914662\n",
      "Loss for  41528 th iteration => 0.00723638877518\n",
      "Loss for  41529 th iteration => 0.00723619841522\n",
      "Loss for  41530 th iteration => 0.00723601004653\n",
      "Loss for  41531 th iteration => 0.00723590126855\n",
      "Loss for  41532 th iteration => 0.00723564512394\n",
      "Loss for  41533 th iteration => 0.00723550116524\n",
      "Loss for  41534 th iteration => 0.00723528013537\n",
      "Loss for  41535 th iteration => 0.00723510125158\n",
      "Loss for  41536 th iteration => 0.00723491508179\n",
      "Loss for  41537 th iteration => 0.00723470152541\n",
      "Loss for  41538 th iteration => 0.00723455089608\n",
      "Loss for  41539 th iteration => 0.00723437090148\n",
      "Loss for  41540 th iteration => 0.00723412623758\n",
      "Loss for  41541 th iteration => 0.00723397423581\n",
      "Loss for  41542 th iteration => 0.00723375876743\n",
      "Loss for  41543 th iteration => 0.00723357772453\n",
      "Loss for  41544 th iteration => 0.00723339125963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  41545 th iteration => 0.00723318200998\n",
      "Loss for  41546 th iteration => 0.00723312283819\n",
      "Loss for  41547 th iteration => 0.00723280419282\n",
      "Loss for  41548 th iteration => 0.00723267546196\n",
      "Loss for  41549 th iteration => 0.00723244055141\n",
      "Loss for  41550 th iteration => 0.0072322745202\n",
      "Loss for  41551 th iteration => 0.00723207683311\n",
      "Loss for  41552 th iteration => 0.00723187378195\n",
      "Loss for  41553 th iteration => 0.00723171303905\n",
      "Loss for  41554 th iteration => 0.00723147324482\n",
      "Loss for  41555 th iteration => 0.00723134917032\n",
      "Loss for  41556 th iteration => 0.00723108484676\n",
      "Loss for  41557 th iteration => 0.00723089625165\n",
      "Loss for  41558 th iteration => 0.00723079788636\n",
      "Loss for  41559 th iteration => 0.00723053185851\n",
      "Loss for  41560 th iteration => 0.00723039830549\n",
      "Loss for  41561 th iteration => 0.00723016739937\n",
      "Loss for  41562 th iteration => 0.0072299989142\n",
      "Loss for  41563 th iteration => 0.00722980287521\n",
      "Loss for  41564 th iteration => 0.0072295997103\n",
      "Loss for  41565 th iteration => 0.00722943872528\n",
      "Loss for  41566 th iteration => 0.00722927709345\n",
      "Loss for  41567 th iteration => 0.00722908732385\n",
      "Loss for  41568 th iteration => 0.00722887348117\n",
      "Loss for  41569 th iteration => 0.00722864986097\n",
      "Loss for  41570 th iteration => 0.00722857273763\n",
      "Loss for  41571 th iteration => 0.00722829996962\n",
      "Loss for  41572 th iteration => 0.00722815582107\n",
      "Loss for  41573 th iteration => 0.00722794980456\n",
      "Loss for  41574 th iteration => 0.00722773936008\n",
      "Loss for  41575 th iteration => 0.00722759936934\n",
      "Loss for  41576 th iteration => 0.0072273343109\n",
      "Loss for  41577 th iteration => 0.00722717760616\n",
      "Loss for  41578 th iteration => 0.00722693426475\n",
      "Loss for  41579 th iteration => 0.00722681418562\n",
      "Loss for  41580 th iteration => 0.00722655044542\n",
      "Loss for  41581 th iteration => 0.00722636184186\n",
      "Loss for  41582 th iteration => 0.00722625997184\n",
      "Loss for  41583 th iteration => 0.00722599789748\n",
      "Loss for  41584 th iteration => 0.00722586088075\n",
      "Loss for  41585 th iteration => 0.00722563388718\n",
      "Loss for  41586 th iteration => 0.00722546197895\n",
      "Loss for  41587 th iteration => 0.00722526981196\n",
      "Loss for  41588 th iteration => 0.00722506326426\n",
      "Loss for  41589 th iteration => 0.00722490603878\n",
      "Loss for  41590 th iteration => 0.00722473767811\n",
      "Loss for  41591 th iteration => 0.00722455506466\n",
      "Loss for  41592 th iteration => 0.00722433843955\n",
      "Loss for  41593 th iteration => 0.00722411814675\n",
      "Loss for  41594 th iteration => 0.00722403392872\n",
      "Loss for  41595 th iteration => 0.00722376867985\n",
      "Loss for  41596 th iteration => 0.00722361753048\n",
      "Loss for  41597 th iteration => 0.00722341893973\n",
      "Loss for  41598 th iteration => 0.00722320158697\n",
      "Loss for  41599 th iteration => 0.00722306892996\n",
      "Loss for  41600 th iteration => 0.00722280039871\n",
      "Loss for  41601 th iteration => 0.00722264776594\n",
      "Loss for  41602 th iteration => 0.00722240084295\n",
      "Loss for  41603 th iteration => 0.00722228479255\n",
      "Loss for  41604 th iteration => 0.00722202162081\n",
      "Loss for  41605 th iteration => 0.00722183302886\n",
      "Loss for  41606 th iteration => 0.00722172760716\n",
      "Loss for  41607 th iteration => 0.00722146953221\n",
      "Loss for  41608 th iteration => 0.00722132900526\n",
      "Loss for  41609 th iteration => 0.00722110596973\n",
      "Loss for  41610 th iteration => 0.00722093059236\n",
      "Loss for  41611 th iteration => 0.00722074234241\n",
      "Loss for  41612 th iteration => 0.00722053236629\n",
      "Loss for  41613 th iteration => 0.00722037895314\n",
      "Loss for  41614 th iteration => 0.00722020379892\n",
      "Loss for  41615 th iteration => 0.00722002840514\n",
      "Loss for  41616 th iteration => 0.00721980895828\n",
      "Loss for  41617 th iteration => 0.00721959202421\n",
      "Loss for  41618 th iteration => 0.00721950066266\n",
      "Loss for  41619 th iteration => 0.00721924298058\n",
      "Loss for  41620 th iteration => 0.00721908478234\n",
      "Loss for  41621 th iteration => 0.00721889366423\n",
      "Loss for  41622 th iteration => 0.0072186700262\n",
      "Loss for  41623 th iteration => 0.00721848494284\n",
      "Loss for  41624 th iteration => 0.00721827071155\n",
      "Loss for  41625 th iteration => 0.0072181225233\n",
      "Loss for  41626 th iteration => 0.00721787454442\n",
      "Loss for  41627 th iteration => 0.00721768513504\n",
      "Loss for  41628 th iteration => 0.00721749573714\n",
      "Loss for  41629 th iteration => 0.00721730772568\n",
      "Loss for  41630 th iteration => 0.0072171975454\n",
      "Loss for  41631 th iteration => 0.00721694474417\n",
      "Loss for  41632 th iteration => 0.0072167993321\n",
      "Loss for  41633 th iteration => 0.00721658169617\n",
      "Loss for  41634 th iteration => 0.00721640130923\n",
      "Loss for  41635 th iteration => 0.00721621858268\n",
      "Loss for  41636 th iteration => 0.00721600347458\n",
      "Loss for  41637 th iteration => 0.00721585540468\n",
      "Loss for  41638 th iteration => 0.00721560954047\n",
      "Loss for  41639 th iteration => 0.00721542025215\n",
      "Loss for  41640 th iteration => 0.00721523147868\n",
      "Loss for  41641 th iteration => 0.00721511829499\n",
      "Loss for  41642 th iteration => 0.00721486599692\n",
      "Loss for  41643 th iteration => 0.0072147236557\n",
      "Loss for  41644 th iteration => 0.00721450112077\n",
      "Loss for  41645 th iteration => 0.00721442473197\n",
      "Loss for  41646 th iteration => 0.00721415267352\n",
      "Loss for  41647 th iteration => 0.00721400926764\n",
      "Loss for  41648 th iteration => 0.00721380395281\n",
      "Loss for  41649 th iteration => 0.00721359425862\n",
      "Loss for  41650 th iteration => 0.00721345496219\n",
      "Loss for  41651 th iteration => 0.00721319056416\n",
      "Loss for  41652 th iteration => 0.00721303434888\n",
      "Loss for  41653 th iteration => 0.00721279197324\n",
      "Loss for  41654 th iteration => 0.00721267237734\n",
      "Loss for  41655 th iteration => 0.00721240922069\n",
      "Loss for  41656 th iteration => 0.00721222165597\n",
      "Loss for  41657 th iteration => 0.00721212032842\n",
      "Loss for  41658 th iteration => 0.0072118591575\n",
      "Loss for  41659 th iteration => 0.00721172269565\n",
      "Loss for  41660 th iteration => 0.00721149659298\n",
      "Loss for  41661 th iteration => 0.00721132525219\n",
      "Loss for  41662 th iteration => 0.00721113396341\n",
      "Loss for  41663 th iteration => 0.00721092799586\n",
      "Loss for  41664 th iteration => 0.00721077245841\n",
      "Loss for  41665 th iteration => 0.00721060333041\n",
      "Loss for  41666 th iteration => 0.00721042291751\n",
      "Loss for  41667 th iteration => 0.00721020538539\n",
      "Loss for  41668 th iteration => 0.00720998796092\n",
      "Loss for  41669 th iteration => 0.0072099019187\n",
      "Loss for  41670 th iteration => 0.00720963992037\n",
      "Loss for  41671 th iteration => 0.00720948699028\n",
      "Loss for  41672 th iteration => 0.00720929160704\n",
      "Loss for  41673 th iteration => 0.007209072516\n",
      "Loss for  41674 th iteration => 0.00720894302447\n",
      "Loss for  41675 th iteration => 0.00720867328193\n",
      "Loss for  41676 th iteration => 0.00720852250743\n",
      "Loss for  41677 th iteration => 0.00720827518915\n",
      "Loss for  41678 th iteration => 0.00720816097316\n",
      "Loss for  41679 th iteration => 0.00720789776888\n",
      "Loss for  41680 th iteration => 0.00720771078755\n",
      "Loss for  41681 th iteration => 0.00720760464837\n",
      "Loss for  41682 th iteration => 0.00720734872765\n",
      "Loss for  41683 th iteration => 0.00720720751162\n",
      "Loss for  41684 th iteration => 0.00720698660188\n",
      "Loss for  41685 th iteration => 0.00720681056373\n",
      "Loss for  41686 th iteration => 0.0072066244112\n",
      "Loss for  41687 th iteration => 0.00720641380254\n",
      "Loss for  41688 th iteration => 0.00720626372665\n",
      "Loss for  41689 th iteration => 0.00720608562533\n",
      "Loss for  41690 th iteration => 0.00720591459644\n",
      "Loss for  41691 th iteration => 0.00720569273393\n",
      "Loss for  41692 th iteration => 0.00720548018705\n",
      "Loss for  41693 th iteration => 0.00720538481528\n",
      "Loss for  41694 th iteration => 0.00720513255423\n",
      "Loss for  41695 th iteration => 0.00720497041941\n",
      "Loss for  41696 th iteration => 0.00720478464929\n",
      "Loss for  41697 th iteration => 0.00720455815289\n",
      "Loss for  41698 th iteration => 0.00720437613825\n",
      "Loss for  41699 th iteration => 0.00720416030789\n",
      "Loss for  41700 th iteration => 0.00720401514883\n",
      "Loss for  41701 th iteration => 0.00720376660112\n",
      "Loss for  41702 th iteration => 0.00720357791823\n",
      "Loss for  41703 th iteration => 0.00720338924677\n",
      "Loss for  41704 th iteration => 0.00720320332868\n",
      "Loss for  41705 th iteration => 0.00720309135897\n",
      "Loss for  41706 th iteration => 0.00720284177531\n",
      "Loss for  41707 th iteration => 0.00720269461665\n",
      "Loss for  41708 th iteration => 0.00720248015546\n",
      "Loss for  41709 th iteration => 0.00720229806448\n",
      "Loss for  41710 th iteration => 0.00720211847013\n",
      "Loss for  41711 th iteration => 0.00720190170027\n",
      "Loss for  41712 th iteration => 0.00720175672028\n",
      "Loss for  41713 th iteration => 0.00720151028452\n",
      "Loss for  41714 th iteration => 0.00720132172218\n",
      "Loss for  41715 th iteration => 0.0072011353585\n",
      "Loss for  41716 th iteration => 0.00720101959435\n",
      "Loss for  41717 th iteration => 0.00720077130638\n",
      "Loss for  41718 th iteration => 0.00720062642401\n",
      "Loss for  41719 th iteration => 0.00720040973424\n",
      "Loss for  41720 th iteration => 0.00720032719705\n",
      "Loss for  41721 th iteration => 0.00720006268203\n",
      "Loss for  41722 th iteration => 0.00719991323127\n",
      "Loss for  41723 th iteration => 0.00719971535707\n",
      "Loss for  41724 th iteration => 0.00719949971968\n",
      "Loss for  41725 th iteration => 0.00719936776293\n",
      "Loss for  41726 th iteration => 0.00719909901903\n",
      "Loss for  41727 th iteration => 0.00719894697341\n",
      "Loss for  41728 th iteration => 0.00719870190304\n",
      "Loss for  41729 th iteration => 0.0071985864231\n",
      "Loss for  41730 th iteration => 0.00719832287581\n",
      "Loss for  41731 th iteration => 0.00719822278347\n",
      "Loss for  41732 th iteration => 0.00719796180071\n",
      "Loss for  41733 th iteration => 0.00719782657356\n",
      "Loss for  41734 th iteration => 0.0071976006594\n",
      "Loss for  41735 th iteration => 0.00719743055311\n",
      "Loss for  41736 th iteration => 0.00719723945285\n",
      "Loss for  41737 th iteration => 0.00719703471993\n",
      "Loss for  41738 th iteration => 0.00719687918306\n",
      "Loss for  41739 th iteration => 0.00719671051262\n",
      "Loss for  41740 th iteration => 0.007196531057\n",
      "Loss for  41741 th iteration => 0.00719631436921\n",
      "Loss for  41742 th iteration => 0.0071960979619\n",
      "Loss for  41743 th iteration => 0.00719601134499\n",
      "Loss for  41744 th iteration => 0.00719575132947\n",
      "Loss for  41745 th iteration => 0.0071955978473\n",
      "Loss for  41746 th iteration => 0.00719540442455\n",
      "Loss for  41747 th iteration => 0.00719518480339\n",
      "Loss for  41748 th iteration => 0.00719505725069\n",
      "Loss for  41749 th iteration => 0.00719478793858\n",
      "Loss for  41750 th iteration => 0.00719463834779\n",
      "Loss for  41751 th iteration => 0.00719439127387\n",
      "Loss for  41752 th iteration => 0.00719427822997\n",
      "Loss for  41753 th iteration => 0.00719401622117\n",
      "Loss for  41754 th iteration => 0.0071938296843\n",
      "Loss for  41755 th iteration => 0.00719372338434\n",
      "Loss for  41756 th iteration => 0.00719346903801\n",
      "Loss for  41757 th iteration => 0.00719332767837\n",
      "Loss for  41758 th iteration => 0.00719310832574\n",
      "Loss for  41759 th iteration => 0.00719293216126\n",
      "Loss for  41760 th iteration => 0.00719274754846\n",
      "Loss for  41761 th iteration => 0.00719253683083\n",
      "Loss for  41762 th iteration => 0.00719238859075\n",
      "Loss for  41763 th iteration => 0.00719220890662\n",
      "Loss for  41764 th iteration => 0.00719196875133\n",
      "Loss for  41765 th iteration => 0.00719181663268\n",
      "Loss for  41766 th iteration => 0.00719160738034\n",
      "Loss for  41767 th iteration => 0.00719150809108\n",
      "Loss for  41768 th iteration => 0.00719126121086\n",
      "Loss for  41769 th iteration => 0.00719109504523\n",
      "Loss for  41770 th iteration => 0.0071909147688\n",
      "Loss for  41771 th iteration => 0.00719068592452\n",
      "Loss for  41772 th iteration => 0.00719050719915\n",
      "Loss for  41773 th iteration => 0.00719028946035\n",
      "Loss for  41774 th iteration => 0.0071901476541\n",
      "Loss for  41775 th iteration => 0.00718989929049\n",
      "Loss for  41776 th iteration => 0.00718971131785\n",
      "Loss for  41777 th iteration => 0.00718952335665\n",
      "Loss for  41778 th iteration => 0.00718933540686\n",
      "Loss for  41779 th iteration => 0.00718914746847\n",
      "Loss for  41780 th iteration => 0.00718896000813\n",
      "Loss for  41781 th iteration => 0.00718886359262\n",
      "Loss for  41782 th iteration => 0.00718859990276\n",
      "Loss for  41783 th iteration => 0.00718846832236\n",
      "Loss for  41784 th iteration => 0.00718823973096\n",
      "Loss for  41785 th iteration => 0.00718807324189\n",
      "Loss for  41786 th iteration => 0.0071878794937\n",
      "Loss for  41787 th iteration => 0.00718767834901\n",
      "Loss for  41788 th iteration => 0.00718752036925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  41789 th iteration => 0.00718735721384\n",
      "Loss for  41790 th iteration => 0.00718717322136\n",
      "Loss for  41791 th iteration => 0.00718695905567\n",
      "Loss for  41792 th iteration => 0.00718674128346\n",
      "Loss for  41793 th iteration => 0.00718665985705\n",
      "Loss for  41794 th iteration => 0.00718639562555\n",
      "Loss for  41795 th iteration => 0.00718624728351\n",
      "Loss for  41796 th iteration => 0.00718604969507\n",
      "Loss for  41797 th iteration => 0.00718583516404\n",
      "Loss for  41798 th iteration => 0.00718570349556\n",
      "Loss for  41799 th iteration => 0.00718543715445\n",
      "Loss for  41800 th iteration => 0.00718528533088\n",
      "Loss for  41801 th iteration => 0.00718504143336\n",
      "Loss for  41802 th iteration => 0.0071849261779\n",
      "Loss for  41803 th iteration => 0.00718466451396\n",
      "Loss for  41804 th iteration => 0.00718447883704\n",
      "Loss for  41805 th iteration => 0.00718437498915\n",
      "Loss for  41806 th iteration => 0.00718411915242\n",
      "Loss for  41807 th iteration => 0.0071839802304\n",
      "Loss for  41808 th iteration => 0.00718375940166\n",
      "Loss for  41809 th iteration => 0.0071835856607\n",
      "Loss for  41810 th iteration => 0.00718339958574\n",
      "Loss for  41811 th iteration => 0.00718319145007\n",
      "Loss for  41812 th iteration => 0.00718310902524\n",
      "Loss for  41813 th iteration => 0.00718280998351\n",
      "Loss for  41814 th iteration => 0.00718273324653\n",
      "Loss for  41815 th iteration => 0.00718244715214\n",
      "Loss for  41816 th iteration => 0.0071822735796\n",
      "Loss for  41817 th iteration => 0.00718217057276\n",
      "Loss for  41818 th iteration => 0.00718189900362\n",
      "Loss for  41819 th iteration => 0.00718178780512\n",
      "Loss for  41820 th iteration => 0.00718152467545\n",
      "Loss for  41821 th iteration => 0.00718134685244\n",
      "Loss for  41822 th iteration => 0.00718116612748\n",
      "Loss for  41823 th iteration => 0.00718095139152\n",
      "Loss for  41824 th iteration => 0.0071808075035\n",
      "Loss for  41825 th iteration => 0.00718055945133\n",
      "Loss for  41826 th iteration => 0.00718037195953\n",
      "Loss for  41827 th iteration => 0.00718018548464\n",
      "Loss for  41828 th iteration => 0.00718007796049\n",
      "Loss for  41829 th iteration => 0.00717982629527\n",
      "Loss for  41830 th iteration => 0.00717968355385\n",
      "Loss for  41831 th iteration => 0.00717946703903\n",
      "Loss for  41832 th iteration => 0.00717928933785\n",
      "Loss for  41833 th iteration => 0.0071791077169\n",
      "Loss for  41834 th iteration => 0.00717889531028\n",
      "Loss for  41835 th iteration => 0.00717874832986\n",
      "Loss for  41836 th iteration => 0.00717850476316\n",
      "Loss for  41837 th iteration => 0.00717831737991\n",
      "Loss for  41838 th iteration => 0.00717813159524\n",
      "Loss for  41839 th iteration => 0.00717801784494\n",
      "Loss for  41840 th iteration => 0.00717776989968\n",
      "Loss for  41841 th iteration => 0.00717762701974\n",
      "Loss for  41842 th iteration => 0.00717740979624\n",
      "Loss for  41843 th iteration => 0.00717733056952\n",
      "Loss for  41844 th iteration => 0.007177065092\n",
      "Loss for  41845 th iteration => 0.00717691893955\n",
      "Loss for  41846 th iteration => 0.0071767201153\n",
      "Loss for  41847 th iteration => 0.00717650776357\n",
      "Loss for  41848 th iteration => 0.00717637486967\n",
      "Loss for  41849 th iteration => 0.00717610917849\n",
      "Loss for  41850 th iteration => 0.00717595700576\n",
      "Loss for  41851 th iteration => 0.00717571440936\n",
      "Loss for  41852 th iteration => 0.00717559880563\n",
      "Loss for  41853 th iteration => 0.00717533686513\n",
      "Loss for  41854 th iteration => 0.00717523831753\n",
      "Loss for  41855 th iteration => 0.007174978135\n",
      "Loss for  41856 th iteration => 0.00717484445994\n",
      "Loss for  41857 th iteration => 0.00717461933841\n",
      "Loss for  41858 th iteration => 0.00717445079196\n",
      "Loss for  41859 th iteration => 0.00717426047633\n",
      "Loss for  41860 th iteration => 0.00717405731141\n",
      "Loss for  41861 th iteration => 0.00717390284583\n",
      "Loss for  41862 th iteration => 0.0071737346437\n",
      "Loss for  41863 th iteration => 0.00717355704902\n",
      "Loss for  41864 th iteration => 0.00717334088605\n",
      "Loss for  41865 th iteration => 0.00717312697155\n",
      "Loss for  41866 th iteration => 0.00717303934012\n",
      "Loss for  41867 th iteration => 0.00717278265757\n",
      "Loss for  41868 th iteration => 0.00717262820825\n",
      "Loss for  41869 th iteration => 0.0071724380717\n",
      "Loss for  41870 th iteration => 0.00717221752945\n",
      "Loss for  41871 th iteration => 0.00717209321743\n",
      "Loss for  41872 th iteration => 0.00717182378623\n",
      "Loss for  41873 th iteration => 0.00717167638063\n",
      "Loss for  41874 th iteration => 0.00717143017733\n",
      "Loss for  41875 th iteration => 0.00717124315375\n",
      "Loss for  41876 th iteration => 0.00717105614154\n",
      "Loss for  41877 th iteration => 0.00717087160764\n",
      "Loss for  41878 th iteration => 0.00717076411159\n",
      "Loss for  41879 th iteration => 0.00717051332873\n",
      "Loss for  41880 th iteration => 0.00717037071437\n",
      "Loss for  41881 th iteration => 0.0071701549833\n",
      "Loss for  41882 th iteration => 0.00716997750685\n",
      "Loss for  41883 th iteration => 0.0071697965723\n",
      "Loss for  41884 th iteration => 0.00716958547596\n",
      "Loss for  41885 th iteration => 0.00716950368143\n",
      "Loss for  41886 th iteration => 0.00716920759128\n",
      "Loss for  41887 th iteration => 0.00716905656649\n",
      "Loss for  41888 th iteration => 0.00716884676036\n",
      "Loss for  41889 th iteration => 0.0071686667188\n",
      "Loss for  41890 th iteration => 0.00716848589094\n",
      "Loss for  41891 th iteration => 0.00716827750191\n",
      "Loss for  41892 th iteration => 0.00716822109238\n",
      "Loss for  41893 th iteration => 0.00716790857859\n",
      "Loss for  41894 th iteration => 0.00716778038212\n",
      "Loss for  41895 th iteration => 0.00716755155004\n",
      "Loss for  41896 th iteration => 0.00716738611897\n",
      "Loss for  41897 th iteration => 0.00716719444414\n",
      "Loss for  41898 th iteration => 0.00716699205935\n",
      "Loss for  41899 th iteration => 0.00716683726198\n",
      "Loss for  41900 th iteration => 0.00716659820087\n",
      "Loss for  41901 th iteration => 0.00716648000464\n",
      "Loss for  41902 th iteration => 0.00716621928137\n",
      "Loss for  41903 th iteration => 0.00716603480357\n",
      "Loss for  41904 th iteration => 0.00716593495851\n",
      "Loss for  41905 th iteration => 0.00716567700898\n",
      "Loss for  41906 th iteration => 0.00716554206863\n",
      "Loss for  41907 th iteration => 0.00716531914796\n",
      "Loss for  41908 th iteration => 0.00716514936811\n",
      "Loss for  41909 th iteration => 0.00716496122148\n",
      "Loss for  41910 th iteration => 0.007164757563\n",
      "Loss for  41911 th iteration => 0.00716467493615\n",
      "Loss for  41912 th iteration => 0.00716437796679\n",
      "Loss for  41913 th iteration => 0.00716430103206\n",
      "Loss for  41914 th iteration => 0.00716401401148\n",
      "Loss for  41915 th iteration => 0.00716393103893\n",
      "Loss for  41916 th iteration => 0.00716367064345\n",
      "Loss for  41917 th iteration => 0.00716352080498\n",
      "Loss for  41918 th iteration => 0.00716332700342\n",
      "Loss for  41919 th iteration => 0.00716311102439\n",
      "Loss for  41920 th iteration => 0.00716298309491\n",
      "Loss for  41921 th iteration => 0.00716271678108\n",
      "Loss for  41922 th iteration => 0.00716256777035\n",
      "Loss for  41923 th iteration => 0.00716232339203\n",
      "Loss for  41924 th iteration => 0.0071622109233\n",
      "Loss for  41925 th iteration => 0.00716195087051\n",
      "Loss for  41926 th iteration => 0.00716176608176\n",
      "Loss for  41927 th iteration => 0.0071616614727\n",
      "Loss for  41928 th iteration => 0.00716140869896\n",
      "Loss for  41929 th iteration => 0.00716126904981\n",
      "Loss for  41930 th iteration => 0.00716105124988\n",
      "Loss for  41931 th iteration => 0.0071608768159\n",
      "Loss for  41932 th iteration => 0.00716069373547\n",
      "Loss for  41933 th iteration => 0.00716048520786\n",
      "Loss for  41934 th iteration => 0.0071604030887\n",
      "Loss for  41935 th iteration => 0.00716010638367\n",
      "Loss for  41936 th iteration => 0.00715995778842\n",
      "Loss for  41937 th iteration => 0.00715974656984\n",
      "Loss for  41938 th iteration => 0.00715965288155\n",
      "Loss for  41939 th iteration => 0.00715940365276\n",
      "Loss for  41940 th iteration => 0.00715924305781\n",
      "Loss for  41941 th iteration => 0.00715906046344\n",
      "Loss for  41942 th iteration => 0.00715883674913\n",
      "Loss for  41943 th iteration => 0.0071586575814\n",
      "Loss for  41944 th iteration => 0.00715844352572\n",
      "Loss for  41945 th iteration => 0.00715830128855\n",
      "Loss for  41946 th iteration => 0.00715805623004\n",
      "Loss for  41947 th iteration => 0.00715786988604\n",
      "Loss for  41948 th iteration => 0.00715768355343\n",
      "Loss for  41949 th iteration => 0.00715749723221\n",
      "Loss for  41950 th iteration => 0.00715731092233\n",
      "Loss for  41951 th iteration => 0.00715712462377\n",
      "Loss for  41952 th iteration => 0.00715694053512\n",
      "Loss for  41953 th iteration => 0.00715684216262\n",
      "Loss for  41954 th iteration => 0.00715658366752\n",
      "Loss for  41955 th iteration => 0.00715645019688\n",
      "Loss for  41956 th iteration => 0.00715622673339\n",
      "Loss for  41957 th iteration => 0.00715605842062\n",
      "Loss for  41958 th iteration => 0.00715586973367\n",
      "Loss for  41959 th iteration => 0.00715566783455\n",
      "Loss for  41960 th iteration => 0.00715558474342\n",
      "Loss for  41961 th iteration => 0.00715528916026\n",
      "Loss for  41962 th iteration => 0.00715521176276\n",
      "Loss for  41963 th iteration => 0.00715492556246\n",
      "Loss for  41964 th iteration => 0.0071548432421\n",
      "Loss for  41965 th iteration => 0.00715458311403\n",
      "Loss for  41966 th iteration => 0.00715443393333\n",
      "Loss for  41967 th iteration => 0.00715424039378\n",
      "Loss for  41968 th iteration => 0.00715402507773\n",
      "Loss for  41969 th iteration => 0.00715389740521\n",
      "Loss for  41970 th iteration => 0.0071536313852\n",
      "Loss for  41971 th iteration => 0.00715348270849\n",
      "Loss for  41972 th iteration => 0.00715323892364\n",
      "Loss for  41973 th iteration => 0.00715312678405\n",
      "Loss for  41974 th iteration => 0.00715286683288\n",
      "Loss for  41975 th iteration => 0.00715268284217\n",
      "Loss for  41976 th iteration => 0.00715257881365\n",
      "Loss for  41977 th iteration => 0.00715232638022\n",
      "Loss for  41978 th iteration => 0.00715218732002\n",
      "Loss for  41979 th iteration => 0.00715196985192\n",
      "Loss for  41980 th iteration => 0.00715179601536\n",
      "Loss for  41981 th iteration => 0.00715161325823\n",
      "Loss for  41982 th iteration => 0.0071514059536\n",
      "Loss for  41983 th iteration => 0.00715132352038\n",
      "Loss for  41984 th iteration => 0.00715102772658\n",
      "Loss for  41985 th iteration => 0.00715095097506\n",
      "Loss for  41986 th iteration => 0.00715067039183\n",
      "Loss for  41987 th iteration => 0.00715057665677\n",
      "Loss for  41988 th iteration => 0.00715032831304\n",
      "Loss for  41989 th iteration => 0.00715016786458\n",
      "Loss for  41990 th iteration => 0.00714998596317\n",
      "Loss for  41991 th iteration => 0.00714976228261\n",
      "Loss for  41992 th iteration => 0.00714958359219\n",
      "Loss for  41993 th iteration => 0.00714937004566\n",
      "Loss for  41994 th iteration => 0.00714922817922\n",
      "Loss for  41995 th iteration => 0.00714898322454\n",
      "Loss for  41996 th iteration => 0.00714879734593\n",
      "Loss for  41997 th iteration => 0.00714861147866\n",
      "Loss for  41998 th iteration => 0.00714842801649\n",
      "Loss for  41999 th iteration => 0.00714831742443\n",
      "Loss for  42000 th iteration => 0.00714807202687\n",
      "Loss for  42001 th iteration => 0.00714792630218\n",
      "Loss for  42002 th iteration => 0.00714771597036\n",
      "Loss for  42003 th iteration => 0.00714753537011\n",
      "Loss for  42004 th iteration => 0.00714735984793\n",
      "Loss for  42005 th iteration => 0.00714714462599\n",
      "Loss for  42006 th iteration => 0.00714700366054\n",
      "Loss for  42007 th iteration => 0.00714676046561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  42008 th iteration => 0.00714657626832\n",
      "Loss for  42009 th iteration => 0.00714646121951\n",
      "Loss for  42010 th iteration => 0.00714621777443\n",
      "Loss for  42011 th iteration => 0.00714607362734\n",
      "Loss for  42012 th iteration => 0.0071458608963\n",
      "Loss for  42013 th iteration => 0.0071457766083\n",
      "Loss for  42014 th iteration => 0.00714551934922\n",
      "Loss for  42015 th iteration => 0.00714536824122\n",
      "Loss for  42016 th iteration => 0.00714517753067\n",
      "Loss for  42017 th iteration => 0.00714496032683\n",
      "Loss for  42018 th iteration => 0.00714483544414\n",
      "Loss for  42019 th iteration => 0.00714456781411\n",
      "Loss for  42020 th iteration => 0.00714442086595\n",
      "Loss for  42021 th iteration => 0.00714417628682\n",
      "Loss for  42022 th iteration => 0.00714406585365\n",
      "Loss for  42023 th iteration => 0.00714380550136\n",
      "Loss for  42024 th iteration => 0.00714370518438\n",
      "Loss for  42025 th iteration => 0.00714344995483\n",
      "Loss for  42026 th iteration => 0.00714331457239\n",
      "Loss for  42027 th iteration => 0.0071430943417\n",
      "Loss for  42028 th iteration => 0.00714292414978\n",
      "Loss for  42029 th iteration => 0.00714273866295\n",
      "Loss for  42030 th iteration => 0.00714253391436\n",
      "Loss for  42031 th iteration => 0.0071423851824\n",
      "Loss for  42032 th iteration => 0.00714221038693\n",
      "Loss for  42033 th iteration => 0.00714204252568\n",
      "Loss for  42034 th iteration => 0.00714182318791\n",
      "Loss for  42035 th iteration => 0.00714161711899\n",
      "Loss for  42036 th iteration => 0.00714151962807\n",
      "Loss for  42037 th iteration => 0.00714127592859\n",
      "Loss for  42038 th iteration => 0.00714111179133\n",
      "Loss for  42039 th iteration => 0.00714093446759\n",
      "Loss for  42040 th iteration => 0.00714070786848\n",
      "Loss for  42041 th iteration => 0.0071405318066\n",
      "Loss for  42042 th iteration => 0.0071403165725\n",
      "Loss for  42043 th iteration => 0.00714017729861\n",
      "Loss for  42044 th iteration => 0.00713993142724\n",
      "Loss for  42045 th iteration => 0.00713974601089\n",
      "Loss for  42046 th iteration => 0.00713956153756\n",
      "Loss for  42047 th iteration => 0.00713945356635\n",
      "Loss for  42048 th iteration => 0.00713920645682\n",
      "Loss for  42049 th iteration => 0.00713906333127\n",
      "Loss for  42050 th iteration => 0.007138851309\n",
      "Loss for  42051 th iteration => 0.00713867328665\n",
      "Loss for  42052 th iteration => 0.00713849609506\n",
      "Loss for  42053 th iteration => 0.00713828343029\n",
      "Loss for  42054 th iteration => 0.00713814081598\n",
      "Loss for  42055 th iteration => 0.00713789949819\n",
      "Loss for  42056 th iteration => 0.00713771496576\n",
      "Loss for  42057 th iteration => 0.00713760123482\n",
      "Loss for  42058 th iteration => 0.00713735737689\n",
      "Loss for  42059 th iteration => 0.00713721453475\n",
      "Loss for  42060 th iteration => 0.00713700030878\n",
      "Loss for  42061 th iteration => 0.00713691861364\n",
      "Loss for  42062 th iteration => 0.0071366596685\n",
      "Loss for  42063 th iteration => 0.00713651112964\n",
      "Loss for  42064 th iteration => 0.00713631875676\n",
      "Loss for  42065 th iteration => 0.00713610409842\n",
      "Loss for  42066 th iteration => 0.00713597757705\n",
      "Loss for  42067 th iteration => 0.00713571241705\n",
      "Loss for  42068 th iteration => 0.00713556466051\n",
      "Loss for  42069 th iteration => 0.0071353217817\n",
      "Loss for  42070 th iteration => 0.00713521055172\n",
      "Loss for  42071 th iteration => 0.00713495160151\n",
      "Loss for  42072 th iteration => 0.00713476845458\n",
      "Loss for  42073 th iteration => 0.00713466525035\n",
      "Loss for  42074 th iteration => 0.0071344138048\n",
      "Loss for  42075 th iteration => 0.00713427558634\n",
      "Loss for  42076 th iteration => 0.00713405908855\n",
      "Loss for  42077 th iteration => 0.0071338861113\n",
      "Loss for  42078 th iteration => 0.00713370430677\n",
      "Loss for  42079 th iteration => 0.00713349855779\n",
      "Loss for  42080 th iteration => 0.00713341542609\n",
      "Loss for  42081 th iteration => 0.00713312214775\n",
      "Loss for  42082 th iteration => 0.00713304469662\n",
      "Loss for  42083 th iteration => 0.00713276744748\n",
      "Loss for  42084 th iteration => 0.00713267124506\n",
      "Loss for  42085 th iteration => 0.00713242715571\n",
      "Loss for  42086 th iteration => 0.00713226429882\n",
      "Loss for  42087 th iteration => 0.00713208659343\n",
      "Loss for  42088 th iteration => 0.00713186146861\n",
      "Loss for  42089 th iteration => 0.00713168538321\n",
      "Loss for  42090 th iteration => 0.00713147106763\n",
      "Loss for  42091 th iteration => 0.00713133177397\n",
      "Loss for  42092 th iteration => 0.007131087063\n",
      "Loss for  42093 th iteration => 0.00713090209648\n",
      "Loss for  42094 th iteration => 0.00713071732072\n",
      "Loss for  42095 th iteration => 0.00713061039832\n",
      "Loss for  42096 th iteration => 0.00713036313665\n",
      "Loss for  42097 th iteration => 0.00713022106034\n",
      "Loss for  42098 th iteration => 0.00713000888541\n",
      "Loss for  42099 th iteration => 0.00712983191292\n",
      "Loss for  42100 th iteration => 0.00712965456795\n",
      "Loss for  42101 th iteration => 0.00712944295384\n",
      "Loss for  42102 th iteration => 0.00712930018523\n",
      "Loss for  42103 th iteration => 0.00712906006957\n",
      "Loss for  42104 th iteration => 0.00712887581834\n",
      "Loss for  42105 th iteration => 0.007128762038\n",
      "Loss for  42106 th iteration => 0.00712851912445\n",
      "Loss for  42107 th iteration => 0.00712837623717\n",
      "Loss for  42108 th iteration => 0.00712816272053\n",
      "Loss for  42109 th iteration => 0.00712808038274\n",
      "Loss for  42110 th iteration => 0.00712782296577\n",
      "Loss for  42111 th iteration => 0.00712767380281\n",
      "Loss for  42112 th iteration => 0.00712748293978\n",
      "Loss for  42113 th iteration => 0.00712726767539\n",
      "Loss for  42114 th iteration => 0.00712714264603\n",
      "Loss for  42115 th iteration => 0.00712687754368\n",
      "Loss for  42116 th iteration => 0.00712673079994\n",
      "Loss for  42117 th iteration => 0.00712648780948\n",
      "Loss for  42118 th iteration => 0.0071263775828\n",
      "Loss for  42119 th iteration => 0.00712611932859\n",
      "Loss for  42120 th iteration => 0.00712593631932\n",
      "Loss for  42121 th iteration => 0.00712583317829\n",
      "Loss for  42122 th iteration => 0.00712558255976\n",
      "Loss for  42123 th iteration => 0.00712544441674\n",
      "Loss for  42124 th iteration => 0.00712522873367\n",
      "Loss for  42125 th iteration => 0.00712505584415\n",
      "Loss for  42126 th iteration => 0.00712487484199\n",
      "Loss for  42127 th iteration => 0.00712466932609\n",
      "Loss for  42128 th iteration => 0.00712458583228\n",
      "Loss for  42129 th iteration => 0.00712429444763\n",
      "Loss for  42130 th iteration => 0.0071241442974\n",
      "Loss for  42131 th iteration => 0.00712393988375\n",
      "Loss for  42132 th iteration => 0.00712384073275\n",
      "Loss for  42133 th iteration => 0.00712360053707\n",
      "Loss for  42134 th iteration => 0.00712343460583\n",
      "Loss for  42135 th iteration => 0.00712326091933\n",
      "Loss for  42136 th iteration => 0.00712303387149\n",
      "Loss for  42137 th iteration => 0.00712286029329\n",
      "Loss for  42138 th iteration => 0.00712264432274\n",
      "Loss for  42139 th iteration => 0.00712250760609\n",
      "Loss for  42140 th iteration => 0.00712226285419\n",
      "Loss for  42141 th iteration => 0.0071220783345\n",
      "Loss for  42142 th iteration => 0.00712189382613\n",
      "Loss for  42143 th iteration => 0.00712170932906\n",
      "Loss for  42144 th iteration => 0.00712152484327\n",
      "Loss for  42145 th iteration => 0.00712134122753\n",
      "Loss for  42146 th iteration => 0.007121245188\n",
      "Loss for  42147 th iteration => 0.00712098796294\n",
      "Loss for  42148 th iteration => 0.00712085684867\n",
      "Loss for  42149 th iteration => 0.0071206346315\n",
      "Loss for  42150 th iteration => 0.00712046869895\n",
      "Loss for  42151 th iteration => 0.00712028123417\n",
      "Loss for  42152 th iteration => 0.0071200821033\n",
      "Loss for  42153 th iteration => 0.00711999750504\n",
      "Loss for  42154 th iteration => 0.00711970703541\n",
      "Loss for  42155 th iteration => 0.0071196281302\n",
      "Loss for  42156 th iteration => 0.00711934806301\n",
      "Loss for  42157 th iteration => 0.00711926181456\n",
      "Loss for  42158 th iteration => 0.00711900917086\n",
      "Loss for  42159 th iteration => 0.00711885616201\n",
      "Loss for  42160 th iteration => 0.00711867000794\n",
      "Loss for  42161 th iteration => 0.0071184509613\n",
      "Loss for  42162 th iteration => 0.0071183305777\n",
      "Loss for  42163 th iteration => 0.00711806314975\n",
      "Loss for  42164 th iteration => 0.00711791916847\n",
      "Loss for  42165 th iteration => 0.0071176756537\n",
      "Loss for  42166 th iteration => 0.00711749137011\n",
      "Loss for  42167 th iteration => 0.00711730709779\n",
      "Loss for  42168 th iteration => 0.00711712529082\n",
      "Loss for  42169 th iteration => 0.00711701988589\n",
      "Loss for  42170 th iteration => 0.00711677244657\n",
      "Loss for  42171 th iteration => 0.00711663198114\n",
      "Loss for  42172 th iteration => 0.00711641953543\n",
      "Loss for  42173 th iteration => 0.00711624426607\n",
      "Loss for  42174 th iteration => 0.00711606723665\n",
      "Loss for  42175 th iteration => 0.00711592107347\n",
      "Loss for  42176 th iteration => 0.00711572741852\n",
      "Loss for  42177 th iteration => 0.00711552019634\n",
      "Loss for  42178 th iteration => 0.00711531423585\n",
      "Loss for  42179 th iteration => 0.00711513579769\n",
      "Loss for  42180 th iteration => 0.00711495879773\n",
      "Loss for  42181 th iteration => 0.00711475181911\n",
      "Loss for  42182 th iteration => 0.00711469698333\n",
      "Loss for  42183 th iteration => 0.00711438979021\n",
      "Loss for  42184 th iteration => 0.00711426212374\n",
      "Loss for  42185 th iteration => 0.00711403817222\n",
      "Loss for  42186 th iteration => 0.00711387332217\n",
      "Loss for  42187 th iteration => 0.00711368647652\n",
      "Loss for  42188 th iteration => 0.00711348472419\n",
      "Loss for  42189 th iteration => 0.00711333470419\n",
      "Loss for  42190 th iteration => 0.00711309632742\n",
      "Loss for  42191 th iteration => 0.0071129828563\n",
      "Loss for  42192 th iteration => 0.00711272514682\n",
      "Loss for  42193 th iteration => 0.00711254320158\n",
      "Loss for  42194 th iteration => 0.0071124438526\n",
      "Loss for  42195 th iteration => 0.00711219080687\n",
      "Loss for  42196 th iteration => 0.00711205643353\n",
      "Loss for  42197 th iteration => 0.00711183834541\n",
      "Loss for  42198 th iteration => 0.00711166920371\n",
      "Loss for  42199 th iteration => 0.00711148602011\n",
      "Loss for  42200 th iteration => 0.00711135121383\n",
      "Loss for  42201 th iteration => 0.00711114664421\n",
      "Loss for  42202 th iteration => 0.00711094797152\n",
      "Loss for  42203 th iteration => 0.00711080701825\n",
      "Loss for  42204 th iteration => 0.00711056515508\n",
      "Loss for  42205 th iteration => 0.00711046528019\n",
      "Loss for  42206 th iteration => 0.00711019783913\n",
      "Loss for  42207 th iteration => 0.00711008978994\n",
      "Loss for  42208 th iteration => 0.00710983133255\n",
      "Loss for  42209 th iteration => 0.00710965614066\n",
      "Loss for  42210 th iteration => 0.00710948004378\n",
      "Loss for  42211 th iteration => 0.00710926799374\n",
      "Loss for  42212 th iteration => 0.00710912867846\n",
      "Loss for  42213 th iteration => 0.00710888419888\n",
      "Loss for  42214 th iteration => 0.00710870036076\n",
      "Loss for  42215 th iteration => 0.00710851810953\n",
      "Loss for  42216 th iteration => 0.00710841219364\n",
      "Loss for  42217 th iteration => 0.00710816616417\n",
      "Loss for  42218 th iteration => 0.00710802511506\n",
      "Loss for  42219 th iteration => 0.00710781415149\n",
      "Loss for  42220 th iteration => 0.00710763822703\n",
      "Loss for  42221 th iteration => 0.00710746207246\n",
      "Loss for  42222 th iteration => 0.00710725152732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  42223 th iteration => 0.00710710992801\n",
      "Loss for  42224 th iteration => 0.00710686957736\n",
      "Loss for  42225 th iteration => 0.00710668795616\n",
      "Loss for  42226 th iteration => 0.00710657439333\n",
      "Loss for  42227 th iteration => 0.00710633349961\n",
      "Loss for  42228 th iteration => 0.00710619085425\n",
      "Loss for  42229 th iteration => 0.00710598129537\n",
      "Loss for  42230 th iteration => 0.00710589796345\n",
      "Loss for  42231 th iteration => 0.00710564375016\n",
      "Loss for  42232 th iteration => 0.0071054936581\n",
      "Loss for  42233 th iteration => 0.00710530593442\n",
      "Loss for  42234 th iteration => 0.00710508980434\n",
      "Loss for  42235 th iteration => 0.0071049678516\n",
      "Loss for  42236 th iteration => 0.00710470127106\n",
      "Loss for  42237 th iteration => 0.00710455678128\n",
      "Loss for  42238 th iteration => 0.00710431380367\n",
      "Loss for  42239 th iteration => 0.00710420579208\n",
      "Loss for  42240 th iteration => 0.00710394797816\n",
      "Loss for  42241 th iteration => 0.00710384805244\n",
      "Loss for  42242 th iteration => 0.00710359644759\n",
      "Loss for  42243 th iteration => 0.00710346150761\n",
      "Loss for  42244 th iteration => 0.00710324485019\n",
      "Loss for  42245 th iteration => 0.00710307515206\n",
      "Loss for  42246 th iteration => 0.00710289318693\n",
      "Loss for  42247 th iteration => 0.00710269074289\n",
      "Loss for  42248 th iteration => 0.00710260739878\n",
      "Loss for  42249 th iteration => 0.00710231744311\n",
      "Loss for  42250 th iteration => 0.00710223978034\n",
      "Loss for  42251 th iteration => 0.00710196501316\n",
      "Loss for  42252 th iteration => 0.00710187013568\n",
      "Loss for  42253 th iteration => 0.00710162781197\n",
      "Loss for  42254 th iteration => 0.00710146631888\n",
      "Loss for  42255 th iteration => 0.00710129034096\n",
      "Loss for  42256 th iteration => 0.00710106661788\n",
      "Loss for  42257 th iteration => 0.00710089222972\n",
      "Loss for  42258 th iteration => 0.00710067934695\n",
      "Loss for  42259 th iteration => 0.00710054172553\n",
      "Loss for  42260 th iteration => 0.00710029851768\n",
      "Loss for  42261 th iteration => 0.00710011511441\n",
      "Loss for  42262 th iteration => 0.00709993241348\n",
      "Loss for  42263 th iteration => 0.00709982593287\n",
      "Loss for  42264 th iteration => 0.00709958132801\n",
      "Loss for  42265 th iteration => 0.00709943973123\n",
      "Loss for  42266 th iteration => 0.00709923017518\n",
      "Loss for  42267 th iteration => 0.00709905372009\n",
      "Loss for  42268 th iteration => 0.00709887895596\n",
      "Loss for  42269 th iteration => 0.00709866789723\n",
      "Loss for  42270 th iteration => 0.00709852767129\n",
      "Loss for  42271 th iteration => 0.0070982886668\n",
      "Loss for  42272 th iteration => 0.00709810716614\n",
      "Loss for  42273 th iteration => 0.00709799228796\n",
      "Loss for  42274 th iteration => 0.00709775356952\n",
      "Loss for  42275 th iteration => 0.00709760962552\n",
      "Loss for  42276 th iteration => 0.00709740179959\n",
      "Loss for  42277 th iteration => 0.00709731500709\n",
      "Loss for  42278 th iteration => 0.00709706509562\n",
      "Loss for  42279 th iteration => 0.00709691159633\n",
      "Loss for  42280 th iteration => 0.00709672812149\n",
      "Loss for  42281 th iteration => 0.00709650863662\n",
      "Loss for  42282 th iteration => 0.00709639088066\n",
      "Loss for  42283 th iteration => 0.00709612299962\n",
      "Loss for  42284 th iteration => 0.00709598101943\n",
      "Loss for  42285 th iteration => 0.00709573772684\n",
      "Loss for  42286 th iteration => 0.00709555455628\n",
      "Loss for  42287 th iteration => 0.00709537228913\n",
      "Loss for  42288 th iteration => 0.00709527055902\n",
      "Loss for  42289 th iteration => 0.00709502165\n",
      "Loss for  42290 th iteration => 0.00709488483976\n",
      "Loss for  42291 th iteration => 0.00709467094366\n",
      "Loss for  42292 th iteration => 0.00709449931059\n",
      "Loss for  42293 th iteration => 0.00709432017106\n",
      "Loss for  42294 th iteration => 0.00709411597478\n",
      "Loss for  42295 th iteration => 0.00709403258297\n",
      "Loss for  42296 th iteration => 0.00709374527631\n",
      "Loss for  42297 th iteration => 0.00709359478031\n",
      "Loss for  42298 th iteration => 0.00709339207833\n",
      "Loss for  42299 th iteration => 0.00709321262109\n",
      "Loss for  42300 th iteration => 0.00709303884107\n",
      "Loss for  42301 th iteration => 0.00709283290119\n",
      "Loss for  42302 th iteration => 0.00709277783202\n",
      "Loss for  42303 th iteration => 0.00709247380346\n",
      "Loss for  42304 th iteration => 0.00709234388401\n",
      "Loss for  42305 th iteration => 0.00709212437423\n",
      "Loss for  42306 th iteration => 0.0070919573309\n",
      "Loss for  42307 th iteration => 0.00709177486733\n",
      "Loss for  42308 th iteration => 0.00709157098103\n",
      "Loss for  42309 th iteration => 0.00709142528383\n",
      "Loss for  42310 th iteration => 0.00709118483199\n",
      "Loss for  42311 th iteration => 0.00709107562478\n",
      "Loss for  42312 th iteration => 0.0070908184565\n",
      "Loss for  42313 th iteration => 0.00709072092837\n",
      "Loss for  42314 th iteration => 0.00709046825307\n",
      "Loss for  42315 th iteration => 0.00709033570524\n",
      "Loss for  42316 th iteration => 0.00709011798267\n",
      "Loss for  42317 th iteration => 0.00708995067153\n",
      "Loss for  42318 th iteration => 0.00708976764625\n",
      "Loss for  42319 th iteration => 0.00708956802684\n",
      "Loss for  42320 th iteration => 0.00708948435213\n",
      "Loss for  42321 th iteration => 0.00708919604623\n",
      "Loss for  42322 th iteration => 0.00708911805468\n",
      "Loss for  42323 th iteration => 0.0070888438457\n",
      "Loss for  42324 th iteration => 0.00708875085984\n",
      "Loss for  42325 th iteration => 0.00708850796287\n",
      "Loss for  42326 th iteration => 0.00708834836373\n",
      "Loss for  42327 th iteration => 0.00708817181047\n",
      "Loss for  42328 th iteration => 0.00708794881797\n",
      "Loss for  42329 th iteration => 0.00708777457694\n",
      "Loss for  42330 th iteration => 0.00708756287355\n",
      "Loss for  42331 th iteration => 0.00708742539371\n",
      "Loss for  42332 th iteration => 0.00708718209099\n",
      "Loss for  42333 th iteration => 0.00708699935193\n",
      "Loss for  42334 th iteration => 0.00708681801565\n",
      "Loss for  42335 th iteration => 0.00708671269086\n",
      "Loss for  42336 th iteration => 0.00708646824848\n",
      "Loss for  42337 th iteration => 0.00708632781843\n",
      "Loss for  42338 th iteration => 0.00708611841387\n",
      "Loss for  42339 th iteration => 0.0070859431365\n",
      "Loss for  42340 th iteration => 0.00708576851278\n",
      "Loss for  42341 th iteration => 0.00708555864284\n",
      "Loss for  42342 th iteration => 0.00708541854616\n",
      "Loss for  42343 th iteration => 0.00708517986566\n",
      "Loss for  42344 th iteration => 0.00708506897009\n",
      "Loss for  42345 th iteration => 0.00708482758686\n",
      "Loss for  42346 th iteration => 0.0070846875898\n",
      "Loss for  42347 th iteration => 0.00708447552597\n",
      "Loss for  42348 th iteration => 0.00708439638281\n",
      "Loss for  42349 th iteration => 0.00708414015456\n",
      "Loss for  42350 th iteration => 0.00708399422505\n",
      "Loss for  42351 th iteration => 0.00708380451282\n",
      "Loss for  42352 th iteration => 0.00708359251887\n",
      "Loss for  42353 th iteration => 0.0070834686042\n",
      "Loss for  42354 th iteration => 0.00708320635431\n",
      "Loss for  42355 th iteration => 0.00708306143405\n",
      "Loss for  42356 th iteration => 0.00708282104735\n",
      "Loss for  42357 th iteration => 0.00708271261737\n",
      "Loss for  42358 th iteration => 0.0070824566743\n",
      "Loss for  42359 th iteration => 0.00708227629322\n",
      "Loss for  42360 th iteration => 0.00708217453806\n",
      "Loss for  42361 th iteration => 0.00708192692529\n",
      "Loss for  42362 th iteration => 0.00708179021265\n",
      "Loss for  42363 th iteration => 0.00708157749054\n",
      "Loss for  42364 th iteration => 0.00708140607621\n",
      "Loss for  42365 th iteration => 0.00708122888166\n",
      "Loss for  42366 th iteration => 0.00708108718927\n",
      "Loss for  42367 th iteration => 0.00708089248883\n",
      "Loss for  42368 th iteration => 0.00708068814159\n",
      "Loss for  42369 th iteration => 0.00708048388486\n",
      "Loss for  42370 th iteration => 0.00708038858914\n",
      "Loss for  42371 th iteration => 0.00708014889978\n",
      "Loss for  42372 th iteration => 0.00707998685343\n",
      "Loss for  42373 th iteration => 0.00707981364451\n",
      "Loss for  42374 th iteration => 0.00707959065388\n",
      "Loss for  42375 th iteration => 0.00707941844002\n",
      "Loss for  42376 th iteration => 0.00707920550282\n",
      "Loss for  42377 th iteration => 0.00707907012932\n",
      "Loss for  42378 th iteration => 0.00707882876047\n",
      "Loss for  42379 th iteration => 0.00707864644065\n",
      "Loss for  42380 th iteration => 0.00707846413211\n",
      "Loss for  42381 th iteration => 0.0070782818348\n",
      "Loss for  42382 th iteration => 0.0070780995487\n",
      "Loss for  42383 th iteration => 0.00707791786323\n",
      "Loss for  42384 th iteration => 0.00707782279464\n",
      "Loss for  42385 th iteration => 0.00707756896311\n",
      "Loss for  42386 th iteration => 0.00707743886463\n",
      "Loss for  42387 th iteration => 0.00707721999585\n",
      "Loss for  42388 th iteration => 0.00707705512427\n",
      "Loss for  42389 th iteration => 0.0070768709624\n",
      "Loss for  42390 th iteration => 0.0070766736197\n",
      "Loss for  42391 th iteration => 0.0070765893248\n",
      "Loss for  42392 th iteration => 0.00707630293207\n",
      "Loss for  42393 th iteration => 0.00707622432316\n",
      "Loss for  42394 th iteration => 0.00707595066375\n",
      "Loss for  42395 th iteration => 0.007075859732\n",
      "Loss for  42396 th iteration => 0.00707561607651\n",
      "Loss for  42397 th iteration => 0.0070754585282\n",
      "Loss for  42398 th iteration => 0.0070752812199\n",
      "Loss for  42399 th iteration => 0.00707505992788\n",
      "Loss for  42400 th iteration => 0.00707488543201\n",
      "Loss for  42401 th iteration => 0.00707467528138\n",
      "Loss for  42402 th iteration => 0.00707453754612\n",
      "Loss for  42403 th iteration => 0.00707429521014\n",
      "Loss for  42404 th iteration => 0.00707411312202\n",
      "Loss for  42405 th iteration => 0.00707393228408\n",
      "Loss for  42406 th iteration => 0.00707382813718\n",
      "Loss for  42407 th iteration => 0.00707358381123\n",
      "Loss for  42408 th iteration => 0.00707344456577\n",
      "Loss for  42409 th iteration => 0.00707323527086\n",
      "Loss for  42410 th iteration => 0.00707306118492\n",
      "Loss for  42411 th iteration => 0.0070728866639\n",
      "Loss for  42412 th iteration => 0.0070726779924\n",
      "Loss for  42413 th iteration => 0.00707253799129\n",
      "Loss for  42414 th iteration => 0.00707230048676\n",
      "Loss for  42415 th iteration => 0.00707218960282\n",
      "Loss for  42416 th iteration => 0.00707194950085\n",
      "Loss for  42417 th iteration => 0.00707180952568\n",
      "Loss for  42418 th iteration => 0.00707159905551\n",
      "Loss for  42419 th iteration => 0.00707151942264\n",
      "Loss for  42420 th iteration => 0.00707126496248\n",
      "Loss for  42421 th iteration => 0.00707111857488\n",
      "Loss for  42422 th iteration => 0.00707093059951\n",
      "Loss for  42423 th iteration => 0.00707071817825\n",
      "Loss for  42424 th iteration => 0.00707059597\n",
      "Loss for  42425 th iteration => 0.00707033356035\n",
      "Loss for  42426 th iteration => 0.00707018974714\n",
      "Loss for  42427 th iteration => 0.00706994955922\n",
      "Loss for  42428 th iteration => 0.00706984221846\n",
      "Loss for  42429 th iteration => 0.00706958663312\n",
      "Loss for  42430 th iteration => 0.00706940732744\n",
      "Loss for  42431 th iteration => 0.00706930545358\n",
      "Loss for  42432 th iteration => 0.00706905924533\n",
      "Loss for  42433 th iteration => 0.0070689224361\n",
      "Loss for  42434 th iteration => 0.00706871109635\n",
      "Loss for  42435 th iteration => 0.0070685396075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  42436 th iteration => 0.00706836453513\n",
      "Loss for  42437 th iteration => 0.00706822128457\n",
      "Loss for  42438 th iteration => 0.00706802941305\n",
      "Loss for  42439 th iteration => 0.00706782384324\n",
      "Loss for  42440 th iteration => 0.00706762253124\n",
      "Loss for  42441 th iteration => 0.0070675248154\n",
      "Loss for  42442 th iteration => 0.007067288811\n",
      "Loss for  42443 th iteration => 0.00706712440305\n",
      "Loss for  42444 th iteration => 0.00706695482107\n",
      "Loss for  42445 th iteration => 0.00706673021539\n",
      "Loss for  42446 th iteration => 0.00706656017264\n",
      "Loss for  42447 th iteration => 0.00706634637641\n",
      "Loss for  42448 th iteration => 0.00706621314218\n",
      "Loss for  42449 th iteration => 0.00706597165142\n",
      "Loss for  42450 th iteration => 0.00706578998103\n",
      "Loss for  42451 th iteration => 0.00706560832188\n",
      "Loss for  42452 th iteration => 0.00706542667393\n",
      "Loss for  42453 th iteration => 0.00706524503717\n",
      "Loss for  42454 th iteration => 0.00706506490274\n",
      "Loss for  42455 th iteration => 0.00706496864048\n",
      "Loss for  42456 th iteration => 0.00706471728064\n",
      "Loss for  42457 th iteration => 0.00706458602414\n",
      "Loss for  42458 th iteration => 0.00706436959142\n",
      "Loss for  42459 th iteration => 0.00706420359724\n",
      "Loss for  42460 th iteration => 0.00706402275032\n",
      "Loss for  42461 th iteration => 0.00706388914524\n",
      "Loss for  42462 th iteration => 0.0070636881119\n",
      "Loss for  42463 th iteration => 0.00706349071717\n",
      "Loss for  42464 th iteration => 0.00706335362135\n",
      "Loss for  42465 th iteration => 0.00706319534524\n",
      "Loss for  42466 th iteration => 0.00706294930046\n",
      "Loss for  42467 th iteration => 0.00706279539581\n",
      "Loss for  42468 th iteration => 0.0070626157244\n",
      "Loss for  42469 th iteration => 0.0070623980839\n",
      "Loss for  42470 th iteration => 0.00706222233395\n",
      "Loss for  42471 th iteration => 0.00706201470205\n",
      "Loss for  42472 th iteration => 0.00706187572546\n",
      "Loss for  42473 th iteration => 0.00706163568337\n",
      "Loss for  42474 th iteration => 0.00706145423284\n",
      "Loss for  42475 th iteration => 0.00706127285092\n",
      "Loss for  42476 th iteration => 0.00706117029259\n",
      "Loss for  42477 th iteration => 0.00706092565168\n",
      "Loss for  42478 th iteration => 0.00706078798991\n",
      "Loss for  42479 th iteration => 0.00706057838476\n",
      "Loss for  42480 th iteration => 0.00706040587799\n",
      "Loss for  42481 th iteration => 0.00706023105109\n",
      "Loss for  42482 th iteration => 0.00706002395459\n",
      "Loss for  42483 th iteration => 0.00705988365162\n",
      "Loss for  42484 th iteration => 0.00705964739106\n",
      "Loss for  42485 th iteration => 0.00705953701561\n",
      "Loss for  42486 th iteration => 0.00705929767632\n",
      "Loss for  42487 th iteration => 0.00705915821047\n",
      "Loss for  42488 th iteration => 0.00705894792154\n",
      "Loss for  42489 th iteration => 0.00705878217734\n",
      "Loss for  42490 th iteration => 0.00705868528493\n",
      "Loss for  42491 th iteration => 0.00705842005966\n",
      "Loss for  42492 th iteration => 0.00705831500003\n",
      "Loss for  42493 th iteration => 0.00705805798069\n",
      "Loss for  42494 th iteration => 0.00705794484028\n",
      "Loss for  42495 th iteration => 0.00705771162469\n",
      "Loss for  42496 th iteration => 0.00705750470844\n",
      "Loss for  42497 th iteration => 0.0070573654341\n",
      "Loss for  42498 th iteration => 0.00705712349603\n",
      "Loss for  42499 th iteration => 0.00705694227583\n",
      "Loss for  42500 th iteration => 0.00705676318743\n",
      "Loss for  42501 th iteration => 0.00705666122002\n",
      "Loss for  42502 th iteration => 0.00705641640867\n",
      "Loss for  42503 th iteration => 0.0070562794204\n",
      "Loss for  42504 th iteration => 0.00705606956256\n",
      "Loss for  42505 th iteration => 0.0070558978107\n",
      "Loss for  42506 th iteration => 0.00705572407451\n",
      "Loss for  42507 th iteration => 0.0070555796217\n",
      "Loss for  42508 th iteration => 0.00705539027769\n",
      "Loss for  42509 th iteration => 0.0070551847033\n",
      "Loss for  42510 th iteration => 0.00705498356336\n",
      "Loss for  42511 th iteration => 0.0070548064155\n",
      "Loss for  42512 th iteration => 0.00705463418748\n",
      "Loss for  42513 th iteration => 0.00705442993902\n",
      "Loss for  42514 th iteration => 0.00705437631073\n",
      "Loss for  42515 th iteration => 0.00705407494851\n",
      "Loss for  42516 th iteration => 0.00705394689174\n",
      "Loss for  42517 th iteration => 0.0070537293666\n",
      "Loss for  42518 th iteration => 0.0070535642184\n",
      "Loss for  42519 th iteration => 0.00705338370679\n",
      "Loss for  42520 th iteration => 0.00705318174836\n",
      "Loss for  42521 th iteration => 0.00705303797013\n",
      "Loss for  42522 th iteration => 0.00705279947924\n",
      "Loss for  42523 th iteration => 0.00705269215768\n",
      "Loss for  42524 th iteration => 0.00705243778034\n",
      "Loss for  42525 th iteration => 0.00705234041795\n",
      "Loss for  42526 th iteration => 0.00705209141634\n",
      "Loss for  42527 th iteration => 0.00705195908206\n",
      "Loss for  42528 th iteration => 0.00705174498516\n",
      "Loss for  42529 th iteration => 0.00705157793555\n",
      "Loss for  42530 th iteration => 0.00705139923356\n",
      "Loss for  42531 th iteration => 0.00705126354915\n",
      "Loss for  42532 th iteration => 0.00705106583787\n",
      "Loss for  42533 th iteration => 0.007050866413\n",
      "Loss for  42534 th iteration => 0.00705073257004\n",
      "Loss for  42535 th iteration => 0.00705057220485\n",
      "Loss for  42536 th iteration => 0.00705032950971\n",
      "Loss for  42537 th iteration => 0.00705017355175\n",
      "Loss for  42538 th iteration => 0.00704999717215\n",
      "Loss for  42539 th iteration => 0.0070497786949\n",
      "Loss for  42540 th iteration => 0.00704960524144\n",
      "Loss for  42541 th iteration => 0.00704939659697\n",
      "Loss for  42542 th iteration => 0.00704925988622\n",
      "Loss for  42543 th iteration => 0.00704902054948\n",
      "Loss for  42544 th iteration => 0.00704883973426\n",
      "Loss for  42545 th iteration => 0.00704865943729\n",
      "Loss for  42546 th iteration => 0.00704855486611\n",
      "Loss for  42547 th iteration => 0.00704831348918\n",
      "Loss for  42548 th iteration => 0.0070481738492\n",
      "Loss for  42549 th iteration => 0.00704796747338\n",
      "Loss for  42550 th iteration => 0.00704779302286\n",
      "Loss for  42551 th iteration => 0.00704762139084\n",
      "Loss for  42552 th iteration => 0.00704741238488\n",
      "Loss for  42553 th iteration => 0.0070472752425\n",
      "Loss for  42554 th iteration => 0.00704703943995\n",
      "Loss for  42555 th iteration => 0.00704692779033\n",
      "Loss for  42556 th iteration => 0.00704669097836\n",
      "Loss for  42557 th iteration => 0.00704655026853\n",
      "Loss for  42558 th iteration => 0.0070463425543\n",
      "Loss for  42559 th iteration => 0.00704625973648\n",
      "Loss for  42560 th iteration => 0.00704601094768\n",
      "Loss for  42561 th iteration => 0.00704586147209\n",
      "Loss for  42562 th iteration => 0.00704567907199\n",
      "Loss for  42563 th iteration => 0.00704546436485\n",
      "Loss for  42564 th iteration => 0.00704528765891\n",
      "Loss for  42565 th iteration => 0.00704508267968\n",
      "Loss for  42566 th iteration => 0.00704494274992\n",
      "Loss for  42567 th iteration => 0.00704470370157\n",
      "Loss for  42568 th iteration => 0.00704452310311\n",
      "Loss for  42569 th iteration => 0.00704434296833\n",
      "Loss for  42570 th iteration => 0.00704424181127\n",
      "Loss for  42571 th iteration => 0.00704399746414\n",
      "Loss for  42572 th iteration => 0.00704386121004\n",
      "Loss for  42573 th iteration => 0.00704365189214\n",
      "Loss for  42574 th iteration => 0.00704348079966\n",
      "Loss for  42575 th iteration => 0.00704330625327\n",
      "Loss for  42576 th iteration => 0.00704310057789\n",
      "Loss for  42577 th iteration => 0.00704296054845\n",
      "Loss for  42578 th iteration => 0.00704272514886\n",
      "Loss for  42579 th iteration => 0.00704261589001\n",
      "Loss for  42580 th iteration => 0.00704237712772\n",
      "Loss for  42581 th iteration => 0.00704223878884\n",
      "Loss for  42582 th iteration => 0.00704202938243\n",
      "Loss for  42583 th iteration => 0.00704195166347\n",
      "Loss for  42584 th iteration => 0.00704169822877\n",
      "Loss for  42585 th iteration => 0.00704155379837\n",
      "Loss for  42586 th iteration => 0.00704136680588\n",
      "Loss for  42587 th iteration => 0.0070411563836\n",
      "Loss for  42588 th iteration => 0.00704103511715\n",
      "Loss for  42589 th iteration => 0.00704077472833\n",
      "Loss for  42590 th iteration => 0.00704063204315\n",
      "Loss for  42591 th iteration => 0.00704039370829\n",
      "Loss for  42592 th iteration => 0.00704028746911\n",
      "Loss for  42593 th iteration => 0.0070400339229\n",
      "Loss for  42594 th iteration => 0.00703993645629\n",
      "Loss for  42595 th iteration => 0.00703968879517\n",
      "Loss for  42596 th iteration => 0.00703955637154\n",
      "Loss for  42597 th iteration => 0.00703934360018\n",
      "Loss for  42598 th iteration => 0.00703917647619\n",
      "Loss for  42599 th iteration => 0.00703899845857\n",
      "Loss for  42600 th iteration => 0.00703886121264\n",
      "Loss for  42601 th iteration => 0.00703866628526\n",
      "Loss for  42602 th iteration => 0.00703846609171\n",
      "Loss for  42603 th iteration => 0.00703826319822\n",
      "Loss for  42604 th iteration => 0.00703809198932\n",
      "Loss for  42605 th iteration => 0.00703799405882\n",
      "Loss for  42606 th iteration => 0.00703773193627\n",
      "Loss for  42607 th iteration => 0.00703762584666\n",
      "Loss for  42608 th iteration => 0.00703737735048\n",
      "Loss for  42609 th iteration => 0.00703719950635\n",
      "Loss for  42610 th iteration => 0.00703703333247\n",
      "Loss for  42611 th iteration => 0.00703681866514\n",
      "Loss for  42612 th iteration => 0.00703668923731\n",
      "Loss for  42613 th iteration => 0.00703644829374\n",
      "Loss for  42614 th iteration => 0.00703626810981\n",
      "Loss for  42615 th iteration => 0.00703608793707\n",
      "Loss for  42616 th iteration => 0.00703590777548\n",
      "Loss for  42617 th iteration => 0.00703572819319\n",
      "Loss for  42618 th iteration => 0.0070356333066\n",
      "Loss for  42619 th iteration => 0.00703538350611\n",
      "Loss for  42620 th iteration => 0.00703525363858\n",
      "Loss for  42621 th iteration => 0.00703503875165\n",
      "Loss for  42622 th iteration => 0.00703487416018\n",
      "Loss for  42623 th iteration => 0.00703469416892\n",
      "Loss for  42624 th iteration => 0.00703456170448\n",
      "Loss for  42625 th iteration => 0.00703436244603\n",
      "Loss for  42626 th iteration => 0.00703416623066\n",
      "Loss for  42627 th iteration => 0.00703403047463\n",
      "Loss for  42628 th iteration => 0.00703379206993\n",
      "Loss for  42629 th iteration => 0.00703369505738\n",
      "Loss for  42630 th iteration => 0.00703343244867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  42631 th iteration => 0.00703332729087\n",
      "Loss for  42632 th iteration => 0.00703307571644\n",
      "Loss for  42633 th iteration => 0.00703290144981\n",
      "Loss for  42634 th iteration => 0.00703273210215\n",
      "Loss for  42635 th iteration => 0.00703252107949\n",
      "Loss for  42636 th iteration => 0.00703238841097\n",
      "Loss for  42637 th iteration => 0.0070321478056\n",
      "Loss for  42638 th iteration => 0.00703196800252\n",
      "Loss for  42639 th iteration => 0.00703186461017\n",
      "Loss for  42640 th iteration => 0.00703162372309\n",
      "Loss for  42641 th iteration => 0.00703148526658\n",
      "Loss for  42642 th iteration => 0.00703127937584\n",
      "Loss for  42643 th iteration => 0.00703110611371\n",
      "Loss for  42644 th iteration => 0.00703093496171\n",
      "Loss for  42645 th iteration => 0.00703072714931\n",
      "Loss for  42646 th iteration => 0.00703059048162\n",
      "Loss for  42647 th iteration => 0.00703035568345\n",
      "Loss for  42648 th iteration => 0.00703024488747\n",
      "Loss for  42649 th iteration => 0.00703000888892\n",
      "Loss for  42650 th iteration => 0.00702986904153\n",
      "Loss for  42651 th iteration => 0.00702966205415\n",
      "Loss for  42652 th iteration => 0.00702949623648\n",
      "Loss for  42653 th iteration => 0.00702939969353\n",
      "Loss for  42654 th iteration => 0.00702913704273\n",
      "Loss for  42655 th iteration => 0.00702903235029\n",
      "Loss for  42656 th iteration => 0.00702877849436\n",
      "Loss for  42657 th iteration => 0.00702860670138\n",
      "Loss for  42658 th iteration => 0.00702843531707\n",
      "Loss for  42659 th iteration => 0.00702822675018\n",
      "Loss for  42660 th iteration => 0.00702809206281\n",
      "Loss for  42661 th iteration => 0.00702785122124\n",
      "Loss for  42662 th iteration => 0.00702767220581\n",
      "Loss for  42663 th iteration => 0.00702757081681\n",
      "Loss for  42664 th iteration => 0.00702732836157\n",
      "Loss for  42665 th iteration => 0.00702719189447\n",
      "Loss for  42666 th iteration => 0.00702698444943\n",
      "Loss for  42667 th iteration => 0.00702681316298\n",
      "Loss for  42668 th iteration => 0.00702664047033\n",
      "Loss for  42669 th iteration => 0.0070264346201\n",
      "Loss for  42670 th iteration => 0.00702629642519\n",
      "Loss for  42671 th iteration => 0.00702606167603\n",
      "Loss for  42672 th iteration => 0.00702595266914\n",
      "Loss for  42673 th iteration => 0.00702571531489\n",
      "Loss for  42674 th iteration => 0.00702557724725\n",
      "Loss for  42675 th iteration => 0.00702536946682\n",
      "Loss for  42676 th iteration => 0.00702529096302\n",
      "Loss for  42677 th iteration => 0.0070250399504\n",
      "Loss for  42678 th iteration => 0.00702489479171\n",
      "Loss for  42679 th iteration => 0.00702471016532\n",
      "Loss for  42680 th iteration => 0.00702449906999\n",
      "Loss for  42681 th iteration => 0.00702438011495\n",
      "Loss for  42682 th iteration => 0.0070241195565\n",
      "Loss for  42683 th iteration => 0.00702397866899\n",
      "Loss for  42684 th iteration => 0.0070237403581\n",
      "Loss for  42685 th iteration => 0.00702356081413\n",
      "Loss for  42686 th iteration => 0.00702338271598\n",
      "Loss for  42687 th iteration => 0.00702328302978\n",
      "Loss for  42688 th iteration => 0.00702303927354\n",
      "Loss for  42689 th iteration => 0.00702290457785\n",
      "Loss for  42690 th iteration => 0.00702269576346\n",
      "Loss for  42691 th iteration => 0.00702252631608\n",
      "Loss for  42692 th iteration => 0.00702235315099\n",
      "Loss for  42693 th iteration => 0.00702221128653\n",
      "Loss for  42694 th iteration => 0.00702202267099\n",
      "Loss for  42695 th iteration => 0.00702181976414\n",
      "Loss for  42696 th iteration => 0.00702161999867\n",
      "Loss for  42697 th iteration => 0.0070214448278\n",
      "Loss for  42698 th iteration => 0.00702127395689\n",
      "Loss for  42699 th iteration => 0.00702107144523\n",
      "Loss for  42700 th iteration => 0.00702101867427\n",
      "Loss for  42701 th iteration => 0.00702071992552\n",
      "Loss for  42702 th iteration => 0.00702059309456\n",
      "Loss for  42703 th iteration => 0.00702037766451\n",
      "Loss for  42704 th iteration => 0.00702021378117\n",
      "Loss for  42705 th iteration => 0.00702003532548\n",
      "Loss for  42706 th iteration => 0.00701983467106\n",
      "Loss for  42707 th iteration => 0.00701969290945\n",
      "Loss for  42708 th iteration => 0.00701945576181\n",
      "Loss for  42709 th iteration => 0.00701935041746\n",
      "Loss for  42710 th iteration => 0.00701909884698\n",
      "Loss for  42711 th iteration => 0.00701900043225\n",
      "Loss for  42712 th iteration => 0.0070187557972\n",
      "Loss for  42713 th iteration => 0.00701862246266\n",
      "Loss for  42714 th iteration => 0.00701841268013\n",
      "Loss for  42715 th iteration => 0.0070182446823\n",
      "Loss for  42716 th iteration => 0.0070180709833\n",
      "Loss for  42717 th iteration => 0.00701793199573\n",
      "Loss for  42718 th iteration => 0.00701774086572\n",
      "Loss for  42719 th iteration => 0.00701753883515\n",
      "Loss for  42720 th iteration => 0.00701733926568\n",
      "Loss for  42721 th iteration => 0.00701724535754\n",
      "Loss for  42722 th iteration => 0.00701701053052\n",
      "Loss for  42723 th iteration => 0.00701685001159\n",
      "Loss for  42724 th iteration => 0.00701668152694\n",
      "Loss for  42725 th iteration => 0.00701646023413\n",
      "Loss for  42726 th iteration => 0.00701629210066\n",
      "Loss for  42727 th iteration => 0.00701608145859\n",
      "Loss for  42728 th iteration => 0.00701595008187\n",
      "Loss for  42729 th iteration => 0.00701571117934\n",
      "Loss for  42730 th iteration => 0.00701553203459\n",
      "Loss for  42731 th iteration => 0.00701535290101\n",
      "Loss for  42732 th iteration => 0.00701517377857\n",
      "Loss for  42733 th iteration => 0.00701499524271\n",
      "Loss for  42734 th iteration => 0.00701490209715\n",
      "Loss for  42735 th iteration => 0.00701465262519\n",
      "Loss for  42736 th iteration => 0.00701452450212\n",
      "Loss for  42737 th iteration => 0.00701430994012\n",
      "Loss for  42738 th iteration => 0.00701414709689\n",
      "Loss for  42739 th iteration => 0.007013967357\n",
      "Loss for  42740 th iteration => 0.00701383738872\n",
      "Loss for  42741 th iteration => 0.00701363769397\n",
      "Loss for  42742 th iteration => 0.00701344398595\n",
      "Loss for  42743 th iteration => 0.00701330778277\n",
      "Loss for  42744 th iteration => 0.00701307122699\n",
      "Loss for  42745 th iteration => 0.0070129751046\n",
      "Loss for  42746 th iteration => 0.00701271366825\n",
      "Loss for  42747 th iteration => 0.00701260940615\n",
      "Loss for  42748 th iteration => 0.00701235830679\n",
      "Loss for  42749 th iteration => 0.00701218571888\n",
      "Loss for  42750 th iteration => 0.00701201675384\n",
      "Loss for  42751 th iteration => 0.00701180742786\n",
      "Loss for  42752 th iteration => 0.00701167512391\n",
      "Loss for  42753 th iteration => 0.00701143551729\n",
      "Loss for  42754 th iteration => 0.00701125759399\n",
      "Loss for  42755 th iteration => 0.00701115415048\n",
      "Loss for  42756 th iteration => 0.00701091537138\n",
      "Loss for  42757 th iteration => 0.0070107768908\n",
      "Loss for  42758 th iteration => 0.00701057308086\n",
      "Loss for  42759 th iteration => 0.00701039982178\n",
      "Loss for  42760 th iteration => 0.00701023072337\n",
      "Loss for  42761 th iteration => 0.00701002294118\n",
      "Loss for  42762 th iteration => 0.00700988829982\n",
      "Loss for  42763 th iteration => 0.00700965442596\n",
      "Loss for  42764 th iteration => 0.00700954372847\n",
      "Loss for  42765 th iteration => 0.00700930968846\n",
      "Loss for  42766 th iteration => 0.00700916996686\n",
      "Loss for  42767 th iteration => 0.00700896574014\n",
      "Loss for  42768 th iteration => 0.00700888252009\n",
      "Loss for  42769 th iteration => 0.00700863781234\n",
      "Loss for  42770 th iteration => 0.00700848804037\n",
      "Loss for  42771 th iteration => 0.00700830961666\n",
      "Loss for  42772 th iteration => 0.00700809504261\n",
      "Loss for  42773 th iteration => 0.00700792147918\n",
      "Loss for  42774 th iteration => 0.00700771712736\n",
      "Loss for  42775 th iteration => 0.00700758027884\n",
      "Loss for  42776 th iteration => 0.00700734244703\n",
      "Loss for  42777 th iteration => 0.00700716372271\n",
      "Loss for  42778 th iteration => 0.00700698752789\n",
      "Loss for  42779 th iteration => 0.00700688399121\n",
      "Loss for  42780 th iteration => 0.00700664572503\n",
      "Loss for  42781 th iteration => 0.00700650716694\n",
      "Loss for  42782 th iteration => 0.00700630385431\n",
      "Loss for  42783 th iteration => 0.0070061305332\n",
      "Loss for  42784 th iteration => 0.00700596191665\n",
      "Loss for  42785 th iteration => 0.00700575408773\n",
      "Loss for  42786 th iteration => 0.00700561991297\n",
      "Loss for  42787 th iteration => 0.0070053861726\n",
      "Loss for  42788 th iteration => 0.00700527510637\n",
      "Loss for  42789 th iteration => 0.00700504185573\n",
      "Loss for  42790 th iteration => 0.0070049017795\n",
      "Loss for  42791 th iteration => 0.00700470009363\n",
      "Loss for  42792 th iteration => 0.00700461584862\n",
      "Loss for  42793 th iteration => 0.00700437257704\n",
      "Loss for  42794 th iteration => 0.00700422180934\n",
      "Loss for  42795 th iteration => 0.00700404479279\n",
      "Loss for  42796 th iteration => 0.00700382839607\n",
      "Loss for  42797 th iteration => 0.00700365568909\n",
      "Loss for  42798 th iteration => 0.00700345091716\n",
      "Loss for  42799 th iteration => 0.00700331490678\n",
      "Loss for  42800 th iteration => 0.00700307555166\n",
      "Loss for  42801 th iteration => 0.00700289817688\n",
      "Loss for  42802 th iteration => 0.00700279907519\n",
      "Loss for  42803 th iteration => 0.00700255679887\n",
      "Loss for  42804 th iteration => 0.00700242263132\n",
      "Loss for  42805 th iteration => 0.00700221535279\n",
      "Loss for  42806 th iteration => 0.00700204637841\n",
      "Loss for  42807 th iteration => 0.00700187383958\n",
      "Loss for  42808 th iteration => 0.00700167031422\n",
      "Loss for  42809 th iteration => 0.00700153226015\n",
      "Loss for  42810 th iteration => 0.00700129913891\n",
      "Loss for  42811 th iteration => 0.00700119124366\n",
      "Loss for  42812 th iteration => 0.00700095524185\n",
      "Loss for  42813 th iteration => 0.00700081830395\n",
      "Loss for  42814 th iteration => 0.00700061279157\n",
      "Loss for  42815 th iteration => 0.00700053536884\n",
      "Loss for  42816 th iteration => 0.00700028571436\n",
      "Loss for  42817 th iteration => 0.00700014168846\n",
      "Loss for  42818 th iteration => 0.0069999583692\n",
      "Loss for  42819 th iteration => 0.00699974845684\n",
      "Loss for  42820 th iteration => 0.00699963075943\n",
      "Loss for  42821 th iteration => 0.00699937072464\n",
      "Loss for  42822 th iteration => 0.00699923103441\n",
      "Loss for  42823 th iteration => 0.00699899387549\n",
      "Loss for  42824 th iteration => 0.00699889056823\n",
      "Loss for  42825 th iteration => 0.00699864047361\n",
      "Loss for  42826 th iteration => 0.00699854113103\n",
      "Loss for  42827 th iteration => 0.0069982994465\n",
      "Loss for  42828 th iteration => 0.0069981652246\n",
      "Loss for  42829 th iteration => 0.00699795835205\n",
      "Loss for  42830 th iteration => 0.00699778950726\n",
      "Loss for  42831 th iteration => 0.0069976193706\n",
      "Loss for  42832 th iteration => 0.00699747713068\n",
      "Loss for  42833 th iteration => 0.00699729124952\n",
      "Loss for  42834 th iteration => 0.0069970875396\n",
      "Loss for  42835 th iteration => 0.00699689210241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  42836 th iteration => 0.00699679406358\n",
      "Loss for  42837 th iteration => 0.00699656535487\n",
      "Loss for  42838 th iteration => 0.00699640080885\n",
      "Loss for  42839 th iteration => 0.00699623833979\n",
      "Loss for  42840 th iteration => 0.00699601477679\n",
      "Loss for  42841 th iteration => 0.00699585030208\n",
      "Loss for  42842 th iteration => 0.00699563807066\n",
      "Loss for  42843 th iteration => 0.00699551029762\n",
      "Loss for  42844 th iteration => 0.00699527178427\n",
      "Loss for  42845 th iteration => 0.00699509366217\n",
      "Loss for  42846 th iteration => 0.00699491555119\n",
      "Loss for  42847 th iteration => 0.0069947374513\n",
      "Loss for  42848 th iteration => 0.00699456102631\n",
      "Loss for  42849 th iteration => 0.00699446533359\n",
      "Loss for  42850 th iteration => 0.00699422041988\n",
      "Loss for  42851 th iteration => 0.00699408981039\n",
      "Loss for  42852 th iteration => 0.00699387974595\n",
      "Loss for  42853 th iteration => 0.00699371447665\n",
      "Loss for  42854 th iteration => 0.00699354044027\n",
      "Loss for  42855 th iteration => 0.00699340437403\n",
      "Loss for  42856 th iteration => 0.00699321275397\n",
      "Loss for  42857 th iteration => 0.00699301307248\n",
      "Loss for  42858 th iteration => 0.00699288593701\n",
      "Loss for  42859 th iteration => 0.00699272376919\n",
      "Loss for  42860 th iteration => 0.00699248852454\n",
      "Loss for  42861 th iteration => 0.00699233097085\n",
      "Loss for  42862 th iteration => 0.0069921618765\n",
      "Loss for  42863 th iteration => 0.00699194334594\n",
      "Loss for  42864 th iteration => 0.0069917750925\n",
      "Loss for  42865 th iteration => 0.00699156707764\n",
      "Loss for  42866 th iteration => 0.00699143547225\n",
      "Loss for  42867 th iteration => 0.00699119843684\n",
      "Loss for  42868 th iteration => 0.00699102051952\n",
      "Loss for  42869 th iteration => 0.00699084539397\n",
      "Loss for  42870 th iteration => 0.00699073792813\n",
      "Loss for  42871 th iteration => 0.00699050517011\n",
      "Loss for  42872 th iteration => 0.00699036275099\n",
      "Loss for  42873 th iteration => 0.0069901648785\n",
      "Loss for  42874 th iteration => 0.00698998776394\n",
      "Loss for  42875 th iteration => 0.00698982452003\n",
      "Loss for  42876 th iteration => 0.00698961296476\n",
      "Loss for  42877 th iteration => 0.00698948472989\n",
      "Loss for  42878 th iteration => 0.00698931570872\n",
      "Loss for  42879 th iteration => 0.0069890843445\n",
      "Loss for  42880 th iteration => 0.00698894397391\n",
      "Loss for  42881 th iteration => 0.00698874166778\n",
      "Loss for  42882 th iteration => 0.00698865628215\n",
      "Loss for  42883 th iteration => 0.00698841571787\n",
      "Loss for  42884 th iteration => 0.00698826384917\n",
      "Loss for  42885 th iteration => 0.00698808950075\n",
      "Loss for  42886 th iteration => 0.00698787486752\n",
      "Loss for  42887 th iteration => 0.00698770375138\n",
      "Loss for  42888 th iteration => 0.00698749898494\n",
      "Loss for  42889 th iteration => 0.00698736454831\n",
      "Loss for  42890 th iteration => 0.00698712851778\n",
      "Loss for  42891 th iteration => 0.00698695080278\n",
      "Loss for  42892 th iteration => 0.00698677514801\n",
      "Loss for  42893 th iteration => 0.00698667068148\n",
      "Loss for  42894 th iteration => 0.00698643533916\n",
      "Loss for  42895 th iteration => 0.00698629589278\n",
      "Loss for  42896 th iteration => 0.00698609546241\n",
      "Loss for  42897 th iteration => 0.00698592129445\n",
      "Loss for  42898 th iteration => 0.00698575551869\n",
      "Loss for  42899 th iteration => 0.00698554688426\n",
      "Loss for  42900 th iteration => 0.00698541576319\n",
      "Loss for  42901 th iteration => 0.00698524946598\n",
      "Loss for  42902 th iteration => 0.00698501647791\n",
      "Loss for  42903 th iteration => 0.00698487812397\n",
      "Loss for  42904 th iteration => 0.00698467411719\n",
      "Loss for  42905 th iteration => 0.00698450960825\n",
      "Loss for  42906 th iteration => 0.00698441531749\n",
      "Loss for  42907 th iteration => 0.00698415488312\n",
      "Loss for  42908 th iteration => 0.00698405245839\n",
      "Loss for  42909 th iteration => 0.00698380155324\n",
      "Loss for  42910 th iteration => 0.00698363141062\n",
      "Loss for  42911 th iteration => 0.00698346283307\n",
      "Loss for  42912 th iteration => 0.0069832559739\n",
      "Loss for  42913 th iteration => 0.00698312403578\n",
      "Loss for  42914 th iteration => 0.00698288574197\n",
      "Loss for  42915 th iteration => 0.00698270973879\n",
      "Loss for  42916 th iteration => 0.00698260786602\n",
      "Loss for  42917 th iteration => 0.0069823703435\n",
      "Loss for  42918 th iteration => 0.00698223346611\n",
      "Loss for  42919 th iteration => 0.00698203088019\n",
      "Loss for  42920 th iteration => 0.00698185925683\n",
      "Loss for  42921 th iteration => 0.00698169134978\n",
      "Loss for  42922 th iteration => 0.00698148523594\n",
      "Loss for  42923 th iteration => 0.00698135175318\n",
      "Loss for  42924 th iteration => 0.00698112012776\n",
      "Loss for  42925 th iteration => 0.00698100908423\n",
      "Loss for  42926 th iteration => 0.00698077821654\n",
      "Loss for  42927 th iteration => 0.00698063818468\n",
      "Loss for  42928 th iteration => 0.00698043849845\n",
      "Loss for  42929 th iteration => 0.00698035318113\n",
      "Loss for  42930 th iteration => 0.00698011335801\n",
      "Loss for  42931 th iteration => 0.00697996158713\n",
      "Loss for  42932 th iteration => 0.00697978795069\n",
      "Loss for  42933 th iteration => 0.00697957188718\n",
      "Loss for  42934 th iteration => 0.00697940142632\n",
      "Loss for  42935 th iteration => 0.00697919684191\n",
      "Loss for  42936 th iteration => 0.00697906304004\n",
      "Loss for  42937 th iteration => 0.00697882531595\n",
      "Loss for  42938 th iteration => 0.0069786491573\n",
      "Loss for  42939 th iteration => 0.00697854944774\n",
      "Loss for  42940 th iteration => 0.00697831017147\n",
      "Loss for  42941 th iteration => 0.00697817544118\n",
      "Loss for  42942 th iteration => 0.00697797111753\n",
      "Loss for  42943 th iteration => 0.00697780162542\n",
      "Loss for  42944 th iteration => 0.00697763199641\n",
      "Loss for  42945 th iteration => 0.00697742799822\n",
      "Loss for  42946 th iteration => 0.00697729280901\n",
      "Loss for  42947 th iteration => 0.00697706162149\n",
      "Loss for  42948 th iteration => 0.00697695197209\n",
      "Loss for  42949 th iteration => 0.0069767201175\n",
      "Loss for  42950 th iteration => 0.00697658146902\n",
      "Loss for  42951 th iteration => 0.00697638069289\n",
      "Loss for  42952 th iteration => 0.00697629825319\n",
      "Loss for  42953 th iteration => 0.00697605596514\n",
      "Loss for  42954 th iteration => 0.00697590704481\n",
      "Loss for  42955 th iteration => 0.00697573097045\n",
      "Loss for  42956 th iteration => 0.00697551665911\n",
      "Loss for  42957 th iteration => 0.0069753449298\n",
      "Loss for  42958 th iteration => 0.00697514200925\n",
      "Loss for  42959 th iteration => 0.00697500695077\n",
      "Loss for  42960 th iteration => 0.00697476951945\n",
      "Loss for  42961 th iteration => 0.00697459345967\n",
      "Loss for  42962 th iteration => 0.00697449535467\n",
      "Loss for  42963 th iteration => 0.00697425487985\n",
      "Loss for  42964 th iteration => 0.00697412174499\n",
      "Loss for  42965 th iteration => 0.00697391623186\n",
      "Loss for  42966 th iteration => 0.00697374832622\n",
      "Loss for  42967 th iteration => 0.00697357751663\n",
      "Loss for  42968 th iteration => 0.00697337509612\n",
      "Loss for  42969 th iteration => 0.00697323873505\n",
      "Loss for  42970 th iteration => 0.00697300794229\n",
      "Loss for  42971 th iteration => 0.00697289924661\n",
      "Loss for  42972 th iteration => 0.00697266684277\n",
      "Loss for  42973 th iteration => 0.00697252914266\n",
      "Loss for  42974 th iteration => 0.00697232795829\n",
      "Loss for  42975 th iteration => 0.00697224741709\n",
      "Loss for  42976 th iteration => 0.00697200363696\n",
      "Loss for  42977 th iteration => 0.00697185660126\n",
      "Loss for  42978 th iteration => 0.00697167904871\n",
      "Loss for  42979 th iteration => 0.00697146623285\n",
      "Loss for  42980 th iteration => 0.00697135419685\n",
      "Loss for  42981 th iteration => 0.0069710926776\n",
      "Loss for  42982 th iteration => 0.00697095657548\n",
      "Loss for  42983 th iteration => 0.00697071935176\n",
      "Loss for  42984 th iteration => 0.00697054338978\n",
      "Loss for  42985 th iteration => 0.00697044687218\n",
      "Loss for  42986 th iteration => 0.00697020518624\n",
      "Loss for  42987 th iteration => 0.0069700737037\n",
      "Loss for  42988 th iteration => 0.00696986691478\n",
      "Loss for  42989 th iteration => 0.00696970072548\n",
      "Loss for  42990 th iteration => 0.00696953017266\n",
      "Loss for  42991 th iteration => 0.00696938984075\n",
      "Loss for  42992 th iteration => 0.00696920489029\n",
      "Loss for  42993 th iteration => 0.00696900347241\n",
      "Loss for  42994 th iteration => 0.00696880803568\n",
      "Loss for  42995 th iteration => 0.00696863382337\n",
      "Loss for  42996 th iteration => 0.00696846723112\n",
      "Loss for  42997 th iteration => 0.00696826658851\n",
      "Loss for  42998 th iteration => 0.00696821546426\n",
      "Loss for  42999 th iteration => 0.00696792132099\n",
      "Loss for  43000 th iteration => 0.00696779461949\n",
      "Loss for  43001 th iteration => 0.00696758427443\n",
      "Loss for  43002 th iteration => 0.00696742060916\n",
      "Loss for  43003 th iteration => 0.00696724714977\n",
      "Loss for  43004 th iteration => 0.0069670468018\n",
      "Loss for  43005 th iteration => 0.00696690994802\n",
      "Loss for  43006 th iteration => 0.00696667319498\n",
      "Loss for  43007 th iteration => 0.00696657390687\n",
      "Loss for  43008 th iteration => 0.00696640298699\n",
      "Loss for  43009 th iteration => 0.00696616062325\n",
      "Loss for  43010 th iteration => 0.00696603027224\n",
      "Loss for  43011 th iteration => 0.00696582271908\n",
      "Loss for  43012 th iteration => 0.00696565774691\n",
      "Loss for  43013 th iteration => 0.00696548623152\n",
      "Loss for  43014 th iteration => 0.00696534839224\n",
      "Loss for  43015 th iteration => 0.00696516128603\n",
      "Loss for  43016 th iteration => 0.00696496132507\n",
      "Loss for  43017 th iteration => 0.00696476599397\n",
      "Loss for  43018 th iteration => 0.00696467089752\n",
      "Loss for  43019 th iteration => 0.00696444240874\n",
      "Loss for  43020 th iteration => 0.00696428084583\n",
      "Loss for  43021 th iteration => 0.00696411855674\n",
      "Loss for  43022 th iteration => 0.00696389815616\n",
      "Loss for  43023 th iteration => 0.00696373441046\n",
      "Loss for  43024 th iteration => 0.00696352465265\n",
      "Loss for  43025 th iteration => 0.00696339758301\n",
      "Loss for  43026 th iteration => 0.00696316165545\n",
      "Loss for  43027 th iteration => 0.00696298513243\n",
      "Loss for  43028 th iteration => 0.0069628086205\n",
      "Loss for  43029 th iteration => 0.00696263211963\n",
      "Loss for  43030 th iteration => 0.00696245712385\n",
      "Loss for  43031 th iteration => 0.00696236210747\n",
      "Loss for  43032 th iteration => 0.00696211968672\n",
      "Loss for  43033 th iteration => 0.00696198979452\n",
      "Loss for  43034 th iteration => 0.00696178218196\n",
      "Loss for  43035 th iteration => 0.00696161767099\n",
      "Loss for  43036 th iteration => 0.00696144650426\n",
      "Loss for  43037 th iteration => 0.00696130920592\n",
      "Loss for  43038 th iteration => 0.00696112195616\n",
      "Loss for  43039 th iteration => 0.00696092186082\n",
      "Loss for  43040 th iteration => 0.00696072706066\n",
      "Loss for  43041 th iteration => 0.00696063258036\n",
      "Loss for  43042 th iteration => 0.00696040387147\n",
      "Loss for  43043 th iteration => 0.00696024293082\n",
      "Loss for  43044 th iteration => 0.00696008041561\n",
      "Loss for  43045 th iteration => 0.00695986014963\n",
      "Loss for  43046 th iteration => 0.00695969626068\n",
      "Loss for  43047 th iteration => 0.00695948704896\n",
      "Loss for  43048 th iteration => 0.00695935983119\n",
      "Loss for  43049 th iteration => 0.00695912369992\n",
      "Loss for  43050 th iteration => 0.00695894737781\n",
      "Loss for  43051 th iteration => 0.00695877106677\n",
      "Loss for  43052 th iteration => 0.00695859476678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  43053 th iteration => 0.00695842054996\n",
      "Loss for  43054 th iteration => 0.00695832562993\n",
      "Loss for  43055 th iteration => 0.00695808350981\n",
      "Loss for  43056 th iteration => 0.0069579537208\n",
      "Loss for  43057 th iteration => 0.00695774640203\n",
      "Loss for  43058 th iteration => 0.00695758200105\n",
      "Loss for  43059 th iteration => 0.00695741166009\n",
      "Loss for  43060 th iteration => 0.00695727426719\n",
      "Loss for  43061 th iteration => 0.0069570875051\n",
      "Loss for  43062 th iteration => 0.00695688683743\n",
      "Loss for  43063 th iteration => 0.00695669301182\n",
      "Loss for  43064 th iteration => 0.00695659850441\n",
      "Loss for  43065 th iteration => 0.00695637021436\n",
      "Loss for  43066 th iteration => 0.00695620926133\n",
      "Loss for  43067 th iteration => 0.00695604715038\n",
      "Loss for  43068 th iteration => 0.00695582654449\n",
      "Loss for  43069 th iteration => 0.00695566286223\n",
      "Loss for  43070 th iteration => 0.0069554538486\n",
      "Loss for  43071 th iteration => 0.00695532682828\n",
      "Loss for  43072 th iteration => 0.00695509033998\n",
      "Loss for  43073 th iteration => 0.00695491421853\n",
      "Loss for  43074 th iteration => 0.00695473810816\n",
      "Loss for  43075 th iteration => 0.00695456200883\n",
      "Loss for  43076 th iteration => 0.00695438870839\n",
      "Loss for  43077 th iteration => 0.00695429357278\n",
      "Loss for  43078 th iteration => 0.00695405206302\n",
      "Loss for  43079 th iteration => 0.00695392206913\n",
      "Loss for  43080 th iteration => 0.00695371535002\n",
      "Loss for  43081 th iteration => 0.00695355126298\n",
      "Loss for  43082 th iteration => 0.00695344382435\n",
      "Loss for  43083 th iteration => 0.00695319279427\n",
      "Loss for  43084 th iteration => 0.00695309109736\n",
      "Loss for  43085 th iteration => 0.00695283443521\n",
      "Loss for  43086 th iteration => 0.00695274127711\n",
      "Loss for  43087 th iteration => 0.00695257642301\n",
      "Loss for  43088 th iteration => 0.00695231833846\n",
      "Loss for  43089 th iteration => 0.00695221672947\n",
      "Loss for  43090 th iteration => 0.00695197067105\n",
      "Loss for  43091 th iteration => 0.00695179889471\n",
      "Loss for  43092 th iteration => 0.00695163507537\n",
      "Loss for  43093 th iteration => 0.00695142665475\n",
      "Loss for  43094 th iteration => 0.00695129940258\n",
      "Loss for  43095 th iteration => 0.00695106277626\n",
      "Loss for  43096 th iteration => 0.00695088894108\n",
      "Loss for  43097 th iteration => 0.00695078429365\n",
      "Loss for  43098 th iteration => 0.00695055266601\n",
      "Loss for  43099 th iteration => 0.00695041309425\n",
      "Loss for  43100 th iteration => 0.00695021632295\n",
      "Loss for  43101 th iteration => 0.00695004208509\n",
      "Loss for  43102 th iteration => 0.00694987991281\n",
      "Loss for  43103 th iteration => 0.00694967126393\n",
      "Loss for  43104 th iteration => 0.00694954423754\n",
      "Loss for  43105 th iteration => 0.00694937695132\n",
      "Loss for  43106 th iteration => 0.00694914854247\n",
      "Loss for  43107 th iteration => 0.00694900919836\n",
      "Loss for  43108 th iteration => 0.00694881002923\n",
      "Loss for  43109 th iteration => 0.00694872442703\n",
      "Loss for  43110 th iteration => 0.00694848797988\n",
      "Loss for  43111 th iteration => 0.00694833600111\n",
      "Loss for  43112 th iteration => 0.00694816566455\n",
      "Loss for  43113 th iteration => 0.00694795175575\n",
      "Loss for  43114 th iteration => 0.00694778374378\n",
      "Loss for  43115 th iteration => 0.00694757986511\n",
      "Loss for  43116 th iteration => 0.00694744847168\n",
      "Loss for  43117 th iteration => 0.00694721430324\n",
      "Loss for  43118 th iteration => 0.00694703857356\n",
      "Loss for  43119 th iteration => 0.00694686586485\n",
      "Loss for  43120 th iteration => 0.00694676053799\n",
      "Loss for  43121 th iteration => 0.00694652997998\n",
      "Loss for  43122 th iteration => 0.00694638974792\n",
      "Loss for  43123 th iteration => 0.00694619402717\n",
      "Loss for  43124 th iteration => 0.00694601914792\n",
      "Loss for  43125 th iteration => 0.00694585800732\n",
      "Loss for  43126 th iteration => 0.00694564873577\n",
      "Loss for  43127 th iteration => 0.00694552364581\n",
      "Loss for  43128 th iteration => 0.00694535459899\n",
      "Loss for  43129 th iteration => 0.00694512744755\n",
      "Loss for  43130 th iteration => 0.006944987254\n",
      "Loss for  43131 th iteration => 0.00694479044169\n",
      "Loss for  43132 th iteration => 0.00694470277593\n",
      "Loss for  43133 th iteration => 0.00694446877167\n",
      "Loss for  43134 th iteration => 0.00694431476876\n",
      "Loss for  43135 th iteration => 0.00694414683594\n",
      "Loss for  43136 th iteration => 0.00694393104467\n",
      "Loss for  43137 th iteration => 0.00694376440805\n",
      "Loss for  43138 th iteration => 0.00694355956431\n",
      "Loss for  43139 th iteration => 0.00694342952456\n",
      "Loss for  43140 th iteration => 0.00694319447561\n",
      "Loss for  43141 th iteration => 0.00694301991573\n",
      "Loss for  43142 th iteration => 0.00694291862765\n",
      "Loss for  43143 th iteration => 0.00694268442726\n",
      "Loss for  43144 th iteration => 0.00694254819108\n",
      "Loss for  43145 th iteration => 0.00694234887065\n",
      "Loss for  43146 th iteration => 0.00694217794505\n",
      "Loss for  43147 th iteration => 0.00694201324682\n",
      "Loss for  43148 th iteration => 0.0069418078873\n",
      "Loss for  43149 th iteration => 0.00694167783658\n",
      "Loss for  43150 th iteration => 0.00694151360988\n",
      "Loss for  43151 th iteration => 0.00694128399962\n",
      "Loss for  43152 th iteration => 0.00694114662455\n",
      "Loss for  43153 th iteration => 0.00694094595663\n",
      "Loss for  43154 th iteration => 0.00694078251133\n",
      "Loss for  43155 th iteration => 0.00694068989946\n",
      "Loss for  43156 th iteration => 0.00694043210279\n",
      "Loss for  43157 th iteration => 0.00694033137563\n",
      "Loss for  43158 th iteration => 0.00694008419417\n",
      "Loss for  43159 th iteration => 0.00693991472529\n",
      "Loss for  43160 th iteration => 0.00693974977421\n",
      "Loss for  43161 th iteration => 0.00693954365701\n",
      "Loss for  43162 th iteration => 0.00693941527704\n",
      "Loss for  43163 th iteration => 0.00693917921222\n",
      "Loss for  43164 th iteration => 0.0069390061222\n",
      "Loss for  43165 th iteration => 0.00693890339058\n",
      "Loss for  43166 th iteration => 0.00693867102012\n",
      "Loss for  43167 th iteration => 0.00693853336579\n",
      "Loss for  43168 th iteration => 0.00693833584998\n",
      "Loss for  43169 th iteration => 0.00693816353133\n",
      "Loss for  43170 th iteration => 0.00693800061267\n",
      "Loss for  43171 th iteration => 0.00693779388494\n",
      "Loss for  43172 th iteration => 0.00693766655152\n",
      "Loss for  43173 th iteration => 0.00693750011662\n",
      "Loss for  43174 th iteration => 0.00693727187479\n",
      "Loss for  43175 th iteration => 0.00693713354065\n",
      "Loss for  43176 th iteration => 0.00693693511738\n",
      "Loss for  43177 th iteration => 0.00693685101265\n",
      "Loss for  43178 th iteration => 0.00693661423069\n",
      "Loss for  43179 th iteration => 0.00693646376403\n",
      "Loss for  43180 th iteration => 0.00693629307833\n",
      "Loss for  43181 th iteration => 0.00693607982345\n",
      "Loss for  43182 th iteration => 0.00693591189286\n",
      "Loss for  43183 th iteration => 0.00693570911175\n",
      "Loss for  43184 th iteration => 0.00693557778842\n",
      "Loss for  43185 th iteration => 0.0069353436386\n",
      "Loss for  43186 th iteration => 0.0069351691216\n",
      "Loss for  43187 th iteration => 0.00693506950079\n",
      "Loss for  43188 th iteration => 0.00693483441008\n",
      "Loss for  43189 th iteration => 0.00693469983526\n",
      "Loss for  43190 th iteration => 0.00693449963034\n",
      "Loss for  43191 th iteration => 0.0069343303604\n",
      "Loss for  43192 th iteration => 0.0069341647833\n",
      "Loss for  43193 th iteration => 0.00693396107394\n",
      "Loss for  43194 th iteration => 0.0069338300044\n",
      "Loss for  43195 th iteration => 0.00693366724656\n",
      "Loss for  43196 th iteration => 0.00693343727874\n",
      "Loss for  43197 th iteration => 0.00693330103455\n",
      "Loss for  43198 th iteration => 0.00693310001113\n",
      "Loss for  43199 th iteration => 0.00693293759311\n",
      "Loss for  43200 th iteration => 0.0069328453938\n",
      "Loss for  43201 th iteration => 0.00693258795585\n",
      "Loss for  43202 th iteration => 0.00693248764198\n",
      "Loss for  43203 th iteration => 0.00693224022165\n",
      "Loss for  43204 th iteration => 0.00693207190668\n",
      "Loss for  43205 th iteration => 0.00693190657497\n",
      "Loss for  43206 th iteration => 0.00693170161262\n",
      "Loss for  43207 th iteration => 0.00693157285104\n",
      "Loss for  43208 th iteration => 0.00693133727589\n",
      "Loss for  43209 th iteration => 0.00693116460657\n",
      "Loss for  43210 th iteration => 0.00693106270753\n",
      "Loss for  43211 th iteration => 0.00693083027609\n",
      "Loss for  43212 th iteration => 0.00693069345874\n",
      "Loss for  43213 th iteration => 0.0069304958775\n",
      "Loss for  43214 th iteration => 0.0069303244003\n",
      "Loss for  43215 th iteration => 0.0069301614117\n",
      "Loss for  43216 th iteration => 0.00692995552997\n",
      "Loss for  43217 th iteration => 0.0069298282749\n",
      "Loss for  43218 th iteration => 0.00692966228746\n",
      "Loss for  43219 th iteration => 0.00692943434386\n",
      "Loss for  43220 th iteration => 0.0069292964885\n",
      "Loss for  43221 th iteration => 0.00692909861612\n",
      "Loss for  43222 th iteration => 0.00692901496774\n",
      "Loss for  43223 th iteration => 0.00692877849266\n",
      "Loss for  43224 th iteration => 0.00692862849919\n",
      "Loss for  43225 th iteration => 0.00692845810377\n",
      "Loss for  43226 th iteration => 0.00692824508447\n",
      "Loss for  43227 th iteration => 0.00692807754395\n",
      "Loss for  43228 th iteration => 0.00692787515154\n",
      "Loss for  43229 th iteration => 0.00692774420766\n",
      "Loss for  43230 th iteration => 0.00692751020239\n",
      "Loss for  43231 th iteration => 0.00692733642476\n",
      "Loss for  43232 th iteration => 0.00692723693131\n",
      "Loss for  43233 th iteration => 0.00692700248013\n",
      "Loss for  43234 th iteration => 0.00692686804583\n",
      "Loss for  43235 th iteration => 0.00692666846728\n",
      "Loss for  43236 th iteration => 0.00692649935095\n",
      "Loss for  43237 th iteration => 0.0069263343871\n",
      "Loss for  43238 th iteration => 0.00692613084443\n",
      "Loss for  43239 th iteration => 0.00692600077903\n",
      "Loss for  43240 th iteration => 0.00692583760726\n",
      "Loss for  43241 th iteration => 0.0069256084905\n",
      "Loss for  43242 th iteration => 0.00692547217521\n",
      "Loss for  43243 th iteration => 0.00692527203552\n",
      "Loss for  43244 th iteration => 0.00692519169294\n",
      "Loss for  43245 th iteration => 0.00692495230306\n",
      "Loss for  43246 th iteration => 0.00692480557915\n",
      "Loss for  43247 th iteration => 0.00692463230505\n",
      "Loss for  43248 th iteration => 0.00692442204524\n",
      "Loss for  43249 th iteration => 0.00692425295894\n",
      "Loss for  43250 th iteration => 0.00692405247783\n",
      "Loss for  43251 th iteration => 0.00692392000639\n",
      "Loss for  43252 th iteration => 0.00692368726747\n",
      "Loss for  43253 th iteration => 0.00692351270225\n",
      "Loss for  43254 th iteration => 0.00692334117631\n",
      "Loss for  43255 th iteration => 0.00692323851661\n",
      "Loss for  43256 th iteration => 0.00692300760589\n",
      "Loss for  43257 th iteration => 0.00692287005519\n",
      "Loss for  43258 th iteration => 0.00692267396742\n",
      "Loss for  43259 th iteration => 0.00692250178394\n",
      "Loss for  43260 th iteration => 0.00692234026177\n",
      "Loss for  43261 th iteration => 0.00692213370061\n",
      "Loss for  43262 th iteration => 0.00692200884025\n",
      "Loss for  43263 th iteration => 0.00692184086479\n",
      "Loss for  43264 th iteration => 0.00692161495959\n",
      "Loss for  43265 th iteration => 0.00692147585163\n",
      "Loss for  43266 th iteration => 0.00692128110509\n",
      "Loss for  43267 th iteration => 0.00692119432824\n",
      "Loss for  43268 th iteration => 0.00692096172348\n",
      "Loss for  43269 th iteration => 0.00692080866313\n",
      "Loss for  43270 th iteration => 0.00692064207687\n",
      "Loss for  43271 th iteration => 0.00692042670024\n",
      "Loss for  43272 th iteration => 0.00692026136042\n",
      "Loss for  43273 th iteration => 0.00692005755702\n",
      "Loss for  43274 th iteration => 0.00691992878089\n",
      "Loss for  43275 th iteration => 0.00691969410538\n",
      "Loss for  43276 th iteration => 0.00691952167887\n",
      "Loss for  43277 th iteration => 0.00691942095359\n",
      "Loss for  43278 th iteration => 0.0069191884908\n",
      "Loss for  43279 th iteration => 0.00691905285771\n",
      "Loss for  43280 th iteration => 0.00691885523457\n",
      "Loss for  43281 th iteration => 0.00691868495221\n",
      "Loss for  43282 th iteration => 0.00691852191108\n",
      "Loss for  43283 th iteration => 0.00691831723482\n",
      "Loss for  43284 th iteration => 0.00691819017424\n",
      "Loss for  43285 th iteration => 0.00691802445264\n",
      "Loss for  43286 th iteration => 0.00691779773436\n",
      "Loss for  43287 th iteration => 0.00691765980791\n",
      "Loss for  43288 th iteration => 0.00691746334843\n",
      "Loss for  43289 th iteration => 0.00691737907036\n",
      "Loss for  43290 th iteration => 0.00691714435198\n",
      "Loss for  43291 th iteration => 0.00691699376508\n",
      "Loss for  43292 th iteration => 0.00691682509048\n",
      "Loss for  43293 th iteration => 0.00691661188045\n",
      "Loss for  43294 th iteration => 0.00691644543253\n",
      "Loss for  43295 th iteration => 0.00691624310483\n",
      "Loss for  43296 th iteration => 0.00691611323331\n",
      "Loss for  43297 th iteration => 0.00691587963445\n",
      "Loss for  43298 th iteration => 0.0069157065584\n",
      "Loss for  43299 th iteration => 0.00691560722426\n",
      "Loss for  43300 th iteration => 0.00691537374941\n",
      "Loss for  43301 th iteration => 0.00691523949742\n",
      "Loss for  43302 th iteration => 0.00691504087219\n",
      "Loss for  43303 th iteration => 0.00691487196108\n",
      "Loss for  43304 th iteration => 0.00691470792764\n",
      "Loss for  43305 th iteration => 0.00691450461301\n",
      "Loss for  43306 th iteration => 0.00691437607155\n",
      "Loss for  43307 th iteration => 0.00691421193207\n",
      "Loss for  43308 th iteration => 0.00691398482977\n",
      "Loss for  43309 th iteration => 0.00691384765842\n",
      "Loss for  43310 th iteration => 0.00691365015766\n",
      "Loss for  43311 th iteration => 0.00691356741985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  43312 th iteration => 0.00691333154037\n",
      "Loss for  43313 th iteration => 0.00691318248123\n",
      "Loss for  43314 th iteration => 0.00691301265803\n",
      "Loss for  43315 th iteration => 0.00691280090126\n",
      "Loss for  43316 th iteration => 0.00691263386512\n",
      "Loss for  43317 th iteration => 0.00691243249655\n",
      "Loss for  43318 th iteration => 0.00691230204304\n",
      "Loss for  43319 th iteration => 0.00691206928408\n",
      "Loss for  43320 th iteration => 0.00691189577698\n",
      "Loss for  43321 th iteration => 0.00691179735936\n",
      "Loss for  43322 th iteration => 0.00691156334414\n",
      "Loss for  43323 th iteration => 0.00691143000452\n",
      "Loss for  43324 th iteration => 0.00691123084304\n",
      "Loss for  43325 th iteration => 0.00691106284028\n",
      "Loss for  43326 th iteration => 0.00691089827456\n",
      "Loss for  43327 th iteration => 0.00691069586437\n",
      "Loss for  43328 th iteration => 0.00691056646919\n",
      "Loss for  43329 th iteration => 0.006910403327\n",
      "Loss for  43330 th iteration => 0.00691017621421\n",
      "Loss for  43331 th iteration => 0.00691003942666\n",
      "Loss for  43332 th iteration => 0.00690984146855\n",
      "Loss for  43333 th iteration => 0.00690975943642\n",
      "Loss for  43334 th iteration => 0.0069095232251\n",
      "Loss for  43335 th iteration => 0.00690937487032\n",
      "Loss for  43336 th iteration => 0.00690920471668\n",
      "Loss for  43337 th iteration => 0.00690899379272\n",
      "Loss for  43338 th iteration => 0.00690882662094\n",
      "Loss for  43339 th iteration => 0.00690862576173\n",
      "Loss for  43340 th iteration => 0.00690849517316\n",
      "Loss for  43341 th iteration => 0.00690826304878\n",
      "Loss for  43342 th iteration => 0.00690808930072\n",
      "Loss for  43343 th iteration => 0.00690799138488\n",
      "Loss for  43344 th iteration => 0.00690775724143\n",
      "Loss for  43345 th iteration => 0.00690762440455\n",
      "Loss for  43346 th iteration => 0.00690742511385\n",
      "Loss for  43347 th iteration => 0.00690725761486\n",
      "Loss for  43348 th iteration => 0.00690709291886\n",
      "Loss for  43349 th iteration => 0.00690689101354\n",
      "Loss for  43350 th iteration => 0.00690676131148\n",
      "Loss for  43351 th iteration => 0.00690659865742\n",
      "Loss for  43352 th iteration => 0.0069063718593\n",
      "Loss for  43353 th iteration => 0.00690623513227\n",
      "Loss for  43354 th iteration => 0.00690603724322\n",
      "Loss for  43355 th iteration => 0.00690587491158\n",
      "Loss for  43356 th iteration => 0.00690578370043\n",
      "Loss for  43357 th iteration => 0.00690552793007\n",
      "Loss for  43358 th iteration => 0.00690542861876\n",
      "Loss for  43359 th iteration => 0.0069051840983\n",
      "Loss for  43360 th iteration => 0.0069050151098\n",
      "Loss for  43361 th iteration => 0.00690485309129\n",
      "Loss for  43362 th iteration => 0.00690464751114\n",
      "Loss for  43363 th iteration => 0.00690452200704\n",
      "Loss for  43364 th iteration => 0.00690428731957\n",
      "Loss for  43365 th iteration => 0.00690411647143\n",
      "Loss for  43366 th iteration => 0.00690401393502\n",
      "Loss for  43367 th iteration => 0.00690378477745\n",
      "Loss for  43368 th iteration => 0.00690364738445\n",
      "Loss for  43369 th iteration => 0.00690345301539\n",
      "Loss for  43370 th iteration => 0.00690328102392\n",
      "Loss for  43371 th iteration => 0.00690312118614\n",
      "Loss for  43372 th iteration => 0.00690291501167\n",
      "Loss for  43373 th iteration => 0.0069028555011\n",
      "Loss for  43374 th iteration => 0.00690257129922\n",
      "Loss for  43375 th iteration => 0.00690243531838\n",
      "Loss for  43376 th iteration => 0.00690223705757\n",
      "Loss for  43377 th iteration => 0.00690207498722\n",
      "Loss for  43378 th iteration => 0.00690198376764\n",
      "Loss for  43379 th iteration => 0.00690172837733\n",
      "Loss for  43380 th iteration => 0.00690162905806\n",
      "Loss for  43381 th iteration => 0.00690138486965\n",
      "Loss for  43382 th iteration => 0.00690121642622\n",
      "Loss for  43383 th iteration => 0.00690105423613\n",
      "Loss for  43384 th iteration => 0.00690084919944\n",
      "Loss for  43385 th iteration => 0.00690072352533\n",
      "Loss for  43386 th iteration => 0.00690048950363\n",
      "Loss for  43387 th iteration => 0.00690031847632\n",
      "Loss for  43388 th iteration => 0.0069002162781\n",
      "Loss for  43389 th iteration => 0.0068999871549\n",
      "Loss for  43390 th iteration => 0.00689985010037\n",
      "Loss for  43391 th iteration => 0.00689965576536\n",
      "Loss for  43392 th iteration => 0.00689948411275\n",
      "Loss for  43393 th iteration => 0.0068993243086\n",
      "Loss for  43394 th iteration => 0.00689911831298\n",
      "Loss for  43395 th iteration => 0.00689899517894\n",
      "Loss for  43396 th iteration => 0.0068988272068\n",
      "Loss for  43397 th iteration => 0.00689860424037\n",
      "Loss for  43398 th iteration => 0.00689846447745\n",
      "Loss for  43399 th iteration => 0.00689827276997\n",
      "Loss for  43400 th iteration => 0.00689818287288\n",
      "Loss for  43401 th iteration => 0.00689795560118\n",
      "Loss for  43402 th iteration => 0.00689779952023\n",
      "Loss for  43403 th iteration => 0.00689763816828\n",
      "Loss for  43404 th iteration => 0.00689742155838\n",
      "Loss for  43405 th iteration => 0.00689725970982\n",
      "Loss for  43406 th iteration => 0.0068970547071\n",
      "Loss for  43407 th iteration => 0.00689692936913\n",
      "Loss for  43408 th iteration => 0.00689669577963\n",
      "Loss for  43409 th iteration => 0.00689652470434\n",
      "Loss for  43410 th iteration => 0.00689642255468\n",
      "Loss for  43411 th iteration => 0.00689619375251\n",
      "Loss for  43412 th iteration => 0.00689605675294\n",
      "Loss for  43413 th iteration => 0.00689586273255\n",
      "Loss for  43414 th iteration => 0.00689569114128\n",
      "Loss for  43415 th iteration => 0.00689553164536\n",
      "Loss for  43416 th iteration => 0.00689532571747\n",
      "Loss for  43417 th iteration => 0.00689520285619\n",
      "Loss for  43418 th iteration => 0.00689503484127\n",
      "Loss for  43419 th iteration => 0.00689481253217\n",
      "Loss for  43420 th iteration => 0.00689467248788\n",
      "Loss for  43421 th iteration => 0.0068944813504\n",
      "Loss for  43422 th iteration => 0.00689439066558\n",
      "Loss for  43423 th iteration => 0.00689416454462\n",
      "Loss for  43424 th iteration => 0.00689400769463\n",
      "Loss for  43425 th iteration => 0.00689384747489\n",
      "Loss for  43426 th iteration => 0.00689363060071\n",
      "Loss for  43427 th iteration => 0.00689346942493\n",
      "Loss for  43428 th iteration => 0.00689326412684\n",
      "Loss for  43429 th iteration => 0.00689313945211\n",
      "Loss for  43430 th iteration => 0.00689290614605\n",
      "Loss for  43431 th iteration => 0.00689273516171\n",
      "Loss for  43432 th iteration => 0.00689263275401\n",
      "Loss for  43433 th iteration => 0.00689240457739\n",
      "Loss for  43434 th iteration => 0.00689226732994\n",
      "Loss for  43435 th iteration => 0.00689207392496\n",
      "Loss for  43436 th iteration => 0.0068919020959\n",
      "Loss for  43437 th iteration => 0.0068917432053\n",
      "Loss for  43438 th iteration => 0.00689153704966\n",
      "Loss for  43439 th iteration => 0.00689141486298\n",
      "Loss for  43440 th iteration => 0.00689124643269\n",
      "Loss for  43441 th iteration => 0.00689102501885\n",
      "Loss for  43442 th iteration => 0.00689088445649\n",
      "Loss for  43443 th iteration => 0.00689069425873\n",
      "Loss for  43444 th iteration => 0.00689060225432\n",
      "Loss for  43445 th iteration => 0.00689037781238\n",
      "Loss for  43446 th iteration => 0.00689021966861\n",
      "Loss for  43447 th iteration => 0.00689006110226\n",
      "Loss for  43448 th iteration => 0.00688984357015\n",
      "Loss for  43449 th iteration => 0.00688968335538\n",
      "Loss for  43450 th iteration => 0.00688947747525\n",
      "Loss for  43451 th iteration => 0.00688935374843\n",
      "Loss for  43452 th iteration => 0.00688912059711\n",
      "Loss for  43453 th iteration => 0.00688894982447\n",
      "Loss for  43454 th iteration => 0.00688884689046\n",
      "Loss for  43455 th iteration => 0.0068886196058\n",
      "Loss for  43456 th iteration => 0.00688848184544\n",
      "Loss for  43457 th iteration => 0.00688828931903\n",
      "Loss for  43458 th iteration => 0.00688811699039\n",
      "Loss for  43459 th iteration => 0.00688795896506\n",
      "Loss for  43460 th iteration => 0.00688775234577\n",
      "Loss for  43461 th iteration => 0.00688769275247\n",
      "Loss for  43462 th iteration => 0.00688741186504\n",
      "Loss for  43463 th iteration => 0.00688727479189\n",
      "Loss for  43464 th iteration => 0.00688707909974\n",
      "Loss for  43465 th iteration => 0.00688691581726\n",
      "Loss for  43466 th iteration => 0.00688682469207\n",
      "Loss for  43467 th iteration => 0.00688657068989\n",
      "Loss for  43468 th iteration => 0.00688647147655\n",
      "Loss for  43469 th iteration => 0.00688623059902\n",
      "Loss for  43470 th iteration => 0.00688606053692\n",
      "Loss for  43471 th iteration => 0.00688590143405\n",
      "Loss for  43472 th iteration => 0.00688569482118\n",
      "Loss for  43473 th iteration => 0.00688557219185\n",
      "Loss for  43474 th iteration => 0.00688533913063\n",
      "Loss for  43475 th iteration => 0.00688516871183\n",
      "Loss for  43476 th iteration => 0.00688506493755\n",
      "Loss for  43477 th iteration => 0.00688483885764\n",
      "Loss for  43478 th iteration => 0.00688470027181\n",
      "Loss for  43479 th iteration => 0.00688450893539\n",
      "Loss for  43480 th iteration => 0.00688433579594\n",
      "Loss for  43481 th iteration => 0.00688417894595\n",
      "Loss for  43482 th iteration => 0.0068839715998\n",
      "Loss for  43483 th iteration => 0.0068839124551\n",
      "Loss for  43484 th iteration => 0.00688363231579\n",
      "Loss for  43485 th iteration => 0.00688349472762\n",
      "Loss for  43486 th iteration => 0.00688329991613\n",
      "Loss for  43487 th iteration => 0.00688313636163\n",
      "Loss for  43488 th iteration => 0.0068830448945\n",
      "Loss for  43489 th iteration => 0.00688279160331\n",
      "Loss for  43490 th iteration => 0.006882692053\n",
      "Loss for  43491 th iteration => 0.00688245249593\n",
      "Loss for  43492 th iteration => 0.00688228144311\n",
      "Loss for  43493 th iteration => 0.0068821236939\n",
      "Loss for  43494 th iteration => 0.00688191610775\n",
      "Loss for  43495 th iteration => 0.00688179481467\n",
      "Loss for  43496 th iteration => 0.00688156173653\n",
      "Loss for  43497 th iteration => 0.00688139177019\n",
      "Loss for  43498 th iteration => 0.00688128693416\n",
      "Loss for  43499 th iteration => 0.00688106227884\n",
      "Loss for  43500 th iteration => 0.00688092264877\n",
      "Loss for  43501 th iteration => 0.00688073271945\n",
      "Loss for  43502 th iteration => 0.00688055855313\n",
      "Loss for  43503 th iteration => 0.0068804030929\n",
      "Loss for  43504 th iteration => 0.00688019488648\n",
      "Loss for  43505 th iteration => 0.00688013622295\n",
      "Loss for  43506 th iteration => 0.00687985691544\n",
      "Loss for  43507 th iteration => 0.00687971863468\n",
      "Loss for  43508 th iteration => 0.00687952488001\n",
      "Loss for  43509 th iteration => 0.00687936097485\n",
      "Loss for  43510 th iteration => 0.00687926912232\n",
      "Loss for  43511 th iteration => 0.00687901658504\n",
      "Loss for  43512 th iteration => 0.00687891665492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  43513 th iteration => 0.00687867855527\n",
      "Loss for  43514 th iteration => 0.00687850629895\n",
      "Loss for  43515 th iteration => 0.00687835011454\n",
      "Loss for  43516 th iteration => 0.00687814134498\n",
      "Loss for  43517 th iteration => 0.00687802159666\n",
      "Loss for  43518 th iteration => 0.00687778840885\n",
      "Loss for  43519 th iteration => 0.00687761898043\n",
      "Loss for  43520 th iteration => 0.00687751288899\n",
      "Loss for  43521 th iteration => 0.00687728985038\n",
      "Loss for  43522 th iteration => 0.00687714898481\n",
      "Loss for  43523 th iteration => 0.00687696065234\n",
      "Loss for  43524 th iteration => 0.00687678527023\n",
      "Loss for  43525 th iteration => 0.00687663138719\n",
      "Loss for  43526 th iteration => 0.00687642220349\n",
      "Loss for  43527 th iteration => 0.00687636404897\n",
      "Loss for  43528 th iteration => 0.0068760856477\n",
      "Loss for  43529 th iteration => 0.0068759465185\n",
      "Loss for  43530 th iteration => 0.00687575427324\n",
      "Loss for  43531 th iteration => 0.00687566360182\n",
      "Loss for  43532 th iteration => 0.00687543927766\n",
      "Loss for  43533 th iteration => 0.00687528248548\n",
      "Loss for  43534 th iteration => 0.00687512401868\n",
      "Loss for  43535 th iteration => 0.00687490914455\n",
      "Loss for  43536 th iteration => 0.00687474973322\n",
      "Loss for  43537 th iteration => 0.00687454451622\n",
      "Loss for  43538 th iteration => 0.00687442158384\n",
      "Loss for  43539 th iteration => 0.00687419125535\n",
      "Loss for  43540 th iteration => 0.00687401944476\n",
      "Loss for  43541 th iteration => 0.0068739167445\n",
      "Loss for  43542 th iteration => 0.00687369068089\n",
      "Loss for  43543 th iteration => 0.00687355316882\n",
      "Loss for  43544 th iteration => 0.00687336184886\n",
      "Loss for  43545 th iteration => 0.00687318978318\n",
      "Loss for  43546 th iteration => 0.00687303294955\n",
      "Loss for  43547 th iteration => 0.00687282658531\n",
      "Loss for  43548 th iteration => 0.00687270515762\n",
      "Loss for  43549 th iteration => 0.00687253800104\n",
      "Loss for  43550 th iteration => 0.00687231892323\n",
      "Loss for  43551 th iteration => 0.00687217787262\n",
      "Loss for  43552 th iteration => 0.00687198876606\n",
      "Loss for  43553 th iteration => 0.0068718936272\n",
      "Loss for  43554 th iteration => 0.0068716741094\n",
      "Loss for  43555 th iteration => 0.00687151291747\n",
      "Loss for  43556 th iteration => 0.00687135918972\n",
      "Loss for  43557 th iteration => 0.00687114132191\n",
      "Loss for  43558 th iteration => 0.0068709845758\n",
      "Loss for  43559 th iteration => 0.00687077708246\n",
      "Loss for  43560 th iteration => 0.00687065678102\n",
      "Loss for  43561 th iteration => 0.00687042583804\n",
      "Loss for  43562 th iteration => 0.00687025495411\n",
      "Loss for  43563 th iteration => 0.00687015015768\n",
      "Loss for  43564 th iteration => 0.00686992654544\n",
      "Loss for  43565 th iteration => 0.00686978696985\n",
      "Loss for  43566 th iteration => 0.00686959806871\n",
      "Loss for  43567 th iteration => 0.00686942397179\n",
      "Loss for  43568 th iteration => 0.00686926952477\n",
      "Loss for  43569 th iteration => 0.00686906116126\n",
      "Loss for  43570 th iteration => 0.00686894282433\n",
      "Loss for  43571 th iteration => 0.00686877300611\n",
      "Loss for  43572 th iteration => 0.00686855625976\n",
      "Loss for  43573 th iteration => 0.00686841326224\n",
      "Loss for  43574 th iteration => 0.00686822732791\n",
      "Loss for  43575 th iteration => 0.00686812766479\n",
      "Loss for  43576 th iteration => 0.00686791300925\n",
      "Loss for  43577 th iteration => 0.00686774736154\n",
      "Loss for  43578 th iteration => 0.00686759842795\n",
      "Loss for  43579 th iteration => 0.00686737752879\n",
      "Loss for  43580 th iteration => 0.00686722347823\n",
      "Loss for  43581 th iteration => 0.00686701367788\n",
      "Loss for  43582 th iteration => 0.00686689603729\n",
      "Loss for  43583 th iteration => 0.00686666447149\n",
      "Loss for  43584 th iteration => 0.00686649452615\n",
      "Loss for  43585 th iteration => 0.00686638759496\n",
      "Loss for  43586 th iteration => 0.00686616647196\n",
      "Loss for  43587 th iteration => 0.00686602479461\n",
      "Loss for  43588 th iteration => 0.00686583834978\n",
      "Loss for  43589 th iteration => 0.00686566218377\n",
      "Loss for  43590 th iteration => 0.0068655101605\n",
      "Loss for  43591 th iteration => 0.00686529980781\n",
      "Loss for  43592 th iteration => 0.00686524132082\n",
      "Loss for  43593 th iteration => 0.00686496672224\n",
      "Loss for  43594 th iteration => 0.00686482599944\n",
      "Loss for  43595 th iteration => 0.00686463612678\n",
      "Loss for  43596 th iteration => 0.00686446961651\n",
      "Loss for  43597 th iteration => 0.00686437741178\n",
      "Loss for  43598 th iteration => 0.0068641266788\n",
      "Loss for  43599 th iteration => 0.00686402641254\n",
      "Loss for  43600 th iteration => 0.00686379340794\n",
      "Loss for  43601 th iteration => 0.00686361780629\n",
      "Loss for  43602 th iteration => 0.00686346639779\n",
      "Loss for  43603 th iteration => 0.00686325434253\n",
      "Loss for  43604 th iteration => 0.00686313931061\n",
      "Loss for  43605 th iteration => 0.00686290715287\n",
      "Loss for  43606 th iteration => 0.00686273819267\n",
      "Loss for  43607 th iteration => 0.00686262901448\n",
      "Loss for  43608 th iteration => 0.00686241049273\n",
      "Loss for  43609 th iteration => 0.00686226660039\n",
      "Loss for  43610 th iteration => 0.0068620827249\n",
      "Loss for  43611 th iteration => 0.00686190437556\n",
      "Loss for  43612 th iteration => 0.00686175489004\n",
      "Loss for  43613 th iteration => 0.00686154296094\n",
      "Loss for  43614 th iteration => 0.0068614851421\n",
      "Loss for  43615 th iteration => 0.00686121180629\n",
      "Loss for  43616 th iteration => 0.00686106945291\n",
      "Loss for  43617 th iteration => 0.00686088209131\n",
      "Loss for  43618 th iteration => 0.00686078310204\n",
      "Loss for  43619 th iteration => 0.00686056848538\n",
      "Loss for  43620 th iteration => 0.00686040351757\n",
      "Loss for  43621 th iteration => 0.00686025461698\n",
      "Loss for  43622 th iteration => 0.0068600354177\n",
      "Loss for  43623 th iteration => 0.00685988168449\n",
      "Loss for  43624 th iteration => 0.00685967228422\n",
      "Loss for  43625 th iteration => 0.0068595549591\n",
      "Loss for  43626 th iteration => 0.00685932525608\n",
      "Loss for  43627 th iteration => 0.00685915429719\n",
      "Loss for  43628 th iteration => 0.00685904766124\n",
      "Loss for  43629 th iteration => 0.00685882695715\n",
      "Loss for  43630 th iteration => 0.00685868557972\n",
      "Loss for  43631 th iteration => 0.00685849954909\n",
      "Loss for  43632 th iteration => 0.00685832368778\n",
      "Loss for  43633 th iteration => 0.00685817207388\n",
      "Loss for  43634 th iteration => 0.00685796198317\n",
      "Loss for  43635 th iteration => 0.00685784616952\n",
      "Loss for  43636 th iteration => 0.00685767505592\n",
      "Loss for  43637 th iteration => 0.00685746135506\n",
      "Loss for  43638 th iteration => 0.00685731641549\n",
      "Loss for  43639 th iteration => 0.00685713322415\n",
      "Loss for  43640 th iteration => 0.00685702827765\n",
      "Loss for  43641 th iteration => 0.00685681994652\n",
      "Loss for  43642 th iteration => 0.00685664910898\n",
      "Loss for  43643 th iteration => 0.00685650640689\n",
      "Loss for  43644 th iteration => 0.00685628311524\n",
      "Loss for  43645 th iteration => 0.00685613285288\n",
      "Loss for  43646 th iteration => 0.00685592037441\n",
      "Loss for  43647 th iteration => 0.00685580647582\n",
      "Loss for  43648 th iteration => 0.00685557580265\n",
      "Loss for  43649 th iteration => 0.00685540610924\n",
      "Loss for  43650 th iteration => 0.00685529661442\n",
      "Loss for  43651 th iteration => 0.00685507911854\n",
      "Loss for  43652 th iteration => 0.00685493492399\n",
      "Loss for  43653 th iteration => 0.00685475205995\n",
      "Loss for  43654 th iteration => 0.00685457342279\n",
      "Loss for  43655 th iteration => 0.00685442493431\n",
      "Loss for  43656 th iteration => 0.0068542121337\n",
      "Loss for  43657 th iteration => 0.0068541540691\n",
      "Loss for  43658 th iteration => 0.00685388342677\n",
      "Loss for  43659 th iteration => 0.00685374021815\n",
      "Loss for  43660 th iteration => 0.00685355389801\n",
      "Loss for  43661 th iteration => 0.00685338498548\n",
      "Loss for  43662 th iteration => 0.00685329222656\n",
      "Loss for  43663 th iteration => 0.00685304312685\n",
      "Loss for  43664 th iteration => 0.00685294232111\n",
      "Loss for  43665 th iteration => 0.0068527139313\n",
      "Loss for  43666 th iteration => 0.00685253491259\n",
      "Loss for  43667 th iteration => 0.00685238797986\n",
      "Loss for  43668 th iteration => 0.00685217256236\n",
      "Loss for  43669 th iteration => 0.00685206195151\n",
      "Loss for  43670 th iteration => 0.00685183038006\n",
      "Loss for  43671 th iteration => 0.00685166195589\n",
      "Loss for  43672 th iteration => 0.0068515495803\n",
      "Loss for  43673 th iteration => 0.00685133531472\n",
      "Loss for  43674 th iteration => 0.00685118827912\n",
      "Loss for  43675 th iteration => 0.00685100860576\n",
      "Loss for  43676 th iteration => 0.00685082716685\n",
      "Loss for  43677 th iteration => 0.00685068182988\n",
      "Loss for  43678 th iteration => 0.00685046876256\n",
      "Loss for  43679 th iteration => 0.00685035192217\n",
      "Loss for  43680 th iteration => 0.00685013965275\n",
      "Loss for  43681 th iteration => 0.0068499939606\n",
      "Loss for  43682 th iteration => 0.00684981160132\n",
      "Loss for  43683 th iteration => 0.00684970403518\n",
      "Loss for  43684 th iteration => 0.0068494990805\n",
      "Loss for  43685 th iteration => 0.00684932551505\n",
      "Loss for  43686 th iteration => 0.00684918629724\n",
      "Loss for  43687 th iteration => 0.00684896203295\n",
      "Loss for  43688 th iteration => 0.0068488140845\n",
      "Loss for  43689 th iteration => 0.00684859996713\n",
      "Loss for  43690 th iteration => 0.00684848844434\n",
      "Loss for  43691 th iteration => 0.00684825867979\n",
      "Loss for  43692 th iteration => 0.00684808782258\n",
      "Loss for  43693 th iteration => 0.00684791697629\n",
      "Loss for  43694 th iteration => 0.00684774614088\n",
      "Loss for  43695 th iteration => 0.0068475780043\n",
      "Loss for  43696 th iteration => 0.00684747527306\n",
      "Loss for  43697 th iteration => 0.00684725173582\n",
      "Loss for  43698 th iteration => 0.00684711441326\n",
      "Loss for  43699 th iteration => 0.00684692539985\n",
      "Loss for  43700 th iteration => 0.00684675601651\n",
      "Loss for  43701 th iteration => 0.00684665150939\n",
      "Loss for  43702 th iteration => 0.00684640806756\n",
      "Loss for  43703 th iteration => 0.00684630925471\n",
      "Loss for  43704 th iteration => 0.00684607503446\n",
      "Loss for  43705 th iteration => 0.00684596053648\n",
      "Loss for  43706 th iteration => 0.00684576283449\n",
      "Loss for  43707 th iteration => 0.00684558243841\n",
      "Loss for  43708 th iteration => 0.00684545037263\n",
      "Loss for  43709 th iteration => 0.006845221276\n",
      "Loss for  43710 th iteration => 0.0068450773761\n",
      "Loss for  43711 th iteration => 0.00684486128399\n",
      "Loss for  43712 th iteration => 0.00684469058615\n",
      "Loss for  43713 th iteration => 0.00684451989932\n",
      "Loss for  43714 th iteration => 0.00684434922346\n",
      "Loss for  43715 th iteration => 0.00684417855853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  43716 th iteration => 0.00684400910852\n",
      "Loss for  43717 th iteration => 0.00684390741734\n",
      "Loss for  43718 th iteration => 0.00684368322536\n",
      "Loss for  43719 th iteration => 0.00684354684375\n",
      "Loss for  43720 th iteration => 0.00684335727431\n",
      "Loss for  43721 th iteration => 0.00684318769123\n",
      "Loss for  43722 th iteration => 0.00684308304226\n",
      "Loss for  43723 th iteration => 0.00684284006621\n",
      "Loss for  43724 th iteration => 0.00684274112762\n",
      "Loss for  43725 th iteration => 0.00684250560439\n",
      "Loss for  43726 th iteration => 0.00684232866174\n",
      "Loss for  43727 th iteration => 0.00684217713776\n",
      "Loss for  43728 th iteration => 0.00684197494168\n",
      "Loss for  43729 th iteration => 0.00684187639215\n",
      "Loss for  43730 th iteration => 0.00684165027536\n",
      "Loss for  43731 th iteration => 0.00684151471635\n",
      "Loss for  43732 th iteration => 0.00684132552996\n",
      "Loss for  43733 th iteration => 0.00684115324521\n",
      "Loss for  43734 th iteration => 0.00684100070646\n",
      "Loss for  43735 th iteration => 0.00684079197628\n",
      "Loss for  43736 th iteration => 0.00684067580582\n",
      "Loss for  43737 th iteration => 0.00684044439568\n",
      "Loss for  43738 th iteration => 0.0068402770448\n",
      "Loss for  43739 th iteration => 0.00684017066257\n",
      "Loss for  43740 th iteration => 0.00683995152727\n",
      "Loss for  43741 th iteration => 0.00683981044785\n",
      "Loss for  43742 th iteration => 0.00683962594178\n",
      "Loss for  43743 th iteration => 0.00683945042242\n",
      "Loss for  43744 th iteration => 0.00683930028919\n",
      "Loss for  43745 th iteration => 0.00683909209021\n",
      "Loss for  43746 th iteration => 0.00683903400594\n",
      "Loss for  43747 th iteration => 0.00683876087118\n",
      "Loss for  43748 th iteration => 0.00683862002962\n",
      "Loss for  43749 th iteration => 0.00683843466094\n",
      "Loss for  43750 th iteration => 0.00683833729502\n",
      "Loss for  43751 th iteration => 0.00683812321368\n",
      "Loss for  43752 th iteration => 0.00683795992125\n",
      "Loss for  43753 th iteration => 0.00683781150465\n",
      "Loss for  43754 th iteration => 0.00683759270251\n",
      "Loss for  43755 th iteration => 0.00683743989574\n",
      "Loss for  43756 th iteration => 0.00683723177597\n",
      "Loss for  43757 th iteration => 0.0068371153435\n",
      "Loss for  43758 th iteration => 0.00683688546375\n",
      "Loss for  43759 th iteration => 0.00683671697042\n",
      "Loss for  43760 th iteration => 0.00683661124293\n",
      "Loss for  43761 th iteration => 0.00683639180041\n",
      "Loss for  43762 th iteration => 0.00683625137154\n",
      "Loss for  43763 th iteration => 0.0068360665624\n",
      "Loss for  43764 th iteration => 0.00683589168954\n",
      "Loss for  43765 th iteration => 0.00683574125723\n",
      "Loss for  43766 th iteration => 0.00683553278121\n",
      "Loss for  43767 th iteration => 0.00683547442683\n",
      "Loss for  43768 th iteration => 0.00683520274795\n",
      "Loss for  43769 th iteration => 0.00683506210484\n",
      "Loss for  43770 th iteration => 0.0068348757235\n",
      "Loss for  43771 th iteration => 0.00683477856501\n",
      "Loss for  43772 th iteration => 0.00683456462081\n",
      "Loss for  43773 th iteration => 0.00683440153741\n",
      "Loss for  43774 th iteration => 0.00683425325641\n",
      "Loss for  43775 th iteration => 0.00683403555911\n",
      "Loss for  43776 th iteration => 0.00683388291138\n",
      "Loss for  43777 th iteration => 0.00683367497767\n",
      "Loss for  43778 th iteration => 0.00683355870494\n",
      "Loss for  43779 th iteration => 0.00683333015846\n",
      "Loss for  43780 th iteration => 0.00683316070533\n",
      "Loss for  43781 th iteration => 0.00683305524102\n",
      "Loss for  43782 th iteration => 0.00683283588045\n",
      "Loss for  43783 th iteration => 0.00683269571537\n",
      "Loss for  43784 th iteration => 0.00683251098753\n",
      "Loss for  43785 th iteration => 0.00683233637917\n",
      "Loss for  43786 th iteration => 0.00683218602743\n",
      "Loss for  43787 th iteration => 0.00683197723018\n",
      "Loss for  43788 th iteration => 0.00683186344255\n",
      "Loss for  43789 th iteration => 0.00683169199364\n",
      "Loss for  43790 th iteration => 0.00683148097472\n",
      "Loss for  43791 th iteration => 0.00683133590924\n",
      "Loss for  43792 th iteration => 0.00683115656836\n",
      "Loss for  43793 th iteration => 0.00683104932524\n",
      "Loss for  43794 th iteration => 0.00683084576716\n",
      "Loss for  43795 th iteration => 0.00683067274111\n",
      "Loss for  43796 th iteration => 0.00683053470498\n",
      "Loss for  43797 th iteration => 0.00683030985868\n",
      "Loss for  43798 th iteration => 0.00683016292053\n",
      "Loss for  43799 th iteration => 0.00682994968263\n",
      "Loss for  43800 th iteration => 0.00682983904763\n",
      "Loss for  43801 th iteration => 0.00682960853114\n",
      "Loss for  43802 th iteration => 0.00682944127257\n",
      "Loss for  43803 th iteration => 0.00682933088711\n",
      "Loss for  43804 th iteration => 0.00682911678332\n",
      "Loss for  43805 th iteration => 0.00682897176382\n",
      "Loss for  43806 th iteration => 0.00682879222623\n",
      "Loss for  43807 th iteration => 0.0068286128294\n",
      "Loss for  43808 th iteration => 0.00682846760218\n",
      "Loss for  43809 th iteration => 0.00682825629516\n",
      "Loss for  43810 th iteration => 0.00682813987096\n",
      "Loss for  43811 th iteration => 0.00682792933685\n",
      "Loss for  43812 th iteration => 0.00682778408885\n",
      "Loss for  43813 th iteration => 0.00682760440472\n",
      "Loss for  43814 th iteration => 0.00682749635173\n",
      "Loss for  43815 th iteration => 0.00682729400648\n",
      "Loss for  43816 th iteration => 0.00682712002897\n",
      "Loss for  43817 th iteration => 0.00682698334662\n",
      "Loss for  43818 th iteration => 0.00682675862233\n",
      "Loss for  43819 th iteration => 0.00682661260721\n",
      "Loss for  43820 th iteration => 0.00682639874232\n",
      "Loss for  43821 th iteration => 0.00682628910939\n",
      "Loss for  43822 th iteration => 0.00682605952826\n",
      "Loss for  43823 th iteration => 0.00682588975547\n",
      "Loss for  43824 th iteration => 0.00682571999356\n",
      "Loss for  43825 th iteration => 0.0068255508028\n",
      "Loss for  43826 th iteration => 0.00682545311159\n",
      "Loss for  43827 th iteration => 0.00682522668215\n",
      "Loss for  43828 th iteration => 0.00682509438371\n",
      "Loss for  43829 th iteration => 0.00682490249382\n",
      "Loss for  43830 th iteration => 0.00682473696817\n",
      "Loss for  43831 th iteration => 0.00682463160438\n",
      "Loss for  43832 th iteration => 0.00682439115183\n",
      "Loss for  43833 th iteration => 0.00682429148617\n",
      "Loss for  43834 th iteration => 0.00682405707312\n",
      "Loss for  43835 th iteration => 0.0068239479924\n",
      "Loss for  43836 th iteration => 0.00682374701042\n",
      "Loss for  43837 th iteration => 0.00682357202451\n",
      "Loss for  43838 th iteration => 0.00682343668624\n",
      "Loss for  43839 th iteration => 0.00682321214821\n",
      "Loss for  43840 th iteration => 0.00682306697008\n",
      "Loss for  43841 th iteration => 0.00682285332173\n",
      "Loss for  43842 th iteration => 0.00682268369663\n",
      "Loss for  43843 th iteration => 0.00682251408252\n",
      "Loss for  43844 th iteration => 0.00682234447938\n",
      "Loss for  43845 th iteration => 0.00682217488716\n",
      "Loss for  43846 th iteration => 0.00682200538231\n",
      "Loss for  43847 th iteration => 0.00682190655139\n",
      "Loss for  43848 th iteration => 0.00682168163289\n",
      "Loss for  43849 th iteration => 0.0068215481231\n",
      "Loss for  43850 th iteration => 0.00682135781547\n",
      "Loss for  43851 th iteration => 0.00682119077014\n",
      "Loss for  43852 th iteration => 0.0068210856215\n",
      "Loss for  43853 th iteration => 0.00682084528061\n",
      "Loss for  43854 th iteration => 0.00682074584135\n",
      "Loss for  43855 th iteration => 0.00682051226053\n",
      "Loss for  43856 th iteration => 0.00682033666746\n",
      "Loss for  43857 th iteration => 0.00682018592654\n",
      "Loss for  43858 th iteration => 0.00681998492941\n",
      "Loss for  43859 th iteration => 0.00681988726879\n",
      "Loss for  43860 th iteration => 0.00681966238693\n",
      "Loss for  43861 th iteration => 0.00681952774698\n",
      "Loss for  43862 th iteration => 0.00681933976537\n",
      "Loss for  43863 th iteration => 0.00681916842978\n",
      "Loss for  43864 th iteration => 0.0068190170657\n",
      "Loss for  43865 th iteration => 0.00681880931472\n",
      "Loss for  43866 th iteration => 0.00681869428888\n",
      "Loss for  43867 th iteration => 0.00681846423842\n",
      "Loss for  43868 th iteration => 0.0068182977294\n",
      "Loss for  43869 th iteration => 0.00681819223222\n",
      "Loss for  43870 th iteration => 0.00681797433265\n",
      "Loss for  43871 th iteration => 0.00681783417439\n",
      "Loss for  43872 th iteration => 0.00681765086793\n",
      "Loss for  43873 th iteration => 0.00681747630573\n",
      "Loss for  43874 th iteration => 0.0068173273361\n",
      "Loss for  43875 th iteration => 0.00681712057432\n",
      "Loss for  43876 th iteration => 0.00681706207899\n",
      "Loss for  43877 th iteration => 0.00681679182106\n",
      "Loss for  43878 th iteration => 0.00681665054291\n",
      "Loss for  43879 th iteration => 0.00681646835053\n",
      "Loss for  43880 th iteration => 0.00681636851319\n",
      "Loss for  43881 th iteration => 0.00681615899028\n",
      "Loss for  43882 th iteration => 0.00681599332279\n",
      "Loss for  43883 th iteration => 0.00681584936914\n",
      "Loss for  43884 th iteration => 0.00681562929368\n",
      "Loss for  43885 th iteration => 0.00681547948933\n",
      "Loss for  43886 th iteration => 0.00681527053152\n",
      "Loss for  43887 th iteration => 0.0068151570488\n",
      "Loss for  43888 th iteration => 0.00681492770612\n",
      "Loss for  43889 th iteration => 0.00681476082132\n",
      "Loss for  43890 th iteration => 0.00681465429068\n",
      "Loss for  43891 th iteration => 0.00681443776087\n",
      "Loss for  43892 th iteration => 0.00681429658548\n",
      "Loss for  43893 th iteration => 0.00681411463248\n",
      "Loss for  43894 th iteration => 0.00681393906937\n",
      "Loss for  43895 th iteration => 0.00681379143701\n",
      "Loss for  43896 th iteration => 0.00681358337528\n",
      "Loss for  43897 th iteration => 0.00681352489572\n",
      "Loss for  43898 th iteration => 0.00681325668216\n",
      "Loss for  43899 th iteration => 0.00681311428501\n",
      "Loss for  43900 th iteration => 0.00681293313932\n",
      "Loss for  43901 th iteration => 0.00681283055085\n",
      "Loss for  43902 th iteration => 0.00681262410398\n",
      "Loss for  43903 th iteration => 0.00681245572619\n",
      "Loss for  43904 th iteration => 0.006812314808\n",
      "Loss for  43905 th iteration => 0.00681209364585\n",
      "Loss for  43906 th iteration => 0.00681194560372\n",
      "Loss for  43907 th iteration => 0.00681173523762\n",
      "Loss for  43908 th iteration => 0.00681162349798\n",
      "Loss for  43909 th iteration => 0.00681139477073\n",
      "Loss for  43910 th iteration => 0.00681122759866\n",
      "Loss for  43911 th iteration => 0.00681111984179\n",
      "Loss for  43912 th iteration => 0.00681090487311\n",
      "Loss for  43913 th iteration => 0.00681076249011\n",
      "Loss for  43914 th iteration => 0.00681058207965\n",
      "Loss for  43915 th iteration => 0.00681040532741\n",
      "Loss for  43916 th iteration => 0.00681025921913\n",
      "Loss for  43917 th iteration => 0.00681004973965\n",
      "Loss for  43918 th iteration => 0.00680999131233\n",
      "Loss for  43919 th iteration => 0.00680972520536\n",
      "Loss for  43920 th iteration => 0.00680958154245\n",
      "Loss for  43921 th iteration => 0.00680940167321\n",
      "Loss for  43922 th iteration => 0.00680929600542\n",
      "Loss for  43923 th iteration => 0.00680909296043\n",
      "Loss for  43924 th iteration => 0.00680892154853\n",
      "Loss for  43925 th iteration => 0.00680878398727\n",
      "Loss for  43926 th iteration => 0.00680856149474\n",
      "Loss for  43927 th iteration => 0.00680841539202\n",
      "Loss for  43928 th iteration => 0.00680820344126\n",
      "Loss for  43929 th iteration => 0.00680809361968\n",
      "Loss for  43930 th iteration => 0.00680786542732\n",
      "Loss for  43931 th iteration => 0.00680769804586\n",
      "Loss for  43932 th iteration => 0.00680758889243\n",
      "Loss for  43933 th iteration => 0.0068073756539\n",
      "Loss for  43934 th iteration => 0.00680723189499\n",
      "Loss for  43935 th iteration => 0.00680705319407\n",
      "Loss for  43936 th iteration => 0.0068068750864\n",
      "Loss for  43937 th iteration => 0.00680673066723\n",
      "Loss for  43938 th iteration => 0.00680652129288\n",
      "Loss for  43939 th iteration => 0.0068064046843\n",
      "Loss for  43940 th iteration => 0.00680619643178\n",
      "Loss for  43941 th iteration => 0.00680605102877\n",
      "Loss for  43942 th iteration => 0.00680587326037\n",
      "Loss for  43943 th iteration => 0.00680576328449\n",
      "Loss for  43944 th iteration => 0.00680556492735\n",
      "Loss for  43945 th iteration => 0.00680538911337\n",
      "Loss for  43946 th iteration => 0.00680525633353\n",
      "Loss for  43947 th iteration => 0.00680503136643\n",
      "Loss for  43948 th iteration => 0.00680488808179\n",
      "Loss for  43949 th iteration => 0.00680467486171\n",
      "Loss for  43950 th iteration => 0.00680450612139\n",
      "Loss for  43951 th iteration => 0.00680433739205\n",
      "Loss for  43952 th iteration => 0.00680416867364\n",
      "Loss for  43953 th iteration => 0.00680399996613\n",
      "Loss for  43954 th iteration => 0.00680383180258\n",
      "Loss for  43955 th iteration => 0.00680373277965\n",
      "Loss for  43956 th iteration => 0.00680350979764\n",
      "Loss for  43957 th iteration => 0.00680337613871\n",
      "Loss for  43958 th iteration => 0.00680318772476\n",
      "Loss for  43959 th iteration => 0.00680302141497\n",
      "Loss for  43960 th iteration => 0.00680291626151\n",
      "Loss for  43961 th iteration => 0.00680267769176\n",
      "Loss for  43962 th iteration => 0.00680257824108\n",
      "Loss for  43963 th iteration => 0.00680234695562\n",
      "Loss for  43964 th iteration => 0.00680217046035\n",
      "Loss for  43965 th iteration => 0.00680202307743\n",
      "Loss for  43966 th iteration => 0.00680189396768\n",
      "Loss for  43967 th iteration => 0.00680166712969\n",
      "Loss for  43968 th iteration => 0.0068015361795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  43969 th iteration => 0.00680134625603\n",
      "Loss for  43970 th iteration => 0.00680117859628\n",
      "Loss for  43971 th iteration => 0.0068010253041\n",
      "Loss for  43972 th iteration => 0.00680082121555\n",
      "Loss for  43973 th iteration => 0.00680070427487\n",
      "Loss for  43974 th iteration => 0.00680047721209\n",
      "Loss for  43975 th iteration => 0.00680030956691\n",
      "Loss for  43976 th iteration => 0.00680020757602\n",
      "Loss for  43977 th iteration => 0.00679998791389\n",
      "Loss for  43978 th iteration => 0.00679985125682\n",
      "Loss for  43979 th iteration => 0.00679966619278\n",
      "Loss for  43980 th iteration => 0.00679949512705\n",
      "Loss for  43981 th iteration => 0.00679934440441\n",
      "Loss for  43982 th iteration => 0.00679914004262\n",
      "Loss for  43983 th iteration => 0.00679908073206\n",
      "Loss for  43984 th iteration => 0.00679881240771\n",
      "Loss for  43985 th iteration => 0.00679867293872\n",
      "Loss for  43986 th iteration => 0.00679848942795\n",
      "Loss for  43987 th iteration => 0.00679839229903\n",
      "Loss for  43988 th iteration => 0.00679818179747\n",
      "Loss for  43989 th iteration => 0.00679801885335\n",
      "Loss for  43990 th iteration => 0.00679787390653\n",
      "Loss for  43991 th iteration => 0.00679765671599\n",
      "Loss for  43992 th iteration => 0.00679750685998\n",
      "Loss for  43993 th iteration => 0.00679729969958\n",
      "Loss for  43994 th iteration => 0.00679718615541\n",
      "Loss for  43995 th iteration => 0.00679695894769\n",
      "Loss for  43996 th iteration => 0.00679679171605\n",
      "Loss for  43997 th iteration => 0.00679668697212\n",
      "Loss for  43998 th iteration => 0.00679647038861\n",
      "Loss for  43999 th iteration => 0.00679633101576\n",
      "Loss for  44000 th iteration => 0.00679614899317\n",
      "Loss for  44001 th iteration => 0.00679597524855\n",
      "Loss for  44002 th iteration => 0.00679582753057\n",
      "Loss for  44003 th iteration => 0.00679562083836\n",
      "Loss for  44004 th iteration => 0.00679556182525\n",
      "Loss for  44005 th iteration => 0.00679529614719\n",
      "Loss for  44006 th iteration => 0.00679515420749\n",
      "Loss for  44007 th iteration => 0.00679497386668\n",
      "Loss for  44008 th iteration => 0.00679487094892\n",
      "Loss for  44009 th iteration => 0.00679466654176\n",
      "Loss for  44010 th iteration => 0.00679449788967\n",
      "Loss for  44011 th iteration => 0.00679435895679\n",
      "Loss for  44012 th iteration => 0.00679413842203\n",
      "Loss for  44013 th iteration => 0.00679399197587\n",
      "Loss for  44014 th iteration => 0.00679378176927\n",
      "Loss for  44015 th iteration => 0.00679367159554\n",
      "Loss for  44016 th iteration => 0.00679344425735\n",
      "Loss for  44017 th iteration => 0.00679327743184\n",
      "Loss for  44018 th iteration => 0.00679316994452\n",
      "Loss for  44019 th iteration => 0.00679295642955\n",
      "Loss for  44020 th iteration => 0.00679281435034\n",
      "Loss for  44021 th iteration => 0.00679263535935\n",
      "Loss for  44022 th iteration => 0.00679245894502\n",
      "Loss for  44023 th iteration => 0.00679231422211\n",
      "Loss for  44024 th iteration => 0.00679210598318\n",
      "Loss for  44025 th iteration => 0.00679199002504\n",
      "Loss for  44026 th iteration => 0.00679178251123\n",
      "Loss for  44027 th iteration => 0.00679163777411\n",
      "Loss for  44028 th iteration => 0.00679146119164\n",
      "Loss for  44029 th iteration => 0.00679135159038\n",
      "Loss for  44030 th iteration => 0.0067911542309\n",
      "Loss for  44031 th iteration => 0.00679097883343\n",
      "Loss for  44032 th iteration => 0.00679084700984\n",
      "Loss for  44033 th iteration => 0.00679062223856\n",
      "Loss for  44034 th iteration => 0.00679047989778\n",
      "Loss for  44035 th iteration => 0.0067902669465\n",
      "Loss for  44036 th iteration => 0.00679009890576\n",
      "Loss for  44037 th iteration => 0.00678993087599\n",
      "Loss for  44038 th iteration => 0.00678976285714\n",
      "Loss for  44039 th iteration => 0.00678959484917\n",
      "Loss for  44040 th iteration => 0.00678942782787\n",
      "Loss for  44041 th iteration => 0.00678932914051\n",
      "Loss for  44042 th iteration => 0.00678910720411\n",
      "Loss for  44043 th iteration => 0.00678897391024\n",
      "Loss for  44044 th iteration => 0.00678878651242\n",
      "Loss for  44045 th iteration => 0.00678862129172\n",
      "Loss for  44046 th iteration => 0.00678851614538\n",
      "Loss for  44047 th iteration => 0.00678827896408\n",
      "Loss for  44048 th iteration => 0.00678817951603\n",
      "Loss for  44049 th iteration => 0.00678794947405\n",
      "Loss for  44050 th iteration => 0.00678777274587\n",
      "Loss for  44051 th iteration => 0.00678762802032\n",
      "Loss for  44052 th iteration => 0.00678749732212\n",
      "Loss for  44053 th iteration => 0.00678727278705\n",
      "Loss for  44054 th iteration => 0.00678714094996\n",
      "Loss for  44055 th iteration => 0.00678695328843\n",
      "Loss for  44056 th iteration => 0.0067867847825\n",
      "Loss for  44057 th iteration => 0.00678663371163\n",
      "Loss for  44058 th iteration => 0.00678642881728\n",
      "Loss for  44059 th iteration => 0.00678631405761\n",
      "Loss for  44060 th iteration => 0.00678608671634\n",
      "Loss for  44061 th iteration => 0.0067859206782\n",
      "Loss for  44062 th iteration => 0.00678581799414\n",
      "Loss for  44063 th iteration => 0.00678560039928\n",
      "Loss for  44064 th iteration => 0.00678546309131\n",
      "Loss for  44065 th iteration => 0.00678528005234\n",
      "Loss for  44066 th iteration => 0.00678510837765\n",
      "Loss for  44067 th iteration => 0.00678495963821\n",
      "Loss for  44068 th iteration => 0.00678475581275\n",
      "Loss for  44069 th iteration => 0.00678469652611\n",
      "Loss for  44070 th iteration => 0.00678442995819\n",
      "Loss for  44071 th iteration => 0.00678428930206\n",
      "Loss for  44072 th iteration => 0.00678410985239\n",
      "Loss for  44073 th iteration => 0.00678400893685\n",
      "Loss for  44074 th iteration => 0.00678380356714\n",
      "Loss for  44075 th iteration => 0.00678363693251\n",
      "Loss for  44076 th iteration => 0.00678349702218\n",
      "Loss for  44077 th iteration => 0.00678327696079\n",
      "Loss for  44078 th iteration => 0.00678313030437\n",
      "Loss for  44079 th iteration => 0.00678292136514\n",
      "Loss for  44080 th iteration => 0.00678281096823\n",
      "Loss for  44081 th iteration => 0.00678258304632\n",
      "Loss for  44082 th iteration => 0.00678241783098\n",
      "Loss for  44083 th iteration => 0.00678231148246\n",
      "Loss for  44084 th iteration => 0.00678209787133\n",
      "Loss for  44085 th iteration => 0.00678195694707\n",
      "Loss for  44086 th iteration => 0.0067817778438\n",
      "Loss for  44087 th iteration => 0.00678160260048\n",
      "Loss for  44088 th iteration => 0.00678145782606\n",
      "Loss for  44089 th iteration => 0.00678130496123\n",
      "Loss for  44090 th iteration => 0.00678115056594\n",
      "Loss for  44091 th iteration => 0.00678095399844\n",
      "Loss for  44092 th iteration => 0.00678077129406\n",
      "Loss for  44093 th iteration => 0.00678067264908\n",
      "Loss for  44094 th iteration => 0.00678046534159\n",
      "Loss for  44095 th iteration => 0.00678030095016\n",
      "Loss for  44096 th iteration => 0.00678015912932\n",
      "Loss for  44097 th iteration => 0.00677994237469\n",
      "Loss for  44098 th iteration => 0.00677979467086\n",
      "Loss for  44099 th iteration => 0.00677958708988\n",
      "Loss for  44100 th iteration => 0.00677947566251\n",
      "Loss for  44101 th iteration => 0.00677925041364\n",
      "Loss for  44102 th iteration => 0.00677908309531\n",
      "Loss for  44103 th iteration => 0.00677897780283\n",
      "Loss for  44104 th iteration => 0.00677876346204\n",
      "Loss for  44105 th iteration => 0.00677862357987\n",
      "Loss for  44106 th iteration => 0.00677844376079\n",
      "Loss for  44107 th iteration => 0.00677826954592\n",
      "Loss for  44108 th iteration => 0.0067781239924\n",
      "Loss for  44109 th iteration => 0.00677791700144\n",
      "Loss for  44110 th iteration => 0.00677780220718\n",
      "Loss for  44111 th iteration => 0.00677759489701\n",
      "Loss for  44112 th iteration => 0.00677745132989\n",
      "Loss for  44113 th iteration => 0.00677727419992\n",
      "Loss for  44114 th iteration => 0.00677716627541\n",
      "Loss for  44115 th iteration => 0.00677696859496\n",
      "Loss for  44116 th iteration => 0.00677679489701\n",
      "Loss for  44117 th iteration => 0.00677666273008\n",
      "Loss for  44118 th iteration => 0.00677643980916\n",
      "Loss for  44119 th iteration => 0.00677629761851\n",
      "Loss for  44120 th iteration => 0.00677608611806\n",
      "Loss for  44121 th iteration => 0.00677591876373\n",
      "Loss for  44122 th iteration => 0.00677575142035\n",
      "Loss for  44123 th iteration => 0.00677558408788\n",
      "Loss for  44124 th iteration => 0.00677541676629\n",
      "Loss for  44125 th iteration => 0.00677524990021\n",
      "Loss for  44126 th iteration => 0.00677515195339\n",
      "Loss for  44127 th iteration => 0.00677493063648\n",
      "Loss for  44128 th iteration => 0.00677479810275\n",
      "Loss for  44129 th iteration => 0.00677461130481\n",
      "Loss for  44130 th iteration => 0.00677444670661\n",
      "Loss for  44131 th iteration => 0.00677434159605\n",
      "Loss for  44132 th iteration => 0.00677410574814\n",
      "Loss for  44133 th iteration => 0.00677400633263\n",
      "Loss for  44134 th iteration => 0.00677377805447\n",
      "Loss for  44135 th iteration => 0.00677360150314\n",
      "Loss for  44136 th iteration => 0.00677345785808\n",
      "Loss for  44137 th iteration => 0.00677332708402\n",
      "Loss for  44138 th iteration => 0.00677310416137\n",
      "Loss for  44139 th iteration => 0.00677297209674\n",
      "Loss for  44140 th iteration => 0.00677278601685\n",
      "Loss for  44141 th iteration => 0.00677261731403\n",
      "Loss for  44142 th iteration => 0.00677246779419\n",
      "Loss for  44143 th iteration => 0.00677226273344\n",
      "Loss for  44144 th iteration => 0.00677214949435\n",
      "Loss for  44145 th iteration => 0.00677192299008\n",
      "Loss for  44146 th iteration => 0.00677175761694\n",
      "Loss for  44147 th iteration => 0.00677165455487\n",
      "Loss for  44148 th iteration => 0.00677143869061\n",
      "Loss for  44149 th iteration => 0.00677130103811\n",
      "Loss for  44150 th iteration => 0.00677111969628\n",
      "Loss for  44151 th iteration => 0.00677094771036\n",
      "Loss for  44152 th iteration => 0.0067708006348\n",
      "Loss for  44153 th iteration => 0.00677059682632\n",
      "Loss for  44154 th iteration => 0.0067705376018\n",
      "Loss for  44155 th iteration => 0.00677027334584\n",
      "Loss for  44156 th iteration => 0.00677013183389\n",
      "Loss for  44157 th iteration => 0.00676995499416\n",
      "Loss for  44158 th iteration => 0.00676985120295\n",
      "Loss for  44159 th iteration => 0.00676965003572\n",
      "Loss for  44160 th iteration => 0.00676948060733\n",
      "Loss for  44161 th iteration => 0.00676934481822\n",
      "Loss for  44162 th iteration => 0.00676912324189\n",
      "Loss for  44163 th iteration => 0.00676897920525\n",
      "Loss for  44164 th iteration => 0.00676876903681\n",
      "Loss for  44165 th iteration => 0.00676866121617\n",
      "Loss for  44166 th iteration => 0.00676843365076\n",
      "Loss for  44167 th iteration => 0.00676826954892\n",
      "Loss for  44168 th iteration => 0.00676816183328\n",
      "Loss for  44169 th iteration => 0.00676795093525\n",
      "Loss for  44170 th iteration => 0.00676780868921\n",
      "Loss for  44171 th iteration => 0.00676763225376\n",
      "Loss for  44172 th iteration => 0.00676745573366\n",
      "Loss for  44173 th iteration => 0.00676731422374\n",
      "Loss for  44174 th iteration => 0.00676715840657\n",
      "Loss for  44175 th iteration => 0.00676695001848\n",
      "Loss for  44176 th iteration => 0.00676680855184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  44177 th iteration => 0.00676663013054\n",
      "Loss for  44178 th iteration => 0.00676652613777\n",
      "Loss for  44179 th iteration => 0.00676632555327\n",
      "Loss for  44180 th iteration => 0.00676615577396\n",
      "Loss for  44181 th iteration => 0.00676602071626\n",
      "Loss for  44182 th iteration => 0.0067658003656\n",
      "Loss for  44183 th iteration => 0.00676565686509\n",
      "Loss for  44184 th iteration => 0.00676544642776\n",
      "Loss for  44185 th iteration => 0.00676533922769\n",
      "Loss for  44186 th iteration => 0.0067651135627\n",
      "Loss for  44187 th iteration => 0.00676494674706\n",
      "Loss for  44188 th iteration => 0.00676477994224\n",
      "Loss for  44189 th iteration => 0.00676461322736\n",
      "Loss for  44190 th iteration => 0.006764517815\n",
      "Loss for  44191 th iteration => 0.00676429495692\n",
      "Loss for  44192 th iteration => 0.00676416503937\n",
      "Loss for  44193 th iteration => 0.00676397661879\n",
      "Loss for  44194 th iteration => 0.00676381454804\n",
      "Loss for  44195 th iteration => 0.00676370913586\n",
      "Loss for  44196 th iteration => 0.00676347463055\n",
      "Loss for  44197 th iteration => 0.00676337490059\n",
      "Loss for  44198 th iteration => 0.00676314891062\n",
      "Loss for  44199 th iteration => 0.00676303453622\n",
      "Loss for  44200 th iteration => 0.0067628446087\n",
      "Loss for  44201 th iteration => 0.00676266570945\n",
      "Loss for  44202 th iteration => 0.0067624918303\n",
      "Loss for  44203 th iteration => 0.00676231190522\n",
      "Loss for  44204 th iteration => 0.00676217460393\n",
      "Loss for  44205 th iteration => 0.00676196240378\n",
      "Loss for  44206 th iteration => 0.00676179573122\n",
      "Loss for  44207 th iteration => 0.0067616290696\n",
      "Loss for  44208 th iteration => 0.00676146241891\n",
      "Loss for  44209 th iteration => 0.00676129577908\n",
      "Loss for  44210 th iteration => 0.00676113111662\n",
      "Loss for  44211 th iteration => 0.00676102937443\n",
      "Loss for  44212 th iteration => 0.0067608132092\n",
      "Loss for  44213 th iteration => 0.0067606768869\n",
      "Loss for  44214 th iteration => 0.00676049523378\n",
      "Loss for  44215 th iteration => 0.0067603245884\n",
      "Loss for  44216 th iteration => 0.0067601773538\n",
      "Loss for  44217 th iteration => 0.00676003101038\n",
      "Loss for  44218 th iteration => 0.00675987214157\n",
      "Loss for  44219 th iteration => 0.00675967899115\n",
      "Loss for  44220 th iteration => 0.00675949552686\n",
      "Loss for  44221 th iteration => 0.00675940222672\n",
      "Loss for  44222 th iteration => 0.0067591916141\n",
      "Loss for  44223 th iteration => 0.00675903257511\n",
      "Loss for  44224 th iteration => 0.00675888744204\n",
      "Loss for  44225 th iteration => 0.00675867403033\n",
      "Loss for  44226 th iteration => 0.00675852494184\n",
      "Loss for  44227 th iteration => 0.00675832080047\n",
      "Loss for  44228 th iteration => 0.00675820797759\n",
      "Loss for  44229 th iteration => 0.00675798362643\n",
      "Loss for  44230 th iteration => 0.00675781773585\n",
      "Loss for  44231 th iteration => 0.00675771514243\n",
      "Loss for  44232 th iteration => 0.00675750014289\n",
      "Loss for  44233 th iteration => 0.00675736297866\n",
      "Loss for  44234 th iteration => 0.00675718248192\n",
      "Loss for  44235 th iteration => 0.0067570110039\n",
      "Loss for  44236 th iteration => 0.00675686475377\n",
      "Loss for  44237 th iteration => 0.00675666086836\n",
      "Loss for  44238 th iteration => 0.00675660143884\n",
      "Loss for  44239 th iteration => 0.00675634007516\n",
      "Loss for  44240 th iteration => 0.00675619824439\n",
      "Loss for  44241 th iteration => 0.00675602217541\n",
      "Loss for  44242 th iteration => 0.00675591703042\n",
      "Loss for  44243 th iteration => 0.00675571852955\n",
      "Loss for  44244 th iteration => 0.00675554780584\n",
      "Loss for  44245 th iteration => 0.00675541462518\n",
      "Loss for  44246 th iteration => 0.00675519337316\n",
      "Loss for  44247 th iteration => 0.00675505098004\n",
      "Loss for  44248 th iteration => 0.00675484052586\n",
      "Loss for  44249 th iteration => 0.00675473431885\n",
      "Loss for  44250 th iteration => 0.00675450835652\n",
      "Loss for  44251 th iteration => 0.00675434424793\n",
      "Loss for  44252 th iteration => 0.00675423588575\n",
      "Loss for  44253 th iteration => 0.00675402696048\n",
      "Loss for  44254 th iteration => 0.00675388410093\n",
      "Loss for  44255 th iteration => 0.00675370960525\n",
      "Loss for  44256 th iteration => 0.0067535325045\n",
      "Loss for  44257 th iteration => 0.00675339269222\n",
      "Loss for  44258 th iteration => 0.00675323640016\n",
      "Loss for  44259 th iteration => 0.00675303053698\n",
      "Loss for  44260 th iteration => 0.00675288790311\n",
      "Loss for  44261 th iteration => 0.00675271158637\n",
      "Loss for  44262 th iteration => 0.00675260433481\n",
      "Loss for  44263 th iteration => 0.00675240830889\n",
      "Loss for  44264 th iteration => 0.0067522353552\n",
      "Loss for  44265 th iteration => 0.00675210477233\n",
      "Loss for  44266 th iteration => 0.00675188332982\n",
      "Loss for  44267 th iteration => 0.00675174250454\n",
      "Loss for  44268 th iteration => 0.00675153285322\n",
      "Loss for  44269 th iteration => 0.00675136668461\n",
      "Loss for  44270 th iteration => 0.00675120052692\n",
      "Loss for  44271 th iteration => 0.00675103438012\n",
      "Loss for  44272 th iteration => 0.00675086824417\n",
      "Loss for  44273 th iteration => 0.00675070227514\n",
      "Loss for  44274 th iteration => 0.00675060455082\n",
      "Loss for  44275 th iteration => 0.00675038535396\n",
      "Loss for  44276 th iteration => 0.0067502530913\n",
      "Loss for  44277 th iteration => 0.00675006836487\n",
      "Loss for  44278 th iteration => 0.00674990437583\n",
      "Loss for  44279 th iteration => 0.00674979938656\n",
      "Loss for  44280 th iteration => 0.00674956578461\n",
      "Loss for  44281 th iteration => 0.00674946648247\n",
      "Loss for  44282 th iteration => 0.006749241591\n",
      "Loss for  44283 th iteration => 0.00674906440483\n",
      "Loss for  44284 th iteration => 0.00674892414902\n",
      "Loss for  44285 th iteration => 0.0067487917352\n",
      "Loss for  44286 th iteration => 0.00674857255185\n",
      "Loss for  44287 th iteration => 0.00674843914799\n",
      "Loss for  44288 th iteration => 0.00674825673978\n",
      "Loss for  44289 th iteration => 0.006748086765\n",
      "Loss for  44290 th iteration => 0.00674794084969\n",
      "Loss for  44291 th iteration => 0.00674773458375\n",
      "Loss for  44292 th iteration => 0.00674762488255\n",
      "Loss for  44293 th iteration => 0.00674739925957\n",
      "Loss for  44294 th iteration => 0.00674723559534\n",
      "Loss for  44295 th iteration => 0.00674713096836\n",
      "Loss for  44296 th iteration => 0.00674691899948\n",
      "Loss for  44297 th iteration => 0.00674677985274\n",
      "Loss for  44298 th iteration => 0.00674660233573\n",
      "Loss for  44299 th iteration => 0.00674642892576\n",
      "Loss for  44300 th iteration => 0.00674628632727\n",
      "Loss for  44301 th iteration => 0.00674613369313\n",
      "Loss for  44302 th iteration => 0.00674598239558\n",
      "Loss for  44303 th iteration => 0.00674578616019\n",
      "Loss for  44304 th iteration => 0.00674560754293\n",
      "Loss for  44305 th iteration => 0.00674550695458\n",
      "Loss for  44306 th iteration => 0.00674530490425\n",
      "Loss for  44307 th iteration => 0.00674513871334\n",
      "Loss for  44308 th iteration => 0.00674500200717\n",
      "Loss for  44309 th iteration => 0.00674478447094\n",
      "Loss for  44310 th iteration => 0.006744640345\n",
      "Loss for  44311 th iteration => 0.00674443261692\n",
      "Loss for  44312 th iteration => 0.00674432468723\n",
      "Loss for  44313 th iteration => 0.00674410045164\n",
      "Loss for  44314 th iteration => 0.00674393587554\n",
      "Loss for  44315 th iteration => 0.00674382970014\n",
      "Loss for  44316 th iteration => 0.00674361958902\n",
      "Loss for  44317 th iteration => 0.00674347891149\n",
      "Loss for  44318 th iteration => 0.00674330323463\n",
      "Loss for  44319 th iteration => 0.0067431283114\n",
      "Loss for  44320 th iteration => 0.00674298681323\n",
      "Loss for  44321 th iteration => 0.0067427822048\n",
      "Loss for  44322 th iteration => 0.00674266531548\n",
      "Loss for  44323 th iteration => 0.00674246345124\n",
      "Loss for  44324 th iteration => 0.00674231786881\n",
      "Loss for  44325 th iteration => 0.00674214811776\n",
      "Loss for  44326 th iteration => 0.00674203352212\n",
      "Loss for  44327 th iteration => 0.00674184579494\n",
      "Loss for  44328 th iteration => 0.00674166674553\n",
      "Loss for  44329 th iteration => 0.00674149444913\n",
      "Loss for  44330 th iteration => 0.00674131498601\n",
      "Loss for  44331 th iteration => 0.00674117922166\n",
      "Loss for  44332 th iteration => 0.00674096740845\n",
      "Loss for  44333 th iteration => 0.00674080174877\n",
      "Loss for  44334 th iteration => 0.00674063610001\n",
      "Loss for  44335 th iteration => 0.00674047046214\n",
      "Loss for  44336 th iteration => 0.00674030483511\n",
      "Loss for  44337 th iteration => 0.00674014215233\n",
      "Loss for  44338 th iteration => 0.00674004005727\n",
      "Loss for  44339 th iteration => 0.00673982624058\n",
      "Loss for  44340 th iteration => 0.00673968961725\n",
      "Loss for  44341 th iteration => 0.00673951026091\n",
      "Loss for  44342 th iteration => 0.00673933936599\n",
      "Loss for  44343 th iteration => 0.00673919560769\n",
      "Loss for  44344 th iteration => 0.00673904700019\n",
      "Loss for  44345 th iteration => 0.00673889236311\n",
      "Loss for  44346 th iteration => 0.0067386973232\n",
      "Loss for  44347 th iteration => 0.00673851832862\n",
      "Loss for  44348 th iteration => 0.00673842165691\n",
      "Loss for  44349 th iteration => 0.00673821637485\n",
      "Loss for  44350 th iteration => 0.00673805408319\n",
      "Loss for  44351 th iteration => 0.00673791416278\n",
      "Loss for  44352 th iteration => 0.00673769833466\n",
      "Loss for  44353 th iteration => 0.00673755249843\n",
      "Loss for  44354 th iteration => 0.00673734715832\n",
      "Loss for  44355 th iteration => 0.00673723752231\n",
      "Loss for  44356 th iteration => 0.00673701282597\n",
      "Loss for  44357 th iteration => 0.00673684953911\n",
      "Loss for  44358 th iteration => 0.00673674522001\n",
      "Loss for  44359 th iteration => 0.00673653393257\n",
      "Loss for  44360 th iteration => 0.00673639511105\n",
      "Loss for  44361 th iteration => 0.00673621825817\n",
      "Loss for  44362 th iteration => 0.00673604519067\n",
      "Loss for  44363 th iteration => 0.00673590335306\n",
      "Loss for  44364 th iteration => 0.00673575044973\n",
      "Loss for  44365 th iteration => 0.00673560039978\n",
      "Loss for  44366 th iteration => 0.00673540420265\n",
      "Loss for  44367 th iteration => 0.0067352266781\n",
      "Loss for  44368 th iteration => 0.00673512551716\n",
      "Loss for  44369 th iteration => 0.00673492501381\n",
      "Loss for  44370 th iteration => 0.00673475829432\n",
      "Loss for  44371 th iteration => 0.00673462309154\n",
      "Loss for  44372 th iteration => 0.00673440554284\n",
      "Loss for  44373 th iteration => 0.00673426232904\n",
      "Loss for  44374 th iteration => 0.00673405469852\n",
      "Loss for  44375 th iteration => 0.00673394765694\n",
      "Loss for  44376 th iteration => 0.0067337239699\n",
      "Loss for  44377 th iteration => 0.00673356012029\n",
      "Loss for  44378 th iteration => 0.00673345349038\n",
      "Loss for  44379 th iteration => 0.00673324481815\n",
      "Loss for  44380 th iteration => 0.00673310371264\n",
      "Loss for  44381 th iteration => 0.00673292944819\n",
      "Loss for  44382 th iteration => 0.00673275412332\n",
      "Loss for  44383 th iteration => 0.006732614321\n",
      "Loss for  44384 th iteration => 0.00673245950237\n",
      "Loss for  44385 th iteration => 0.00673225492484\n",
      "Loss for  44386 th iteration => 0.00673211301348\n",
      "Loss for  44387 th iteration => 0.00673193755495\n",
      "Loss for  44388 th iteration => 0.00673183098189\n",
      "Loss for  44389 th iteration => 0.00673163623759\n",
      "Loss for  44390 th iteration => 0.00673146425237\n",
      "Loss for  44391 th iteration => 0.00673128811501\n",
      "Loss for  44392 th iteration => 0.00673111344788\n",
      "Loss for  44393 th iteration => 0.00673097387895\n",
      "Loss for  44394 th iteration => 0.00673076554033\n",
      "Loss for  44395 th iteration => 0.00673060036916\n",
      "Loss for  44396 th iteration => 0.00673043520894\n",
      "Loss for  44397 th iteration => 0.00673027005962\n",
      "Loss for  44398 th iteration => 0.00673010492117\n",
      "Loss for  44399 th iteration => 0.00672994031455\n",
      "Loss for  44400 th iteration => 0.00672984187461\n",
      "Loss for  44401 th iteration => 0.00672962538974\n",
      "Loss for  44402 th iteration => 0.00672949239528\n",
      "Loss for  44403 th iteration => 0.00672931039686\n",
      "Loss for  44404 th iteration => 0.00672914310508\n",
      "Loss for  44405 th iteration => 0.00672899533673\n",
      "Loss for  44406 th iteration => 0.00672879625088\n",
      "Loss for  44407 th iteration => 0.00672873621638\n",
      "Loss for  44408 th iteration => 0.00672847533179\n",
      "Loss for  44409 th iteration => 0.00672833578792\n",
      "Loss for  44410 th iteration => 0.00672816067344\n",
      "Loss for  44411 th iteration => 0.00672805940464\n",
      "Loss for  44412 th iteration => 0.00672785966971\n",
      "Loss for  44413 th iteration => 0.00672769287613\n",
      "Loss for  44414 th iteration => 0.00672755840833\n",
      "Loss for  44415 th iteration => 0.0067273395627\n",
      "Loss for  44416 th iteration => 0.00672719685814\n",
      "Loss for  44417 th iteration => 0.00672698940882\n",
      "Loss for  44418 th iteration => 0.00672688285461\n",
      "Loss for  44419 th iteration => 0.00672665770878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  44420 th iteration => 0.0067264959599\n",
      "Loss for  44421 th iteration => 0.00672638938776\n",
      "Loss for  44422 th iteration => 0.00672618132582\n",
      "Loss for  44423 th iteration => 0.00672604030094\n",
      "Loss for  44424 th iteration => 0.00672586662399\n",
      "Loss for  44425 th iteration => 0.00672569140236\n",
      "Loss for  44426 th iteration => 0.0067255537967\n",
      "Loss for  44427 th iteration => 0.0067253969872\n",
      "Loss for  44428 th iteration => 0.00672519333966\n",
      "Loss for  44429 th iteration => 0.00672505118833\n",
      "Loss for  44430 th iteration => 0.00672487865971\n",
      "Loss for  44431 th iteration => 0.00672477122459\n",
      "Loss for  44432 th iteration => 0.00672457799769\n",
      "Loss for  44433 th iteration => 0.00672440496902\n",
      "Loss for  44434 th iteration => 0.00672427707771\n",
      "Loss for  44435 th iteration => 0.00672405502671\n",
      "Loss for  44436 th iteration => 0.00672391609291\n",
      "Loss for  44437 th iteration => 0.00672370624484\n",
      "Loss for  44438 th iteration => 0.00672354141644\n",
      "Loss for  44439 th iteration => 0.00672337659894\n",
      "Loss for  44440 th iteration => 0.00672321179229\n",
      "Loss for  44441 th iteration => 0.00672304699645\n",
      "Loss for  44442 th iteration => 0.00672288438839\n",
      "Loss for  44443 th iteration => 0.00672278617655\n",
      "Loss for  44444 th iteration => 0.00672257010595\n",
      "Loss for  44445 th iteration => 0.00672243742742\n",
      "Loss for  44446 th iteration => 0.006722257891\n",
      "Loss for  44447 th iteration => 0.00672213861194\n",
      "Loss for  44448 th iteration => 0.0067219563924\n",
      "Loss for  44449 th iteration => 0.00672177399193\n",
      "Loss for  44450 th iteration => 0.00672165465317\n",
      "Loss for  44451 th iteration => 0.00672142388419\n",
      "Loss for  44452 th iteration => 0.00672128244867\n",
      "Loss for  44453 th iteration => 0.00672115435857\n",
      "Loss for  44454 th iteration => 0.00672093391521\n",
      "Loss for  44455 th iteration => 0.00672080443277\n",
      "Loss for  44456 th iteration => 0.00672062074152\n",
      "Loss for  44457 th iteration => 0.00672045471124\n",
      "Loss for  44458 th iteration => 0.0067203074898\n",
      "Loss for  44459 th iteration => 0.00672010519149\n",
      "Loss for  44460 th iteration => 0.006719994161\n",
      "Loss for  44461 th iteration => 0.00671977062262\n",
      "Loss for  44462 th iteration => 0.00671960829275\n",
      "Loss for  44463 th iteration => 0.00671950594845\n",
      "Loss for  44464 th iteration => 0.00671929433027\n",
      "Loss for  44465 th iteration => 0.00671915749974\n",
      "Loss for  44466 th iteration => 0.0067189802999\n",
      "Loss for  44467 th iteration => 0.00671880923963\n",
      "Loss for  44468 th iteration => 0.00671866749312\n",
      "Loss for  44469 th iteration => 0.00671851681085\n",
      "Loss for  44470 th iteration => 0.00671836617218\n",
      "Loss for  44471 th iteration => 0.00671817091141\n",
      "Loss for  44472 th iteration => 0.00671799437735\n",
      "Loss for  44473 th iteration => 0.00671789492932\n",
      "Loss for  44474 th iteration => 0.00671769433904\n",
      "Loss for  44475 th iteration => 0.00671752937662\n",
      "Loss for  44476 th iteration => 0.00671739404333\n",
      "Loss for  44477 th iteration => 0.00671717760136\n",
      "Loss for  44478 th iteration => 0.0067170344915\n",
      "Loss for  44479 th iteration => 0.00671682842238\n",
      "Loss for  44480 th iteration => 0.00671672145739\n",
      "Loss for  44481 th iteration => 0.0067164982534\n",
      "Loss for  44482 th iteration => 0.00671633598316\n",
      "Loss for  44483 th iteration => 0.00671622996764\n",
      "Loss for  44484 th iteration => 0.00671602231644\n",
      "Loss for  44485 th iteration => 0.00671588185793\n",
      "Loss for  44486 th iteration => 0.00671570858195\n",
      "Loss for  44487 th iteration => 0.0067155339365\n",
      "Loss for  44488 th iteration => 0.00671539604319\n",
      "Loss for  44489 th iteration => 0.00671524045047\n",
      "Loss for  44490 th iteration => 0.00671503757641\n",
      "Loss for  44491 th iteration => 0.00671489562907\n",
      "Loss for  44492 th iteration => 0.00671472284798\n",
      "Loss for  44493 th iteration => 0.00671461557007\n",
      "Loss for  44494 th iteration => 0.0067144231407\n",
      "Loss for  44495 th iteration => 0.00671425041415\n",
      "Loss for  44496 th iteration => 0.00671407573547\n",
      "Loss for  44497 th iteration => 0.00671390128318\n",
      "Loss for  44498 th iteration => 0.00671376312832\n",
      "Loss for  44499 th iteration => 0.00671355466793\n",
      "Loss for  44500 th iteration => 0.00671339032392\n",
      "Loss for  44501 th iteration => 0.00671322599083\n",
      "Loss for  44502 th iteration => 0.00671306166862\n",
      "Loss for  44503 th iteration => 0.00671289735724\n",
      "Loss for  44504 th iteration => 0.00671273492331\n",
      "Loss for  44505 th iteration => 0.00671263574484\n",
      "Loss for  44506 th iteration => 0.00671242162499\n",
      "Loss for  44507 th iteration => 0.006712287941\n",
      "Loss for  44508 th iteration => 0.0067121082587\n",
      "Loss for  44509 th iteration => 0.006711940326\n",
      "Loss for  44510 th iteration => 0.00671179593295\n",
      "Loss for  44511 th iteration => 0.00671165033757\n",
      "Loss for  44512 th iteration => 0.00671149528272\n",
      "Loss for  44513 th iteration => 0.00671130271045\n",
      "Loss for  44514 th iteration => 0.00671112422673\n",
      "Loss for  44515 th iteration => 0.00671102976038\n",
      "Loss for  44516 th iteration => 0.00671082485706\n",
      "Loss for  44517 th iteration => 0.00671066484143\n",
      "Loss for  44518 th iteration => 0.00671052523\n",
      "Loss for  44519 th iteration => 0.00671031194081\n",
      "Loss for  44520 th iteration => 0.00671016649682\n",
      "Loss for  44521 th iteration => 0.00670996340907\n",
      "Loss for  44522 th iteration => 0.0067098541244\n",
      "Loss for  44523 th iteration => 0.00670963154356\n",
      "Loss for  44524 th iteration => 0.00670946952299\n",
      "Loss for  44525 th iteration => 0.00670936590542\n",
      "Loss for  44526 th iteration => 0.00670915651596\n",
      "Loss for  44527 th iteration => 0.0067090184454\n",
      "Loss for  44528 th iteration => 0.0067088434411\n",
      "Loss for  44529 th iteration => 0.00670867117381\n",
      "Loss for  44530 th iteration => 0.0067085316653\n",
      "Loss for  44531 th iteration => 0.00670837786195\n",
      "Loss for  44532 th iteration => 0.00670823128161\n",
      "Loss for  44533 th iteration => 0.00670803482436\n",
      "Loss for  44534 th iteration => 0.00670786057042\n",
      "Loss for  44535 th iteration => 0.00670775766539\n",
      "Loss for  44536 th iteration => 0.00670756146517\n",
      "Loss for  44537 th iteration => 0.00670739312364\n",
      "Loss for  44538 th iteration => 0.00670726210308\n",
      "Loss for  44539 th iteration => 0.00670704415459\n",
      "Loss for  44540 th iteration => 0.00670690347914\n",
      "Loss for  44541 th iteration => 0.00670669596713\n",
      "Loss for  44542 th iteration => 0.00670659139699\n",
      "Loss for  44543 th iteration => 0.00670636885232\n",
      "Loss for  44544 th iteration => 0.00670620716796\n",
      "Loss for  44545 th iteration => 0.00670609927453\n",
      "Loss for  44546 th iteration => 0.00670589445257\n",
      "Loss for  44547 th iteration => 0.00670575215661\n",
      "Loss for  44548 th iteration => 0.00670558166949\n",
      "Loss for  44549 th iteration => 0.00670540522671\n",
      "Loss for  44550 th iteration => 0.00670527037664\n",
      "Loss for  44551 th iteration => 0.00670511279552\n",
      "Loss for  44552 th iteration => 0.00670491281478\n",
      "Loss for  44553 th iteration => 0.00670476896278\n",
      "Loss for  44554 th iteration => 0.00670459920727\n",
      "Loss for  44555 th iteration => 0.00670448740462\n",
      "Loss for  44556 th iteration => 0.00670430042564\n",
      "Loss for  44557 th iteration => 0.00670412533415\n",
      "Loss for  44558 th iteration => 0.00670395371895\n",
      "Loss for  44559 th iteration => 0.0067037771983\n",
      "Loss for  44560 th iteration => 0.00670364205926\n",
      "Loss for  44561 th iteration => 0.00670343397809\n",
      "Loss for  44562 th iteration => 0.00670327011961\n",
      "Loss for  44563 th iteration => 0.00670310627203\n",
      "Loss for  44564 th iteration => 0.0067029424353\n",
      "Loss for  44565 th iteration => 0.0067027786094\n",
      "Loss for  44566 th iteration => 0.00670261703106\n",
      "Loss for  44567 th iteration => 0.00670251535325\n",
      "Loss for  44568 th iteration => 0.00670230467966\n",
      "Loss for  44569 th iteration => 0.00670216854428\n",
      "Loss for  44570 th iteration => 0.00670199226039\n",
      "Loss for  44571 th iteration => 0.00670182192383\n",
      "Loss for  44572 th iteration => 0.00670168139044\n",
      "Loss for  44573 th iteration => 0.00670153051841\n",
      "Loss for  44574 th iteration => 0.00670138166296\n",
      "Loss for  44575 th iteration => 0.00670118641036\n",
      "Loss for  44576 th iteration => 0.0067010117021\n",
      "Loss for  44577 th iteration => 0.00670091161184\n",
      "Loss for  44578 th iteration => 0.00670071325082\n",
      "Loss for  44579 th iteration => 0.00670054771751\n",
      "Loss for  44580 th iteration => 0.00670041454284\n",
      "Loss for  44581 th iteration => 0.00670019814775\n",
      "Loss for  44582 th iteration => 0.00670005631613\n",
      "Loss for  44583 th iteration => 0.0066998506134\n",
      "Loss for  44584 th iteration => 0.00669974488725\n",
      "Loss for  44585 th iteration => 0.00669952244824\n",
      "Loss for  44586 th iteration => 0.00669936149451\n",
      "Loss for  44587 th iteration => 0.00669925491094\n",
      "Loss for  44588 th iteration => 0.00669904943094\n",
      "Loss for  44589 th iteration => 0.00669890844784\n",
      "Loss for  44590 th iteration => 0.00669873729967\n",
      "Loss for  44591 th iteration => 0.00669856217278\n",
      "Loss for  44592 th iteration => 0.00669842714129\n",
      "Loss for  44593 th iteration => 0.00669826998691\n",
      "Loss for  44594 th iteration => 0.00669806983397\n",
      "Loss for  44595 th iteration => 0.0066979268101\n",
      "Loss for  44596 th iteration => 0.00669775738438\n",
      "Loss for  44597 th iteration => 0.00669764727261\n",
      "Loss for  44598 th iteration => 0.00669745924958\n",
      "Loss for  44599 th iteration => 0.00669728487175\n",
      "Loss for  44600 th iteration => 0.00669711271606\n",
      "Loss for  44601 th iteration => 0.00669693739281\n",
      "Loss for  44602 th iteration => 0.00669680170568\n",
      "Loss for  44603 th iteration => 0.00669659345476\n",
      "Loss for  44604 th iteration => 0.0066964299239\n",
      "Loss for  44605 th iteration => 0.00669626640393\n",
      "Loss for  44606 th iteration => 0.00669610289482\n",
      "Loss for  44607 th iteration => 0.00669593939651\n",
      "Loss for  44608 th iteration => 0.0066957788216\n",
      "Loss for  44609 th iteration => 0.00669567783526\n",
      "Loss for  44610 th iteration => 0.00669546711812\n",
      "Loss for  44611 th iteration => 0.00669533168469\n",
      "Loss for  44612 th iteration => 0.00669515534679\n",
      "Loss for  44613 th iteration => 0.00669498572257\n",
      "Loss for  44614 th iteration => 0.00669484582554\n",
      "Loss for  44615 th iteration => 0.00669469567794\n",
      "Loss for  44616 th iteration => 0.00669454674042\n",
      "Loss for  44617 th iteration => 0.00669435116535\n",
      "Loss for  44618 th iteration => 0.00669417754938\n",
      "Loss for  44619 th iteration => 0.00669407804151\n",
      "Loss for  44620 th iteration => 0.00669387973806\n",
      "Loss for  44621 th iteration => 0.00669371480962\n",
      "Loss for  44622 th iteration => 0.00669358167029\n",
      "Loss for  44623 th iteration => 0.00669336517734\n",
      "Loss for  44624 th iteration => 0.00669322340349\n",
      "Loss for  44625 th iteration => 0.00669301830316\n",
      "Loss for  44626 th iteration => 0.00669291262025\n",
      "Loss for  44627 th iteration => 0.00669269009812\n",
      "Loss for  44628 th iteration => 0.00669258920323\n",
      "Loss for  44629 th iteration => 0.00669237868997\n",
      "Loss for  44630 th iteration => 0.00669224334264\n",
      "Loss for  44631 th iteration => 0.00669206721392\n",
      "Loss for  44632 th iteration => 0.00669189767067\n",
      "Loss for  44633 th iteration => 0.00669175596616\n",
      "Loss for  44634 th iteration => 0.00669160561128\n",
      "Loss for  44635 th iteration => 0.00669145717446\n",
      "Loss for  44636 th iteration => 0.00669126401968\n",
      "Loss for  44637 th iteration => 0.00669108858488\n",
      "Loss for  44638 th iteration => 0.00669092463626\n",
      "Loss for  44639 th iteration => 0.00669083866432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  44640 th iteration => 0.00669059838914\n",
      "Loss for  44641 th iteration => 0.00669050444852\n",
      "Loss for  44642 th iteration => 0.00669028745895\n",
      "Loss for  44643 th iteration => 0.00669011293189\n",
      "Loss for  44644 th iteration => 0.00668997703522\n",
      "Loss for  44645 th iteration => 0.00668976849586\n",
      "Loss for  44646 th iteration => 0.00668960529707\n",
      "Loss for  44647 th iteration => 0.00668944340886\n",
      "Loss for  44648 th iteration => 0.00668933657237\n",
      "Loss for  44649 th iteration => 0.00668913231489\n",
      "Loss for  44650 th iteration => 0.00668899101038\n",
      "Loss for  44651 th iteration => 0.00668882115292\n",
      "Loss for  44652 th iteration => 0.00668864563731\n",
      "Loss for  44653 th iteration => 0.0066885109507\n",
      "Loss for  44654 th iteration => 0.00668835465397\n",
      "Loss for  44655 th iteration => 0.00668815593553\n",
      "Loss for  44656 th iteration => 0.00668801238696\n",
      "Loss for  44657 th iteration => 0.00668784233519\n",
      "Loss for  44658 th iteration => 0.00668767027544\n",
      "Loss for  44659 th iteration => 0.00668752869327\n",
      "Loss for  44660 th iteration => 0.00668733455365\n",
      "Loss for  44661 th iteration => 0.00668723836889\n",
      "Loss for  44662 th iteration => 0.00668702464302\n",
      "Loss for  44663 th iteration => 0.00668689175524\n",
      "Loss for  44664 th iteration => 0.00668671465377\n",
      "Loss for  44665 th iteration => 0.00668654534499\n",
      "Loss for  44666 th iteration => 0.00668640458683\n",
      "Loss for  44667 th iteration => 0.0066861991357\n",
      "Loss for  44668 th iteration => 0.00668609444311\n",
      "Loss for  44669 th iteration => 0.0066858724303\n",
      "Loss for  44670 th iteration => 0.00668577070023\n",
      "Loss for  44671 th iteration => 0.00668556166117\n",
      "Loss for  44672 th iteration => 0.00668542550499\n",
      "Loss for  44673 th iteration => 0.0066852508242\n",
      "Loss for  44674 th iteration => 0.00668508049818\n",
      "Loss for  44675 th iteration => 0.00668494124122\n",
      "Loss for  44676 th iteration => 0.00668478938912\n",
      "Loss for  44677 th iteration => 0.0066846430768\n",
      "Loss for  44678 th iteration => 0.00668444820655\n",
      "Loss for  44679 th iteration => 0.00668427482025\n",
      "Loss for  44680 th iteration => 0.00668417342408\n",
      "Loss for  44681 th iteration => 0.00668397792572\n",
      "Loss for  44682 th iteration => 0.00668381116535\n",
      "Loss for  44683 th iteration => 0.00668368077518\n",
      "Loss for  44684 th iteration => 0.00668346447231\n",
      "Loss for  44685 th iteration => 0.00668332447049\n",
      "Loss for  44686 th iteration => 0.00668311855651\n",
      "Loss for  44687 th iteration => 0.00668301461818\n",
      "Loss for  44688 th iteration => 0.00668279350115\n",
      "Loss for  44689 th iteration => 0.00668263337612\n",
      "Loss for  44690 th iteration => 0.0066825255406\n",
      "Loss for  44691 th iteration => 0.00668232288712\n",
      "Loss for  44692 th iteration => 0.00668218069787\n",
      "Loss for  44693 th iteration => 0.0066820123305\n",
      "Loss for  44694 th iteration => 0.00668183604298\n",
      "Loss for  44695 th iteration => 0.00668170389254\n",
      "Loss for  44696 th iteration => 0.0066815455854\n",
      "Loss for  44697 th iteration => 0.00668134819278\n",
      "Loss for  44698 th iteration => 0.00668120402613\n",
      "Loss for  44699 th iteration => 0.00668103741516\n",
      "Loss for  44700 th iteration => 0.00668092386238\n",
      "Loss for  44701 th iteration => 0.00668074082297\n",
      "Loss for  44702 th iteration => 0.00668056487555\n",
      "Loss for  44703 th iteration => 0.00668039576608\n",
      "Loss for  44704 th iteration => 0.00668021902239\n",
      "Loss for  44705 th iteration => 0.00668008632405\n",
      "Loss for  44706 th iteration => 0.00667987885079\n",
      "Loss for  44707 th iteration => 0.00667971611901\n",
      "Loss for  44708 th iteration => 0.00667955339811\n",
      "Loss for  44709 th iteration => 0.00667939068802\n",
      "Loss for  44710 th iteration => 0.00667922819402\n",
      "Loss for  44711 th iteration => 0.00667913027036\n",
      "Loss for  44712 th iteration => 0.00667891806784\n",
      "Loss for  44713 th iteration => 0.00667878568788\n",
      "Loss for  44714 th iteration => 0.0066786078737\n",
      "Loss for  44715 th iteration => 0.00667844129418\n",
      "Loss for  44716 th iteration => 0.00667829786586\n",
      "Loss for  44717 th iteration => 0.00667815223175\n",
      "Loss for  44718 th iteration => 0.00667800035467\n",
      "Loss for  44719 th iteration => 0.00667780973275\n",
      "Loss for  44720 th iteration => 0.0066776331817\n",
      "Loss for  44721 th iteration => 0.00667747161034\n",
      "Loss for  44722 th iteration => 0.00667738668151\n",
      "Loss for  44723 th iteration => 0.00667714663895\n",
      "Loss for  44724 th iteration => 0.00667705374278\n",
      "Loss for  44725 th iteration => 0.00667683526558\n",
      "Loss for  44726 th iteration => 0.00667666354597\n",
      "Loss for  44727 th iteration => 0.00667652611913\n",
      "Loss for  44728 th iteration => 0.0066763183043\n",
      "Loss for  44729 th iteration => 0.00667621689601\n",
      "Loss for  44730 th iteration => 0.00667599574089\n",
      "Loss for  44731 th iteration => 0.00667589108581\n",
      "Loss for  44732 th iteration => 0.00667568589203\n",
      "Loss for  44733 th iteration => 0.0066755468581\n",
      "Loss for  44734 th iteration => 0.00667537597542\n",
      "Loss for  44735 th iteration => 0.00667520281853\n",
      "Loss for  44736 th iteration => 0.00667506715035\n",
      "Loss for  44737 th iteration => 0.00667491275148\n",
      "Loss for  44738 th iteration => 0.00667471328133\n",
      "Loss for  44739 th iteration => 0.00667457181115\n",
      "Loss for  44740 th iteration => 0.00667440189265\n",
      "Loss for  44741 th iteration => 0.00667429394556\n",
      "Loss for  44742 th iteration => 0.006674105947\n",
      "Loss for  44743 th iteration => 0.00667393422396\n",
      "Loss for  44744 th iteration => 0.00667376253446\n",
      "Loss for  44745 th iteration => 0.0066735889881\n",
      "Loss for  44746 th iteration => 0.00667345373013\n",
      "Loss for  44747 th iteration => 0.00667324779669\n",
      "Loss for  44748 th iteration => 0.00667308538006\n",
      "Loss for  44749 th iteration => 0.00667292297431\n",
      "Loss for  44750 th iteration => 0.0066727605794\n",
      "Loss for  44751 th iteration => 0.00667259819529\n",
      "Loss for  44752 th iteration => 0.00667243849221\n",
      "Loss for  44753 th iteration => 0.00667233730653\n",
      "Loss for  44754 th iteration => 0.00667212899012\n",
      "Loss for  44755 th iteration => 0.0066719934037\n",
      "Loss for  44756 th iteration => 0.00667181942024\n",
      "Loss for  44757 th iteration => 0.00667164968917\n",
      "Loss for  44758 th iteration => 0.00667151198112\n",
      "Loss for  44759 th iteration => 0.0066713604795\n",
      "Loss for  44760 th iteration => 0.00667121507031\n",
      "Loss for  44761 th iteration => 0.00667101967717\n",
      "Loss for  44762 th iteration => 0.00667084819888\n",
      "Loss for  44763 th iteration => 0.00667074704814\n",
      "Loss for  44764 th iteration => 0.00667055255327\n",
      "Loss for  44765 th iteration => 0.00667038609401\n",
      "Loss for  44766 th iteration => 0.00667025665222\n",
      "Loss for  44767 th iteration => 0.00667004004309\n",
      "Loss for  44768 th iteration => 0.00666990077035\n",
      "Loss for  44769 th iteration => 0.00666969542313\n",
      "Loss for  44770 th iteration => 0.00666959218075\n",
      "Loss for  44771 th iteration => 0.00666937156787\n",
      "Loss for  44772 th iteration => 0.00666926887847\n",
      "Loss for  44773 th iteration => 0.00666906235079\n",
      "Loss for  44774 th iteration => 0.00666892527453\n",
      "Loss for  44775 th iteration => 0.00666875306593\n",
      "Loss for  44776 th iteration => 0.00666858185887\n",
      "Loss for  44777 th iteration => 0.00666844447991\n",
      "Loss for  44778 th iteration => 0.00666829207671\n",
      "Loss for  44779 th iteration => 0.006668091903\n",
      "Loss for  44780 th iteration => 0.0066679517623\n",
      "Loss for  44781 th iteration => 0.00666778055578\n",
      "Loss for  44782 th iteration => 0.00666767541671\n",
      "Loss for  44783 th iteration => 0.00666748524075\n",
      "Loss for  44784 th iteration => 0.0066673156988\n",
      "Loss for  44785 th iteration => 0.00666714284649\n",
      "Loss for  44786 th iteration => 0.006666971089\n",
      "Loss for  44787 th iteration => 0.00666683467145\n",
      "Loss for  44788 th iteration => 0.00666662963883\n",
      "Loss for  44789 th iteration => 0.00666646753704\n",
      "Loss for  44790 th iteration => 0.00666630544612\n",
      "Loss for  44791 th iteration => 0.00666614336604\n",
      "Loss for  44792 th iteration => 0.00666598129675\n",
      "Loss for  44793 th iteration => 0.00666582156959\n",
      "Loss for  44794 th iteration => 0.0066657216164\n",
      "Loss for  44795 th iteration => 0.00666551269494\n",
      "Loss for  44796 th iteration => 0.00666537834181\n",
      "Loss for  44797 th iteration => 0.00666520375247\n",
      "Loss for  44798 th iteration => 0.00666503525559\n",
      "Loss for  44799 th iteration => 0.00666489681311\n",
      "Loss for  44800 th iteration => 0.00666474697663\n",
      "Loss for  44801 th iteration => 0.00666460052737\n",
      "Loss for  44802 th iteration => 0.00666440621954\n",
      "Loss for  44803 th iteration => 0.00666423436464\n",
      "Loss for  44804 th iteration => 0.00666413470806\n",
      "Loss for  44805 th iteration => 0.00666393934177\n",
      "Loss for  44806 th iteration => 0.00666377438367\n",
      "Loss for  44807 th iteration => 0.00666364406364\n",
      "Loss for  44808 th iteration => 0.00666342865417\n",
      "Loss for  44809 th iteration => 0.00666328893764\n",
      "Loss for  44810 th iteration => 0.00666308466452\n",
      "Loss for  44811 th iteration => 0.0066629809732\n",
      "Loss for  44812 th iteration => 0.00666276085698\n",
      "Loss for  44813 th iteration => 0.00666265884654\n",
      "Loss for  44814 th iteration => 0.00666245226398\n",
      "Loss for  44815 th iteration => 0.00666231587412\n",
      "Loss for  44816 th iteration => 0.00666214360318\n",
      "Loss for  44817 th iteration => 0.00666197308999\n",
      "Loss for  44818 th iteration => 0.00666183571961\n",
      "Loss for  44819 th iteration => 0.00666168370869\n",
      "Loss for  44820 th iteration => 0.00666148386028\n",
      "Loss for  44821 th iteration => 0.00666134402615\n",
      "Loss for  44822 th iteration => 0.00666117313498\n",
      "Loss for  44823 th iteration => 0.00666106850292\n",
      "Loss for  44824 th iteration => 0.00666087843637\n",
      "Loss for  44825 th iteration => 0.00666070934551\n",
      "Loss for  44826 th iteration => 0.00666053659643\n",
      "Loss for  44827 th iteration => 0.0066603653694\n",
      "Loss for  44828 th iteration => 0.00666022904301\n",
      "Loss for  44829 th iteration => 0.00666002434254\n",
      "Loss for  44830 th iteration => 0.00665986255519\n",
      "Loss for  44831 th iteration => 0.00665970077872\n",
      "Loss for  44832 th iteration => 0.00665953901308\n",
      "Loss for  44833 th iteration => 0.00665937725823\n",
      "Loss for  44834 th iteration => 0.00665921801687\n",
      "Loss for  44835 th iteration => 0.00665911817024\n",
      "Loss for  44836 th iteration => 0.0066589097627\n",
      "Loss for  44837 th iteration => 0.00665877553044\n",
      "Loss for  44838 th iteration => 0.00665860144074\n",
      "Loss for  44839 th iteration => 0.00665843307895\n",
      "Loss for  44840 th iteration => 0.00665829540329\n",
      "Loss for  44841 th iteration => 0.00665814524206\n",
      "Loss for  44842 th iteration => 0.00665799973016\n",
      "Loss for  44843 th iteration => 0.0066578051272\n",
      "Loss for  44844 th iteration => 0.00665763429264\n",
      "Loss for  44845 th iteration => 0.00665753411998\n",
      "Loss for  44846 th iteration => 0.00665733987989\n",
      "Loss for  44847 th iteration => 0.00665717443877\n",
      "Loss for  44848 th iteration => 0.00665704521218\n",
      "Loss for  44849 th iteration => 0.00665682951256\n",
      "Loss for  44850 th iteration => 0.00665669044476\n",
      "Loss for  44851 th iteration => 0.00665648615951\n",
      "Loss for  44852 th iteration => 0.00665638309859\n",
      "Loss for  44853 th iteration => 0.00665616344515\n",
      "Loss for  44854 th iteration => 0.00665606110741\n",
      "Loss for  44855 th iteration => 0.00665585546982\n",
      "Loss for  44856 th iteration => 0.00665571877217\n",
      "Loss for  44857 th iteration => 0.00665554742674\n",
      "Loss for  44858 th iteration => 0.00665537662509\n",
      "Loss for  44859 th iteration => 0.00665524060136\n",
      "Loss for  44860 th iteration => 0.00665508773821\n",
      "Loss for  44861 th iteration => 0.00665488901487\n",
      "Loss for  44862 th iteration => 0.00665474869177\n",
      "Loss for  44863 th iteration => 0.00665457935823\n",
      "Loss for  44864 th iteration => 0.00665447344156\n",
      "Loss for  44865 th iteration => 0.00665428526461\n",
      "Loss for  44866 th iteration => 0.00665411528136\n",
      "Loss for  44867 th iteration => 0.00665394362202\n",
      "Loss for  44868 th iteration => 0.00665377194447\n",
      "Loss for  44869 th iteration => 0.00665363668387\n",
      "Loss for  44870 th iteration => 0.00665343187738\n",
      "Loss for  44871 th iteration => 0.00665327040399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  44872 th iteration => 0.00665310894146\n",
      "Loss for  44873 th iteration => 0.00665294748976\n",
      "Loss for  44874 th iteration => 0.00665278604883\n",
      "Loss for  44875 th iteration => 0.00665262768711\n",
      "Loss for  44876 th iteration => 0.00665252706601\n",
      "Loss for  44877 th iteration => 0.00665232004762\n",
      "Loss for  44878 th iteration => 0.00665218506571\n",
      "Loss for  44879 th iteration => 0.0066520123404\n",
      "Loss for  44880 th iteration => 0.00665184341128\n",
      "Loss for  44881 th iteration => 0.00665176010757\n",
      "Loss for  44882 th iteration => 0.0066515142065\n",
      "Loss for  44883 th iteration => 0.00665143652376\n",
      "Loss for  44884 th iteration => 0.00665120363767\n",
      "Loss for  44885 th iteration => 0.0066510439048\n",
      "Loss for  44886 th iteration => 0.00665096068296\n",
      "Loss for  44887 th iteration => 0.00665072143332\n",
      "Loss for  44888 th iteration => 0.00665063025355\n",
      "Loss for  44889 th iteration => 0.00665041178588\n",
      "Loss for  44890 th iteration => 0.00665024274634\n",
      "Loss for  44891 th iteration => 0.00665010512978\n",
      "Loss for  44892 th iteration => 0.00664990003288\n",
      "Loss for  44893 th iteration => 0.00664979839707\n",
      "Loss for  44894 th iteration => 0.00664957914206\n",
      "Loss for  44895 th iteration => 0.00664947581946\n",
      "Loss for  44896 th iteration => 0.00664927177991\n",
      "Loss for  44897 th iteration => 0.00664913412394\n",
      "Loss for  44898 th iteration => 0.00664896435008\n",
      "Loss for  44899 th iteration => 0.0066487926164\n",
      "Loss for  44900 th iteration => 0.00664865866471\n",
      "Loss for  44901 th iteration => 0.00664850444406\n",
      "Loss for  44902 th iteration => 0.00664830706715\n",
      "Loss for  44903 th iteration => 0.00664816603528\n",
      "Loss for  44904 th iteration => 0.00664799880708\n",
      "Loss for  44905 th iteration => 0.00664789063953\n",
      "Loss for  44906 th iteration => 0.00664770531115\n",
      "Loss for  44907 th iteration => 0.00664753361083\n",
      "Loss for  44908 th iteration => 0.00664736378155\n",
      "Loss for  44909 th iteration => 0.00664719091571\n",
      "Loss for  44910 th iteration => 0.00664705745414\n",
      "Loss for  44911 th iteration => 0.00664685221778\n",
      "Loss for  44912 th iteration => 0.00664669105768\n",
      "Loss for  44913 th iteration => 0.00664652990844\n",
      "Loss for  44914 th iteration => 0.00664636876999\n",
      "Loss for  44915 th iteration => 0.0066462082313\n",
      "Loss for  44916 th iteration => 0.00664611185922\n",
      "Loss for  44917 th iteration => 0.00664590121352\n",
      "Loss for  44918 th iteration => 0.00664577044132\n",
      "Loss for  44919 th iteration => 0.00664559412785\n",
      "Loss for  44920 th iteration => 0.00664542921201\n",
      "Loss for  44921 th iteration => 0.00664528791649\n",
      "Loss for  44922 th iteration => 0.00664514316428\n",
      "Loss for  44923 th iteration => 0.00664499348108\n",
      "Loss for  44924 th iteration => 0.006644803349\n",
      "Loss for  44925 th iteration => 0.00664462942916\n",
      "Loss for  44926 th iteration => 0.00664453422061\n",
      "Loss for  44927 th iteration => 0.00664433624953\n",
      "Loss for  44928 th iteration => 0.00664417575827\n",
      "Loss for  44929 th iteration => 0.00664404281516\n",
      "Loss for  44930 th iteration => 0.00664383150195\n",
      "Loss for  44931 th iteration => 0.00664369067448\n",
      "Loss for  44932 th iteration => 0.0066434893754\n",
      "Loss for  44933 th iteration => 0.00664338456007\n",
      "Loss for  44934 th iteration => 0.00664316611521\n",
      "Loss for  44935 th iteration => 0.00664300823492\n",
      "Loss for  44936 th iteration => 0.0066429025286\n",
      "Loss for  44937 th iteration => 0.00664270147752\n",
      "Loss for  44938 th iteration => 0.00664256148189\n",
      "Loss for  44939 th iteration => 0.00664239465257\n",
      "Loss for  44940 th iteration => 0.00664222083403\n",
      "Loss for  44941 th iteration => 0.00664213866539\n",
      "Loss for  44942 th iteration => 0.00664189665385\n",
      "Loss for  44943 th iteration => 0.00664175823756\n",
      "Loss for  44944 th iteration => 0.00664158740785\n",
      "Loss for  44945 th iteration => 0.00664142353022\n",
      "Loss for  44946 th iteration => 0.00664133920513\n",
      "Loss for  44947 th iteration => 0.0066411029391\n",
      "Loss for  44948 th iteration => 0.00664096395855\n",
      "Loss for  44949 th iteration => 0.00664079729182\n",
      "Loss for  44950 th iteration => 0.0066406219142\n",
      "Loss for  44951 th iteration => 0.00664049156674\n",
      "Loss for  44952 th iteration => 0.00664028529983\n",
      "Loss for  44953 th iteration => 0.00664012445276\n",
      "Loss for  44954 th iteration => 0.00663996361651\n",
      "Loss for  44955 th iteration => 0.00663980279105\n",
      "Loss for  44956 th iteration => 0.00663964399295\n",
      "Loss for  44957 th iteration => 0.00663954493614\n",
      "Loss for  44958 th iteration => 0.00663933757801\n",
      "Loss for  44959 th iteration => 0.0066392041677\n",
      "Loss for  44960 th iteration => 0.0066390310953\n",
      "Loss for  44961 th iteration => 0.00663886358749\n",
      "Loss for  44962 th iteration => 0.00663872667098\n",
      "Loss for  44963 th iteration => 0.00663857687678\n",
      "Loss for  44964 th iteration => 0.0066384328175\n",
      "Loss for  44965 th iteration => 0.00663823919268\n",
      "Loss for  44966 th iteration => 0.00663806950008\n",
      "Loss for  44967 th iteration => 0.00663796906987\n",
      "Loss for  44968 th iteration => 0.00663777689959\n",
      "Loss for  44969 th iteration => 0.00663761128163\n",
      "Loss for  44970 th iteration => 0.00663748404493\n",
      "Loss for  44971 th iteration => 0.00663726891559\n",
      "Loss for  44972 th iteration => 0.00663713138712\n",
      "Loss for  44973 th iteration => 0.00663692743965\n",
      "Loss for  44974 th iteration => 0.00663682587357\n",
      "Loss for  44975 th iteration => 0.00663660763416\n",
      "Loss for  44976 th iteration => 0.00663650469968\n",
      "Loss for  44977 th iteration => 0.00663630148932\n",
      "Loss for  44978 th iteration => 0.00663616424362\n",
      "Loss for  44979 th iteration => 0.00663599527682\n",
      "Loss for  44980 th iteration => 0.0066358239755\n",
      "Loss for  44981 th iteration => 0.00663569056944\n",
      "Loss for  44982 th iteration => 0.00663553691251\n",
      "Loss for  44983 th iteration => 0.00663534072433\n",
      "Loss for  44984 th iteration => 0.00663519974265\n",
      "Loss for  44985 th iteration => 0.00663503327968\n",
      "Loss for  44986 th iteration => 0.00663492463941\n",
      "Loss for  44987 th iteration => 0.00663474098215\n",
      "Loss for  44988 th iteration => 0.00663456965405\n",
      "Loss for  44989 th iteration => 0.00663440093351\n",
      "Loss for  44990 th iteration => 0.0066342282027\n",
      "Loss for  44991 th iteration => 0.00663409581862\n",
      "Loss for  44992 th iteration => 0.00663389164036\n",
      "Loss for  44993 th iteration => 0.00663373109507\n",
      "Loss for  44994 th iteration => 0.00663357056062\n",
      "Loss for  44995 th iteration => 0.00663341003696\n",
      "Loss for  44996 th iteration => 0.00663325007468\n",
      "Loss for  44997 th iteration => 0.00663315304744\n",
      "Loss for  44998 th iteration => 0.00663294426759\n",
      "Loss for  44999 th iteration => 0.00663281287493\n",
      "Loss for  45000 th iteration => 0.00663263839267\n",
      "Loss for  45001 th iteration => 0.00663247289085\n",
      "Loss for  45002 th iteration => 0.00663233352554\n",
      "Loss for  45003 th iteration => 0.00663218679959\n",
      "Loss for  45004 th iteration => 0.00663204028202\n",
      "Loss for  45005 th iteration => 0.00663184946991\n",
      "Loss for  45006 th iteration => 0.00663167760126\n",
      "Loss for  45007 th iteration => 0.0066315800583\n",
      "Loss for  45008 th iteration => 0.00663138560851\n",
      "Loss for  45009 th iteration => 0.00663122286333\n",
      "Loss for  45010 th iteration => 0.00663109336168\n",
      "Loss for  45011 th iteration => 0.00663088108403\n",
      "Loss for  45012 th iteration => 0.00663074234677\n",
      "Loss for  45013 th iteration => 0.00663054020643\n",
      "Loss for  45014 th iteration => 0.00663043743878\n",
      "Loss for  45015 th iteration => 0.00663021974321\n",
      "Loss for  45016 th iteration => 0.00663011818312\n",
      "Loss for  45017 th iteration => 0.00662991420246\n",
      "Loss for  45018 th iteration => 0.00662977832708\n",
      "Loss for  45019 th iteration => 0.00662960859401\n",
      "Loss for  45020 th iteration => 0.00662943865912\n",
      "Loss for  45021 th iteration => 0.00662930368799\n",
      "Loss for  45022 th iteration => 0.00662915203281\n",
      "Loss for  45023 th iteration => 0.00662895560254\n",
      "Loss for  45024 th iteration => 0.00662881546443\n",
      "Loss for  45025 th iteration => 0.00662864765323\n",
      "Loss for  45026 th iteration => 0.00662854072465\n",
      "Loss for  45027 th iteration => 0.0066283559562\n",
      "Loss for  45028 th iteration => 0.00662818660341\n",
      "Loss for  45029 th iteration => 0.00662801731228\n",
      "Loss for  45030 th iteration => 0.00662784575445\n",
      "Loss for  45031 th iteration => 0.00662771279904\n",
      "Loss for  45032 th iteration => 0.00662751003287\n",
      "Loss for  45033 th iteration => 0.00662734978915\n",
      "Loss for  45034 th iteration => 0.00662718955628\n",
      "Loss for  45035 th iteration => 0.00662702933419\n",
      "Loss for  45036 th iteration => 0.00662686912286\n",
      "Loss for  45037 th iteration => 0.00662671193456\n",
      "Loss for  45038 th iteration => 0.00662660995632\n",
      "Loss for  45039 th iteration => 0.00662640671621\n",
      "Loss for  45040 th iteration => 0.00662627044721\n",
      "Loss for  45041 th iteration => 0.00662610143025\n",
      "Loss for  45042 th iteration => 0.0066259315142\n",
      "Loss for  45043 th iteration => 0.00662584909364\n",
      "Loss for  45044 th iteration => 0.00662560599704\n",
      "Loss for  45045 th iteration => 0.00662547028684\n",
      "Loss for  45046 th iteration => 0.00662529828855\n",
      "Loss for  45047 th iteration => 0.00662513725621\n",
      "Loss for  45048 th iteration => 0.00662505439539\n",
      "Loss for  45049 th iteration => 0.006624817223\n",
      "Loss for  45050 th iteration => 0.00662472641003\n",
      "Loss for  45051 th iteration => 0.00662451263825\n",
      "Loss for  45052 th iteration => 0.00662434102292\n",
      "Loss for  45053 th iteration => 0.0066242084224\n",
      "Loss for  45054 th iteration => 0.00662400298308\n",
      "Loss for  45055 th iteration => 0.00662384290967\n",
      "Loss for  45056 th iteration => 0.00662368284704\n",
      "Loss for  45057 th iteration => 0.00662352279514\n",
      "Loss for  45058 th iteration => 0.00662336511277\n",
      "Loss for  45059 th iteration => 0.00662326880763\n",
      "Loss for  45060 th iteration => 0.00662306020635\n",
      "Loss for  45061 th iteration => 0.0066229299545\n",
      "Loss for  45062 th iteration => 0.00662280365971\n",
      "Loss for  45063 th iteration => 0.00662260340377\n",
      "Loss for  45064 th iteration => 0.00662248283212\n",
      "Loss for  45065 th iteration => 0.00662227696121\n",
      "Loss for  45066 th iteration => 0.00662216205326\n",
      "Loss for  45067 th iteration => 0.00662196681402\n",
      "Loss for  45068 th iteration => 0.00662177581537\n",
      "Loss for  45069 th iteration => 0.00662168289935\n",
      "Loss for  45070 th iteration => 0.00662147206046\n",
      "Loss for  45071 th iteration => 0.00662134256814\n",
      "Loss for  45072 th iteration => 0.00662116822723\n",
      "Loss for  45073 th iteration => 0.00662100243985\n",
      "Loss for  45074 th iteration => 0.00662086431654\n",
      "Loss for  45075 th iteration => 0.00662066251201\n",
      "Loss for  45076 th iteration => 0.00662056032931\n",
      "Loss for  45077 th iteration => 0.00662034315275\n",
      "Loss for  45078 th iteration => 0.00662024158661\n",
      "Loss for  45079 th iteration => 0.00662003853199\n",
      "Loss for  45080 th iteration => 0.00661990268117\n",
      "Loss for  45081 th iteration => 0.00661973384362\n",
      "Loss for  45082 th iteration => 0.00661956396357\n",
      "Loss for  45083 th iteration => 0.00661943157152\n",
      "Loss for  45084 th iteration => 0.00661927801313\n",
      "Loss for  45085 th iteration => 0.006619082305\n",
      "Loss for  45086 th iteration => 0.00661894239377\n",
      "Loss for  45087 th iteration => 0.00661877768288\n",
      "Loss for  45088 th iteration => 0.00661867020039\n",
      "Loss for  45089 th iteration => 0.00661848688915\n",
      "Loss for  45090 th iteration => 0.00661831554845\n",
      "Loss for  45091 th iteration => 0.00661814748132\n",
      "Loss for  45092 th iteration => 0.00661797565279\n",
      "Loss for  45093 th iteration => 0.00661784388463\n",
      "Loss for  45094 th iteration => 0.00661763922512\n",
      "Loss for  45095 th iteration => 0.00661747945001\n",
      "Loss for  45096 th iteration => 0.00661731968572\n",
      "Loss for  45097 th iteration => 0.0066171599322\n",
      "Loss for  45098 th iteration => 0.00661700227835\n",
      "Loss for  45099 th iteration => 0.00661690536297\n",
      "Loss for  45100 th iteration => 0.00661669798734\n",
      "Loss for  45101 th iteration => 0.00661656674813\n",
      "Loss for  45102 th iteration => 0.0066163936286\n",
      "Loss for  45103 th iteration => 0.00661622832146\n",
      "Loss for  45104 th iteration => 0.00661609163264\n",
      "Loss for  45105 th iteration => 0.00661594441178\n",
      "Loss for  45106 th iteration => 0.00661579988505\n",
      "Loss for  45107 th iteration => 0.00661560767475\n",
      "Loss for  45108 th iteration => 0.00661543910596\n",
      "Loss for  45109 th iteration => 0.00661534047121\n",
      "Loss for  45110 th iteration => 0.00661514860307\n",
      "Loss for  45111 th iteration => 0.00661498485398\n",
      "Loss for  45112 th iteration => 0.00661485784686\n",
      "Loss for  45113 th iteration => 0.00661464394037\n",
      "Loss for  45114 th iteration => 0.00661450704704\n",
      "Loss for  45115 th iteration => 0.00661430462461\n",
      "Loss for  45116 th iteration => 0.00661420364985\n",
      "Loss for  45117 th iteration => 0.00661398694209\n",
      "Loss for  45118 th iteration => 0.006613884503\n",
      "Loss for  45119 th iteration => 0.00661368291098\n",
      "Loss for  45120 th iteration => 0.0066135462099\n",
      "Loss for  45121 th iteration => 0.00661337881231\n",
      "Loss for  45122 th iteration => 0.0066132081045\n",
      "Loss for  45123 th iteration => 0.0066130771321\n",
      "Loss for  45124 th iteration => 0.00661292279997\n",
      "Loss for  45125 th iteration => 0.00661272863649\n",
      "Loss for  45126 th iteration => 0.00661258779126\n",
      "Loss for  45127 th iteration => 0.00661242450196\n",
      "Loss for  45128 th iteration => 0.00661231472916\n",
      "Loss for  45129 th iteration => 0.00661213428309\n",
      "Loss for  45130 th iteration => 0.00661196192097\n",
      "Loss for  45131 th iteration => 0.00661179548625\n",
      "Loss for  45132 th iteration => 0.00661162263977\n",
      "Loss for  45133 th iteration => 0.00661149247685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  45134 th iteration => 0.00661128825591\n",
      "Loss for  45135 th iteration => 0.00661112878116\n",
      "Loss for  45136 th iteration => 0.00661096931722\n",
      "Loss for  45137 th iteration => 0.00661080986404\n",
      "Loss for  45138 th iteration => 0.00661065253824\n",
      "Loss for  45139 th iteration => 0.00661055434493\n",
      "Loss for  45140 th iteration => 0.00661034883417\n",
      "Loss for  45141 th iteration => 0.00661021634453\n",
      "Loss for  45142 th iteration => 0.00661004506242\n",
      "Loss for  45143 th iteration => 0.00660987853214\n",
      "Loss for  45144 th iteration => 0.00660974379809\n",
      "Loss for  45145 th iteration => 0.00660959380366\n",
      "Loss for  45146 th iteration => 0.00660945262346\n",
      "Loss for  45147 th iteration => 0.00660925917955\n",
      "Loss for  45148 th iteration => 0.00660909252556\n",
      "Loss for  45149 th iteration => 0.00660899088263\n",
      "Loss for  45150 th iteration => 0.00660880259297\n",
      "Loss for  45151 th iteration => 0.00660863589729\n",
      "Loss for  45152 th iteration => 0.00660851240747\n",
      "Loss for  45153 th iteration => 0.00660829698762\n",
      "Loss for  45154 th iteration => 0.00660816208915\n",
      "Loss for  45155 th iteration => 0.00660795830725\n",
      "Loss for  45156 th iteration => 0.00660779899438\n",
      "Loss for  45157 th iteration => 0.00660764217018\n",
      "Loss for  45158 th iteration => 0.00660753774781\n",
      "Loss for  45159 th iteration => 0.00660733875219\n",
      "Loss for  45160 th iteration => 0.00660720002274\n",
      "Loss for  45161 th iteration => 0.00660703526645\n",
      "Loss for  45162 th iteration => 0.00660686265922\n",
      "Loss for  45163 th iteration => 0.00660678123229\n",
      "Loss for  45164 th iteration => 0.00660654284087\n",
      "Loss for  45165 th iteration => 0.0066064047338\n",
      "Loss for  45166 th iteration => 0.00660623693009\n",
      "Loss for  45167 th iteration => 0.00660607040076\n",
      "Loss for  45168 th iteration => 0.00660593097754\n",
      "Loss for  45169 th iteration => 0.00660574075388\n",
      "Loss for  45170 th iteration => 0.00660564965877\n",
      "Loss for  45171 th iteration => 0.00660543849859\n",
      "Loss for  45172 th iteration => 0.00660531085029\n",
      "Loss for  45173 th iteration => 0.00660513616502\n",
      "Loss for  45174 th iteration => 0.00660497224466\n",
      "Loss for  45175 th iteration => 0.00660483375406\n",
      "Loss for  45176 th iteration => 0.00660463383941\n",
      "Loss for  45177 th iteration => 0.00660453126658\n",
      "Loss for  45178 th iteration => 0.00660431516525\n",
      "Loss for  45179 th iteration => 0.00660421475648\n",
      "Loss for  45180 th iteration => 0.00660401204213\n",
      "Loss for  45181 th iteration => 0.00660387737598\n",
      "Loss for  45182 th iteration => 0.00660370885144\n",
      "Loss for  45183 th iteration => 0.00660354038421\n",
      "Loss for  45184 th iteration => 0.0066034584468\n",
      "Loss for  45185 th iteration => 0.00660321685483\n",
      "Loss for  45186 th iteration => 0.00660308225472\n",
      "Loss for  45187 th iteration => 0.00660291124288\n",
      "Loss for  45188 th iteration => 0.00660275131709\n",
      "Loss for  45189 th iteration => 0.00660266900061\n",
      "Loss for  45190 th iteration => 0.00660243338579\n",
      "Loss for  45191 th iteration => 0.00660234313027\n",
      "Loss for  45192 th iteration => 0.0066021309894\n",
      "Loss for  45193 th iteration => 0.00660196011294\n",
      "Loss for  45194 th iteration => 0.00660182886075\n",
      "Loss for  45195 th iteration => 0.00660162457563\n",
      "Loss for  45196 th iteration => 0.00660146555966\n",
      "Loss for  45197 th iteration => 0.00660130655444\n",
      "Loss for  45198 th iteration => 0.00660114755995\n",
      "Loss for  45199 th iteration => 0.00660099130331\n",
      "Loss for  45200 th iteration => 0.00660089474734\n",
      "Loss for  45201 th iteration => 0.00660068848102\n",
      "Loss for  45202 th iteration => 0.00660055844032\n",
      "Loss for  45203 th iteration => 0.00660043316438\n",
      "Loss for  45204 th iteration => 0.00660023400362\n",
      "Loss for  45205 th iteration => 0.00660011444026\n",
      "Loss for  45206 th iteration => 0.00659990967487\n",
      "Loss for  45207 th iteration => 0.00659979576483\n",
      "Loss for  45208 th iteration => 0.0065996024494\n",
      "Loss for  45209 th iteration => 0.00659941293432\n",
      "Loss for  45210 th iteration => 0.00659931883599\n",
      "Loss for  45211 th iteration => 0.00659911125523\n",
      "Loss for  45212 th iteration => 0.00659898065091\n",
      "Loss for  45213 th iteration => 0.00659880949801\n",
      "Loss for  45214 th iteration => 0.00659864266833\n",
      "Loss for  45215 th iteration => 0.00659850766353\n",
      "Loss for  45216 th iteration => 0.00659830488579\n",
      "Loss for  45217 th iteration => 0.00659820587275\n",
      "Loss for  45218 th iteration => 0.00659804794449\n",
      "Loss for  45219 th iteration => 0.00659784298558\n",
      "Loss for  45220 th iteration => 0.00659771112642\n",
      "Loss for  45221 th iteration => 0.00659754038348\n",
      "Loss for  45222 th iteration => 0.00659737449643\n",
      "Loss for  45223 th iteration => 0.00659723850992\n",
      "Loss for  45224 th iteration => 0.00659709025653\n",
      "Loss for  45225 th iteration => 0.0065968936246\n",
      "Loss for  45226 th iteration => 0.00659675672778\n",
      "Loss for  45227 th iteration => 0.0065965891506\n",
      "Loss for  45228 th iteration => 0.00659648674071\n",
      "Loss for  45229 th iteration => 0.00659630042683\n",
      "Loss for  45230 th iteration => 0.00659613390017\n",
      "Loss for  45231 th iteration => 0.00659596479889\n",
      "Loss for  45232 th iteration => 0.00659579609974\n",
      "Loss for  45233 th iteration => 0.00659566328021\n",
      "Loss for  45234 th iteration => 0.00659546162771\n",
      "Loss for  45235 th iteration => 0.00659530289801\n",
      "Loss for  45236 th iteration => 0.00659514417913\n",
      "Loss for  45237 th iteration => 0.00659498547102\n",
      "Loss for  45238 th iteration => 0.0065948275372\n",
      "Loss for  45239 th iteration => 0.00659473247723\n",
      "Loss for  45240 th iteration => 0.00659452531977\n",
      "Loss for  45241 th iteration => 0.00659439596252\n",
      "Loss for  45242 th iteration => 0.00659422303463\n",
      "Loss for  45243 th iteration => 0.00659405963601\n",
      "Loss for  45244 th iteration => 0.00659392219881\n",
      "Loss for  45245 th iteration => 0.00659377715779\n",
      "Loss for  45246 th iteration => 0.00659363250669\n",
      "Loss for  45247 th iteration => 0.00659344297954\n",
      "Loss for  45248 th iteration => 0.00659327419758\n",
      "Loss for  45249 th iteration => 0.00659317681431\n",
      "Loss for  45250 th iteration => 0.00659298574179\n",
      "Loss for  45251 th iteration => 0.00659282331767\n",
      "Loss for  45252 th iteration => 0.0065926970335\n",
      "Loss for  45253 th iteration => 0.00659248512371\n",
      "Loss for  45254 th iteration => 0.00659234925745\n",
      "Loss for  45255 th iteration => 0.00659214791498\n",
      "Loss for  45256 th iteration => 0.00659204792634\n",
      "Loss for  45257 th iteration => 0.00659183274082\n",
      "Loss for  45258 th iteration => 0.00659173050524\n",
      "Loss for  45259 th iteration => 0.00659153077323\n",
      "Loss for  45260 th iteration => 0.00659139432176\n",
      "Loss for  45261 th iteration => 0.00659122873815\n",
      "Loss for  45262 th iteration => 0.00659105832583\n",
      "Loss for  45263 th iteration => 0.0065909288233\n",
      "Loss for  45264 th iteration => 0.00659077507292\n",
      "Loss for  45265 th iteration => 0.00659058301344\n",
      "Loss for  45266 th iteration => 0.00659044217181\n",
      "Loss for  45267 th iteration => 0.00659028073561\n",
      "Loss for  45268 th iteration => 0.00659016927068\n",
      "Loss for  45269 th iteration => 0.00658999254501\n",
      "Loss for  45270 th iteration => 0.00658981990909\n",
      "Loss for  45271 th iteration => 0.00658965615888\n",
      "Loss for  45272 th iteration => 0.00658948274497\n",
      "Loss for  45273 th iteration => 0.00658935520479\n",
      "Loss for  45274 th iteration => 0.0065891523161\n",
      "Loss for  45275 th iteration => 0.00658899388548\n",
      "Loss for  45276 th iteration => 0.00658883546566\n",
      "Loss for  45277 th iteration => 0.00658867705656\n",
      "Loss for  45278 th iteration => 0.00658852096751\n",
      "Loss for  45279 th iteration => 0.00658842128974\n",
      "Loss for  45280 th iteration => 0.00658821931627\n",
      "Loss for  45281 th iteration => 0.00658808540838\n",
      "Loss for  45282 th iteration => 0.00658791759751\n",
      "Loss for  45283 th iteration => 0.00658774976358\n",
      "Loss for  45284 th iteration => 0.00658766835221\n",
      "Loss for  45285 th iteration => 0.00658742794634\n",
      "Loss for  45286 th iteration => 0.0065872939523\n",
      "Loss for  45287 th iteration => 0.00658712380763\n",
      "Loss for  45288 th iteration => 0.00658696447628\n",
      "Loss for  45289 th iteration => 0.00658688245706\n",
      "Loss for  45290 th iteration => 0.00658664802321\n",
      "Loss for  45291 th iteration => 0.00658655807512\n",
      "Loss for  45292 th iteration => 0.00658634735702\n",
      "Loss for  45293 th iteration => 0.00658617675139\n",
      "Loss for  45294 th iteration => 0.00658604669471\n",
      "Loss for  45295 th iteration => 0.00658584323753\n",
      "Loss for  45296 th iteration => 0.00658568496521\n",
      "Loss for  45297 th iteration => 0.00658552670364\n",
      "Loss for  45298 th iteration => 0.00658536845277\n",
      "Loss for  45299 th iteration => 0.00658521323656\n",
      "Loss for  45300 th iteration => 0.00658511616371\n",
      "Loss for  45301 th iteration => 0.00658491187855\n",
      "Loss for  45302 th iteration => 0.0065847816182\n",
      "Loss for  45303 th iteration => 0.00658465724023\n",
      "Loss for  45304 th iteration => 0.00658445866912\n",
      "Loss for  45305 th iteration => 0.00658433999543\n",
      "Loss for  45306 th iteration => 0.0065841358278\n",
      "Loss for  45307 th iteration => 0.00658402279931\n",
      "Loss for  45308 th iteration => 0.00658383107091\n",
      "Loss for  45309 th iteration => 0.00658370805845\n",
      "Loss for  45310 th iteration => 0.00658349689632\n",
      "Loss for  45311 th iteration => 0.00658337132193\n",
      "Loss for  45312 th iteration => 0.00658319661157\n",
      "Loss for  45313 th iteration => 0.00658303478831\n",
      "Loss for  45314 th iteration => 0.00658289624946\n",
      "Loss for  45315 th iteration => 0.00658269845515\n",
      "Loss for  45316 th iteration => 0.00658259581084\n",
      "Loss for  45317 th iteration => 0.00658238128147\n",
      "Loss for  45318 th iteration => 0.00658228207524\n",
      "Loss for  45319 th iteration => 0.00658208020407\n",
      "Loss for  45320 th iteration => 0.00658194677014\n",
      "Loss for  45321 th iteration => 0.00658177905913\n",
      "Loss for  45322 th iteration => 0.00658161165277\n",
      "Loss for  45323 th iteration => 0.00658147938862\n",
      "Loss for  45324 th iteration => 0.00658132899273\n",
      "Loss for  45325 th iteration => 0.00658113521832\n",
      "Loss for  45326 th iteration => 0.00658099697289\n",
      "Loss for  45327 th iteration => 0.00658083328635\n",
      "Loss for  45328 th iteration => 0.00658072657332\n",
      "Loss for  45329 th iteration => 0.00658054598546\n",
      "Loss for  45330 th iteration => 0.00658037658506\n",
      "Loss for  45331 th iteration => 0.00658021112536\n",
      "Loss for  45332 th iteration => 0.00658004030261\n",
      "Loss for  45333 th iteration => 0.00657991105799\n",
      "Loss for  45334 th iteration => 0.00657970915001\n",
      "Loss for  45335 th iteration => 0.00657955116298\n",
      "Loss for  45336 th iteration => 0.00657939318673\n",
      "Loss for  45337 th iteration => 0.00657923522123\n",
      "Loss for  45338 th iteration => 0.00657907926637\n",
      "Loss for  45339 th iteration => 0.00657898161545\n",
      "Loss for  45340 th iteration => 0.00657877849952\n",
      "Loss for  45341 th iteration => 0.0065786466186\n",
      "Loss for  45342 th iteration => 0.00657847766514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  45343 th iteration => 0.0065783118095\n",
      "Loss for  45344 th iteration => 0.00657817936047\n",
      "Loss for  45345 th iteration => 0.006578029022\n",
      "Loss for  45346 th iteration => 0.00657789108686\n",
      "Loss for  45347 th iteration => 0.006577698329\n",
      "Loss for  45348 th iteration => 0.00657753460751\n",
      "Loss for  45349 th iteration => 0.00657743121218\n",
      "Loss for  45350 th iteration => 0.00657724756379\n",
      "Loss for  45351 th iteration => 0.00657707931385\n",
      "Loss for  45352 th iteration => 0.0065769120781\n",
      "Loss for  45353 th iteration => 0.00657674338694\n",
      "Loss for  45354 th iteration => 0.00657661229267\n",
      "Loss for  45355 th iteration => 0.00657640952388\n",
      "Loss for  45356 th iteration => 0.0065762518439\n",
      "Loss for  45357 th iteration => 0.00657615126388\n",
      "Loss for  45358 th iteration => 0.00657595138306\n",
      "Loss for  45359 th iteration => 0.00657581645758\n",
      "Loss for  45360 th iteration => 0.00657565085427\n",
      "Loss for  45361 th iteration => 0.00657548184036\n",
      "Loss for  45362 th iteration => 0.00657535025829\n",
      "Loss for  45363 th iteration => 0.0065751524644\n",
      "Loss for  45364 th iteration => 0.00657499544886\n",
      "Loss for  45365 th iteration => 0.00657489287359\n",
      "Loss for  45366 th iteration => 0.00657469252303\n",
      "Loss for  45367 th iteration => 0.00657456146022\n",
      "Loss for  45368 th iteration => 0.00657439026762\n",
      "Loss for  45369 th iteration => 0.00657429961955\n",
      "Loss for  45370 th iteration => 0.00657410357644\n",
      "Loss for  45371 th iteration => 0.0065739478861\n",
      "Loss for  45372 th iteration => 0.00657381663322\n",
      "Loss for  45373 th iteration => 0.00657360926961\n",
      "Loss for  45374 th iteration => 0.00657347119868\n",
      "Loss for  45375 th iteration => 0.00657327383288\n",
      "Loss for  45376 th iteration => 0.00657317163379\n",
      "Loss for  45377 th iteration => 0.0065729576433\n",
      "Loss for  45378 th iteration => 0.00657285861485\n",
      "Loss for  45379 th iteration => 0.0065726574388\n",
      "Loss for  45380 th iteration => 0.00657252420702\n",
      "Loss for  45381 th iteration => 0.00657235716682\n",
      "Loss for  45382 th iteration => 0.00657218998679\n",
      "Loss for  45383 th iteration => 0.00657205904481\n",
      "Loss for  45384 th iteration => 0.0065719078747\n",
      "Loss for  45385 th iteration => 0.00657171524222\n",
      "Loss for  45386 th iteration => 0.00657157675113\n",
      "Loss for  45387 th iteration => 0.00657141495332\n",
      "Loss for  45388 th iteration => 0.0065713073017\n",
      "Loss for  45389 th iteration => 0.00657112850925\n",
      "Loss for  45390 th iteration => 0.00657095822011\n",
      "Loss for  45391 th iteration => 0.00657079387175\n",
      "Loss for  45392 th iteration => 0.00657062283793\n",
      "Loss for  45393 th iteration => 0.00657049467387\n",
      "Loss for  45394 th iteration => 0.00657029247806\n",
      "Loss for  45395 th iteration => 0.0065701349341\n",
      "Loss for  45396 th iteration => 0.0065699774009\n",
      "Loss for  45397 th iteration => 0.00656981987844\n",
      "Loss for  45398 th iteration => 0.00656966518712\n",
      "Loss for  45399 th iteration => 0.00656956707111\n",
      "Loss for  45400 th iteration => 0.00656936528903\n",
      "Loss for  45401 th iteration => 0.00656923297497\n",
      "Loss for  45402 th iteration => 0.00656906532348\n",
      "Loss for  45403 th iteration => 0.00656889972342\n",
      "Loss for  45404 th iteration => 0.00656881860288\n",
      "Loss for  45405 th iteration => 0.0065685784347\n",
      "Loss for  45406 th iteration => 0.00656844551528\n",
      "Loss for  45407 th iteration => 0.00656827653028\n",
      "Loss for  45408 th iteration => 0.00656818120195\n",
      "Loss for  45409 th iteration => 0.00656799043527\n",
      "Loss for  45410 th iteration => 0.00656783000081\n",
      "Loss for  45411 th iteration => 0.00656770408803\n",
      "Loss for  45412 th iteration => 0.00656749431956\n",
      "Loss for  45413 th iteration => 0.00656735940834\n",
      "Loss for  45414 th iteration => 0.00656715942919\n",
      "Loss for  45415 th iteration => 0.00656706042887\n",
      "Loss for  45416 th iteration => 0.00656684555732\n",
      "Loss for  45417 th iteration => 0.00656668818505\n",
      "Loss for  45418 th iteration => 0.00656653155392\n",
      "Loss for  45419 th iteration => 0.00656644055732\n",
      "Loss for  45420 th iteration => 0.00656623192273\n",
      "Loss for  45421 th iteration => 0.00656610679965\n",
      "Loss for  45422 th iteration => 0.0065659344787\n",
      "Loss for  45423 th iteration => 0.00656582032153\n",
      "Loss for  45424 th iteration => 0.00656564744887\n",
      "Loss for  45425 th iteration => 0.00656547083396\n",
      "Loss for  45426 th iteration => 0.00656536253451\n",
      "Loss for  45427 th iteration => 0.00656519510407\n",
      "Loss for  45428 th iteration => 0.00656502181705\n",
      "Loss for  45429 th iteration => 0.00656484909104\n",
      "Loss for  45430 th iteration => 0.0065646878448\n",
      "Loss for  45431 th iteration => 0.00656451430512\n",
      "Loss for  45432 th iteration => 0.00656438920138\n",
      "Loss for  45433 th iteration => 0.00656418753993\n",
      "Loss for  45434 th iteration => 0.0065640302829\n",
      "Loss for  45435 th iteration => 0.00656387303662\n",
      "Loss for  45436 th iteration => 0.00656371580106\n",
      "Loss for  45437 th iteration => 0.00656356134187\n",
      "Loss for  45438 th iteration => 0.0065634605126\n",
      "Loss for  45439 th iteration => 0.00656326199852\n",
      "Loss for  45440 th iteration => 0.00656312701173\n",
      "Loss for  45441 th iteration => 0.0065629625878\n",
      "Loss for  45442 th iteration => 0.00656279437211\n",
      "Loss for  45443 th iteration => 0.00656271380348\n",
      "Loss for  45444 th iteration => 0.00656247664614\n",
      "Loss for  45445 th iteration => 0.00656234141027\n",
      "Loss for  45446 th iteration => 0.00656217521349\n",
      "Loss for  45447 th iteration => 0.00656207447044\n",
      "Loss for  45448 th iteration => 0.00656188965035\n",
      "Loss for  45449 th iteration => 0.00656172497956\n",
      "Loss for  45450 th iteration => 0.00656155742402\n",
      "Loss for  45451 th iteration => 0.00656139044545\n",
      "Loss for  45452 th iteration => 0.00656125909744\n",
      "Loss for  45453 th iteration => 0.00656105932159\n",
      "Loss for  45454 th iteration => 0.00656090220906\n",
      "Loss for  45455 th iteration => 0.00656074510731\n",
      "Loss for  45456 th iteration => 0.0065605880163\n",
      "Loss for  45457 th iteration => 0.00656043218045\n",
      "Loss for  45458 th iteration => 0.00656033725303\n",
      "Loss for  45459 th iteration => 0.00656013315049\n",
      "Loss for  45460 th iteration => 0.00656000400903\n",
      "Loss for  45461 th iteration => 0.006559834053\n",
      "Loss for  45462 th iteration => 0.00655967095284\n",
      "Loss for  45463 th iteration => 0.00655953725738\n",
      "Loss for  45464 th iteration => 0.00655939035985\n",
      "Loss for  45465 th iteration => 0.00655925070897\n",
      "Loss for  45466 th iteration => 0.00655906028892\n",
      "Loss for  45467 th iteration => 0.00655889632439\n",
      "Loss for  45468 th iteration => 0.00655879563214\n",
      "Loss for  45469 th iteration => 0.00655861099913\n",
      "Loss for  45470 th iteration => 0.00655844545545\n",
      "Loss for  45471 th iteration => 0.006558325423\n",
      "Loss for  45472 th iteration => 0.00655811177961\n",
      "Loss for  45473 th iteration => 0.00655798009502\n",
      "Loss for  45474 th iteration => 0.0065577787904\n",
      "Loss for  45475 th iteration => 0.0065576218383\n",
      "Loss for  45476 th iteration => 0.00655746812111\n",
      "Loss for  45477 th iteration => 0.00655736346389\n",
      "Loss for  45478 th iteration => 0.0065571693549\n",
      "Loss for  45479 th iteration => 0.00655703051566\n",
      "Loss for  45480 th iteration => 0.00655687052123\n",
      "Loss for  45481 th iteration => 0.00655669899289\n",
      "Loss for  45482 th iteration => 0.00655661906836\n",
      "Loss for  45483 th iteration => 0.00655638552829\n",
      "Loss for  45484 th iteration => 0.00655624674483\n",
      "Loss for  45485 th iteration => 0.00655608427689\n",
      "Loss for  45486 th iteration => 0.00655591718161\n",
      "Loss for  45487 th iteration => 0.00655578360142\n",
      "Loss for  45488 th iteration => 0.0065556615735\n",
      "Loss for  45489 th iteration => 0.00655545166385\n",
      "Loss for  45490 th iteration => 0.0065553274971\n",
      "Loss for  45491 th iteration => 0.00655515397642\n",
      "Loss for  45492 th iteration => 0.0065549936233\n",
      "Loss for  45493 th iteration => 0.00655485621179\n",
      "Loss for  45494 th iteration => 0.00655465994964\n",
      "Loss for  45495 th iteration => 0.00655455837082\n",
      "Loss for  45496 th iteration => 0.00655434560611\n",
      "Loss for  45497 th iteration => 0.0065542469401\n",
      "Loss for  45498 th iteration => 0.0065540471236\n",
      "Loss for  45499 th iteration => 0.00655391429734\n",
      "Loss for  45500 th iteration => 0.0065537485737\n",
      "Loss for  45501 th iteration => 0.00655358184197\n",
      "Loss for  45502 th iteration => 0.00655345271499\n",
      "Loss for  45503 th iteration => 0.0065533011166\n",
      "Loss for  45504 th iteration => 0.00655311038081\n",
      "Loss for  45505 th iteration => 0.00655297175629\n",
      "Loss for  45506 th iteration => 0.00655281244343\n",
      "Loss for  45507 th iteration => 0.00655270349616\n",
      "Loss for  45508 th iteration => 0.00655252769124\n",
      "Loss for  45509 th iteration => 0.00655235663979\n",
      "Loss for  45510 th iteration => 0.00655219427956\n",
      "Loss for  45511 th iteration => 0.00655202302863\n",
      "Loss for  45512 th iteration => 0.00655189679702\n",
      "Loss for  45513 th iteration => 0.00655169491638\n",
      "Loss for  45514 th iteration => 0.00655153824506\n",
      "Loss for  45515 th iteration => 0.0065513815845\n",
      "Loss for  45516 th iteration => 0.00655122528497\n",
      "Loss for  45517 th iteration => 0.00655113201128\n",
      "Loss for  45518 th iteration => 0.00655092711249\n",
      "Loss for  45519 th iteration => 0.00655079962767\n",
      "Loss for  45520 th iteration => 0.00655062887247\n",
      "Loss for  45521 th iteration => 0.00655046743194\n",
      "Loss for  45522 th iteration => 0.00655033235569\n",
      "Loss for  45523 th iteration => 0.00655018778084\n",
      "Loss for  45524 th iteration => 0.00655004666071\n",
      "Loss for  45525 th iteration => 0.00654985823235\n",
      "Loss for  45526 th iteration => 0.00654969332548\n",
      "Loss for  45527 th iteration => 0.00654959450566\n",
      "Loss for  45528 th iteration => 0.00654940885028\n",
      "Loss for  45529 th iteration => 0.0065492451936\n",
      "Loss for  45530 th iteration => 0.0065491241245\n",
      "Loss for  45531 th iteration => 0.00654891228821\n",
      "Loss for  45532 th iteration => 0.00654878023653\n",
      "Loss for  45533 th iteration => 0.00654858008054\n",
      "Loss for  45534 th iteration => 0.0065484235587\n",
      "Loss for  45535 th iteration => 0.00654826972639\n",
      "Loss for  45536 th iteration => 0.00654816597276\n",
      "Loss for  45537 th iteration => 0.00654797181314\n",
      "Loss for  45538 th iteration => 0.00654783388949\n",
      "Loss for  45539 th iteration => 0.00654767383244\n",
      "Loss for  45540 th iteration => 0.00654750292611\n",
      "Loss for  45541 th iteration => 0.00654742300309\n",
      "Loss for  45542 th iteration => 0.0065471903721\n",
      "Loss for  45543 th iteration => 0.00654705210614\n",
      "Loss for  45544 th iteration => 0.00654688997399\n",
      "Loss for  45545 th iteration => 0.0065467234079\n",
      "Loss for  45546 th iteration => 0.0065465898221\n",
      "Loss for  45547 th iteration => 0.00654646848688\n",
      "Loss for  45548 th iteration => 0.00654625905524\n",
      "Loss for  45549 th iteration => 0.00654613527882\n",
      "Loss for  45550 th iteration => 0.0065459622172\n",
      "Loss for  45551 th iteration => 0.00654580227329\n",
      "Loss for  45552 th iteration => 0.00654566530202\n",
      "Loss for  45553 th iteration => 0.00654546946783\n",
      "Loss for  45554 th iteration => 0.00654536831056\n",
      "Loss for  45555 th iteration => 0.00654515612482\n",
      "Loss for  45556 th iteration => 0.00654505761767\n",
      "Loss for  45557 th iteration => 0.00654485849088\n",
      "Loss for  45558 th iteration => 0.00654472584403\n",
      "Loss for  45559 th iteration => 0.0065445607896\n",
      "Loss for  45560 th iteration => 0.00654439425771\n",
      "Loss for  45561 th iteration => 0.00654426575027\n",
      "Loss for  45562 th iteration => 0.0065441142765\n",
      "Loss for  45563 th iteration => 0.00654392450244\n",
      "Loss for  45564 th iteration => 0.0065437857844\n",
      "Loss for  45565 th iteration => 0.00654362739125\n",
      "Loss for  45566 th iteration => 0.00654351764914\n",
      "Loss for  45567 th iteration => 0.00654334347256\n",
      "Loss for  45568 th iteration => 0.00654317221326\n",
      "Loss for  45569 th iteration => 0.00654301096044\n",
      "Loss for  45570 th iteration => 0.00654283947432\n",
      "Loss for  45571 th iteration => 0.00654271432312\n",
      "Loss for  45572 th iteration => 0.00654251292994\n",
      "Loss for  45573 th iteration => 0.00654235668838\n",
      "Loss for  45574 th iteration => 0.00654220045755\n",
      "Loss for  45575 th iteration => 0.00654204467451\n",
      "Loss for  45576 th iteration => 0.00654195089687\n",
      "Loss for  45577 th iteration => 0.00654174734645\n",
      "Loss for  45578 th iteration => 0.00654161938609\n",
      "Loss for  45579 th iteration => 0.00654144995091\n",
      "Loss for  45580 th iteration => 0.00654128806304\n",
      "Loss for  45581 th iteration => 0.00654115448641\n",
      "Loss for  45582 th iteration => 0.00654100841966\n",
      "Loss for  45583 th iteration => 0.0065408696214\n",
      "Loss for  45584 th iteration => 0.00654068053534\n",
      "Loss for  45585 th iteration => 0.00654051736476\n",
      "Loss for  45586 th iteration => 0.00654041656901\n",
      "Loss for  45587 th iteration => 0.00654023371585\n",
      "Loss for  45588 th iteration => 0.00654006845368\n",
      "Loss for  45589 th iteration => 0.0065399023189\n",
      "Loss for  45590 th iteration => 0.00653973602481\n",
      "Loss for  45591 th iteration => 0.00653960595938\n",
      "Loss for  45592 th iteration => 0.00653940587434\n",
      "Loss for  45593 th iteration => 0.00653924977954\n",
      "Loss for  45594 th iteration => 0.00653909691027\n",
      "Loss for  45595 th iteration => 0.0065389916916\n",
      "Loss for  45596 th iteration => 0.00653879985893\n",
      "Loss for  45597 th iteration => 0.00653866044786\n",
      "Loss for  45598 th iteration => 0.00653850274003\n",
      "Loss for  45599 th iteration => 0.00653832939215\n",
      "Loss for  45600 th iteration => 0.00653820555434\n",
      "Loss for  45601 th iteration => 0.00653800775391\n",
      "Loss for  45602 th iteration => 0.00653790498615\n",
      "Loss for  45603 th iteration => 0.00653770825411\n",
      "Loss for  45604 th iteration => 0.00653757707077\n",
      "Loss for  45605 th iteration => 0.00653740977624\n",
      "Loss for  45606 th iteration => 0.00653731688763\n",
      "Loss for  45607 th iteration => 0.00653712644913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  45608 th iteration => 0.00653696870509\n",
      "Loss for  45609 th iteration => 0.00653684287167\n",
      "Loss for  45610 th iteration => 0.00653663500568\n",
      "Loss for  45611 th iteration => 0.00653650059389\n",
      "Loss for  45612 th iteration => 0.00653630308101\n",
      "Loss for  45613 th iteration => 0.00653620443863\n",
      "Loss for  45614 th iteration => 0.00653599277456\n",
      "Loss for  45615 th iteration => 0.00653589245854\n",
      "Loss for  45616 th iteration => 0.00653569597685\n",
      "Loss for  45617 th iteration => 0.00653556156534\n",
      "Loss for  45618 th iteration => 0.00653539911191\n",
      "Loss for  45619 th iteration => 0.00653523148379\n",
      "Loss for  45620 th iteration => 0.00653515113104\n",
      "Loss for  45621 th iteration => 0.00653491765469\n",
      "Loss for  45622 th iteration => 0.00653478206751\n",
      "Loss for  45623 th iteration => 0.00653461870329\n",
      "Loss for  45624 th iteration => 0.00653451551199\n",
      "Loss for  45625 th iteration => 0.00653433564069\n",
      "Loss for  45626 th iteration => 0.00653417031513\n",
      "Loss for  45627 th iteration => 0.00653400607474\n",
      "Loss for  45628 th iteration => 0.00653383839785\n",
      "Loss for  45629 th iteration => 0.00653371028395\n",
      "Loss for  45630 th iteration => 0.00653351196807\n",
      "Loss for  45631 th iteration => 0.00653335614484\n",
      "Loss for  45632 th iteration => 0.00653320033238\n",
      "Loss for  45633 th iteration => 0.00653304453063\n",
      "Loss for  45634 th iteration => 0.00653289022214\n",
      "Loss for  45635 th iteration => 0.00653279382803\n",
      "Loss for  45636 th iteration => 0.00653259372539\n",
      "Loss for  45637 th iteration => 0.00653246320274\n",
      "Loss for  45638 th iteration => 0.00653229716133\n",
      "Loss for  45639 th iteration => 0.00653213294336\n",
      "Loss for  45640 th iteration => 0.00653205200093\n",
      "Loss for  45641 th iteration => 0.00653181631009\n",
      "Loss for  45642 th iteration => 0.00653168380811\n",
      "Loss for  45643 th iteration => 0.00653151733061\n",
      "Loss for  45644 th iteration => 0.00653135990399\n",
      "Loss for  45645 th iteration => 0.00653127957004\n",
      "Loss for  45646 th iteration => 0.00653104879028\n",
      "Loss for  45647 th iteration => 0.00653091499852\n",
      "Loss for  45648 th iteration => 0.00653075336291\n",
      "Loss for  45649 th iteration => 0.00653058341203\n",
      "Loss for  45650 th iteration => 0.00653045785836\n",
      "Loss for  45651 th iteration => 0.00653025669092\n",
      "Loss for  45652 th iteration => 0.00653010102201\n",
      "Loss for  45653 th iteration => 0.00652994536382\n",
      "Loss for  45654 th iteration => 0.00652979069427\n",
      "Loss for  45655 th iteration => 0.00652969792727\n",
      "Loss for  45656 th iteration => 0.00652949449725\n",
      "Loss for  45657 th iteration => 0.00652936757079\n",
      "Loss for  45658 th iteration => 0.00652919823283\n",
      "Loss for  45659 th iteration => 0.00652903740192\n",
      "Loss for  45660 th iteration => 0.00652890451164\n",
      "Loss for  45661 th iteration => 0.00652875953326\n",
      "Loss for  45662 th iteration => 0.00652862076584\n",
      "Loss for  45663 th iteration => 0.00652843177538\n",
      "Loss for  45664 th iteration => 0.00652826995851\n",
      "Loss for  45665 th iteration => 0.00652816970532\n",
      "Loss for  45666 th iteration => 0.00652798742434\n",
      "Loss for  45667 th iteration => 0.00652782244926\n",
      "Loss for  45668 th iteration => 0.00652770464059\n",
      "Loss for  45669 th iteration => 0.0065274917303\n",
      "Loss for  45670 th iteration => 0.00652736197713\n",
      "Loss for  45671 th iteration => 0.00652716181111\n",
      "Loss for  45672 th iteration => 0.00652700680552\n",
      "Loss for  45673 th iteration => 0.00652690812142\n",
      "Loss for  45674 th iteration => 0.00652671087152\n",
      "Loss for  45675 th iteration => 0.00652657801254\n",
      "Loss for  45676 th iteration => 0.00652641487002\n",
      "Loss for  45677 th iteration => 0.0065262480916\n",
      "Loss for  45678 th iteration => 0.00652612115707\n",
      "Loss for  45679 th iteration => 0.00652596971208\n",
      "Loss for  45680 th iteration => 0.00652578249411\n",
      "Loss for  45681 th iteration => 0.00652564289121\n",
      "Loss for  45682 th iteration => 0.00652548406627\n",
      "Loss for  45683 th iteration => 0.00652531622509\n",
      "Loss for  45684 th iteration => 0.00652518776281\n",
      "Loss for  45685 th iteration => 0.00652506284907\n",
      "Loss for  45686 th iteration => 0.00652485758736\n",
      "Loss for  45687 th iteration => 0.00652473168547\n",
      "Loss for  45688 th iteration => 0.00652456270668\n",
      "Loss for  45689 th iteration => 0.0065244007238\n",
      "Loss for  45690 th iteration => 0.00652426774911\n",
      "Loss for  45691 th iteration => 0.00652406996159\n",
      "Loss for  45692 th iteration => 0.00652397290937\n",
      "Loss for  45693 th iteration => 0.00652381859249\n",
      "Loss for  45694 th iteration => 0.00652361810921\n",
      "Loss for  45695 th iteration => 0.0065234888026\n",
      "Loss for  45696 th iteration => 0.0065233223773\n",
      "Loss for  45697 th iteration => 0.00652315920013\n",
      "Loss for  45698 th iteration => 0.00652302847896\n",
      "Loss for  45699 th iteration => 0.00652288064433\n",
      "Loss for  45700 th iteration => 0.00652269060184\n",
      "Loss for  45701 th iteration => 0.0065225541378\n",
      "Loss for  45702 th iteration => 0.00652239444174\n",
      "Loss for  45703 th iteration => 0.00652228847723\n",
      "Loss for  45704 th iteration => 0.00652211247254\n",
      "Loss for  45705 th iteration => 0.00652194459059\n",
      "Loss for  45706 th iteration => 0.00652178277631\n",
      "Loss for  45707 th iteration => 0.00652161384259\n",
      "Loss for  45708 th iteration => 0.00652148810079\n",
      "Loss for  45709 th iteration => 0.0065212887538\n",
      "Loss for  45710 th iteration => 0.00652113350224\n",
      "Loss for  45711 th iteration => 0.00652097826142\n",
      "Loss for  45712 th iteration => 0.00652082303129\n",
      "Loss for  45713 th iteration => 0.00652067109313\n",
      "Loss for  45714 th iteration => 0.00652057292993\n",
      "Loss for  45715 th iteration => 0.00652037571121\n",
      "Loss for  45716 th iteration => 0.00652024347374\n",
      "Loss for  45717 th iteration => 0.00652008026213\n",
      "Loss for  45718 th iteration => 0.00651991591263\n",
      "Loss for  45719 th iteration => 0.00651983572555\n",
      "Loss for  45720 th iteration => 0.00651960087264\n",
      "Loss for  45721 th iteration => 0.00651946696015\n",
      "Loss for  45722 th iteration => 0.00651930477878\n",
      "Loss for  45723 th iteration => 0.00651920434992\n",
      "Loss for  45724 th iteration => 0.00651902311624\n",
      "Loss for  45725 th iteration => 0.00651885853919\n",
      "Loss for  45726 th iteration => 0.00651869390416\n",
      "Loss for  45727 th iteration => 0.00651852806336\n",
      "Loss for  45728 th iteration => 0.00651839952411\n",
      "Loss for  45729 th iteration => 0.0065182005189\n",
      "Loss for  45730 th iteration => 0.00651804541058\n",
      "Loss for  45731 th iteration => 0.00651789031301\n",
      "Loss for  45732 th iteration => 0.00651773522614\n",
      "Loss for  45733 th iteration => 0.00651758331094\n",
      "Loss for  45734 th iteration => 0.00651748791112\n",
      "Loss for  45735 th iteration => 0.00651728822289\n",
      "Loss for  45736 th iteration => 0.00651715872941\n",
      "Loss for  45737 th iteration => 0.00651699306761\n",
      "Loss for  45738 th iteration => 0.00651683141223\n",
      "Loss for  45739 th iteration => 0.00651675082484\n",
      "Loss for  45740 th iteration => 0.00651651489742\n",
      "Loss for  45741 th iteration => 0.00651643988435\n",
      "Loss for  45742 th iteration => 0.0065162189896\n",
      "Loss for  45743 th iteration => 0.00651612389231\n",
      "Loss for  45744 th iteration => 0.00651593757585\n",
      "Loss for  45745 th iteration => 0.00651577774441\n",
      "Loss for  45746 th iteration => 0.00651565591278\n",
      "Loss for  45747 th iteration => 0.00651544698556\n",
      "Loss for  45748 th iteration => 0.00651531532013\n",
      "Loss for  45749 th iteration => 0.00651511706949\n",
      "Loss for  45750 th iteration => 0.00651502110729\n",
      "Loss for  45751 th iteration => 0.00651481068139\n",
      "Loss for  45752 th iteration => 0.00651470917146\n",
      "Loss for  45753 th iteration => 0.00651451582485\n",
      "Loss for  45754 th iteration => 0.00651438028786\n",
      "Loss for  45755 th iteration => 0.00651422090126\n",
      "Loss for  45756 th iteration => 0.00651405302692\n",
      "Loss for  45757 th iteration => 0.00651397329524\n",
      "Loss for  45758 th iteration => 0.00651374257793\n",
      "Loss for  45759 th iteration => 0.00651360565462\n",
      "Loss for  45760 th iteration => 0.00651344668188\n",
      "Loss for  45761 th iteration => 0.00651333901748\n",
      "Loss for  45762 th iteration => 0.00651316551791\n",
      "Loss for  45763 th iteration => 0.0065129973352\n",
      "Loss for  45764 th iteration => 0.00651283715106\n",
      "Loss for  45765 th iteration => 0.00651266743433\n",
      "Loss for  45766 th iteration => 0.00651254329397\n",
      "Loss for  45767 th iteration => 0.00651234495956\n",
      "Loss for  45768 th iteration => 0.00651219012441\n",
      "Loss for  45769 th iteration => 0.00651203529999\n",
      "Loss for  45770 th iteration => 0.00651188048625\n",
      "Loss for  45771 th iteration => 0.00651172840992\n",
      "Loss for  45772 th iteration => 0.00651162950991\n",
      "Loss for  45773 th iteration => 0.00651143384583\n",
      "Loss for  45774 th iteration => 0.00651130090113\n",
      "Loss for  45775 th iteration => 0.00651113921465\n",
      "Loss for  45776 th iteration => 0.0065109739353\n",
      "Loss for  45777 th iteration => 0.00651089383308\n",
      "Loss for  45778 th iteration => 0.00651066138616\n",
      "Loss for  45779 th iteration => 0.00651052650424\n",
      "Loss for  45780 th iteration => 0.0065103658408\n",
      "Loss for  45781 th iteration => 0.0065102625669\n",
      "Loss for  45782 th iteration => 0.00651008497631\n",
      "Loss for  45783 th iteration => 0.00650991924091\n",
      "Loss for  45784 th iteration => 0.00650975686489\n",
      "Loss for  45785 th iteration => 0.00650958961574\n",
      "Loss for  45786 th iteration => 0.0065094632993\n",
      "Loss for  45787 th iteration => 0.00650926502772\n",
      "Loss for  45788 th iteration => 0.00650911033552\n",
      "Loss for  45789 th iteration => 0.00650895565406\n",
      "Loss for  45790 th iteration => 0.00650880098328\n",
      "Loss for  45791 th iteration => 0.00650864918472\n",
      "Loss for  45792 th iteration => 0.00650855247783\n",
      "Loss for  45793 th iteration => 0.00650835491085\n",
      "Loss for  45794 th iteration => 0.00650822414656\n",
      "Loss for  45795 th iteration => 0.00650806056985\n",
      "Loss for  45796 th iteration => 0.00650789763203\n",
      "Loss for  45797 th iteration => 0.00650781722574\n",
      "Loss for  45798 th iteration => 0.00650758320239\n",
      "Loss for  45799 th iteration => 0.00650745002699\n",
      "Loss for  45800 th iteration => 0.00650728819775\n",
      "Loss for  45801 th iteration => 0.00650718855987\n",
      "Loss for  45802 th iteration => 0.00650700762839\n",
      "Loss for  45803 th iteration => 0.0065068437733\n",
      "Loss for  45804 th iteration => 0.00650667962132\n",
      "Loss for  45805 th iteration => 0.00650651442651\n",
      "Loss for  45806 th iteration => 0.00650638634498\n",
      "Loss for  45807 th iteration => 0.00650618795032\n",
      "Loss for  45808 th iteration => 0.00650603340107\n",
      "Loss for  45809 th iteration => 0.00650587886255\n",
      "Loss for  45810 th iteration => 0.00650572433472\n",
      "Loss for  45811 th iteration => 0.00650557298132\n",
      "Loss for  45812 th iteration => 0.00650547809679\n",
      "Loss for  45813 th iteration => 0.00650527899563\n",
      "Loss for  45814 th iteration => 0.00650515004539\n",
      "Loss for  45815 th iteration => 0.00650498494279\n",
      "Loss for  45816 th iteration => 0.00650482412009\n",
      "Loss for  45817 th iteration => 0.00650474346925\n",
      "Loss for  45818 th iteration => 0.00650450872237\n",
      "Loss for  45819 th iteration => 0.00650443364156\n",
      "Loss for  45820 th iteration => 0.00650421430941\n",
      "Loss for  45821 th iteration => 0.00650411831347\n",
      "Loss for  45822 th iteration => 0.00650393397875\n",
      "Loss for  45823 th iteration => 0.00650377331259\n",
      "Loss for  45824 th iteration => 0.00650365339934\n",
      "Loss for  45825 th iteration => 0.0065034439732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  45826 th iteration => 0.00650331368487\n",
      "Loss for  45827 th iteration => 0.0065031153966\n",
      "Loss for  45828 th iteration => 0.0065029609955\n",
      "Loss for  45829 th iteration => 0.0065028099132\n",
      "Loss for  45830 th iteration => 0.00650270780031\n",
      "Loss for  45831 th iteration => 0.00650251618122\n",
      "Loss for  45832 th iteration => 0.00650238000469\n",
      "Loss for  45833 th iteration => 0.00650222238205\n",
      "Loss for  45834 th iteration => 0.00650205460209\n",
      "Loss for  45835 th iteration => 0.00650197499606\n",
      "Loss for  45836 th iteration => 0.00650174595366\n",
      "Loss for  45837 th iteration => 0.00650160788097\n",
      "Loss for  45838 th iteration => 0.00650144974278\n",
      "Loss for  45839 th iteration => 0.00650128346543\n",
      "Loss for  45840 th iteration => 0.00650115573673\n",
      "Loss for  45841 th iteration => 0.00650103193945\n",
      "Loss for  45842 th iteration => 0.00650082798159\n",
      "Loss for  45843 th iteration => 0.00650070303541\n",
      "Loss for  45844 th iteration => 0.0065005353074\n",
      "Loss for  45845 th iteration => 0.00650037433305\n",
      "Loss for  45846 th iteration => 0.0065002425565\n",
      "Loss for  45847 th iteration => 0.00650004582992\n",
      "Loss for  45848 th iteration => 0.00649994996736\n",
      "Loss for  45849 th iteration => 0.00649979664594\n",
      "Loss for  45850 th iteration => 0.00649959737512\n",
      "Loss for  45851 th iteration => 0.0064994691169\n",
      "Loss for  45852 th iteration => 0.00649930384842\n",
      "Loss for  45853 th iteration => 0.00649914177503\n",
      "Loss for  45854 th iteration => 0.00649901266645\n",
      "Loss for  45855 th iteration => 0.0064988648377\n",
      "Loss for  45856 th iteration => 0.00649867702466\n",
      "Loss for  45857 th iteration => 0.00649854058975\n",
      "Loss for  45858 th iteration => 0.00649838390741\n",
      "Loss for  45859 th iteration => 0.00649827618726\n",
      "Loss for  45860 th iteration => 0.00649810410408\n",
      "Loss for  45861 th iteration => 0.00649793513316\n",
      "Loss for  45862 th iteration => 0.00649777616012\n",
      "Loss for  45863 th iteration => 0.0064976066542\n",
      "Loss for  45864 th iteration => 0.00649748368084\n",
      "Loss for  45865 th iteration => 0.00649728470753\n",
      "Loss for  45866 th iteration => 0.00649713057351\n",
      "Loss for  45867 th iteration => 0.00649697645021\n",
      "Loss for  45868 th iteration => 0.00649682301347\n",
      "Loss for  45869 th iteration => 0.00649673005058\n",
      "Loss for  45870 th iteration => 0.00649652983854\n",
      "Loss for  45871 th iteration => 0.00649640280443\n",
      "Loss for  45872 th iteration => 0.00649623659643\n",
      "Loss for  45873 th iteration => 0.00649607579601\n",
      "Loss for  45874 th iteration => 0.00649599491403\n",
      "Loss for  45875 th iteration => 0.00649576146977\n",
      "Loss for  45876 th iteration => 0.00649563097894\n",
      "Loss for  45877 th iteration => 0.00649546581444\n",
      "Loss for  45878 th iteration => 0.00649531056926\n",
      "Loss for  45879 th iteration => 0.00649523124719\n",
      "Loss for  45880 th iteration => 0.00649500263085\n",
      "Loss for  45881 th iteration => 0.00649491543704\n",
      "Loss for  45882 th iteration => 0.00649471103649\n",
      "Loss for  45883 th iteration => 0.00649454299572\n",
      "Loss for  45884 th iteration => 0.00649441881844\n",
      "Loss for  45885 th iteration => 0.0064942194853\n",
      "Loss for  45886 th iteration => 0.00649406549629\n",
      "Loss for  45887 th iteration => 0.00649391151794\n",
      "Loss for  45888 th iteration => 0.00649375872758\n",
      "Loss for  45889 th iteration => 0.00649366711994\n",
      "Loss for  45890 th iteration => 0.0064934658157\n",
      "Loss for  45891 th iteration => 0.00649334073442\n",
      "Loss for  45892 th iteration => 0.00649321803951\n",
      "Loss for  45893 th iteration => 0.00649302631963\n",
      "Loss for  45894 th iteration => 0.00649290929831\n",
      "Loss for  45895 th iteration => 0.00649271201195\n",
      "Loss for  45896 th iteration => 0.00649260060581\n",
      "Loss for  45897 th iteration => 0.00649241614442\n",
      "Loss for  45898 th iteration => 0.00649229482526\n",
      "Loss for  45899 th iteration => 0.0064920910286\n",
      "Loss for  45900 th iteration => 0.00649196673293\n",
      "Loss for  45901 th iteration => 0.0064917991584\n",
      "Loss for  45902 th iteration => 0.00649163884235\n",
      "Loss for  45903 th iteration => 0.00649150721148\n",
      "Loss for  45904 th iteration => 0.00649131115107\n",
      "Loss for  45905 th iteration => 0.00649121518868\n",
      "Loss for  45906 th iteration => 0.00649100610714\n",
      "Loss for  45907 th iteration => 0.00649090638175\n",
      "Loss for  45908 th iteration => 0.00649071343801\n",
      "Loss for  45909 th iteration => 0.00649057972575\n",
      "Loss for  45910 th iteration => 0.00649042070192\n",
      "Loss for  45911 th iteration => 0.00649025402794\n",
      "Loss for  45912 th iteration => 0.00649017473323\n",
      "Loss for  45913 th iteration => 0.00648994608128\n",
      "Loss for  45914 th iteration => 0.00648981041261\n",
      "Loss for  45915 th iteration => 0.00648965197185\n",
      "Loss for  45916 th iteration => 0.00648954481324\n",
      "Loss for  45917 th iteration => 0.00648937296287\n",
      "Loss for  45918 th iteration => 0.0064892058716\n",
      "Loss for  45919 th iteration => 0.00648904749072\n",
      "Loss for  45920 th iteration => 0.0064888782064\n",
      "Loss for  45921 th iteration => 0.00648875581258\n",
      "Loss for  45922 th iteration => 0.00648855905814\n",
      "Loss for  45923 th iteration => 0.00648840532792\n",
      "Loss for  45924 th iteration => 0.00648825160842\n",
      "Loss for  45925 th iteration => 0.00648809789958\n",
      "Loss for  45926 th iteration => 0.00648794670328\n",
      "Loss for  45927 th iteration => 0.00648784777011\n",
      "Loss for  45928 th iteration => 0.0064876543156\n",
      "Loss for  45929 th iteration => 0.00648752139926\n",
      "Loss for  45930 th iteration => 0.00648736186098\n",
      "Loss for  45931 th iteration => 0.00648719666582\n",
      "Loss for  45932 th iteration => 0.00648711729775\n",
      "Loss for  45933 th iteration => 0.0064868875836\n",
      "Loss for  45934 th iteration => 0.00648675250333\n",
      "Loss for  45935 th iteration => 0.00648659463385\n",
      "Loss for  45936 th iteration => 0.00648648860378\n",
      "Loss for  45937 th iteration => 0.00648631590427\n",
      "Loss for  45938 th iteration => 0.00648614881846\n",
      "Loss for  45939 th iteration => 0.0064859900377\n",
      "Loss for  45940 th iteration => 0.0064858214392\n",
      "Loss for  45941 th iteration => 0.00648569864003\n",
      "Loss for  45942 th iteration => 0.00648550115532\n",
      "Loss for  45943 th iteration => 0.00648534756736\n",
      "Loss for  45944 th iteration => 0.0064851939901\n",
      "Loss for  45945 th iteration => 0.00648504042351\n",
      "Loss for  45946 th iteration => 0.00648489022621\n",
      "Loss for  45947 th iteration => 0.00648479186867\n",
      "Loss for  45948 th iteration => 0.00648459811865\n",
      "Loss for  45949 th iteration => 0.00648446578418\n",
      "Loss for  45950 th iteration => 0.00648430594418\n",
      "Loss for  45951 th iteration => 0.00648414209203\n",
      "Loss for  45952 th iteration => 0.00648406268669\n",
      "Loss for  45953 th iteration => 0.00648383199213\n",
      "Loss for  45954 th iteration => 0.00648369732774\n",
      "Loss for  45955 th iteration => 0.00648354029535\n",
      "Loss for  45956 th iteration => 0.00648343500563\n",
      "Loss for  45957 th iteration => 0.00648326184277\n",
      "Loss for  45958 th iteration => 0.00648309447403\n",
      "Loss for  45959 th iteration => 0.00648293550704\n",
      "Loss for  45960 th iteration => 0.0064827673819\n",
      "Loss for  45961 th iteration => 0.00648264438853\n",
      "Loss for  45962 th iteration => 0.00648244608211\n",
      "Loss for  45963 th iteration => 0.0064822926363\n",
      "Loss for  45964 th iteration => 0.00648213920119\n",
      "Loss for  45965 th iteration => 0.00648198665382\n",
      "Loss for  45966 th iteration => 0.00648189463317\n",
      "Loss for  45967 th iteration => 0.00648169483779\n",
      "Loss for  45968 th iteration => 0.00648156877582\n",
      "Loss for  45969 th iteration => 0.00648140295464\n",
      "Loss for  45970 th iteration => 0.00648124335651\n",
      "Loss for  45971 th iteration => 0.00648116295846\n",
      "Loss for  45972 th iteration => 0.00648093014057\n",
      "Loss for  45973 th iteration => 0.00648085530771\n",
      "Loss for  45974 th iteration => 0.00648063666503\n",
      "Loss for  45975 th iteration => 0.00648054311807\n",
      "Loss for  45976 th iteration => 0.0064803584783\n",
      "Loss for  45977 th iteration => 0.00648020033178\n",
      "Loss for  45978 th iteration => 0.00648008004366\n",
      "Loss for  45979 th iteration => 0.00647987383103\n",
      "Loss for  45980 th iteration => 0.00647974422137\n",
      "Loss for  45981 th iteration => 0.00647954877082\n",
      "Loss for  45982 th iteration => 0.00647939546198\n",
      "Loss for  45983 th iteration => 0.00647924398386\n",
      "Loss for  45984 th iteration => 0.00647914302962\n",
      "Loss for  45985 th iteration => 0.00647895241118\n",
      "Loss for  45986 th iteration => 0.00647881743833\n",
      "Loss for  45987 th iteration => 0.00647866077141\n",
      "Loss for  45988 th iteration => 0.00647849319679\n",
      "Loss for  45989 th iteration => 0.00647841412136\n",
      "Loss for  45990 th iteration => 0.00647818807616\n",
      "Loss for  45991 th iteration => 0.00647805075518\n",
      "Loss for  45992 th iteration => 0.00647789402698\n",
      "Loss for  45993 th iteration => 0.00647772854193\n",
      "Loss for  45994 th iteration => 0.00647760138243\n",
      "Loss for  45995 th iteration => 0.00647747810631\n",
      "Loss for  45996 th iteration => 0.00647727687334\n",
      "Loss for  45997 th iteration => 0.00647715141554\n",
      "Loss for  45998 th iteration => 0.00647698634933\n",
      "Loss for  45999 th iteration => 0.00647682492615\n",
      "Loss for  46000 th iteration => 0.00647669574878\n",
      "Loss for  46001 th iteration => 0.00647649918997\n",
      "Loss for  46002 th iteration => 0.00647634602316\n",
      "Loss for  46003 th iteration => 0.00647619584253\n",
      "Loss for  46004 th iteration => 0.00647609480072\n",
      "Loss for  46005 th iteration => 0.00647590454592\n",
      "Loss for  46006 th iteration => 0.00647576949965\n",
      "Loss for  46007 th iteration => 0.00647561318228\n",
      "Loss for  46008 th iteration => 0.00647544651077\n",
      "Loss for  46009 th iteration => 0.00647536757864\n",
      "Loss for  46010 th iteration => 0.00647514072356\n",
      "Loss for  46011 th iteration => 0.00647500336084\n",
      "Loss for  46012 th iteration => 0.00647484695155\n",
      "Loss for  46013 th iteration => 0.00647468143686\n",
      "Loss for  46014 th iteration => 0.00647455590489\n",
      "Loss for  46015 th iteration => 0.00647443136496\n",
      "Loss for  46016 th iteration => 0.00647423072282\n",
      "Loss for  46017 th iteration => 0.00647410496529\n",
      "Loss for  46018 th iteration => 0.0064739404738\n",
      "Loss for  46019 th iteration => 0.00647377876685\n",
      "Loss for  46020 th iteration => 0.00647365014829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  46021 th iteration => 0.0064734527672\n",
      "Loss for  46022 th iteration => 0.00647336048391\n",
      "Loss for  46023 th iteration => 0.00647320598751\n",
      "Loss for  46024 th iteration => 0.006473009921\n",
      "Loss for  46025 th iteration => 0.00647288096306\n",
      "Loss for  46026 th iteration => 0.00647271881885\n",
      "Loss for  46027 th iteration => 0.0064725562442\n",
      "Loss for  46028 th iteration => 0.00647247647908\n",
      "Loss for  46029 th iteration => 0.00647224712939\n",
      "Loss for  46030 th iteration => 0.00647211492909\n",
      "Loss for  46031 th iteration => 0.00647195413402\n",
      "Loss for  46032 th iteration => 0.00647185316347\n",
      "Loss for  46033 th iteration => 0.00647167675013\n",
      "Loss for  46034 th iteration => 0.00647151385704\n",
      "Loss for  46035 th iteration => 0.00647135356543\n",
      "Loss for  46036 th iteration => 0.00647118782938\n",
      "Loss for  46037 th iteration => 0.00647106351506\n",
      "Loss for  46038 th iteration => 0.00647086811051\n",
      "Loss for  46039 th iteration => 0.00647071519877\n",
      "Loss for  46040 th iteration => 0.00647056229775\n",
      "Loss for  46041 th iteration => 0.0064704094074\n",
      "Loss for  46042 th iteration => 0.00647025862492\n",
      "Loss for  46043 th iteration => 0.00647016283368\n",
      "Loss for  46044 th iteration => 0.00646996786195\n",
      "Loss for  46045 th iteration => 0.00646983810405\n",
      "Loss for  46046 th iteration => 0.00646967703207\n",
      "Loss for  46047 th iteration => 0.00646951494889\n",
      "Loss for  46048 th iteration => 0.00646943537297\n",
      "Loss for  46049 th iteration => 0.0064692055521\n",
      "Loss for  46050 th iteration => 0.00646907264752\n",
      "Loss for  46051 th iteration => 0.00646891444266\n",
      "Loss for  46052 th iteration => 0.00646881169498\n",
      "Loss for  46053 th iteration => 0.00646863732085\n",
      "Loss for  46054 th iteration => 0.00646847225378\n",
      "Loss for  46055 th iteration => 0.00646831316055\n",
      "Loss for  46056 th iteration => 0.00646814652165\n",
      "Loss for  46057 th iteration => 0.00646802338139\n",
      "Loss for  46058 th iteration => 0.00646782653514\n",
      "Loss for  46059 th iteration => 0.00646767376529\n",
      "Loss for  46060 th iteration => 0.00646752100613\n",
      "Loss for  46061 th iteration => 0.00646736842277\n",
      "Loss for  46062 th iteration => 0.00646727775894\n",
      "Loss for  46063 th iteration => 0.00646707794421\n",
      "Loss for  46064 th iteration => 0.00646695326363\n",
      "Loss for  46065 th iteration => 0.0064667873986\n",
      "Loss for  46066 th iteration => 0.00646662895531\n",
      "Loss for  46067 th iteration => 0.00646649956959\n",
      "Loss for  46068 th iteration => 0.00646635457982\n",
      "Loss for  46069 th iteration => 0.00646622146732\n",
      "Loss for  46070 th iteration => 0.00646603400409\n",
      "Loss for  46071 th iteration => 0.00646587840035\n",
      "Loss for  46072 th iteration => 0.00646577395398\n",
      "Loss for  46073 th iteration => 0.00646560148504\n",
      "Loss for  46074 th iteration => 0.0064654344394\n",
      "Loss for  46075 th iteration => 0.0064652763226\n",
      "Loss for  46076 th iteration => 0.00646510904998\n",
      "Loss for  46077 th iteration => 0.00646498678674\n",
      "Loss for  46078 th iteration => 0.00646478865384\n",
      "Loss for  46079 th iteration => 0.00646463770699\n",
      "Loss for  46080 th iteration => 0.00646453689171\n",
      "Loss for  46081 th iteration => 0.00646434748722\n",
      "Loss for  46082 th iteration => 0.00646421263051\n",
      "Loss for  46083 th iteration => 0.00646405720027\n",
      "Loss for  46084 th iteration => 0.00646388855677\n",
      "Loss for  46085 th iteration => 0.00646376684688\n",
      "Loss for  46086 th iteration => 0.00646357415477\n",
      "Loss for  46087 th iteration => 0.00646347285942\n",
      "Loss for  46088 th iteration => 0.00646328149313\n",
      "Loss for  46089 th iteration => 0.00646315192197\n",
      "Loss for  46090 th iteration => 0.00646299054318\n",
      "Loss for  46091 th iteration => 0.00646289520517\n",
      "Loss for  46092 th iteration => 0.00646271393345\n",
      "Loss for  46093 th iteration => 0.00646255410359\n",
      "Loss for  46094 th iteration => 0.00646243707672\n",
      "Loss for  46095 th iteration => 0.00646222919534\n",
      "Loss for  46096 th iteration => 0.0064621019246\n",
      "Loss for  46097 th iteration => 0.00646190590813\n",
      "Loss for  46098 th iteration => 0.00646175341655\n",
      "Loss for  46099 th iteration => 0.00646160414734\n",
      "Loss for  46100 th iteration => 0.0064615023863\n",
      "Loss for  46101 th iteration => 0.00646131417602\n",
      "Loss for  46102 th iteration => 0.00646117845854\n",
      "Loss for  46103 th iteration => 0.0064610241378\n",
      "Loss for  46104 th iteration => 0.00646085724928\n",
      "Loss for  46105 th iteration => 0.00646077865555\n",
      "Loss for  46106 th iteration => 0.00646055388912\n",
      "Loss for  46107 th iteration => 0.00646041562883\n",
      "Loss for  46108 th iteration => 0.00646026144564\n",
      "Loss for  46109 th iteration => 0.0064600950747\n",
      "Loss for  46110 th iteration => 0.00645997236166\n",
      "Loss for  46111 th iteration => 0.00645984579682\n",
      "Loss for  46112 th iteration => 0.0064596481442\n",
      "Loss for  46113 th iteration => 0.00645952077572\n",
      "Loss for  46114 th iteration => 0.00645935921495\n",
      "Loss for  46115 th iteration => 0.00645919595547\n",
      "Loss for  46116 th iteration => 0.00645907020939\n",
      "Loss for  46117 th iteration => 0.00645887260352\n",
      "Loss for  46118 th iteration => 0.00645872161873\n",
      "Loss for  46119 th iteration => 0.00645862477558\n",
      "Loss for  46120 th iteration => 0.00645843192978\n",
      "Loss for  46121 th iteration => 0.00645830108268\n",
      "Loss for  46122 th iteration => 0.00645814217379\n",
      "Loss for  46123 th iteration => 0.00645797863249\n",
      "Loss for  46124 th iteration => 0.00645789935054\n",
      "Loss for  46125 th iteration => 0.00645767266866\n",
      "Loss for  46126 th iteration => 0.00645753835942\n",
      "Loss for  46127 th iteration => 0.00645738050298\n",
      "Loss for  46128 th iteration => 0.00645721804643\n",
      "Loss for  46129 th iteration => 0.00645709000356\n",
      "Loss for  46130 th iteration => 0.00645696951299\n",
      "Loss for  46131 th iteration => 0.00645676750156\n",
      "Loss for  46132 th iteration => 0.00645664472793\n",
      "Loss for  46133 th iteration => 0.00645647885369\n",
      "Loss for  46134 th iteration => 0.00645632014416\n",
      "Loss for  46135 th iteration => 0.00645619012938\n",
      "Loss for  46136 th iteration => 0.00645599575924\n",
      "Loss for  46137 th iteration => 0.00645590132944\n",
      "Loss for  46138 th iteration => 0.00645569391742\n",
      "Loss for  46139 th iteration => 0.00645559565299\n",
      "Loss for  46140 th iteration => 0.0064554044686\n",
      "Loss for  46141 th iteration => 0.00645527230597\n",
      "Loss for  46142 th iteration => 0.00645511495307\n",
      "Loss for  46143 th iteration => 0.00645495155278\n",
      "Loss for  46144 th iteration => 0.00645487263376\n",
      "Loss for  46145 th iteration => 0.00645464552384\n",
      "Loss for  46146 th iteration => 0.00645451035117\n",
      "Loss for  46147 th iteration => 0.00645435717221\n",
      "Loss for  46148 th iteration => 0.00645424810295\n",
      "Loss for  46149 th iteration => 0.00645408132615\n",
      "Loss for  46150 th iteration => 0.00645391195808\n",
      "Loss for  46151 th iteration => 0.00645375752816\n",
      "Loss for  46152 th iteration => 0.0064535876133\n",
      "Loss for  46153 th iteration => 0.00645346905796\n",
      "Loss for  46154 th iteration => 0.00645327154386\n",
      "Loss for  46155 th iteration => 0.00645311944782\n",
      "Loss for  46156 th iteration => 0.00645296736246\n",
      "Loss for  46157 th iteration => 0.0064528167708\n",
      "Loss for  46158 th iteration => 0.00645272294241\n",
      "Loss for  46159 th iteration => 0.00645252760164\n",
      "Loss for  46160 th iteration => 0.00645239983301\n",
      "Loss for  46161 th iteration => 0.00645223836565\n",
      "Loss for  46162 th iteration => 0.00645207804899\n",
      "Loss for  46163 th iteration => 0.00645199845371\n",
      "Loss for  46164 th iteration => 0.00645176967058\n",
      "Loss for  46165 th iteration => 0.00645163799633\n",
      "Loss for  46166 th iteration => 0.00645148003763\n",
      "Loss for  46167 th iteration => 0.00645137863962\n",
      "Loss for  46168 th iteration => 0.00645120448795\n",
      "Loss for  46169 th iteration => 0.00645104063989\n",
      "Loss for  46170 th iteration => 0.00645088219895\n",
      "Loss for  46171 th iteration => 0.00645071653378\n",
      "Loss for  46172 th iteration => 0.00645059400756\n",
      "Loss for  46173 th iteration => 0.00645039813264\n",
      "Loss for  46174 th iteration => 0.00645024616738\n",
      "Loss for  46175 th iteration => 0.00645009421281\n",
      "Loss for  46176 th iteration => 0.0064499424099\n",
      "Loss for  46177 th iteration => 0.00644985240305\n",
      "Loss for  46178 th iteration => 0.00644965351736\n",
      "Loss for  46179 th iteration => 0.00644952953549\n",
      "Loss for  46180 th iteration => 0.00644936455787\n",
      "Loss for  46181 th iteration => 0.00644920695809\n",
      "Loss for  46182 th iteration => 0.00644912678217\n",
      "Loss for  46183 th iteration => 0.00644889670091\n",
      "Loss for  46184 th iteration => 0.00644882207911\n",
      "Loss for  46185 th iteration => 0.00644860649305\n",
      "Loss for  46186 th iteration => 0.00644851231516\n",
      "Loss for  46187 th iteration => 0.00644833118251\n",
      "Loss for  46188 th iteration => 0.0064481726057\n",
      "Loss for  46189 th iteration => 0.00644801029953\n",
      "Loss for  46190 th iteration => 0.00644784878697\n",
      "Loss for  46191 th iteration => 0.00644772235833\n",
      "Loss for  46192 th iteration => 0.00644752810226\n",
      "Loss for  46193 th iteration => 0.00644737627075\n",
      "Loss for  46194 th iteration => 0.00644722667538\n",
      "Loss for  46195 th iteration => 0.00644712529452\n",
      "Loss for  46196 th iteration => 0.00644693803354\n",
      "Loss for  46197 th iteration => 0.00644680266952\n",
      "Loss for  46198 th iteration => 0.00644664932469\n",
      "Loss for  46199 th iteration => 0.00644648023163\n",
      "Loss for  46200 th iteration => 0.00644636054954\n",
      "Loss for  46201 th iteration => 0.00644616896042\n",
      "Loss for  46202 th iteration => 0.00644606660407\n",
      "Loss for  46203 th iteration => 0.00644587788032\n",
      "Loss for  46204 th iteration => 0.00644574729909\n",
      "Loss for  46205 th iteration => 0.00644558974401\n",
      "Loss for  46206 th iteration => 0.00644549055378\n",
      "Loss for  46207 th iteration => 0.00644531467621\n",
      "Loss for  46208 th iteration => 0.00644515150539\n",
      "Loss for  46209 th iteration => 0.00644499217226\n",
      "Loss for  46210 th iteration => 0.00644482799212\n",
      "Loss for  46211 th iteration => 0.00644470449169\n",
      "Loss for  46212 th iteration => 0.0064445080093\n",
      "Loss for  46213 th iteration => 0.00644435727134\n",
      "Loss for  46214 th iteration => 0.00644425955065\n",
      "Loss for  46215 th iteration => 0.00644406890448\n",
      "Loss for  46216 th iteration => 0.00644393716833\n",
      "Loss for  46217 th iteration => 0.00644378047053\n",
      "Loss for  46218 th iteration => 0.00644361497345\n",
      "Loss for  46219 th iteration => 0.00644349197019\n",
      "Loss for  46220 th iteration => 0.00644330081784\n",
      "Loss for  46221 th iteration => 0.00644320124889\n",
      "Loss for  46222 th iteration => 0.00644301000944\n",
      "Loss for  46223 th iteration => 0.00644288219114\n",
      "Loss for  46224 th iteration => 0.00644272100654\n",
      "Loss for  46225 th iteration => 0.0064426278421\n",
      "Loss for  46226 th iteration => 0.00644244622579\n",
      "Loss for  46227 th iteration => 0.00644228863701\n",
      "Loss for  46228 th iteration => 0.00644217119885\n",
      "Loss for  46229 th iteration => 0.00644196492206\n",
      "Loss for  46230 th iteration => 0.00644183809271\n",
      "Loss for  46231 th iteration => 0.00644164296687\n",
      "Loss for  46232 th iteration => 0.00644149140975\n",
      "Loss for  46233 th iteration => 0.00644134306849\n",
      "Loss for  46234 th iteration => 0.00644124280132\n",
      "Loss for  46235 th iteration => 0.00644105494097\n",
      "Loss for  46236 th iteration => 0.00644092076141\n",
      "Loss for  46237 th iteration => 0.00644076684812\n",
      "Loss for  46238 th iteration => 0.00644064590654\n",
      "Loss for  46239 th iteration => 0.00644049120465\n",
      "Loss for  46240 th iteration => 0.00644031564707\n",
      "Loss for  46241 th iteration => 0.00644015776867\n",
      "Loss for  46242 th iteration => 0.00643999692622\n",
      "Loss for  46243 th iteration => 0.00643986774713\n",
      "Loss for  46244 th iteration => 0.00643974964993\n",
      "Loss for  46245 th iteration => 0.00643954783812\n",
      "Loss for  46246 th iteration => 0.00643942646272\n",
      "Loss for  46247 th iteration => 0.00643926075872\n",
      "Loss for  46248 th iteration => 0.00643910347674\n",
      "Loss for  46249 th iteration => 0.00643897360296\n",
      "Loss for  46250 th iteration => 0.00643878068955\n",
      "Loss for  46251 th iteration => 0.00643868637167\n",
      "Loss for  46252 th iteration => 0.00643847990134\n",
      "Loss for  46253 th iteration => 0.00643838287888\n",
      "Loss for  46254 th iteration => 0.00643819201949\n",
      "Loss for  46255 th iteration => 0.00643806113132\n",
      "Loss for  46256 th iteration => 0.00643790407102\n",
      "Loss for  46257 th iteration => 0.00643774150071\n",
      "Loss for  46258 th iteration => 0.00643766246989\n",
      "Loss for  46259 th iteration => 0.00643743754319\n",
      "Loss for  46260 th iteration => 0.00643730281993\n",
      "Loss for  46261 th iteration => 0.00643715024146\n",
      "Loss for  46262 th iteration => 0.00643704099106\n",
      "Loss for  46263 th iteration => 0.00643687593728\n",
      "Loss for  46264 th iteration => 0.00643670726443\n",
      "Loss for  46265 th iteration => 0.00643655421799\n",
      "Loss for  46266 th iteration => 0.00643638452496\n",
      "Loss for  46267 th iteration => 0.00643626730876\n",
      "Loss for  46268 th iteration => 0.0064360711621\n",
      "Loss for  46269 th iteration => 0.00643591985832\n",
      "Loss for  46270 th iteration => 0.0064357685652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  46271 th iteration => 0.00643561833657\n",
      "Loss for  46272 th iteration => 0.0064355245048\n",
      "Loss for  46273 th iteration => 0.00643533072691\n",
      "Loss for  46274 th iteration => 0.00643520300196\n",
      "Loss for  46275 th iteration => 0.00643504305054\n",
      "Loss for  46276 th iteration => 0.00643488280114\n",
      "Loss for  46277 th iteration => 0.00643480329993\n",
      "Loss for  46278 th iteration => 0.00643457715144\n",
      "Loss for  46279 th iteration => 0.00643444493321\n",
      "Loss for  46280 th iteration => 0.00643428912606\n",
      "Loss for  46281 th iteration => 0.00643418531536\n",
      "Loss for  46282 th iteration => 0.0064340151046\n",
      "Loss for  46283 th iteration => 0.00643385027788\n",
      "Loss for  46284 th iteration => 0.00643369444698\n",
      "Loss for  46285 th iteration => 0.00643352778411\n",
      "Loss for  46286 th iteration => 0.00643340780907\n",
      "Loss for  46287 th iteration => 0.00643321275486\n",
      "Loss for  46288 th iteration => 0.00643306158139\n",
      "Loss for  46289 th iteration => 0.0064329104186\n",
      "Loss for  46290 th iteration => 0.00643275948014\n",
      "Loss for  46291 th iteration => 0.00643266835432\n",
      "Loss for  46292 th iteration => 0.00643247214021\n",
      "Loss for  46293 th iteration => 0.00643234709946\n",
      "Loss for  46294 th iteration => 0.0064321847335\n",
      "Loss for  46295 th iteration => 0.0064320265145\n",
      "Loss for  46296 th iteration => 0.00643194661888\n",
      "Loss for  46297 th iteration => 0.00643171942399\n",
      "Loss for  46298 th iteration => 0.00643158931986\n",
      "Loss for  46299 th iteration => 0.00643143090163\n",
      "Loss for  46300 th iteration => 0.00643133164968\n",
      "Loss for  46301 th iteration => 0.00643115715799\n",
      "Loss for  46302 th iteration => 0.00643099551172\n",
      "Loss for  46303 th iteration => 0.00643083738212\n",
      "Loss for  46304 th iteration => 0.0064306732669\n",
      "Loss for  46305 th iteration => 0.00643055101307\n",
      "Loss for  46306 th iteration => 0.00643035682964\n",
      "Loss for  46307 th iteration => 0.00643020578655\n",
      "Loss for  46308 th iteration => 0.00643005475414\n",
      "Loss for  46309 th iteration => 0.00642990373238\n",
      "Loss for  46310 th iteration => 0.00642975575687\n",
      "Loss for  46311 th iteration => 0.00642966083704\n",
      "Loss for  46312 th iteration => 0.0064294686714\n",
      "Loss for  46313 th iteration => 0.00642933989366\n",
      "Loss for  46314 th iteration => 0.00642918155402\n",
      "Loss for  46315 th iteration => 0.00642906991528\n",
      "Loss for  46316 th iteration => 0.00642890693421\n",
      "Loss for  46317 th iteration => 0.00642873602839\n",
      "Loss for  46318 th iteration => 0.00642857537852\n",
      "Loss for  46319 th iteration => 0.00642848020792\n",
      "Loss for  46320 th iteration => 0.00642830190949\n",
      "Loss for  46321 th iteration => 0.00642814313318\n",
      "Loss for  46322 th iteration => 0.00642798287644\n",
      "Loss for  46323 th iteration => 0.00642782113918\n",
      "Loss for  46324 th iteration => 0.00642769677461\n",
      "Loss for  46325 th iteration => 0.00642750338864\n",
      "Loss for  46326 th iteration => 0.0064273524759\n",
      "Loss for  46327 th iteration => 0.00642720157386\n",
      "Loss for  46328 th iteration => 0.00642705068246\n",
      "Loss for  46329 th iteration => 0.00642690237687\n",
      "Loss for  46330 th iteration => 0.00642680933769\n",
      "Loss for  46331 th iteration => 0.00642661555722\n",
      "Loss for  46332 th iteration => 0.00642648864708\n",
      "Loss for  46333 th iteration => 0.00642632867094\n",
      "Loss for  46334 th iteration => 0.0064261706305\n",
      "Loss for  46335 th iteration => 0.00642609117949\n",
      "Loss for  46336 th iteration => 0.00642586383201\n",
      "Loss for  46337 th iteration => 0.00642573212672\n",
      "Loss for  46338 th iteration => 0.0064255782846\n",
      "Loss for  46339 th iteration => 0.00642547525088\n",
      "Loss for  46340 th iteration => 0.00642530504101\n",
      "Loss for  46341 th iteration => 0.00642513953396\n",
      "Loss for  46342 th iteration => 0.0064249838227\n",
      "Loss for  46343 th iteration => 0.0064248178547\n",
      "Loss for  46344 th iteration => 0.00642469797213\n",
      "Loss for  46345 th iteration => 0.00642450166454\n",
      "Loss for  46346 th iteration => 0.00642435089257\n",
      "Loss for  46347 th iteration => 0.00642420013126\n",
      "Loss for  46348 th iteration => 0.00642405134943\n",
      "Loss for  46349 th iteration => 0.00642396048518\n",
      "Loss for  46350 th iteration => 0.0064237647962\n",
      "Loss for  46351 th iteration => 0.0064236400454\n",
      "Loss for  46352 th iteration => 0.00642347817628\n",
      "Loss for  46353 th iteration => 0.00642332183523\n",
      "Loss for  46354 th iteration => 0.00642324207837\n",
      "Loss for  46355 th iteration => 0.00642301396612\n",
      "Loss for  46356 th iteration => 0.00642293974904\n",
      "Loss for  46357 th iteration => 0.00642272863086\n",
      "Loss for  46358 th iteration => 0.00642262985883\n",
      "Loss for  46359 th iteration => 0.00642245560861\n",
      "Loss for  46360 th iteration => 0.00642229316015\n",
      "Loss for  46361 th iteration => 0.0064221352108\n",
      "Loss for  46362 th iteration => 0.00642197177683\n",
      "Loss for  46363 th iteration => 0.00642184960051\n",
      "Loss for  46364 th iteration => 0.00642165419539\n",
      "Loss for  46365 th iteration => 0.00642150473301\n",
      "Loss for  46366 th iteration => 0.00642140729913\n",
      "Loss for  46367 th iteration => 0.00642121843479\n",
      "Loss for  46368 th iteration => 0.00642108704827\n",
      "Loss for  46369 th iteration => 0.00642093206965\n",
      "Loss for  46370 th iteration => 0.00642076698451\n",
      "Loss for  46371 th iteration => 0.0064206456383\n",
      "Loss for  46372 th iteration => 0.00642045599787\n",
      "Loss for  46373 th iteration => 0.00642035588607\n",
      "Loss for  46374 th iteration => 0.00642016726164\n",
      "Loss for  46375 th iteration => 0.00642003895647\n",
      "Loss for  46376 th iteration => 0.00641988110867\n",
      "Loss for  46377 th iteration => 0.00641978546818\n",
      "Loss for  46378 th iteration => 0.00641960835576\n",
      "Loss for  46379 th iteration => 0.00641944843189\n",
      "Loss for  46380 th iteration => 0.00641933535782\n",
      "Loss for  46381 th iteration => 0.00641912773325\n",
      "Loss for  46382 th iteration => 0.00641900380125\n",
      "Loss for  46383 th iteration => 0.00641880904568\n",
      "Loss for  46384 th iteration => 0.00641865916257\n",
      "Loss for  46385 th iteration => 0.00641856377951\n",
      "Loss for  46386 th iteration => 0.00641837310797\n",
      "Loss for  46387 th iteration => 0.00641824381808\n",
      "Loss for  46388 th iteration => 0.0064180869866\n",
      "Loss for  46389 th iteration => 0.00641792520119\n",
      "Loss for  46390 th iteration => 0.00641784603836\n",
      "Loss for  46391 th iteration => 0.00641762388219\n",
      "Loss for  46392 th iteration => 0.00641748963498\n",
      "Loss for  46393 th iteration => 0.00641733535649\n",
      "Loss for  46394 th iteration => 0.00641717304833\n",
      "Loss for  46395 th iteration => 0.00641704895308\n",
      "Loss for  46396 th iteration => 0.00641692712604\n",
      "Loss for  46397 th iteration => 0.00641673007777\n",
      "Loss for  46398 th iteration => 0.00641660608704\n",
      "Loss for  46399 th iteration => 0.00641644504928\n",
      "Loss for  46400 th iteration => 0.0064162852486\n",
      "Loss for  46401 th iteration => 0.00641615994475\n",
      "Loss for  46402 th iteration => 0.00641596565492\n",
      "Loss for  46403 th iteration => 0.00641581557734\n",
      "Loss for  46404 th iteration => 0.00641572175405\n",
      "Loss for  46405 th iteration => 0.00641552978546\n",
      "Loss for  46406 th iteration => 0.00641540204709\n",
      "Loss for  46407 th iteration => 0.00641524392677\n",
      "Loss for  46408 th iteration => 0.00641508343559\n",
      "Loss for  46409 th iteration => 0.00641500413709\n",
      "Loss for  46410 th iteration => 0.00641478128313\n",
      "Loss for  46411 th iteration => 0.00641464830531\n",
      "Loss for  46412 th iteration => 0.0064144930192\n",
      "Loss for  46413 th iteration => 0.00641433197465\n",
      "Loss for  46414 th iteration => 0.00641420669029\n",
      "Loss for  46415 th iteration => 0.00641408641276\n",
      "Loss for  46416 th iteration => 0.00641388831967\n",
      "Loss for  46417 th iteration => 0.00641376562926\n",
      "Loss for  46418 th iteration => 0.00641360355288\n",
      "Loss for  46419 th iteration => 0.0064134450464\n",
      "Loss for  46420 th iteration => 0.00641331871003\n",
      "Loss for  46421 th iteration => 0.00641312473077\n",
      "Loss for  46422 th iteration => 0.00641297460252\n",
      "Loss for  46423 th iteration => 0.00641288202443\n",
      "Loss for  46424 th iteration => 0.00641268907166\n",
      "Loss for  46425 th iteration => 0.00641256257393\n",
      "Loss for  46426 th iteration => 0.006412403474\n",
      "Loss for  46427 th iteration => 0.00641224408573\n",
      "Loss for  46428 th iteration => 0.00641216470346\n",
      "Loss for  46429 th iteration => 0.00641194126464\n",
      "Loss for  46430 th iteration => 0.00641180930503\n",
      "Loss for  46431 th iteration => 0.00641165326113\n",
      "Loss for  46432 th iteration => 0.0064114932319\n",
      "Loss for  46433 th iteration => 0.00641136714802\n",
      "Loss for  46434 th iteration => 0.00641124799454\n",
      "Loss for  46435 th iteration => 0.0064110491678\n",
      "Loss for  46436 th iteration => 0.00641092746851\n",
      "Loss for  46437 th iteration => 0.00641076466102\n",
      "Loss for  46438 th iteration => 0.00641060714317\n",
      "Loss for  46439 th iteration => 0.00641048007818\n",
      "Loss for  46440 th iteration => 0.00641028701609\n",
      "Loss for  46441 th iteration => 0.00641019548624\n",
      "Loss for  46442 th iteration => 0.00641004565462\n",
      "Loss for  46443 th iteration => 0.00640985170144\n",
      "Loss for  46444 th iteration => 0.00640972650788\n",
      "Loss for  46445 th iteration => 0.00640956633789\n",
      "Loss for  46446 th iteration => 0.00640940808462\n",
      "Loss for  46447 th iteration => 0.00640932866386\n",
      "Loss for  46448 th iteration => 0.00640910449305\n",
      "Loss for  46449 th iteration => 0.00640897382395\n",
      "Loss for  46450 th iteration => 0.00640881836443\n",
      "Loss for  46451 th iteration => 0.00640871591054\n",
      "Loss for  46452 th iteration => 0.00640854662196\n",
      "Loss for  46453 th iteration => 0.00640838342418\n",
      "Loss for  46454 th iteration => 0.00640822890171\n",
      "Loss for  46455 th iteration => 0.00640806329498\n",
      "Loss for  46456 th iteration => 0.00640794456782\n",
      "Loss for  46457 th iteration => 0.00640775117668\n",
      "Loss for  46458 th iteration => 0.0064076011717\n",
      "Loss for  46459 th iteration => 0.00640745117738\n",
      "Loss for  46460 th iteration => 0.00640730119368\n",
      "Loss for  46461 th iteration => 0.00640715463145\n",
      "Loss for  46462 th iteration => 0.00640705803334\n",
      "Loss for  46463 th iteration => 0.00640686958004\n",
      "Loss for  46464 th iteration => 0.00640673920589\n",
      "Loss for  46465 th iteration => 0.00640658506664\n",
      "Loss for  46466 th iteration => 0.00640646910243\n",
      "Loss for  46467 th iteration => 0.00640631244109\n",
      "Loss for  46468 th iteration => 0.0064061395658\n",
      "Loss for  46469 th iteration => 0.00640598364446\n",
      "Loss for  46470 th iteration => 0.00640588271669\n",
      "Loss for  46471 th iteration => 0.00640571216076\n",
      "Loss for  46472 th iteration => 0.00640554991863\n",
      "Loss for  46473 th iteration => 0.00640539467569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  46474 th iteration => 0.00640523004796\n",
      "Loss for  46475 th iteration => 0.00640511060021\n",
      "Loss for  46476 th iteration => 0.0064049173839\n",
      "Loss for  46477 th iteration => 0.0064047675086\n",
      "Loss for  46478 th iteration => 0.00640461764397\n",
      "Loss for  46479 th iteration => 0.00640446783273\n",
      "Loss for  46480 th iteration => 0.00640437795075\n",
      "Loss for  46481 th iteration => 0.00640418305225\n",
      "Loss for  46482 th iteration => 0.006404059322\n",
      "Loss for  46483 th iteration => 0.00640389820518\n",
      "Loss for  46484 th iteration => 0.00640374158016\n",
      "Loss for  46485 th iteration => 0.00640366204746\n",
      "Loss for  46486 th iteration => 0.00640343719538\n",
      "Loss for  46487 th iteration => 0.00640330762342\n",
      "Loss for  46488 th iteration => 0.00640315173417\n",
      "Loss for  46489 th iteration => 0.00640305156204\n",
      "Loss for  46490 th iteration => 0.00640288050536\n",
      "Loss for  46491 th iteration => 0.00640271859078\n",
      "Loss for  46492 th iteration => 0.00640256314023\n",
      "Loss for  46493 th iteration => 0.00640239898115\n",
      "Loss for  46494 th iteration => 0.00640227932114\n",
      "Loss for  46495 th iteration => 0.00640208604378\n",
      "Loss for  46496 th iteration => 0.00640193629818\n",
      "Loss for  46497 th iteration => 0.00640178656325\n",
      "Loss for  46498 th iteration => 0.00640163709528\n",
      "Loss for  46499 th iteration => 0.00640154758777\n",
      "Loss for  46500 th iteration => 0.00640135257084\n",
      "Loss for  46501 th iteration => 0.00640122922045\n",
      "Loss for  46502 th iteration => 0.00640106797983\n",
      "Loss for  46503 th iteration => 0.00640091195831\n",
      "Loss for  46504 th iteration => 0.00640083241265\n",
      "Loss for  46505 th iteration => 0.00640060736473\n",
      "Loss for  46506 th iteration => 0.00640047804253\n",
      "Loss for  46507 th iteration => 0.00640032245307\n",
      "Loss for  46508 th iteration => 0.00640022265036\n",
      "Loss for  46509 th iteration => 0.00640005147684\n",
      "Loss for  46510 th iteration => 0.00639988960052\n",
      "Loss for  46511 th iteration => 0.00639973415488\n",
      "Loss for  46512 th iteration => 0.00639957025316\n",
      "Loss for  46513 th iteration => 0.00639945059091\n",
      "Loss for  46514 th iteration => 0.00639925715848\n",
      "Loss for  46515 th iteration => 0.00639910754251\n",
      "Loss for  46516 th iteration => 0.00639895793721\n",
      "Loss for  46517 th iteration => 0.00639880889851\n",
      "Loss for  46518 th iteration => 0.00639871957276\n",
      "Loss for  46519 th iteration => 0.00639852462896\n",
      "Loss for  46520 th iteration => 0.00639840146795\n",
      "Loss for  46521 th iteration => 0.00639824029287\n",
      "Loss for  46522 th iteration => 0.00639808475586\n",
      "Loss for  46523 th iteration => 0.00639800522912\n",
      "Loss for  46524 th iteration => 0.00639778005631\n",
      "Loss for  46525 th iteration => 0.00639765083011\n",
      "Loss for  46526 th iteration => 0.00639749577945\n",
      "Loss for  46527 th iteration => 0.0063973960005\n",
      "Loss for  46528 th iteration => 0.00639722505369\n",
      "Loss for  46529 th iteration => 0.00639706295721\n",
      "Loss for  46530 th iteration => 0.00639690770656\n",
      "Loss for  46531 th iteration => 0.00639674387319\n",
      "Loss for  46532 th iteration => 0.00639662439658\n",
      "Loss for  46533 th iteration => 0.00639643072549\n",
      "Loss for  46534 th iteration => 0.00639628123908\n",
      "Loss for  46535 th iteration => 0.00639613176333\n",
      "Loss for  46536 th iteration => 0.0063959832305\n",
      "Loss for  46537 th iteration => 0.00639589391364\n",
      "Loss for  46538 th iteration => 0.00639569921478\n",
      "Loss for  46539 th iteration => 0.00639557607227\n",
      "Loss for  46540 th iteration => 0.00639541513255\n",
      "Loss for  46541 th iteration => 0.0063952599727\n",
      "Loss for  46542 th iteration => 0.00639518049376\n",
      "Loss for  46543 th iteration => 0.00639495525996\n",
      "Loss for  46544 th iteration => 0.00639482599206\n",
      "Loss for  46545 th iteration => 0.00639467169353\n",
      "Loss for  46546 th iteration => 0.00639457162903\n",
      "Loss for  46547 th iteration => 0.00639440121633\n",
      "Loss for  46548 th iteration => 0.00639423866891\n",
      "Loss for  46549 th iteration => 0.00639408378328\n",
      "Loss for  46550 th iteration => 0.00639391984915\n",
      "Loss for  46551 th iteration => 0.00639380072621\n",
      "Loss for  46552 th iteration => 0.00639360674227\n",
      "Loss for  46553 th iteration => 0.00639345738534\n",
      "Loss for  46554 th iteration => 0.00639330803905\n",
      "Loss for  46555 th iteration => 0.00639316008028\n",
      "Loss for  46556 th iteration => 0.00639307061716\n",
      "Loss for  46557 th iteration => 0.0063928763174\n",
      "Loss for  46558 th iteration => 0.00639275304003\n",
      "Loss for  46559 th iteration => 0.00639259248806\n",
      "Loss for  46560 th iteration => 0.00639243760845\n",
      "Loss for  46561 th iteration => 0.0063923582035\n",
      "Loss for  46562 th iteration => 0.00639213296625\n",
      "Loss for  46563 th iteration => 0.00639200353337\n",
      "Loss for  46564 th iteration => 0.0063918501773\n",
      "Loss for  46565 th iteration => 0.00639174955043\n",
      "Loss for  46566 th iteration => 0.00639157994692\n",
      "Loss for  46567 th iteration => 0.00639141674249\n",
      "Loss for  46568 th iteration => 0.00639126237396\n",
      "Loss for  46569 th iteration => 0.00639109818778\n",
      "Loss for  46570 th iteration => 0.00639097956882\n",
      "Loss for  46571 th iteration => 0.0063907852062\n",
      "Loss for  46572 th iteration => 0.00639063597865\n",
      "Loss for  46573 th iteration => 0.00639048676175\n",
      "Loss for  46574 th iteration => 0.00639033943769\n",
      "Loss for  46575 th iteration => 0.00639024968905\n",
      "Loss for  46576 th iteration => 0.00639005592675\n",
      "Loss for  46577 th iteration => 0.00638993237683\n",
      "Loss for  46578 th iteration => 0.00638977234938\n",
      "Loss for  46579 th iteration => 0.00638961766243\n",
      "Loss for  46580 th iteration => 0.00638953835525\n",
      "Loss for  46581 th iteration => 0.00638931316642\n",
      "Loss for  46582 th iteration => 0.00638918367403\n",
      "Loss for  46583 th iteration => 0.00638909011767\n",
      "Loss for  46584 th iteration => 0.00638888542482\n",
      "Loss for  46585 th iteration => 0.00638878406673\n",
      "Loss for  46586 th iteration => 0.00638858879717\n",
      "Loss for  46587 th iteration => 0.00638843092732\n",
      "Loss for  46588 th iteration => 0.00638830625941\n",
      "Loss for  46589 th iteration => 0.00638811522076\n",
      "Loss for  46590 th iteration => 0.00638796611185\n",
      "Loss for  46591 th iteration => 0.00638781701362\n",
      "Loss for  46592 th iteration => 0.00638766792603\n",
      "Loss for  46593 th iteration => 0.00638752141638\n",
      "Loss for  46594 th iteration => 0.00638743099332\n",
      "Loss for  46595 th iteration => 0.00638723815608\n",
      "Loss for  46596 th iteration => 0.0063871139473\n",
      "Loss for  46597 th iteration => 0.00638695487704\n",
      "Loss for  46598 th iteration => 0.00638684914705\n",
      "Loss for  46599 th iteration => 0.00638668404448\n",
      "Loss for  46600 th iteration => 0.00638651696515\n",
      "Loss for  46601 th iteration => 0.00638635730478\n",
      "Loss for  46602 th iteration => 0.00638626622393\n",
      "Loss for  46603 th iteration => 0.00638608760838\n",
      "Loss for  46604 th iteration => 0.00638593221987\n",
      "Loss for  46605 th iteration => 0.00638581766769\n",
      "Loss for  46606 th iteration => 0.00638561443292\n",
      "Loss for  46607 th iteration => 0.00638549074677\n",
      "Loss for  46608 th iteration => 0.00638529911841\n",
      "Loss for  46609 th iteration => 0.00638515014153\n",
      "Loss for  46610 th iteration => 0.00638500117529\n",
      "Loss for  46611 th iteration => 0.00638485221964\n",
      "Loss for  46612 th iteration => 0.00638470652543\n",
      "Loss for  46613 th iteration => 0.00638461542049\n",
      "Loss for  46614 th iteration => 0.00638442402429\n",
      "Loss for  46615 th iteration => 0.00638434324184\n",
      "Loss for  46616 th iteration => 0.00638415353789\n",
      "Loss for  46617 th iteration => 0.00638401064705\n",
      "Loss for  46618 th iteration => 0.00638388282228\n",
      "Loss for  46619 th iteration => 0.0063836790603\n",
      "Loss for  46620 th iteration => 0.00638355620413\n",
      "Loss for  46621 th iteration => 0.00638343540052\n",
      "Loss for  46622 th iteration => 0.00638324106949\n",
      "Loss for  46623 th iteration => 0.00638311742337\n",
      "Loss for  46624 th iteration => 0.0063829590817\n",
      "Loss for  46625 th iteration => 0.00638279964704\n",
      "Loss for  46626 th iteration => 0.00638267701796\n",
      "Loss for  46627 th iteration => 0.00638248476206\n",
      "Loss for  46628 th iteration => 0.00638233591444\n",
      "Loss for  46629 th iteration => 0.00638218707745\n",
      "Loss for  46630 th iteration => 0.00638203890992\n",
      "Loss for  46631 th iteration => 0.00638195281413\n",
      "Loss for  46632 th iteration => 0.00638175613989\n",
      "Loss for  46633 th iteration => 0.00638163750598\n",
      "Loss for  46634 th iteration => 0.00638151734286\n",
      "Loss for  46635 th iteration => 0.00638133336983\n",
      "Loss for  46636 th iteration => 0.00638121883478\n",
      "Loss for  46637 th iteration => 0.00638102933983\n",
      "Loss for  46638 th iteration => 0.00638092037537\n",
      "Loss for  46639 th iteration => 0.00638074549596\n",
      "Loss for  46640 th iteration => 0.0063806233658\n",
      "Loss for  46641 th iteration => 0.00638043007139\n",
      "Loss for  46642 th iteration => 0.0063803057029\n",
      "Loss for  46643 th iteration => 0.00638014830572\n",
      "Loss for  46644 th iteration => 0.0063799882399\n",
      "Loss for  46645 th iteration => 0.00637986646439\n",
      "Loss for  46646 th iteration => 0.0063796736882\n",
      "Loss for  46647 th iteration => 0.00637952603527\n",
      "Loss for  46648 th iteration => 0.00637943082821\n",
      "Loss for  46649 th iteration => 0.00637924350441\n",
      "Loss for  46650 th iteration => 0.00637911449841\n",
      "Loss for  46651 th iteration => 0.0063789609071\n",
      "Loss for  46652 th iteration => 0.00637880005614\n",
      "Loss for  46653 th iteration => 0.00637872227631\n",
      "Loss for  46654 th iteration => 0.00637850330956\n",
      "Loss for  46655 th iteration => 0.00637836918813\n",
      "Loss for  46656 th iteration => 0.00637821831487\n",
      "Loss for  46657 th iteration => 0.00637805622607\n",
      "Loss for  46658 th iteration => 0.00637793662473\n",
      "Loss for  46659 th iteration => 0.00637781277525\n",
      "Loss for  46660 th iteration => 0.00637762083297\n",
      "Loss for  46661 th iteration => 0.00637749538128\n",
      "Loss for  46662 th iteration => 0.00637733931439\n",
      "Loss for  46663 th iteration => 0.00637717818704\n",
      "Loss for  46664 th iteration => 0.00637705772023\n",
      "Loss for  46665 th iteration => 0.00637686420448\n",
      "Loss for  46666 th iteration => 0.00637671747303\n",
      "Loss for  46667 th iteration => 0.00637662133651\n",
      "Loss for  46668 th iteration => 0.00637643518974\n",
      "Loss for  46669 th iteration => 0.00637630527484\n",
      "Loss for  46670 th iteration => 0.00637615284006\n",
      "Loss for  46671 th iteration => 0.0063759917546\n",
      "Loss for  46672 th iteration => 0.00637591424251\n",
      "Loss for  46673 th iteration => 0.00637569549737\n",
      "Loss for  46674 th iteration => 0.00637556065079\n",
      "Loss for  46675 th iteration => 0.00637541075195\n",
      "Loss for  46676 th iteration => 0.00637524845472\n",
      "Loss for  46677 th iteration => 0.00637519060498\n",
      "Loss for  46678 th iteration => 0.00637495880147\n",
      "Loss for  46679 th iteration => 0.00637483882355\n",
      "Loss for  46680 th iteration => 0.00637467754538\n",
      "Loss for  46681 th iteration => 0.00637452183416\n",
      "Loss for  46682 th iteration => 0.00637439621353\n",
      "Loss for  46683 th iteration => 0.00637420561393\n",
      "Loss for  46684 th iteration => 0.00637405714553\n",
      "Loss for  46685 th iteration => 0.00637391148162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  46686 th iteration => 0.00637381409778\n",
      "Loss for  46687 th iteration => 0.00637362944451\n",
      "Loss for  46688 th iteration => 0.00637349830572\n",
      "Loss for  46689 th iteration => 0.0063733475814\n",
      "Loss for  46690 th iteration => 0.00637322862172\n",
      "Loss for  46691 th iteration => 0.00637307794872\n",
      "Loss for  46692 th iteration => 0.00637290501805\n",
      "Loss for  46693 th iteration => 0.00637275123384\n",
      "Loss for  46694 th iteration => 0.00637259253631\n",
      "Loss for  46695 th iteration => 0.00637246834186\n",
      "Loss for  46696 th iteration => 0.00637234982124\n",
      "Loss for  46697 th iteration => 0.00637215453545\n",
      "Loss for  46698 th iteration => 0.00637203290498\n",
      "Loss for  46699 th iteration => 0.00637187352287\n",
      "Loss for  46700 th iteration => 0.00637171618877\n",
      "Loss for  46701 th iteration => 0.00637159243462\n",
      "Loss for  46702 th iteration => 0.00637140084094\n",
      "Loss for  46703 th iteration => 0.00637125291262\n",
      "Loss for  46704 th iteration => 0.00637116007087\n",
      "Loss for  46705 th iteration => 0.00637097113322\n",
      "Loss for  46706 th iteration => 0.00637084448991\n",
      "Loss for  46707 th iteration => 0.00637068928737\n",
      "Loss for  46708 th iteration => 0.00637053022926\n",
      "Loss for  46709 th iteration => 0.00637045254533\n",
      "Loss for  46710 th iteration => 0.00637023289914\n",
      "Loss for  46711 th iteration => 0.00637010085814\n",
      "Loss for  46712 th iteration => 0.00636994865521\n",
      "Loss for  46713 th iteration => 0.00636978864649\n",
      "Loss for  46714 th iteration => 0.00636966727923\n",
      "Loss for  46715 th iteration => 0.00636954591982\n",
      "Loss for  46716 th iteration => 0.00636935280461\n",
      "Loss for  46717 th iteration => 0.00636922927756\n",
      "Loss for  46718 th iteration => 0.00636907203466\n",
      "Loss for  46719 th iteration => 0.00636891283509\n",
      "Loss for  46720 th iteration => 0.00636879118915\n",
      "Loss for  46721 th iteration => 0.00636859849154\n",
      "Loss for  46722 th iteration => 0.00636845182179\n",
      "Loss for  46723 th iteration => 0.00636835731016\n",
      "Loss for  46724 th iteration => 0.00636817028589\n",
      "Loss for  46725 th iteration => 0.0063680420018\n",
      "Loss for  46726 th iteration => 0.00636788868361\n",
      "Loss for  46727 th iteration => 0.00636772894045\n",
      "Loss for  46728 th iteration => 0.00636765164439\n",
      "Loss for  46729 th iteration => 0.00636743248887\n",
      "Loss for  46730 th iteration => 0.00636729912933\n",
      "Loss for  46731 th iteration => 0.00636714849092\n",
      "Loss for  46732 th iteration => 0.00636698757758\n",
      "Loss for  46733 th iteration => 0.00636692981189\n",
      "Loss for  46734 th iteration => 0.00636669785046\n",
      "Loss for  46735 th iteration => 0.00636657908922\n",
      "Loss for  46736 th iteration => 0.00636641733898\n",
      "Loss for  46737 th iteration => 0.00636626285588\n",
      "Loss for  46738 th iteration => 0.00636613675179\n",
      "Loss for  46739 th iteration => 0.00636594682039\n",
      "Loss for  46740 th iteration => 0.00636585608965\n",
      "Loss for  46741 th iteration => 0.00636565397101\n",
      "Loss for  46742 th iteration => 0.00636555790834\n",
      "Loss for  46743 th iteration => 0.00636537265284\n",
      "Loss for  46744 th iteration => 0.00636524291791\n",
      "Loss for  46745 th iteration => 0.00636509126861\n",
      "Loss for  46746 th iteration => 0.00636493091226\n",
      "Loss for  46747 th iteration => 0.0063648541028\n",
      "Loss for  46748 th iteration => 0.00636463517407\n",
      "Loss for  46749 th iteration => 0.00636450210329\n",
      "Loss for  46750 th iteration => 0.00636440701951\n",
      "Loss for  46751 th iteration => 0.00636420609427\n",
      "Loss for  46752 th iteration => 0.00636410324615\n",
      "Loss for  46753 th iteration => 0.00636391688723\n",
      "Loss for  46754 th iteration => 0.00636375217239\n",
      "Loss for  46755 th iteration => 0.00636363653404\n",
      "Loss for  46756 th iteration => 0.00636344585087\n",
      "Loss for  46757 th iteration => 0.00636329786892\n",
      "Loss for  46758 th iteration => 0.0063631498976\n",
      "Loss for  46759 th iteration => 0.00636300221217\n",
      "Loss for  46760 th iteration => 0.0063629105687\n",
      "Loss for  46761 th iteration => 0.00636272115067\n",
      "Loss for  46762 th iteration => 0.00636259578866\n",
      "Loss for  46763 th iteration => 0.00636244002297\n",
      "Loss for  46764 th iteration => 0.00636228200029\n",
      "Loss for  46765 th iteration => 0.00636220480014\n",
      "Loss for  46766 th iteration => 0.00636198460516\n",
      "Loss for  46767 th iteration => 0.00636185413888\n",
      "Loss for  46768 th iteration => 0.00636170365652\n",
      "Loss for  46769 th iteration => 0.00636159765098\n",
      "Loss for  46770 th iteration => 0.00636143606817\n",
      "Loss for  46771 th iteration => 0.00636127119559\n",
      "Loss for  46772 th iteration => 0.00636112244671\n",
      "Loss for  46773 th iteration => 0.00636095544753\n",
      "Loss for  46774 th iteration => 0.00636084233317\n",
      "Loss for  46775 th iteration => 0.00636065035812\n",
      "Loss for  46776 th iteration => 0.00636050250479\n",
      "Loss for  46777 th iteration => 0.00636035466209\n",
      "Loss for  46778 th iteration => 0.0063602084457\n",
      "Loss for  46779 th iteration => 0.00636011466417\n",
      "Loss for  46780 th iteration => 0.00635992762484\n",
      "Loss for  46781 th iteration => 0.00635980015908\n",
      "Loss for  46782 th iteration => 0.00635964673788\n",
      "Loss for  46783 th iteration => 0.00635948777379\n",
      "Loss for  46784 th iteration => 0.00635941097061\n",
      "Loss for  46785 th iteration => 0.00635919150555\n",
      "Loss for  46786 th iteration => 0.00635905945348\n",
      "Loss for  46787 th iteration => 0.00635896486956\n",
      "Loss for  46788 th iteration => 0.00635876393543\n",
      "Loss for  46789 th iteration => 0.00635866158561\n",
      "Loss for  46790 th iteration => 0.00635847433132\n",
      "Loss for  46791 th iteration => 0.0063583118574\n",
      "Loss for  46792 th iteration => 0.00635819447393\n",
      "Loss for  46793 th iteration => 0.00635800501589\n",
      "Loss for  46794 th iteration => 0.0063578572804\n",
      "Loss for  46795 th iteration => 0.00635770955555\n",
      "Loss for  46796 th iteration => 0.0063575618413\n",
      "Loss for  46797 th iteration => 0.00635741718157\n",
      "Loss for  46798 th iteration => 0.00635732106723\n",
      "Loss for  46799 th iteration => 0.00635713660088\n",
      "Loss for  46800 th iteration => 0.00635700683696\n",
      "Loss for  46801 th iteration => 0.00635685612358\n",
      "Loss for  46802 th iteration => 0.0063567396543\n",
      "Loss for  46803 th iteration => 0.00635658790559\n",
      "Loss for  46804 th iteration => 0.0063564167013\n",
      "Loss for  46805 th iteration => 0.0063562642641\n",
      "Loss for  46806 th iteration => 0.00635616154217\n",
      "Loss for  46807 th iteration => 0.00635599717092\n",
      "Loss for  46808 th iteration => 0.00635583489425\n",
      "Loss for  46809 th iteration => 0.00635568472002\n",
      "Loss for  46810 th iteration => 0.00635551963575\n",
      "Loss for  46811 th iteration => 0.00635540509977\n",
      "Loss for  46812 th iteration => 0.00635521420604\n",
      "Loss for  46813 th iteration => 0.00635506659909\n",
      "Loss for  46814 th iteration => 0.00635491900277\n",
      "Loss for  46815 th iteration => 0.00635477252839\n",
      "Loss for  46816 th iteration => 0.00635467996267\n",
      "Loss for  46817 th iteration => 0.00635449219943\n",
      "Loss for  46818 th iteration => 0.00635436594892\n",
      "Loss for  46819 th iteration => 0.00635421180436\n",
      "Loss for  46820 th iteration => 0.00635405357344\n",
      "Loss for  46821 th iteration => 0.00635397697819\n",
      "Loss for  46822 th iteration => 0.00635375733262\n",
      "Loss for  46823 th iteration => 0.00635362628324\n",
      "Loss for  46824 th iteration => 0.006353477951\n",
      "Loss for  46825 th iteration => 0.00635337011556\n",
      "Loss for  46826 th iteration => 0.00635321107712\n",
      "Loss for  46827 th iteration => 0.00635304473158\n",
      "Loss for  46828 th iteration => 0.00635289757435\n",
      "Loss for  46829 th iteration => 0.00635272975202\n",
      "Loss for  46830 th iteration => 0.00635261819088\n",
      "Loss for  46831 th iteration => 0.00635242580565\n",
      "Loss for  46832 th iteration => 0.00635227832709\n",
      "Loss for  46833 th iteration => 0.00635213085912\n",
      "Loss for  46834 th iteration => 0.00635198603628\n",
      "Loss for  46835 th iteration => 0.00635189091806\n",
      "Loss for  46836 th iteration => 0.00635170594529\n",
      "Loss for  46837 th iteration => 0.00635157718131\n",
      "Loss for  46838 th iteration => 0.00635142578831\n",
      "Loss for  46839 th iteration => 0.00635126635827\n",
      "Loss for  46840 th iteration => 0.00635119022799\n",
      "Loss for  46841 th iteration => 0.00635097146764\n",
      "Loss for  46842 th iteration => 0.00635083951688\n",
      "Loss for  46843 th iteration => 0.00635074489452\n",
      "Loss for  46844 th iteration => 0.0063505447424\n",
      "Loss for  46845 th iteration => 0.00635044236439\n",
      "Loss for  46846 th iteration => 0.00635025635192\n",
      "Loss for  46847 th iteration => 0.00635009264757\n",
      "Loss for  46848 th iteration => 0.00634997722192\n",
      "Loss for  46849 th iteration => 0.00634978717372\n",
      "Loss for  46850 th iteration => 0.00634963981273\n",
      "Loss for  46851 th iteration => 0.00634949246236\n",
      "Loss for  46852 th iteration => 0.00634934584432\n",
      "Loss for  46853 th iteration => 0.00634925421692\n",
      "Loss for  46854 th iteration => 0.00634906600452\n",
      "Loss for  46855 th iteration => 0.0063489406959\n",
      "Loss for  46856 th iteration => 0.00634878609863\n",
      "Loss for  46857 th iteration => 0.00634862843747\n",
      "Loss for  46858 th iteration => 0.00634855206405\n",
      "Loss for  46859 th iteration => 0.0063483323788\n",
      "Loss for  46860 th iteration => 0.00634820225728\n",
      "Loss for  46861 th iteration => 0.00634805311708\n",
      "Loss for  46862 th iteration => 0.00634794684595\n",
      "Loss for  46863 th iteration => 0.00634778672819\n",
      "Loss for  46864 th iteration => 0.00634762164839\n",
      "Loss for  46865 th iteration => 0.0063474740944\n",
      "Loss for  46866 th iteration => 0.0063473071632\n",
      "Loss for  46867 th iteration => 0.00634719519846\n",
      "Loss for  46868 th iteration => 0.00634700343727\n",
      "Loss for  46869 th iteration => 0.0063468562046\n",
      "Loss for  46870 th iteration => 0.00634670898252\n",
      "Loss for  46871 th iteration => 0.00634656421718\n",
      "Loss for  46872 th iteration => 0.00634646959487\n",
      "Loss for  46873 th iteration => 0.00634628461288\n",
      "Loss for  46874 th iteration => 0.00634615635347\n",
      "Loss for  46875 th iteration => 0.00634600494262\n",
      "Loss for  46876 th iteration => 0.00634584581374\n",
      "Loss for  46877 th iteration => 0.0063457699753\n",
      "Loss for  46878 th iteration => 0.006345551338\n",
      "Loss for  46879 th iteration => 0.00634541978858\n",
      "Loss for  46880 th iteration => 0.00634532527858\n",
      "Loss for  46881 th iteration => 0.00634512550294\n",
      "Loss for  46882 th iteration => 0.00634502324055\n",
      "Loss for  46883 th iteration => 0.00634483748138\n",
      "Loss for  46884 th iteration => 0.00634467424278\n",
      "Loss for  46885 th iteration => 0.00634455883646\n",
      "Loss for  46886 th iteration => 0.00634436921931\n",
      "Loss for  46887 th iteration => 0.00634422210412\n",
      "Loss for  46888 th iteration => 0.00634407499955\n",
      "Loss for  46889 th iteration => 0.00634392861467\n",
      "Loss for  46890 th iteration => 0.00634383709726\n",
      "Loss for  46891 th iteration => 0.00634364925931\n",
      "Loss for  46892 th iteration => 0.00634352407383\n",
      "Loss for  46893 th iteration => 0.00634336983791\n",
      "Loss for  46894 th iteration => 0.0063432122437\n",
      "Loss for  46895 th iteration => 0.00634313622342\n",
      "Loss for  46896 th iteration => 0.00634291680256\n",
      "Loss for  46897 th iteration => 0.0063427869421\n",
      "Loss for  46898 th iteration => 0.00634263803763\n",
      "Loss for  46899 th iteration => 0.00634253183334\n",
      "Loss for  46900 th iteration => 0.00634237212493\n",
      "Loss for  46901 th iteration => 0.00634220718559\n",
      "Loss for  46902 th iteration => 0.00634206005799\n",
      "Loss for  46903 th iteration => 0.00634189319959\n",
      "Loss for  46904 th iteration => 0.00634178164486\n",
      "Loss for  46905 th iteration => 0.00634159013833\n",
      "Loss for  46906 th iteration => 0.00634144315134\n",
      "Loss for  46907 th iteration => 0.00634129617494\n",
      "Loss for  46908 th iteration => 0.00634115180346\n",
      "Loss for  46909 th iteration => 0.00634105693403\n",
      "Loss for  46910 th iteration => 0.0063408726815\n",
      "Loss for  46911 th iteration => 0.00634074419221\n",
      "Loss for  46912 th iteration => 0.00634059349364\n",
      "Loss for  46913 th iteration => 0.00634043421319\n",
      "Loss for  46914 th iteration => 0.00634035878505\n",
      "Loss for  46915 th iteration => 0.00634014054433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  46916 th iteration => 0.00634000914028\n",
      "Loss for  46917 th iteration => 0.00633991458202\n",
      "Loss for  46918 th iteration => 0.00633971534266\n",
      "Loss for  46919 th iteration => 0.00633961303727\n",
      "Loss for  46920 th iteration => 0.006339428008\n",
      "Loss for  46921 th iteration => 0.00633926449643\n",
      "Loss for  46922 th iteration => 0.00633914984379\n",
      "Loss for  46923 th iteration => 0.00633896032575\n",
      "Loss for  46924 th iteration => 0.00633881345611\n",
      "Loss for  46925 th iteration => 0.00633866659709\n",
      "Loss for  46926 th iteration => 0.0063385207475\n",
      "Loss for  46927 th iteration => 0.00633842866927\n",
      "Loss for  46928 th iteration => 0.00633824187256\n",
      "Loss for  46929 th iteration => 0.00633811614713\n",
      "Loss for  46930 th iteration => 0.00633796293164\n",
      "Loss for  46931 th iteration => 0.00633780499479\n",
      "Loss for  46932 th iteration => 0.00633772943373\n",
      "Loss for  46933 th iteration => 0.00633751052639\n",
      "Loss for  46934 th iteration => 0.00633738038672\n",
      "Loss for  46935 th iteration => 0.00633723255944\n",
      "Loss for  46936 th iteration => 0.00633712521065\n",
      "Loss for  46937 th iteration => 0.00633696711558\n",
      "Loss for  46938 th iteration => 0.00633680140921\n",
      "Loss for  46939 th iteration => 0.00633665537295\n",
      "Loss for  46940 th iteration => 0.00633648792608\n",
      "Loss for  46941 th iteration => 0.00633637743857\n",
      "Loss for  46942 th iteration => 0.0063361858912\n",
      "Loss for  46943 th iteration => 0.00633603914962\n",
      "Loss for  46944 th iteration => 0.00633589241862\n",
      "Loss for  46945 th iteration => 0.00633574871042\n",
      "Loss for  46946 th iteration => 0.00633565299202\n",
      "Loss for  46947 th iteration => 0.00633547006708\n",
      "Loss for  46948 th iteration => 0.00633534075296\n",
      "Loss for  46949 th iteration => 0.00633519137915\n",
      "Loss for  46950 th iteration => 0.00633507516572\n",
      "Loss for  46951 th iteration => 0.00633492507201\n",
      "Loss for  46952 th iteration => 0.00633475475169\n",
      "Loss for  46953 th iteration => 0.00633460351804\n",
      "Loss for  46954 th iteration => 0.00633450081458\n",
      "Loss for  46955 th iteration => 0.00633433832869\n",
      "Loss for  46956 th iteration => 0.00633417650074\n",
      "Loss for  46957 th iteration => 0.00633402801756\n",
      "Loss for  46958 th iteration => 0.00633386323971\n",
      "Loss for  46959 th iteration => 0.00633375032821\n",
      "Loss for  46960 th iteration => 0.00633356045631\n",
      "Loss for  46961 th iteration => 0.00633341383218\n",
      "Loss for  46962 th iteration => 0.00633326721866\n",
      "Loss for  46963 th iteration => 0.00633312232013\n",
      "Loss for  46964 th iteration => 0.00633302877187\n",
      "Loss for  46965 th iteration => 0.00633284392028\n",
      "Loss for  46966 th iteration => 0.00633271675707\n",
      "Loss for  46967 th iteration => 0.00633256545455\n",
      "Loss for  46968 th iteration => 0.00633240674954\n",
      "Loss for  46969 th iteration => 0.00633233153253\n",
      "Loss for  46970 th iteration => 0.00633211372855\n",
      "Loss for  46971 th iteration => 0.00633198257902\n",
      "Loss for  46972 th iteration => 0.00633188854183\n",
      "Loss for  46973 th iteration => 0.00633168950385\n",
      "Loss for  46974 th iteration => 0.00633158772226\n",
      "Loss for  46975 th iteration => 0.0063314028162\n",
      "Loss for  46976 th iteration => 0.00633124067594\n",
      "Loss for  46977 th iteration => 0.00633112537183\n",
      "Loss for  46978 th iteration => 0.00633093716452\n",
      "Loss for  46979 th iteration => 0.00633079065765\n",
      "Loss for  46980 th iteration => 0.00633064416141\n",
      "Loss for  46981 th iteration => 0.00633049807949\n",
      "Loss for  46982 th iteration => 0.00633040667954\n",
      "Loss for  46983 th iteration => 0.00633021992307\n",
      "Loss for  46984 th iteration => 0.00633009488829\n",
      "Loss for  46985 th iteration => 0.0063299417007\n",
      "Loss for  46986 th iteration => 0.00632978395012\n",
      "Loss for  46987 th iteration => 0.0063297087038\n",
      "Loss for  46988 th iteration => 0.00632949046535\n",
      "Loss for  46989 th iteration => 0.00632936099889\n",
      "Loss for  46990 th iteration => 0.00632921259028\n",
      "Loss for  46991 th iteration => 0.00632910624028\n",
      "Loss for  46992 th iteration => 0.00632894785535\n",
      "Loss for  46993 th iteration => 0.00632878337561\n",
      "Loss for  46994 th iteration => 0.00632863736111\n",
      "Loss for  46995 th iteration => 0.00632847062575\n",
      "Loss for  46996 th iteration => 0.00632836014287\n",
      "Loss for  46997 th iteration => 0.00632816960749\n",
      "Loss for  46998 th iteration => 0.00632802322856\n",
      "Loss for  46999 th iteration => 0.00632787686022\n",
      "Loss for  47000 th iteration => 0.00632773319122\n",
      "Loss for  47001 th iteration => 0.00632763755325\n",
      "Loss for  47002 th iteration => 0.00632745526301\n",
      "Loss for  47003 th iteration => 0.00632732604853\n",
      "Loss for  47004 th iteration => 0.00632717726901\n",
      "Loss for  47005 th iteration => 0.00632701729059\n",
      "Loss for  47006 th iteration => 0.00632694277847\n",
      "Loss for  47007 th iteration => 0.0063267260471\n",
      "Loss for  47008 th iteration => 0.0063265944111\n",
      "Loss for  47009 th iteration => 0.00632649992543\n",
      "Loss for  47010 th iteration => 0.00632630182196\n",
      "Loss for  47011 th iteration => 0.00632619960211\n",
      "Loss for  47012 th iteration => 0.00632601660818\n",
      "Loss for  47013 th iteration => 0.00632585236171\n",
      "Loss for  47014 th iteration => 0.00632573963369\n",
      "Loss for  47015 th iteration => 0.00632555070005\n",
      "Loss for  47016 th iteration => 0.00632540443816\n",
      "Loss for  47017 th iteration => 0.00632525818688\n",
      "Loss for  47018 th iteration => 0.0063251133912\n",
      "Loss for  47019 th iteration => 0.00632501976657\n",
      "Loss for  47020 th iteration => 0.00632483570525\n",
      "Loss for  47021 th iteration => 0.00632470848579\n",
      "Loss for  47022 th iteration => 0.00632455795346\n",
      "Loss for  47023 th iteration => 0.00632439884515\n",
      "Loss for  47024 th iteration => 0.00632432432204\n",
      "Loss for  47025 th iteration => 0.00632410721272\n",
      "Loss for  47026 th iteration => 0.00632397612778\n",
      "Loss for  47027 th iteration => 0.00632383088495\n",
      "Loss for  47028 th iteration => 0.0063237203854\n",
      "Loss for  47029 th iteration => 0.00632356660074\n",
      "Loss for  47030 th iteration => 0.00632339910086\n",
      "Loss for  47031 th iteration => 0.00632325582825\n",
      "Loss for  47032 th iteration => 0.00632308686295\n",
      "Loss for  47033 th iteration => 0.00632297907876\n",
      "Loss for  47034 th iteration => 0.00632278776737\n",
      "Loss for  47035 th iteration => 0.00632264163316\n",
      "Loss for  47036 th iteration => 0.00632249576582\n",
      "Loss for  47037 th iteration => 0.00632240403132\n",
      "Loss for  47038 th iteration => 0.00632221832068\n",
      "Loss for  47039 th iteration => 0.00632209297626\n",
      "Loss for  47040 th iteration => 0.00632194080968\n",
      "Loss for  47041 th iteration => 0.0063217826343\n",
      "Loss for  47042 th iteration => 0.00632170783246\n",
      "Loss for  47043 th iteration => 0.00632149067577\n",
      "Loss for  47044 th iteration => 0.00632136103963\n",
      "Loss for  47045 th iteration => 0.00632121344031\n",
      "Loss for  47046 th iteration => 0.00632110601726\n",
      "Loss for  47047 th iteration => 0.00632094940157\n",
      "Loss for  47048 th iteration => 0.00632078455795\n",
      "Loss for  47049 th iteration => 0.00632063978357\n",
      "Loss for  47050 th iteration => 0.00632047254676\n",
      "Loss for  47051 th iteration => 0.0063203632741\n",
      "Loss for  47052 th iteration => 0.00632017321145\n",
      "Loss for  47053 th iteration => 0.00632002719438\n",
      "Loss for  47054 th iteration => 0.00631988118788\n",
      "Loss for  47055 th iteration => 0.00631973795156\n",
      "Loss for  47056 th iteration => 0.00631964149393\n",
      "Loss for  47057 th iteration => 0.00631946073174\n",
      "Loss for  47058 th iteration => 0.00631933072801\n",
      "Loss for  47059 th iteration => 0.00631918344623\n",
      "Loss for  47060 th iteration => 0.00631902274542\n",
      "Loss for  47061 th iteration => 0.00631894875658\n",
      "Loss for  47062 th iteration => 0.00631873328553\n",
      "Loss for  47063 th iteration => 0.00631860123427\n",
      "Loss for  47064 th iteration => 0.00631850661592\n",
      "Loss for  47065 th iteration => 0.00631830936421\n",
      "Loss for  47066 th iteration => 0.00631820702068\n",
      "Loss for  47067 th iteration => 0.0063180257328\n",
      "Loss for  47068 th iteration => 0.00631786051496\n",
      "Loss for  47069 th iteration => 0.0063177494642\n",
      "Loss for  47070 th iteration => 0.00631756079192\n",
      "Loss for  47071 th iteration => 0.00631741489169\n",
      "Loss for  47072 th iteration => 0.00631726900205\n",
      "Loss for  47073 th iteration => 0.00631712483154\n",
      "Loss for  47074 th iteration => 0.0063170299601\n",
      "Loss for  47075 th iteration => 0.00631684785131\n",
      "Loss for  47076 th iteration => 0.00631671942039\n",
      "Loss for  47077 th iteration => 0.00631657080536\n",
      "Loss for  47078 th iteration => 0.00631641071426\n",
      "Loss for  47079 th iteration => 0.00631633678132\n",
      "Loss for  47080 th iteration => 0.00631612109131\n",
      "Loss for  47081 th iteration => 0.00631598941165\n",
      "Loss for  47082 th iteration => 0.00631589509635\n",
      "Loss for  47083 th iteration => 0.00631569777516\n",
      "Loss for  47084 th iteration => 0.0063155957324\n",
      "Loss for  47085 th iteration => 0.00631541396693\n",
      "Loss for  47086 th iteration => 0.006315250395\n",
      "Loss for  47087 th iteration => 0.00631513793729\n",
      "Loss for  47088 th iteration => 0.00631495048249\n",
      "Loss for  47089 th iteration => 0.00631480469915\n",
      "Loss for  47090 th iteration => 0.00631465892641\n",
      "Loss for  47091 th iteration => 0.00631451397921\n",
      "Loss for  47092 th iteration => 0.00631442035599\n",
      "Loss for  47093 th iteration => 0.00631423723687\n",
      "Loss for  47094 th iteration => 0.00631411004469\n",
      "Loss for  47095 th iteration => 0.00631396042878\n",
      "Loss for  47096 th iteration => 0.00631380074477\n",
      "Loss for  47097 th iteration => 0.00631372692246\n",
      "Loss for  47098 th iteration => 0.00631351113308\n",
      "Loss for  47099 th iteration => 0.00631338035867\n",
      "Loss for  47100 th iteration => 0.00631323514409\n",
      "Loss for  47101 th iteration => 0.00631312432563\n",
      "Loss for  47102 th iteration => 0.0063129717865\n",
      "Loss for  47103 th iteration => 0.0063128048058\n",
      "Loss for  47104 th iteration => 0.00631266261271\n",
      "Loss for  47105 th iteration => 0.00631249354064\n",
      "Loss for  47106 th iteration => 0.00631238680359\n",
      "Loss for  47107 th iteration => 0.00631219661826\n",
      "Loss for  47108 th iteration => 0.0063120509624\n",
      "Loss for  47109 th iteration => 0.0063119053171\n",
      "Loss for  47110 th iteration => 0.00631176315127\n",
      "Loss for  47111 th iteration => 0.00631166446265\n",
      "Loss for  47112 th iteration => 0.00631148663195\n",
      "Loss for  47113 th iteration => 0.00631135444204\n",
      "Loss for  47114 th iteration => 0.00631121022155\n",
      "Loss for  47115 th iteration => 0.00631108830903\n",
      "Loss for  47116 th iteration => 0.00631094599365\n",
      "Loss for  47117 th iteration => 0.00631077358989\n",
      "Loss for  47118 th iteration => 0.00631062671123\n",
      "Loss for  47119 th iteration => 0.00631051803025\n",
      "Loss for  47120 th iteration => 0.00631036359283\n",
      "Loss for  47121 th iteration => 0.00631019860651\n",
      "Loss for  47122 th iteration => 0.00631005534834\n",
      "Loss for  47123 th iteration => 0.00630988757097\n",
      "Loss for  47124 th iteration => 0.00630977977576\n",
      "Loss for  47125 th iteration => 0.00630959064846\n",
      "Loss for  47126 th iteration => 0.00630944510944\n",
      "Loss for  47127 th iteration => 0.00630929958098\n",
      "Loss for  47128 th iteration => 0.00630915678615\n",
      "Loss for  47129 th iteration => 0.0063090590203\n",
      "Loss for  47130 th iteration => 0.00630888050249\n",
      "Loss for  47131 th iteration => 0.00630874923047\n",
      "Loss for  47132 th iteration => 0.00630860415325\n",
      "Loss for  47133 th iteration => 0.00630844206135\n",
      "Loss for  47134 th iteration => 0.00630836901781\n",
      "Loss for  47135 th iteration => 0.00630815531413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  47136 th iteration => 0.00630802243643\n",
      "Loss for  47137 th iteration => 0.00630792772655\n",
      "Loss for  47138 th iteration => 0.00630773151676\n",
      "Loss for  47139 th iteration => 0.00630762909345\n",
      "Loss for  47140 th iteration => 0.00630745015944\n",
      "Loss for  47141 th iteration => 0.00630728376929\n",
      "Loss for  47142 th iteration => 0.00630717482386\n",
      "Loss for  47143 th iteration => 0.00630698680203\n",
      "Loss for  47144 th iteration => 0.00630684137959\n",
      "Loss for  47145 th iteration => 0.00630669596773\n",
      "Loss for  47146 th iteration => 0.00630655249725\n",
      "Loss for  47147 th iteration => 0.0063064557454\n",
      "Loss for  47148 th iteration => 0.00630627644963\n",
      "Loss for  47149 th iteration => 0.00630614618483\n",
      "Loss for  47150 th iteration => 0.00630600033643\n",
      "Loss for  47151 th iteration => 0.00630583850658\n",
      "Loss for  47152 th iteration => 0.00630576560727\n",
      "Loss for  47153 th iteration => 0.00630555189821\n",
      "Loss for  47154 th iteration => 0.00630541919246\n",
      "Loss for  47155 th iteration => 0.0063053246633\n",
      "Loss for  47156 th iteration => 0.00630512850553\n",
      "Loss for  47157 th iteration => 0.0063050262622\n",
      "Loss for  47158 th iteration => 0.00630484721775\n",
      "Loss for  47159 th iteration => 0.00630468190334\n",
      "Loss for  47160 th iteration => 0.00630457211756\n",
      "Loss for  47161 th iteration => 0.00630438505725\n",
      "Loss for  47162 th iteration => 0.00630423975142\n",
      "Loss for  47163 th iteration => 0.00630409445617\n",
      "Loss for  47164 th iteration => 0.00630395044107\n",
      "Loss for  47165 th iteration => 0.00630385442109\n",
      "Loss for  47166 th iteration => 0.00630367462804\n",
      "Loss for  47167 th iteration => 0.00630354509158\n",
      "Loss for  47168 th iteration => 0.00630339874941\n",
      "Loss for  47169 th iteration => 0.00630323701233\n",
      "Loss for  47170 th iteration => 0.00630316430245\n",
      "Loss for  47171 th iteration => 0.00630295068871\n",
      "Loss for  47172 th iteration => 0.00630281832269\n",
      "Loss for  47173 th iteration => 0.00630267601658\n",
      "Loss for  47174 th iteration => 0.00630256079252\n",
      "Loss for  47175 th iteration => 0.0063024135601\n",
      "Loss for  47176 th iteration => 0.00630224407808\n",
      "Loss for  47177 th iteration => 0.00630210512802\n",
      "Loss for  47178 th iteration => 0.00630193379771\n",
      "Loss for  47179 th iteration => 0.00630183024501\n",
      "Loss for  47180 th iteration => 0.00630164022636\n",
      "Loss for  47181 th iteration => 0.00630149504768\n",
      "Loss for  47182 th iteration => 0.00630135048501\n",
      "Loss for  47183 th iteration => 0.006301255196\n",
      "Loss for  47184 th iteration => 0.00630107490566\n",
      "Loss for  47185 th iteration => 0.00630094609841\n",
      "Loss for  47186 th iteration => 0.00630079926071\n",
      "Loss for  47187 th iteration => 0.00630063774886\n",
      "Loss for  47188 th iteration => 0.00630056494701\n",
      "Loss for  47189 th iteration => 0.00630035171059\n",
      "Loss for  47190 th iteration => 0.00630021986326\n",
      "Loss for  47191 th iteration => 0.00630007665141\n",
      "Loss for  47192 th iteration => 0.00629996241829\n",
      "Loss for  47193 th iteration => 0.00629981442757\n",
      "Loss for  47194 th iteration => 0.00629964603759\n",
      "Loss for  47195 th iteration => 0.00629950673051\n",
      "Loss for  47196 th iteration => 0.00629933599006\n",
      "Loss for  47197 th iteration => 0.00629923208036\n",
      "Loss for  47198 th iteration => 0.00629904279935\n",
      "Loss for  47199 th iteration => 0.00629889773722\n",
      "Loss for  47200 th iteration => 0.00629875273791\n",
      "Loss for  47201 th iteration => 0.00629865794016\n",
      "Loss for  47202 th iteration => 0.00629847739096\n",
      "Loss for  47203 th iteration => 0.00629834907596\n",
      "Loss for  47204 th iteration => 0.00629820197842\n",
      "Loss for  47205 th iteration => 0.00629804054764\n",
      "Loss for  47206 th iteration => 0.0062979676922\n",
      "Loss for  47207 th iteration => 0.00629775491902\n",
      "Loss for  47208 th iteration => 0.00629762339772\n",
      "Loss for  47209 th iteration => 0.00629747958399\n",
      "Loss for  47210 th iteration => 0.00629736590854\n",
      "Loss for  47211 th iteration => 0.00629721759034\n",
      "Loss for  47212 th iteration => 0.00629704996514\n",
      "Loss for  47213 th iteration => 0.00629691053955\n",
      "Loss for  47214 th iteration => 0.00629674015191\n",
      "Loss for  47215 th iteration => 0.00629663612097\n",
      "Loss for  47216 th iteration => 0.00629644746867\n",
      "Loss for  47217 th iteration => 0.00629630252308\n",
      "Loss for  47218 th iteration => 0.00629615758803\n",
      "Loss for  47219 th iteration => 0.00629601579635\n",
      "Loss for  47220 th iteration => 0.00629591498829\n",
      "Loss for  47221 th iteration => 0.0062957406668\n",
      "Loss for  47222 th iteration => 0.00629560641983\n",
      "Loss for  47223 th iteration => 0.00629546547189\n",
      "Loss for  47224 th iteration => 0.00629530072464\n",
      "Loss for  47225 th iteration => 0.00629522889057\n",
      "Loss for  47226 th iteration => 0.00629501827926\n",
      "Loss for  47227 th iteration => 0.00629488368053\n",
      "Loss for  47228 th iteration => 0.00629478836535\n",
      "Loss for  47229 th iteration => 0.00629459393987\n",
      "Loss for  47230 th iteration => 0.00629449093039\n",
      "Loss for  47231 th iteration => 0.00629431626543\n",
      "Loss for  47232 th iteration => 0.00629414664322\n",
      "Loss for  47233 th iteration => 0.00629404207978\n",
      "Loss for  47234 th iteration => 0.00629385425531\n",
      "Loss for  47235 th iteration => 0.00629370942589\n",
      "Loss for  47236 th iteration => 0.00629356460704\n",
      "Loss for  47237 th iteration => 0.00629342239225\n",
      "Loss for  47238 th iteration => 0.00629332203296\n",
      "Loss for  47239 th iteration => 0.00629314749498\n",
      "Loss for  47240 th iteration => 0.00629301369648\n",
      "Loss for  47241 th iteration => 0.00629287253233\n",
      "Loss for  47242 th iteration => 0.00629270770197\n",
      "Loss for  47243 th iteration => 0.00629263609805\n",
      "Loss for  47244 th iteration => 0.00629242569603\n",
      "Loss for  47245 th iteration => 0.00629229106679\n",
      "Loss for  47246 th iteration => 0.00629219581491\n",
      "Loss for  47247 th iteration => 0.00629200155778\n",
      "Loss for  47248 th iteration => 0.00629189861235\n",
      "Loss for  47249 th iteration => 0.0062917241988\n",
      "Loss for  47250 th iteration => 0.00629155509127\n",
      "Loss for  47251 th iteration => 0.00629145024473\n",
      "Loss for  47252 th iteration => 0.00629126313269\n",
      "Loss for  47253 th iteration => 0.00629111841946\n",
      "Loss for  47254 th iteration => 0.00629097371677\n",
      "Loss for  47255 th iteration => 0.00629083118382\n",
      "Loss for  47256 th iteration => 0.00629073104625\n",
      "Loss for  47257 th iteration => 0.0062905565176\n",
      "Loss for  47258 th iteration => 0.00629042294314\n",
      "Loss for  47259 th iteration => 0.006290281786\n",
      "Loss for  47260 th iteration => 0.00629011673614\n",
      "Loss for  47261 th iteration => 0.00629004539817\n",
      "Loss for  47262 th iteration => 0.00628983528747\n",
      "Loss for  47263 th iteration => 0.00628970055163\n",
      "Loss for  47264 th iteration => 0.00628960531374\n",
      "Loss for  47265 th iteration => 0.00628941127411\n",
      "Loss for  47266 th iteration => 0.00628930834405\n",
      "Loss for  47267 th iteration => 0.0062891343264\n",
      "Loss for  47268 th iteration => 0.00628896550708\n",
      "Loss for  47269 th iteration => 0.00628886060268\n",
      "Loss for  47270 th iteration => 0.0062886740991\n",
      "Loss for  47271 th iteration => 0.00628852950202\n",
      "Loss for  47272 th iteration => 0.00628838491549\n",
      "Loss for  47273 th iteration => 0.00628824215888\n",
      "Loss for  47274 th iteration => 0.00628814203798\n",
      "Loss for  47275 th iteration => 0.00628796772257\n",
      "Loss for  47276 th iteration => 0.00628783416947\n",
      "Loss for  47277 th iteration => 0.0062876932209\n",
      "Loss for  47278 th iteration => 0.0062875278283\n",
      "Loss for  47279 th iteration => 0.00628745678831\n",
      "Loss for  47280 th iteration => 0.00628724704352\n",
      "Loss for  47281 th iteration => 0.00628711213182\n",
      "Loss for  47282 th iteration => 0.00628701686363\n",
      "Loss for  47283 th iteration => 0.00628682308563\n",
      "Loss for  47284 th iteration => 0.0062867201272\n",
      "Loss for  47285 th iteration => 0.00628654663604\n",
      "Loss for  47286 th iteration => 0.00628637790036\n",
      "Loss for  47287 th iteration => 0.00628627314152\n",
      "Loss for  47288 th iteration => 0.00628608715274\n",
      "Loss for  47289 th iteration => 0.00628594267177\n",
      "Loss for  47290 th iteration => 0.00628579820137\n",
      "Loss for  47291 th iteration => 0.00628565530617\n",
      "Loss for  47292 th iteration => 0.00628555501679\n",
      "Loss for  47293 th iteration => 0.00628538109872\n",
      "Loss for  47294 th iteration => 0.00628524738396\n",
      "Loss for  47295 th iteration => 0.00628510682594\n",
      "Loss for  47296 th iteration => 0.00628494097924\n",
      "Loss for  47297 th iteration => 0.00628487026585\n",
      "Loss for  47298 th iteration => 0.00628466095484\n",
      "Loss for  47299 th iteration => 0.0062845258042\n",
      "Loss for  47300 th iteration => 0.00628443046597\n",
      "Loss for  47301 th iteration => 0.00628423698917\n",
      "Loss for  47302 th iteration => 0.00628413396312\n",
      "Loss for  47303 th iteration => 0.00628396111646\n",
      "Loss for  47304 th iteration => 0.00628379227965\n",
      "Loss for  47305 th iteration => 0.00628368785007\n",
      "Loss for  47306 th iteration => 0.00628350229175\n",
      "Loss for  47307 th iteration => 0.00628335792686\n",
      "Loss for  47308 th iteration => 0.00628321357253\n",
      "Loss for  47309 th iteration => 0.00628307061531\n",
      "Loss for  47310 th iteration => 0.00628296999025\n",
      "Loss for  47311 th iteration => 0.00628279663575\n",
      "Loss for  47312 th iteration => 0.00628266259405\n",
      "Loss for  47313 th iteration => 0.00628252259089\n",
      "Loss for  47314 th iteration => 0.00628235618943\n",
      "Loss for  47315 th iteration => 0.00628228582821\n",
      "Loss for  47316 th iteration => 0.00628207701276\n",
      "Loss for  47317 th iteration => 0.00628194177438\n",
      "Loss for  47318 th iteration => 0.00628180423728\n",
      "Loss for  47319 th iteration => 0.00628168045471\n",
      "Loss for  47320 th iteration => 0.00628154355943\n",
      "Loss for  47321 th iteration => 0.00628136973499\n",
      "Loss for  47322 th iteration => 0.00628123731745\n",
      "Loss for  47323 th iteration => 0.00628106295294\n",
      "Loss for  47324 th iteration => 0.00628091869003\n",
      "Loss for  47325 th iteration => 0.00628077443777\n",
      "Loss for  47326 th iteration => 0.00628063019613\n",
      "Loss for  47327 th iteration => 0.0062804873317\n",
      "Loss for  47328 th iteration => 0.006280386031\n",
      "Loss for  47329 th iteration => 0.00628021360268\n",
      "Loss for  47330 th iteration => 0.00628007882906\n",
      "Loss for  47331 th iteration => 0.0062799398082\n",
      "Loss for  47332 th iteration => 0.00627977271775\n",
      "Loss for  47333 th iteration => 0.00627970249305\n",
      "Loss for  47334 th iteration => 0.0062794946677\n",
      "Loss for  47335 th iteration => 0.00627935859532\n",
      "Loss for  47336 th iteration => 0.00627921849976\n",
      "Loss for  47337 th iteration => 0.00627905519566\n",
      "Loss for  47338 th iteration => 0.00627899541517\n",
      "Loss for  47339 th iteration => 0.00627878102546\n",
      "Loss for  47340 th iteration => 0.00627865428028\n",
      "Loss for  47341 th iteration => 0.00627850829192\n",
      "Loss for  47342 th iteration => 0.00627834618017\n",
      "Loss for  47343 th iteration => 0.00627823548385\n",
      "Loss for  47344 th iteration => 0.00627804972793\n",
      "Loss for  47345 th iteration => 0.0062779069689\n",
      "Loss for  47346 th iteration => 0.00627780496304\n",
      "Loss for  47347 th iteration => 0.00627763346527\n",
      "Loss for  47348 th iteration => 0.00627749800066\n",
      "Loss for  47349 th iteration => 0.00627735989622\n",
      "Loss for  47350 th iteration => 0.00627719208055\n",
      "Loss for  47351 th iteration => 0.00627712215549\n",
      "Loss for  47352 th iteration => 0.00627691507138\n",
      "Loss for  47353 th iteration => 0.00627677852348\n",
      "Loss for  47354 th iteration => 0.00627663913001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  47355 th iteration => 0.00627647540713\n",
      "Loss for  47356 th iteration => 0.00627641532093\n",
      "Loss for  47357 th iteration => 0.00627620209817\n",
      "Loss for  47358 th iteration => 0.00627607448846\n",
      "Loss for  47359 th iteration => 0.00627592958908\n",
      "Loss for  47360 th iteration => 0.00627576662877\n",
      "Loss for  47361 th iteration => 0.00627565700551\n",
      "Loss for  47362 th iteration => 0.00627547133975\n",
      "Loss for  47363 th iteration => 0.00627532873344\n",
      "Loss for  47364 th iteration => 0.00627522591683\n",
      "Loss for  47365 th iteration => 0.0062750554545\n",
      "Loss for  47366 th iteration => 0.0062749191945\n",
      "Loss for  47367 th iteration => 0.00627478211018\n",
      "Loss for  47368 th iteration => 0.00627461350503\n",
      "Loss for  47369 th iteration => 0.00627454389543\n",
      "Loss for  47370 th iteration => 0.00627433759274\n",
      "Loss for  47371 th iteration => 0.00627420048338\n",
      "Loss for  47372 th iteration => 0.00627406187734\n",
      "Loss for  47373 th iteration => 0.00627389769773\n",
      "Loss for  47374 th iteration => 0.00627383728526\n",
      "Loss for  47375 th iteration => 0.00627362529593\n",
      "Loss for  47376 th iteration => 0.00627349671774\n",
      "Loss for  47377 th iteration => 0.00627335301058\n",
      "Loss for  47378 th iteration => 0.00627318909895\n",
      "Loss for  47379 th iteration => 0.00627308065079\n",
      "Loss for  47380 th iteration => 0.00627289502944\n",
      "Loss for  47381 th iteration => 0.00627275261837\n",
      "Loss for  47382 th iteration => 0.00627264889563\n",
      "Loss for  47383 th iteration => 0.00627247956345\n",
      "Loss for  47384 th iteration => 0.00627234241376\n",
      "Loss for  47385 th iteration => 0.00627220644319\n",
      "Loss for  47386 th iteration => 0.00627203700907\n",
      "Loss for  47387 th iteration => 0.00627192687378\n",
      "Loss for  47388 th iteration => 0.00627176157757\n",
      "Loss for  47389 th iteration => 0.00627162354056\n",
      "Loss for  47390 th iteration => 0.00627148610482\n",
      "Loss for  47391 th iteration => 0.00627132151607\n",
      "Loss for  47392 th iteration => 0.00627122673345\n",
      "Loss for  47393 th iteration => 0.00627104957159\n",
      "Loss for  47394 th iteration => 0.00627091908103\n",
      "Loss for  47395 th iteration => 0.00627077755164\n",
      "Loss for  47396 th iteration => 0.00627061162751\n",
      "Loss for  47397 th iteration => 0.00627050545695\n",
      "Loss for  47398 th iteration => 0.00627031934873\n",
      "Loss for  47399 th iteration => 0.00627017557601\n",
      "Loss for  47400 th iteration => 0.00627003181385\n",
      "Loss for  47401 th iteration => 0.00626989073567\n",
      "Loss for  47402 th iteration => 0.00626979242239\n",
      "Loss for  47403 th iteration => 0.00626961792616\n",
      "Loss for  47404 th iteration => 0.00626948621816\n",
      "Loss for  47405 th iteration => 0.00626934505145\n",
      "Loss for  47406 th iteration => 0.00626918200164\n",
      "Loss for  47407 th iteration => 0.00626911203718\n",
      "Loss for  47408 th iteration => 0.00626890096216\n",
      "Loss for  47409 th iteration => 0.00626876931199\n",
      "Loss for  47410 th iteration => 0.00626867415907\n",
      "Loss for  47411 th iteration => 0.00626848190249\n",
      "Loss for  47412 th iteration => 0.00626837907188\n",
      "Loss for  47413 th iteration => 0.00626820524504\n",
      "Loss for  47414 th iteration => 0.00626803809095\n",
      "Loss for  47415 th iteration => 0.00626793337205\n",
      "Loss for  47416 th iteration => 0.00626774718709\n",
      "Loss for  47417 th iteration => 0.00626760353019\n",
      "Loss for  47418 th iteration => 0.00626745988383\n",
      "Loss for  47419 th iteration => 0.006267319206\n",
      "Loss for  47420 th iteration => 0.00626721953081\n",
      "Loss for  47421 th iteration => 0.00626704661849\n",
      "Loss for  47422 th iteration => 0.00626691356901\n",
      "Loss for  47423 th iteration => 0.00626677396581\n",
      "Loss for  47424 th iteration => 0.00626660973444\n",
      "Loss for  47425 th iteration => 0.00626654028491\n",
      "Loss for  47426 th iteration => 0.00626633008653\n",
      "Loss for  47427 th iteration => 0.00626619778259\n",
      "Loss for  47428 th iteration => 0.00626610230386\n",
      "Loss for  47429 th iteration => 0.00626591060298\n",
      "Loss for  47430 th iteration => 0.00626580745216\n",
      "Loss for  47431 th iteration => 0.00626563503566\n",
      "Loss for  47432 th iteration => 0.00626546659345\n",
      "Loss for  47433 th iteration => 0.00626536338385\n",
      "Loss for  47434 th iteration => 0.00626517709739\n",
      "Loss for  47435 th iteration => 0.00626503355619\n",
      "Loss for  47436 th iteration => 0.00626489002553\n",
      "Loss for  47437 th iteration => 0.00626474977015\n",
      "Loss for  47438 th iteration => 0.0062646486807\n",
      "Loss for  47439 th iteration => 0.00626447740413\n",
      "Loss for  47440 th iteration => 0.00626434296144\n",
      "Loss for  47441 th iteration => 0.00626420497301\n",
      "Loss for  47442 th iteration => 0.00626403952802\n",
      "Loss for  47443 th iteration => 0.00626397059934\n",
      "Loss for  47444 th iteration => 0.00626376130053\n",
      "Loss for  47445 th iteration => 0.00626362832105\n",
      "Loss for  47446 th iteration => 0.00626353250704\n",
      "Loss for  47447 th iteration => 0.00626334137108\n",
      "Loss for  47448 th iteration => 0.00626323789071\n",
      "Loss for  47449 th iteration => 0.00626306691852\n",
      "Loss for  47450 th iteration => 0.00626289713573\n",
      "Loss for  47451 th iteration => 0.0062627954874\n",
      "Loss for  47452 th iteration => 0.00626260907733\n",
      "Loss for  47453 th iteration => 0.00626246565172\n",
      "Loss for  47454 th iteration => 0.00626232227677\n",
      "Loss for  47455 th iteration => 0.00626222606429\n",
      "Loss for  47456 th iteration => 0.00626205014577\n",
      "Loss for  47457 th iteration => 0.00626192052649\n",
      "Loss for  47458 th iteration => 0.00626177794954\n",
      "Loss for  47459 th iteration => 0.00626161517248\n",
      "Loss for  47460 th iteration => 0.00626150807604\n",
      "Loss for  47461 th iteration => 0.00626136088232\n",
      "Loss for  47462 th iteration => 0.00626119471219\n",
      "Loss for  47463 th iteration => 0.00626106089446\n",
      "Loss for  47464 th iteration => 0.00626096475038\n",
      "Loss for  47465 th iteration => 0.00626077417413\n",
      "Loss for  47466 th iteration => 0.00626067037019\n",
      "Loss for  47467 th iteration => 0.00626050099137\n",
      "Loss for  47468 th iteration => 0.00626032958218\n",
      "Loss for  47469 th iteration => 0.00626022977928\n",
      "Loss for  47470 th iteration => 0.00626004311319\n",
      "Loss for  47471 th iteration => 0.00625989980321\n",
      "Loss for  47472 th iteration => 0.00625975703005\n",
      "Loss for  47473 th iteration => 0.00625965901648\n",
      "Loss for  47474 th iteration => 0.00625948511852\n",
      "Loss for  47475 th iteration => 0.00625935372335\n",
      "Loss for  47476 th iteration => 0.00625921314183\n",
      "Loss for  47477 th iteration => 0.00625904861381\n",
      "Loss for  47478 th iteration => 0.00625894379323\n",
      "Loss for  47479 th iteration => 0.00625879486884\n",
      "Loss for  47480 th iteration => 0.00625863019516\n",
      "Loss for  47481 th iteration => 0.00625849553027\n",
      "Loss for  47482 th iteration => 0.00625839905414\n",
      "Loss for  47483 th iteration => 0.00625820903932\n",
      "Loss for  47484 th iteration => 0.00625810490986\n",
      "Loss for  47485 th iteration => 0.00625793713502\n",
      "Loss for  47486 th iteration => 0.00625776482584\n",
      "Loss for  47487 th iteration => 0.00625762160681\n",
      "Loss for  47488 th iteration => 0.00625747839843\n",
      "Loss for  47489 th iteration => 0.00625733520066\n",
      "Loss for  47490 th iteration => 0.00625719323648\n",
      "Loss for  47491 th iteration => 0.00625709297265\n",
      "Loss for  47492 th iteration => 0.00625692156686\n",
      "Loss for  47493 th iteration => 0.00625678788189\n",
      "Loss for  47494 th iteration => 0.00625664983195\n",
      "Loss for  47495 th iteration => 0.00625648323536\n",
      "Loss for  47496 th iteration => 0.00625641501008\n",
      "Loss for  47497 th iteration => 0.00625620733513\n",
      "Loss for  47498 th iteration => 0.00625607342747\n",
      "Loss for  47499 th iteration => 0.00625593323052\n",
      "Loss for  47500 th iteration => 0.00625577200801\n",
      "Loss for  47501 th iteration => 0.00625571193163\n",
      "Loss for  47502 th iteration => 0.00625549940266\n",
      "Loss for  47503 th iteration => 0.0062553736146\n",
      "Loss for  47504 th iteration => 0.0062552287214\n",
      "Loss for  47505 th iteration => 0.00625506763252\n",
      "Loss for  47506 th iteration => 0.00625495796585\n",
      "Loss for  47507 th iteration => 0.00625477346312\n",
      "Loss for  47508 th iteration => 0.0062546322941\n",
      "Loss for  47509 th iteration => 0.00625452980606\n",
      "Loss for  47510 th iteration => 0.00625436084117\n",
      "Loss for  47511 th iteration => 0.00625422496348\n",
      "Loss for  47512 th iteration => 0.00625408932305\n",
      "Loss for  47513 th iteration => 0.00625392107928\n",
      "Loss for  47514 th iteration => 0.00625385339145\n",
      "Loss for  47515 th iteration => 0.00625364701967\n",
      "Loss for  47516 th iteration => 0.00625351140996\n",
      "Loss for  47517 th iteration => 0.0062533731345\n",
      "Loss for  47518 th iteration => 0.00625321097289\n",
      "Loss for  47519 th iteration => 0.00625315027023\n",
      "Loss for  47520 th iteration => 0.00625293987965\n",
      "Loss for  47521 th iteration => 0.00625281171597\n",
      "Loss for  47522 th iteration => 0.00625266941404\n",
      "Loss for  47523 th iteration => 0.00625250598278\n",
      "Loss for  47524 th iteration => 0.00625239887425\n",
      "Loss for  47525 th iteration => 0.00625221378868\n",
      "Loss for  47526 th iteration => 0.00625207339289\n",
      "Loss for  47527 th iteration => 0.00625196872114\n",
      "Loss for  47528 th iteration => 0.00625180215655\n",
      "Loss for  47529 th iteration => 0.00625166412615\n",
      "Loss for  47530 th iteration => 0.0062515308551\n",
      "Loss for  47531 th iteration => 0.00625136171853\n",
      "Loss for  47532 th iteration => 0.00625125199632\n",
      "Loss for  47533 th iteration => 0.00625108811081\n",
      "Loss for  47534 th iteration => 0.00625095054483\n",
      "Loss for  47535 th iteration => 0.00625081446196\n",
      "Loss for  47536 th iteration => 0.00625065150376\n",
      "Loss for  47537 th iteration => 0.00625055579196\n",
      "Loss for  47538 th iteration => 0.00625038137076\n",
      "Loss for  47539 th iteration => 0.00625025003322\n",
      "Loss for  47540 th iteration => 0.00625011116259\n",
      "Loss for  47541 th iteration => 0.00624994447281\n",
      "Loss for  47542 th iteration => 0.00624984087998\n",
      "Loss for  47543 th iteration => 0.00624965474875\n",
      "Loss for  47544 th iteration => 0.00624951190269\n",
      "Loss for  47545 th iteration => 0.00624936983122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  47546 th iteration => 0.00624927562911\n",
      "Loss for  47547 th iteration => 0.00624909884682\n",
      "Loss for  47548 th iteration => 0.00624897125726\n",
      "Loss for  47549 th iteration => 0.00624882779726\n",
      "Loss for  47550 th iteration => 0.00624866706912\n",
      "Loss for  47551 th iteration => 0.00624855945224\n",
      "Loss for  47552 th iteration => 0.00624841377843\n",
      "Loss for  47553 th iteration => 0.00624824653835\n",
      "Loss for  47554 th iteration => 0.00624811572955\n",
      "Loss for  47555 th iteration => 0.0062480199862\n",
      "Loss for  47556 th iteration => 0.00624783015983\n",
      "Loss for  47557 th iteration => 0.00624772676332\n",
      "Loss for  47558 th iteration => 0.00624755607687\n",
      "Loss for  47559 th iteration => 0.00624738678291\n",
      "Loss for  47560 th iteration => 0.00624728600784\n",
      "Loss for  47561 th iteration => 0.00624709917997\n",
      "Loss for  47562 th iteration => 0.00624695644937\n",
      "Loss for  47563 th iteration => 0.00624681538258\n",
      "Loss for  47564 th iteration => 0.00624671848826\n",
      "Loss for  47565 th iteration => 0.00624654461264\n",
      "Loss for  47566 th iteration => 0.0062464143662\n",
      "Loss for  47567 th iteration => 0.00624627377765\n",
      "Loss for  47568 th iteration => 0.00624611083259\n",
      "Loss for  47569 th iteration => 0.00624604285967\n",
      "Loss for  47570 th iteration => 0.00624583241612\n",
      "Loss for  47571 th iteration => 0.00624570205228\n",
      "Loss for  47572 th iteration => 0.00624560713222\n",
      "Loss for  47573 th iteration => 0.00624541669446\n",
      "Loss for  47574 th iteration => 0.00624531411219\n",
      "Loss for  47575 th iteration => 0.00624514196583\n",
      "Loss for  47576 th iteration => 0.00624497666752\n",
      "Loss for  47577 th iteration => 0.00624487212616\n",
      "Loss for  47578 th iteration => 0.00624468827947\n",
      "Loss for  47579 th iteration => 0.00624454565381\n",
      "Loss for  47580 th iteration => 0.0062444030387\n",
      "Loss for  47581 th iteration => 0.0062442630943\n",
      "Loss for  47582 th iteration => 0.00624416325472\n",
      "Loss for  47583 th iteration => 0.00624399253744\n",
      "Loss for  47584 th iteration => 0.00624385938422\n",
      "Loss for  47585 th iteration => 0.00624372191561\n",
      "Loss for  47586 th iteration => 0.00624355683081\n",
      "Loss for  47587 th iteration => 0.00624348962012\n",
      "Loss for  47588 th iteration => 0.00624328063675\n",
      "Loss for  47589 th iteration => 0.00624314907574\n",
      "Loss for  47590 th iteration => 0.00624305348837\n",
      "Loss for  47591 th iteration => 0.00624286394629\n",
      "Loss for  47592 th iteration => 0.00624276070654\n",
      "Loss for  47593 th iteration => 0.0062425909848\n",
      "Loss for  47594 th iteration => 0.00624242281847\n",
      "Loss for  47595 th iteration => 0.00624232135729\n",
      "Loss for  47596 th iteration => 0.00624213669366\n",
      "Loss for  47597 th iteration => 0.00624199418331\n",
      "Loss for  47598 th iteration => 0.00624185173688\n",
      "Loss for  47599 th iteration => 0.00624175542703\n",
      "Loss for  47600 th iteration => 0.00624158140733\n",
      "Loss for  47601 th iteration => 0.00624145174584\n",
      "Loss for  47602 th iteration => 0.00624131101273\n",
      "Loss for  47603 th iteration => 0.00624114824818\n",
      "Loss for  47604 th iteration => 0.00624104248595\n",
      "Loss for  47605 th iteration => 0.00624089637184\n",
      "Loss for  47606 th iteration => 0.00624073098414\n",
      "Loss for  47607 th iteration => 0.00624059811048\n",
      "Loss for  47608 th iteration => 0.00624050189422\n",
      "Loss for  47609 th iteration => 0.00624031320933\n",
      "Loss for  47610 th iteration => 0.00624020935084\n",
      "Loss for  47611 th iteration => 0.00624004210876\n",
      "Loss for  47612 th iteration => 0.00623987117206\n",
      "Loss for  47613 th iteration => 0.0062397287524\n",
      "Loss for  47614 th iteration => 0.00623958634341\n",
      "Loss for  47615 th iteration => 0.00623944394501\n",
      "Loss for  47616 th iteration => 0.00623930287313\n",
      "Loss for  47617 th iteration => 0.00623920307104\n",
      "Loss for  47618 th iteration => 0.00623903277829\n",
      "Loss for  47619 th iteration => 0.00623889959966\n",
      "Loss for  47620 th iteration => 0.00623876261831\n",
      "Loss for  47621 th iteration => 0.00623859631213\n",
      "Loss for  47622 th iteration => 0.0062384953529\n",
      "Loss for  47623 th iteration => 0.00623834510684\n",
      "Loss for  47624 th iteration => 0.00623818279446\n",
      "Loss for  47625 th iteration => 0.00623804487225\n",
      "Loss for  47626 th iteration => 0.00623791069616\n",
      "Loss for  47627 th iteration => 0.00623779755529\n",
      "Loss for  47628 th iteration => 0.00623765383683\n",
      "Loss for  47629 th iteration => 0.0062374932736\n",
      "Loss for  47630 th iteration => 0.00623734941759\n",
      "Loss for  47631 th iteration => 0.00623718891589\n",
      "Loss for  47632 th iteration => 0.00623708023173\n",
      "Loss for  47633 th iteration => 0.00623689654832\n",
      "Loss for  47634 th iteration => 0.00623675673917\n",
      "Loss for  47635 th iteration => 0.00623665367818\n",
      "Loss for  47636 th iteration => 0.00623648685469\n",
      "Loss for  47637 th iteration => 0.00623635046109\n",
      "Loss for  47638 th iteration => 0.00623621690521\n",
      "Loss for  47639 th iteration => 0.00623604835935\n",
      "Loss for  47640 th iteration => 0.00623598219358\n",
      "Loss for  47641 th iteration => 0.00623577661298\n",
      "Loss for  47642 th iteration => 0.00623564134002\n",
      "Loss for  47643 th iteration => 0.00623550430074\n",
      "Loss for  47644 th iteration => 0.0062353430834\n",
      "Loss for  47645 th iteration => 0.00623528211992\n",
      "Loss for  47646 th iteration => 0.00623507384457\n",
      "Loss for  47647 th iteration => 0.0062349450948\n",
      "Loss for  47648 th iteration => 0.00623480494193\n",
      "Loss for  47649 th iteration => 0.00623464099218\n",
      "Loss for  47650 th iteration => 0.00623453596536\n",
      "Loss for  47651 th iteration => 0.0062343512017\n",
      "Loss for  47652 th iteration => 0.00623421262428\n",
      "Loss for  47653 th iteration => 0.00623410636342\n",
      "Loss for  47654 th iteration => 0.0062339429504\n",
      "Loss for  47655 th iteration => 0.00623380339943\n",
      "Loss for  47656 th iteration => 0.00623367321163\n",
      "Loss for  47657 th iteration => 0.00623350444501\n",
      "Loss for  47658 th iteration => 0.00623339418389\n",
      "Loss for  47659 th iteration => 0.00623323240496\n",
      "Loss for  47660 th iteration => 0.00623309435794\n",
      "Loss for  47661 th iteration => 0.00623296032391\n",
      "Loss for  47662 th iteration => 0.00623279869951\n",
      "Loss for  47663 th iteration => 0.00623270131886\n",
      "Loss for  47664 th iteration => 0.00623253012273\n",
      "Loss for  47665 th iteration => 0.00623239719708\n",
      "Loss for  47666 th iteration => 0.00623226147107\n",
      "Loss for  47667 th iteration => 0.00623209327308\n",
      "Loss for  47668 th iteration => 0.00623199274525\n",
      "Loss for  47669 th iteration => 0.00623180649329\n",
      "Loss for  47670 th iteration => 0.00623166444548\n",
      "Loss for  47671 th iteration => 0.00623152463589\n",
      "Loss for  47672 th iteration => 0.00623142781763\n",
      "Loss for  47673 th iteration => 0.00623125520786\n",
      "Loss for  47674 th iteration => 0.006231125082\n",
      "Loss for  47675 th iteration => 0.00623098571492\n",
      "Loss for  47676 th iteration => 0.00623082319259\n",
      "Loss for  47677 th iteration => 0.00623075634268\n",
      "Loss for  47678 th iteration => 0.00623054609834\n",
      "Loss for  47679 th iteration => 0.00623041700259\n",
      "Loss for  47680 th iteration => 0.00623032211891\n",
      "Loss for  47681 th iteration => 0.00623013300109\n",
      "Loss for  47682 th iteration => 0.00623003046868\n",
      "Loss for  47683 th iteration => 0.00622985936136\n",
      "Loss for  47684 th iteration => 0.00622969420027\n",
      "Loss for  47685 th iteration => 0.0062295908593\n",
      "Loss for  47686 th iteration => 0.00622940715373\n",
      "Loss for  47687 th iteration => 0.00622926521059\n",
      "Loss for  47688 th iteration => 0.00622912334115\n",
      "Loss for  47689 th iteration => 0.00622902908893\n",
      "Loss for  47690 th iteration => 0.00622885413517\n",
      "Loss for  47691 th iteration => 0.00622872654839\n",
      "Loss for  47692 th iteration => 0.00622858486422\n",
      "Loss for  47693 th iteration => 0.00622842419136\n",
      "Loss for  47694 th iteration => 0.00622831735728\n",
      "Loss for  47695 th iteration => 0.00622817328284\n",
      "Loss for  47696 th iteration => 0.00622800699335\n",
      "Loss for  47697 th iteration => 0.00622787624863\n",
      "Loss for  47698 th iteration => 0.00622778051925\n",
      "Loss for  47699 th iteration => 0.00622759247468\n",
      "Loss for  47700 th iteration => 0.00622748910936\n",
      "Loss for  47701 th iteration => 0.00622732111767\n",
      "Loss for  47702 th iteration => 0.0062271519636\n",
      "Loss for  47703 th iteration => 0.00622705282133\n",
      "Loss for  47704 th iteration => 0.00622686778916\n",
      "Loss for  47705 th iteration => 0.00622672596116\n",
      "Loss for  47706 th iteration => 0.00622658567334\n",
      "Loss for  47707 th iteration => 0.00622648746107\n",
      "Loss for  47708 th iteration => 0.00622631667458\n",
      "Loss for  47709 th iteration => 0.00622618517745\n",
      "Loss for  47710 th iteration => 0.00622604761098\n",
      "Loss for  47711 th iteration => 0.00622588307689\n",
      "Loss for  47712 th iteration => 0.00622578141003\n",
      "Loss for  47713 th iteration => 0.00622563290384\n",
      "Loss for  47714 th iteration => 0.00622546985043\n",
      "Loss for  47715 th iteration => 0.00622533751966\n",
      "Loss for  47716 th iteration => 0.00622524099312\n",
      "Loss for  47717 th iteration => 0.00622505397288\n",
      "Loss for  47718 th iteration => 0.00622494982261\n",
      "Loss for  47719 th iteration => 0.00622478481406\n",
      "Loss for  47720 th iteration => 0.00622461313438\n",
      "Loss for  47721 th iteration => 0.00622447139673\n",
      "Loss for  47722 th iteration => 0.0062243296697\n",
      "Loss for  47723 th iteration => 0.00622418795325\n",
      "Loss for  47724 th iteration => 0.00622404935185\n",
      "Loss for  47725 th iteration => 0.00622394694619\n",
      "Loss for  47726 th iteration => 0.00622378058318\n",
      "Loss for  47727 th iteration => 0.00622364487609\n",
      "Loss for  47728 th iteration => 0.00622351174961\n",
      "Loss for  47729 th iteration => 0.00622334418673\n",
      "Loss for  47730 th iteration => 0.00622327904757\n",
      "Loss for  47731 th iteration => 0.00622307278975\n",
      "Loss for  47732 th iteration => 0.00622293873454\n",
      "Loss for  47733 th iteration => 0.00622280159571\n",
      "Loss for  47734 th iteration => 0.00622264223896\n",
      "Loss for  47735 th iteration => 0.00622258132693\n",
      "Loss for  47736 th iteration => 0.00622237318745\n",
      "Loss for  47737 th iteration => 0.0062222452171\n",
      "Loss for  47738 th iteration => 0.00622210539691\n",
      "Loss for  47739 th iteration => 0.00622194226489\n",
      "Loss for  47740 th iteration => 0.00622183753259\n",
      "Loss for  47741 th iteration => 0.00622165375579\n",
      "Loss for  47742 th iteration => 0.00622155379741\n",
      "Loss for  47743 th iteration => 0.00622138520751\n",
      "Loss for  47744 th iteration => 0.00622125192327\n",
      "Loss for  47745 th iteration => 0.00622111659427\n",
      "Loss for  47746 th iteration => 0.00622095023274\n",
      "Loss for  47747 th iteration => 0.00622085038216\n",
      "Loss for  47748 th iteration => 0.00622070097038\n",
      "Loss for  47749 th iteration => 0.00622053976411\n",
      "Loss for  47750 th iteration => 0.00622040232928\n",
      "Loss for  47751 th iteration => 0.00622026896392\n",
      "Loss for  47752 th iteration => 0.00622015544661\n",
      "Loss for  47753 th iteration => 0.00621997833708\n",
      "Loss for  47754 th iteration => 0.00621985245852\n",
      "Loss for  47755 th iteration => 0.00621971078583\n",
      "Loss for  47756 th iteration => 0.00621954966798\n",
      "Loss for  47757 th iteration => 0.00621944316057\n",
      "Loss for  47758 th iteration => 0.00621926045179\n",
      "Loss for  47759 th iteration => 0.00621912178375\n",
      "Loss for  47760 th iteration => 0.0062190170791\n",
      "Loss for  47761 th iteration => 0.00621885345705\n",
      "Loss for  47762 th iteration => 0.00621871543266\n",
      "Loss for  47763 th iteration => 0.00621858506541\n",
      "Loss for  47764 th iteration => 0.00621841591186\n",
      "Loss for  47765 th iteration => 0.00621827445704\n",
      "Loss for  47766 th iteration => 0.00621813458678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  47767 th iteration => 0.00621803199019\n",
      "Loss for  47768 th iteration => 0.00621786388294\n",
      "Loss for  47769 th iteration => 0.00621773586166\n",
      "Loss for  47770 th iteration => 0.00621764075444\n",
      "Loss for  47771 th iteration => 0.00621745298385\n",
      "Loss for  47772 th iteration => 0.00621735024263\n",
      "Loss for  47773 th iteration => 0.00621718041468\n",
      "Loss for  47774 th iteration => 0.00621701416589\n",
      "Loss for  47775 th iteration => 0.00621691301419\n",
      "Loss for  47776 th iteration => 0.0062167284379\n",
      "Loss for  47777 th iteration => 0.00621658706078\n",
      "Loss for  47778 th iteration => 0.00621644751793\n",
      "Loss for  47779 th iteration => 0.00621635142871\n",
      "Loss for  47780 th iteration => 0.0062161794134\n",
      "Loss for  47781 th iteration => 0.00621605004974\n",
      "Loss for  47782 th iteration => 0.00621591124408\n",
      "Loss for  47783 th iteration => 0.00621574890728\n",
      "Loss for  47784 th iteration => 0.00621568319842\n",
      "Loss for  47785 th iteration => 0.00621547344354\n",
      "Loss for  47786 th iteration => 0.00621534518301\n",
      "Loss for  47787 th iteration => 0.00621525046374\n",
      "Loss for  47788 th iteration => 0.0062150625152\n",
      "Loss for  47789 th iteration => 0.0062149601583\n",
      "Loss for  47790 th iteration => 0.0062147901559\n",
      "Loss for  47791 th iteration => 0.00621462589616\n",
      "Loss for  47792 th iteration => 0.00621452297326\n",
      "Loss for  47793 th iteration => 0.00621434046501\n",
      "Loss for  47794 th iteration => 0.00621419919236\n",
      "Loss for  47795 th iteration => 0.00621405802754\n",
      "Loss for  47796 th iteration => 0.00621396354864\n",
      "Loss for  47797 th iteration => 0.00621379013963\n",
      "Loss for  47798 th iteration => 0.00621366237026\n",
      "Loss for  47799 th iteration => 0.0062135221869\n",
      "Loss for  47800 th iteration => 0.00621336137516\n",
      "Loss for  47801 th iteration => 0.00621325575228\n",
      "Loss for  47802 th iteration => 0.00621311218155\n",
      "Loss for  47803 th iteration => 0.00621294673092\n",
      "Loss for  47804 th iteration => 0.00621281648489\n",
      "Loss for  47805 th iteration => 0.00621272070833\n",
      "Loss for  47806 th iteration => 0.00621253404366\n",
      "Loss for  47807 th iteration => 0.00621243064485\n",
      "Loss for  47808 th iteration => 0.00621226438626\n",
      "Loss for  47809 th iteration => 0.00621209515661\n",
      "Loss for  47810 th iteration => 0.00621199740376\n",
      "Loss for  47811 th iteration => 0.00621181312757\n",
      "Loss for  47812 th iteration => 0.00621167196979\n",
      "Loss for  47813 th iteration => 0.00621153279041\n",
      "Loss for  47814 th iteration => 0.00621143346021\n",
      "Loss for  47815 th iteration => 0.00621126510458\n",
      "Loss for  47816 th iteration => 0.00621113254365\n",
      "Loss for  47817 th iteration => 0.00621099735409\n",
      "Loss for  47818 th iteration => 0.00621083181149\n",
      "Loss for  47819 th iteration => 0.00621076715082\n",
      "Loss for  47820 th iteration => 0.0062105600323\n",
      "Loss for  47821 th iteration => 0.00621042951303\n",
      "Loss for  47822 th iteration => 0.00621033410975\n",
      "Loss for  47823 th iteration => 0.00621014728138\n",
      "Loss for  47824 th iteration => 0.00621004425243\n",
      "Loss for  47825 th iteration => 0.00620987787247\n",
      "Loss for  47826 th iteration => 0.00620971054722\n",
      "Loss for  47827 th iteration => 0.00620961110698\n",
      "Loss for  47828 th iteration => 0.00620942886137\n",
      "Loss for  47829 th iteration => 0.00620928780785\n",
      "Loss for  47830 th iteration => 0.00620914703994\n",
      "Loss for  47831 th iteration => 0.00620904924381\n",
      "Loss for  47832 th iteration => 0.00620887956993\n",
      "Loss for  47833 th iteration => 0.00620874852793\n",
      "Loss for  47834 th iteration => 0.00620861203521\n",
      "Loss for  47835 th iteration => 0.00620844799498\n",
      "Loss for  47836 th iteration => 0.00620834599379\n",
      "Loss for  47837 th iteration => 0.0062081999296\n",
      "Loss for  47838 th iteration => 0.00620803726982\n",
      "Loss for  47839 th iteration => 0.00620790473435\n",
      "Loss for  47840 th iteration => 0.00620780826163\n",
      "Loss for  47841 th iteration => 0.00620762272882\n",
      "Loss for  47842 th iteration => 0.00620751864597\n",
      "Loss for  47843 th iteration => 0.0062073560559\n",
      "Loss for  47844 th iteration => 0.00620718657395\n",
      "Loss for  47845 th iteration => 0.00620704561055\n",
      "Loss for  47846 th iteration => 0.00620690465777\n",
      "Loss for  47847 th iteration => 0.00620676371556\n",
      "Loss for  47848 th iteration => 0.00620662515977\n",
      "Loss for  47849 th iteration => 0.00620652201991\n",
      "Loss for  47850 th iteration => 0.00620635791317\n",
      "Loss for  47851 th iteration => 0.00620622152437\n",
      "Loss for  47852 th iteration => 0.00620609060185\n",
      "Loss for  47853 th iteration => 0.00620592275171\n",
      "Loss for  47854 th iteration => 0.00620581644151\n",
      "Loss for  47855 th iteration => 0.00620565314133\n",
      "Loss for  47856 th iteration => 0.00620551908262\n",
      "Loss for  47857 th iteration => 0.00620538349005\n",
      "Loss for  47858 th iteration => 0.00620522354471\n",
      "Loss for  47859 th iteration => 0.00620516211519\n",
      "Loss for  47860 th iteration => 0.00620495765431\n",
      "Loss for  47861 th iteration => 0.00620482838461\n",
      "Loss for  47862 th iteration => 0.00620469140248\n",
      "Loss for  47863 th iteration => 0.00620452697212\n",
      "Loss for  47864 th iteration => 0.0062044250769\n",
      "Loss for  47865 th iteration => 0.00620424166592\n",
      "Loss for  47866 th iteration => 0.00620410084594\n",
      "Loss for  47867 th iteration => 0.00620396179354\n",
      "Loss for  47868 th iteration => 0.00620386641208\n",
      "Loss for  47869 th iteration => 0.00620369476448\n",
      "Loss for  47870 th iteration => 0.0062035661885\n",
      "Loss for  47871 th iteration => 0.00620342980029\n",
      "Loss for  47872 th iteration => 0.00620329812651\n",
      "Loss for  47873 th iteration => 0.00620317490419\n",
      "Loss for  47874 th iteration => 0.00620299603686\n",
      "Loss for  47875 th iteration => 0.00620286507916\n",
      "Loss for  47876 th iteration => 0.00620274962084\n",
      "Loss for  47877 th iteration => 0.0062025763976\n",
      "Loss for  47878 th iteration => 0.00620244817907\n",
      "Loss for  47879 th iteration => 0.00620231037835\n",
      "Loss for  47880 th iteration => 0.006202146935\n",
      "Loss for  47881 th iteration => 0.00620204428516\n",
      "Loss for  47882 th iteration => 0.0062018624661\n",
      "Loss for  47883 th iteration => 0.0062017217478\n",
      "Loss for  47884 th iteration => 0.00620158159457\n",
      "Loss for  47885 th iteration => 0.0062014866944\n",
      "Loss for  47886 th iteration => 0.00620131479496\n",
      "Loss for  47887 th iteration => 0.00620118664354\n",
      "Loss for  47888 th iteration => 0.00620104793066\n",
      "Loss for  47889 th iteration => 0.00620088677573\n",
      "Loss for  47890 th iteration => 0.006200782668\n",
      "Loss for  47891 th iteration => 0.00620063903781\n",
      "Loss for  47892 th iteration => 0.00620047432987\n",
      "Loss for  47893 th iteration => 0.00620034473656\n",
      "Loss for  47894 th iteration => 0.00620024906536\n",
      "Loss for  47895 th iteration => 0.00620006339689\n",
      "Loss for  47896 th iteration => 0.00619996011489\n",
      "Loss for  47897 th iteration => 0.00619979490434\n",
      "Loss for  47898 th iteration => 0.00619962571282\n",
      "Loss for  47899 th iteration => 0.00619952900685\n",
      "Loss for  47900 th iteration => 0.00619934506937\n",
      "Loss for  47901 th iteration => 0.00619920446571\n",
      "Loss for  47902 th iteration => 0.00619906661736\n",
      "Loss for  47903 th iteration => 0.00619896615829\n",
      "Loss for  47904 th iteration => 0.00619880001572\n",
      "Loss for  47905 th iteration => 0.00619866637307\n",
      "Loss for  47906 th iteration => 0.00619853334958\n",
      "Loss for  47907 th iteration => 0.00619836711261\n",
      "Loss for  47908 th iteration => 0.00619830380351\n",
      "Loss for  47909 th iteration => 0.00619809726898\n",
      "Loss for  47910 th iteration => 0.00619796716369\n",
      "Loss for  47911 th iteration => 0.00619787171672\n",
      "Loss for  47912 th iteration => 0.00619768603278\n",
      "Loss for  47913 th iteration => 0.00619758297343\n",
      "Loss for  47914 th iteration => 0.00619741808536\n",
      "Loss for  47915 th iteration => 0.00619725043783\n",
      "Loss for  47916 th iteration => 0.00619710991359\n",
      "Loss for  47917 th iteration => 0.0061969694\n",
      "Loss for  47918 th iteration => 0.006196828897\n",
      "Loss for  47919 th iteration => 0.00619668994473\n",
      "Loss for  47920 th iteration => 0.00619658996342\n",
      "Loss for  47921 th iteration => 0.00619642357705\n",
      "Loss for  47922 th iteration => 0.00619629034097\n",
      "Loss for  47923 th iteration => 0.00619615714466\n",
      "Loss for  47924 th iteration => 0.00619599090176\n",
      "Loss for  47925 th iteration => 0.0061958930357\n",
      "Loss for  47926 th iteration => 0.00619574442296\n",
      "Loss for  47927 th iteration => 0.00619558414237\n",
      "Loss for  47928 th iteration => 0.00619544802834\n",
      "Loss for  47929 th iteration => 0.00619531589218\n",
      "Loss for  47930 th iteration => 0.00619520277516\n",
      "Loss for  47931 th iteration => 0.00619502757138\n",
      "Loss for  47932 th iteration => 0.00619490204643\n",
      "Loss for  47933 th iteration => 0.00619476219321\n",
      "Loss for  47934 th iteration => 0.00619460151471\n",
      "Loss for  47935 th iteration => 0.00619449674136\n",
      "Loss for  47936 th iteration => 0.00619431530364\n",
      "Loss for  47937 th iteration => 0.00619421561437\n",
      "Loss for  47938 th iteration => 0.00619404916336\n",
      "Loss for  47939 th iteration => 0.0061939161661\n",
      "Loss for  47940 th iteration => 0.00619378295824\n",
      "Loss for  47941 th iteration => 0.00619361732713\n",
      "Loss for  47942 th iteration => 0.00619347697031\n",
      "Loss for  47943 th iteration => 0.00619333662406\n",
      "Loss for  47944 th iteration => 0.00619319847479\n",
      "Loss for  47945 th iteration => 0.00619309558098\n",
      "Loss for  47946 th iteration => 0.00619292994856\n",
      "Loss for  47947 th iteration => 0.0061928028713\n",
      "Loss for  47948 th iteration => 0.00619270756895\n",
      "Loss for  47949 th iteration => 0.00619252219583\n",
      "Loss for  47950 th iteration => 0.00619241928589\n",
      "Loss for  47951 th iteration => 0.00619225248644\n",
      "Loss for  47952 th iteration => 0.0061920849559\n",
      "Loss for  47953 th iteration => 0.00619198724946\n",
      "Loss for  47954 th iteration => 0.00619180274459\n",
      "Loss for  47955 th iteration => 0.00619166267608\n",
      "Loss for  47956 th iteration => 0.00619156984071\n",
      "Loss for  47957 th iteration => 0.00619139674854\n",
      "Loss for  47958 th iteration => 0.00619127066952\n",
      "Loss for  47959 th iteration => 0.00619113075636\n",
      "Loss for  47960 th iteration => 0.00619097168142\n",
      "Loss for  47961 th iteration => 0.00619086594371\n",
      "Loss for  47962 th iteration => 0.00619072472879\n",
      "Loss for  47963 th iteration => 0.00619055884323\n",
      "Loss for  47964 th iteration => 0.00619043094571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  47965 th iteration => 0.00619033577126\n",
      "Loss for  47966 th iteration => 0.00619015047879\n",
      "Loss for  47967 th iteration => 0.00619004769711\n",
      "Loss for  47968 th iteration => 0.00618988165585\n",
      "Loss for  47969 th iteration => 0.00618971461061\n",
      "Loss for  47970 th iteration => 0.00618961662782\n",
      "Loss for  47971 th iteration => 0.00618943350821\n",
      "Loss for  47972 th iteration => 0.00618929334336\n",
      "Loss for  47973 th iteration => 0.006189155922\n",
      "Loss for  47974 th iteration => 0.00618905677781\n",
      "Loss for  47975 th iteration => 0.00618889018844\n",
      "Loss for  47976 th iteration => 0.00618875787669\n",
      "Loss for  47977 th iteration => 0.00618862439045\n",
      "Loss for  47978 th iteration => 0.00618845934835\n",
      "Loss for  47979 th iteration => 0.00618839658375\n",
      "Loss for  47980 th iteration => 0.00618818948965\n",
      "Loss for  47981 th iteration => 0.00618806083576\n",
      "Loss for  47982 th iteration => 0.00618796572749\n",
      "Loss for  47983 th iteration => 0.00618778057704\n",
      "Loss for  47984 th iteration => 0.0061876778619\n",
      "Loss for  47985 th iteration => 0.00618751260635\n",
      "Loss for  47986 th iteration => 0.00618734605156\n",
      "Loss for  47987 th iteration => 0.00618724778724\n",
      "Loss for  47988 th iteration => 0.00618706605622\n",
      "Loss for  47989 th iteration => 0.00618692599526\n",
      "Loss for  47990 th iteration => 0.00618678757196\n",
      "Loss for  47991 th iteration => 0.00618668867396\n",
      "Loss for  47992 th iteration => 0.00618652204677\n",
      "Loss for  47993 th iteration => 0.00618638998073\n",
      "Loss for  47994 th iteration => 0.00618625645713\n",
      "Loss for  47995 th iteration => 0.00618609146997\n",
      "Loss for  47996 th iteration => 0.00618599303432\n",
      "Loss for  47997 th iteration => 0.00618584588564\n",
      "Loss for  47998 th iteration => 0.00618568500618\n",
      "Loss for  47999 th iteration => 0.00618555388112\n",
      "Loss for  48000 th iteration => 0.00618545741761\n",
      "Loss for  48001 th iteration => 0.00618527384704\n",
      "Loss for  48002 th iteration => 0.00618516979563\n",
      "Loss for  48003 th iteration => 0.00618500917796\n",
      "Loss for  48004 th iteration => 0.00618483958879\n",
      "Loss for  48005 th iteration => 0.00618469961754\n",
      "Loss for  48006 th iteration => 0.0061845596569\n",
      "Loss for  48007 th iteration => 0.00618442035852\n",
      "Loss for  48008 th iteration => 0.00618432143013\n",
      "Loss for  48009 th iteration => 0.00618415506392\n",
      "Loss for  48010 th iteration => 0.00618402290215\n",
      "Loss for  48011 th iteration => 0.0061838897047\n",
      "Loss for  48012 th iteration => 0.00618372455737\n",
      "Loss for  48013 th iteration => 0.00618362573593\n",
      "Loss for  48014 th iteration => 0.00618347939992\n",
      "Loss for  48015 th iteration => 0.00618331883325\n",
      "Loss for  48016 th iteration => 0.00618318409867\n",
      "Loss for  48017 th iteration => 0.00618305110963\n",
      "Loss for  48018 th iteration => 0.00618289258081\n",
      "Loss for  48019 th iteration => 0.00618279665495\n",
      "Loss for  48020 th iteration => 0.00618262832944\n",
      "Loss for  48021 th iteration => 0.00618249708944\n",
      "Loss for  48022 th iteration => 0.00618236400403\n",
      "Loss for  48023 th iteration => 0.00618219772004\n",
      "Loss for  48024 th iteration => 0.00618209960528\n",
      "Loss for  48025 th iteration => 0.00618191898811\n",
      "Loss for  48026 th iteration => 0.00618181322451\n",
      "Loss for  48027 th iteration => 0.00618165390263\n",
      "Loss for  48028 th iteration => 0.00618151493585\n",
      "Loss for  48029 th iteration => 0.00618138875262\n",
      "Loss for  48030 th iteration => 0.00618122074835\n",
      "Loss for  48031 th iteration => 0.00618108094397\n",
      "Loss for  48032 th iteration => 0.0061809425494\n",
      "Loss for  48033 th iteration => 0.00618084065846\n",
      "Loss for  48034 th iteration => 0.00618067509507\n",
      "Loss for  48035 th iteration => 0.00618054821108\n",
      "Loss for  48036 th iteration => 0.00618045318488\n",
      "Loss for  48037 th iteration => 0.00618026861398\n",
      "Loss for  48038 th iteration => 0.00618016598891\n",
      "Loss for  48039 th iteration => 0.00618000026816\n",
      "Loss for  48040 th iteration => 0.00617983357534\n",
      "Loss for  48041 th iteration => 0.00617973609799\n",
      "Loss for  48042 th iteration => 0.00617955300835\n",
      "Loss for  48043 th iteration => 0.00617941328121\n",
      "Loss for  48044 th iteration => 0.00617927703786\n",
      "Loss for  48045 th iteration => 0.00617917750588\n",
      "Loss for  48046 th iteration => 0.00617901216154\n",
      "Loss for  48047 th iteration => 0.00617887949693\n",
      "Loss for  48048 th iteration => 0.00617874722092\n",
      "Loss for  48049 th iteration => 0.00617858231454\n",
      "Loss for  48050 th iteration => 0.00617852037512\n",
      "Loss for  48051 th iteration => 0.00617831335753\n",
      "Loss for  48052 th iteration => 0.0061781855244\n",
      "Loss for  48053 th iteration => 0.00617809044389\n",
      "Loss for  48054 th iteration => 0.00617790613487\n",
      "Loss for  48055 th iteration => 0.00617780345733\n",
      "Loss for  48056 th iteration => 0.00617763888093\n",
      "Loss for  48057 th iteration => 0.0061774721206\n",
      "Loss for  48058 th iteration => 0.00617737491641\n",
      "Loss for  48059 th iteration => 0.00617719296338\n",
      "Loss for  48060 th iteration => 0.00617705333995\n",
      "Loss for  48061 th iteration => 0.00617691632492\n",
      "Loss for  48062 th iteration => 0.00617681653053\n",
      "Loss for  48063 th iteration => 0.00617665165396\n",
      "Loss for  48064 th iteration => 0.00617651873221\n",
      "Loss for  48065 th iteration => 0.00617638691869\n",
      "Loss for  48066 th iteration => 0.00617622111608\n",
      "Loss for  48067 th iteration => 0.0061761249933\n",
      "Loss for  48068 th iteration => 0.00617597658698\n",
      "Loss for  48069 th iteration => 0.00617581755494\n",
      "Loss for  48070 th iteration => 0.00617572532286\n",
      "Loss for  48071 th iteration => 0.00617556612649\n",
      "Loss for  48072 th iteration => 0.00617541085217\n",
      "Loss for  48073 th iteration => 0.00617531446449\n",
      "Loss for  48074 th iteration => 0.0061751129886\n",
      "Loss for  48075 th iteration => 0.00617497344432\n",
      "Loss for  48076 th iteration => 0.00617483391068\n",
      "Loss for  48077 th iteration => 0.00617469438762\n",
      "Loss for  48078 th iteration => 0.00617455683502\n",
      "Loss for  48079 th iteration => 0.00617445627716\n",
      "Loss for  48080 th iteration => 0.00617429239\n",
      "Loss for  48081 th iteration => 0.00617415864979\n",
      "Loss for  48082 th iteration => 0.00617402788054\n",
      "Loss for  48083 th iteration => 0.00617386168743\n",
      "Loss for  48084 th iteration => 0.00617372470846\n",
      "Loss for  48085 th iteration => 0.00617361619778\n",
      "Loss for  48086 th iteration => 0.00617345789545\n",
      "Loss for  48087 th iteration => 0.0061733217586\n",
      "Loss for  48088 th iteration => 0.00617319225175\n",
      "Loss for  48089 th iteration => 0.00617307773857\n",
      "Loss for  48090 th iteration => 0.00617290568599\n",
      "Loss for  48091 th iteration => 0.00617277897131\n",
      "Loss for  48092 th iteration => 0.00617264224532\n",
      "Loss for  48093 th iteration => 0.00617248040115\n",
      "Loss for  48094 th iteration => 0.0061723787311\n",
      "Loss for  48095 th iteration => 0.00617219722014\n",
      "Loss for  48096 th iteration => 0.00617205781624\n",
      "Loss for  48097 th iteration => 0.00617192091486\n",
      "Loss for  48098 th iteration => 0.00617182552682\n",
      "Loss for  48099 th iteration => 0.00617165669224\n",
      "Loss for  48100 th iteration => 0.00617152815168\n",
      "Loss for  48101 th iteration => 0.00617139240529\n",
      "Loss for  48102 th iteration => 0.00617123095899\n",
      "Loss for  48103 th iteration => 0.00617113094584\n",
      "Loss for  48104 th iteration => 0.00617098609446\n",
      "Loss for  48105 th iteration => 0.00617082406333\n",
      "Loss for  48106 th iteration => 0.00617073941975\n",
      "Loss for  48107 th iteration => 0.00617057309404\n",
      "Loss for  48108 th iteration => 0.00617042535383\n",
      "Loss for  48109 th iteration => 0.00617032189117\n",
      "Loss for  48110 th iteration => 0.00617012398004\n",
      "Loss for  48111 th iteration => 0.0061700234331\n",
      "Loss for  48112 th iteration => 0.00616984266494\n",
      "Loss for  48113 th iteration => 0.00616970336486\n",
      "Loss for  48114 th iteration => 0.00616956598722\n",
      "Loss for  48115 th iteration => 0.00616946969575\n",
      "Loss for  48116 th iteration => 0.00616930196613\n",
      "Loss for  48117 th iteration => 0.0061691725362\n",
      "Loss for  48118 th iteration => 0.00616903788074\n",
      "Loss for  48119 th iteration => 0.00616887555902\n",
      "Loss for  48120 th iteration => 0.00616877620772\n",
      "Loss for  48121 th iteration => 0.00616863107262\n",
      "Loss for  48122 th iteration => 0.00616846955371\n",
      "Loss for  48123 th iteration => 0.00616838287833\n",
      "Loss for  48124 th iteration => 0.00616821877749\n",
      "Loss for  48125 th iteration => 0.00616806903896\n",
      "Loss for  48126 th iteration => 0.00616796776785\n",
      "Loss for  48127 th iteration => 0.00616776926743\n",
      "Loss for  48128 th iteration => 0.00616766994225\n",
      "Loss for  48129 th iteration => 0.00616748987051\n",
      "Loss for  48130 th iteration => 0.0061673506742\n",
      "Loss for  48131 th iteration => 0.00616721286479\n",
      "Loss for  48132 th iteration => 0.00616711557478\n",
      "Loss for  48133 th iteration => 0.00616694904463\n",
      "Loss for  48134 th iteration => 0.00616681863126\n",
      "Loss for  48135 th iteration => 0.00616668516021\n",
      "Loss for  48136 th iteration => 0.00616652187003\n",
      "Loss for  48137 th iteration => 0.0061664233077\n",
      "Loss for  48138 th iteration => 0.00616627777152\n",
      "Loss for  48139 th iteration => 0.00616611729511\n",
      "Loss for  48140 th iteration => 0.00616598740176\n",
      "Loss for  48141 th iteration => 0.00616589174142\n",
      "Loss for  48142 th iteration => 0.00616570908694\n",
      "Loss for  48143 th iteration => 0.00616560585231\n",
      "Loss for  48144 th iteration => 0.00616544576101\n",
      "Loss for  48145 th iteration => 0.00616527719952\n",
      "Loss for  48146 th iteration => 0.00616513809266\n",
      "Loss for  48147 th iteration => 0.0061649989964\n",
      "Loss for  48148 th iteration => 0.00616486086375\n",
      "Loss for  48149 th iteration => 0.00616476233664\n",
      "Loss for  48150 th iteration => 0.00616459726715\n",
      "Loss for  48151 th iteration => 0.00616446556554\n",
      "Loss for  48152 th iteration => 0.00616433360615\n",
      "Loss for  48153 th iteration => 0.00616416897732\n",
      "Loss for  48154 th iteration => 0.00616407164491\n",
      "Loss for  48155 th iteration => 0.00616392541366\n",
      "Loss for  48156 th iteration => 0.00616376621668\n",
      "Loss for  48157 th iteration => 0.00616363186537\n",
      "Loss for  48158 th iteration => 0.00616350027649\n",
      "Loss for  48159 th iteration => 0.00616338885317\n",
      "Loss for  48160 th iteration => 0.00616321540588\n",
      "Loss for  48161 th iteration => 0.00616309098612\n",
      "Loss for  48162 th iteration => 0.00616295278891\n",
      "Loss for  48163 th iteration => 0.0061627933155\n",
      "Loss for  48164 th iteration => 0.00616269009865\n",
      "Loss for  48165 th iteration => 0.00616251095013\n",
      "Loss for  48166 th iteration => 0.00616241079372\n",
      "Loss for  48167 th iteration => 0.00616224756977\n",
      "Loss for  48168 th iteration => 0.00616211420806\n",
      "Loss for  48169 th iteration => 0.00616198412492\n",
      "Loss for  48170 th iteration => 0.00616181969929\n",
      "Loss for  48171 th iteration => 0.00616168074856\n",
      "Loss for  48172 th iteration => 0.00616154180839\n",
      "Loss for  48173 th iteration => 0.00616140522205\n",
      "Loss for  48174 th iteration => 0.00616130224686\n",
      "Loss for  48175 th iteration => 0.00616113955085\n",
      "Loss for  48176 th iteration => 0.00616105648932\n",
      "Loss for  48177 th iteration => 0.00616088941475\n",
      "Loss for  48178 th iteration => 0.00616074329367\n",
      "Loss for  48179 th iteration => 0.00616063904547\n",
      "Loss for  48180 th iteration => 0.00616044273483\n",
      "Loss for  48181 th iteration => 0.00616034190731\n",
      "Loss for  48182 th iteration => 0.00616016218928\n",
      "Loss for  48183 th iteration => 0.00616002331581\n",
      "Loss for  48184 th iteration => 0.00615988593194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  48185 th iteration => 0.00615979039482\n",
      "Loss for  48186 th iteration => 0.00615962275194\n",
      "Loss for  48187 th iteration => 0.00615949409934\n",
      "Loss for  48188 th iteration => 0.00615935950773\n",
      "Loss for  48189 th iteration => 0.00615919798612\n",
      "Loss for  48190 th iteration => 0.00615909846013\n",
      "Loss for  48191 th iteration => 0.00615895417306\n",
      "Loss for  48192 th iteration => 0.00615879299623\n",
      "Loss for  48193 th iteration => 0.00615866469347\n",
      "Loss for  48194 th iteration => 0.00615856955359\n",
      "Loss for  48195 th iteration => 0.00615838702016\n",
      "Loss for  48196 th iteration => 0.00615828430868\n",
      "Loss for  48197 th iteration => 0.00615812313913\n",
      "Loss for  48198 th iteration => 0.00615795467052\n",
      "Loss for  48199 th iteration => 0.00615781588645\n",
      "Loss for  48200 th iteration => 0.00615767711297\n",
      "Loss for  48201 th iteration => 0.00615753963742\n",
      "Loss for  48202 th iteration => 0.00615744235627\n",
      "Loss for  48203 th iteration => 0.00615727667814\n",
      "Loss for  48204 th iteration => 0.00615714623626\n",
      "Loss for  48205 th iteration => 0.00615701365454\n",
      "Loss for  48206 th iteration => 0.00615685029907\n",
      "Loss for  48207 th iteration => 0.00615675268121\n",
      "Loss for  48208 th iteration => 0.00615660706705\n",
      "Loss for  48209 th iteration => 0.00615644757472\n",
      "Loss for  48210 th iteration => 0.0061563141696\n",
      "Loss for  48211 th iteration => 0.00615618276448\n",
      "Loss for  48212 th iteration => 0.00615607248592\n",
      "Loss for  48213 th iteration => 0.00615593291382\n",
      "Loss for  48214 th iteration => 0.00615577547763\n",
      "Loss for  48215 th iteration => 0.00615563669085\n",
      "Loss for  48216 th iteration => 0.00615547849492\n",
      "Loss for  48217 th iteration => 0.00615537461713\n",
      "Loss for  48218 th iteration => 0.00615519581849\n",
      "Loss for  48219 th iteration => 0.00615509683314\n",
      "Loss for  48220 th iteration => 0.00615493305607\n",
      "Loss for  48221 th iteration => 0.00615480093331\n",
      "Loss for  48222 th iteration => 0.00615467022938\n",
      "Loss for  48223 th iteration => 0.00615450581856\n",
      "Loss for  48224 th iteration => 0.00615436918492\n",
      "Loss for  48225 th iteration => 0.00615426162383\n",
      "Loss for  48226 th iteration => 0.00615410405693\n",
      "Loss for  48227 th iteration => 0.00615396891008\n",
      "Loss for  48228 th iteration => 0.00615383951078\n",
      "Loss for  48229 th iteration => 0.00615372635071\n",
      "Loss for  48230 th iteration => 0.00615355516763\n",
      "Loss for  48231 th iteration => 0.00615342931788\n",
      "Loss for  48232 th iteration => 0.00615329340308\n",
      "Loss for  48233 th iteration => 0.00615313248182\n",
      "Loss for  48234 th iteration => 0.00615303156522\n",
      "Loss for  48235 th iteration => 0.00615285156679\n",
      "Loss for  48236 th iteration => 0.00615271301582\n",
      "Loss for  48237 th iteration => 0.00615257668691\n",
      "Loss for  48238 th iteration => 0.00615248150198\n",
      "Loss for  48239 th iteration => 0.00615231413954\n",
      "Loss for  48240 th iteration => 0.0061521858619\n",
      "Loss for  48241 th iteration => 0.00615205152804\n",
      "Loss for  48242 th iteration => 0.00615189040396\n",
      "Loss for  48243 th iteration => 0.00615179172357\n",
      "Loss for  48244 th iteration => 0.00615164707043\n",
      "Loss for  48245 th iteration => 0.00615148683748\n",
      "Loss for  48246 th iteration => 0.00615140084166\n",
      "Loss for  48247 th iteration => 0.00615123750834\n",
      "Loss for  48248 th iteration => 0.00615108854575\n",
      "Loss for  48249 th iteration => 0.00615098794665\n",
      "Loss for  48250 th iteration => 0.00615078987526\n",
      "Loss for  48251 th iteration => 0.00615069124725\n",
      "Loss for  48252 th iteration => 0.0061505114581\n",
      "Loss for  48253 th iteration => 0.00615037301066\n",
      "Loss for  48254 th iteration => 0.00615023669809\n",
      "Loss for  48255 th iteration => 0.00615013953117\n",
      "Loss for  48256 th iteration => 0.0061499743461\n",
      "Loss for  48257 th iteration => 0.00614984411261\n",
      "Loss for  48258 th iteration => 0.00614971193005\n",
      "Loss for  48259 th iteration => 0.00614954887598\n",
      "Loss for  48260 th iteration => 0.00614945230044\n",
      "Loss for  48261 th iteration => 0.00614930602006\n",
      "Loss for  48262 th iteration => 0.00614914765614\n",
      "Loss for  48263 th iteration => 0.00614905768395\n",
      "Loss for  48264 th iteration => 0.00614889850871\n",
      "Loss for  48265 th iteration => 0.00614874562684\n",
      "Loss for  48266 th iteration => 0.006148649129\n",
      "Loss for  48267 th iteration => 0.00614844907146\n",
      "Loss for  48268 th iteration => 0.00614831070285\n",
      "Loss for  48269 th iteration => 0.00614817234486\n",
      "Loss for  48270 th iteration => 0.00614803399744\n",
      "Loss for  48271 th iteration => 0.00614789789524\n",
      "Loss for  48272 th iteration => 0.0061477983291\n",
      "Loss for  48273 th iteration => 0.00614763575974\n",
      "Loss for  48274 th iteration => 0.00614750309115\n",
      "Loss for  48275 th iteration => 0.00614737356008\n",
      "Loss for  48276 th iteration => 0.00614720836994\n",
      "Loss for  48277 th iteration => 0.00614710563196\n",
      "Loss for  48278 th iteration => 0.00614694387993\n",
      "Loss for  48279 th iteration => 0.00614681352262\n",
      "Loss for  48280 th iteration => 0.00614667934935\n",
      "Loss for  48281 th iteration => 0.00614652350697\n",
      "Loss for  48282 th iteration => 0.00614646329713\n",
      "Loss for  48283 th iteration => 0.00614626178458\n",
      "Loss for  48284 th iteration => 0.00614613521168\n",
      "Loss for  48285 th iteration => 0.00614600062552\n",
      "Loss for  48286 th iteration => 0.0061458390747\n",
      "Loss for  48287 th iteration => 0.00614573939346\n",
      "Loss for  48288 th iteration => 0.0061455585285\n",
      "Loss for  48289 th iteration => 0.00614542030243\n",
      "Loss for  48290 th iteration => 0.00614528547293\n",
      "Loss for  48291 th iteration => 0.00614518963852\n",
      "Loss for  48292 th iteration => 0.00614502353365\n",
      "Loss for  48293 th iteration => 0.00614489486204\n",
      "Loss for  48294 th iteration => 0.00614479494163\n",
      "Loss for  48295 th iteration => 0.0061446120063\n",
      "Loss for  48296 th iteration => 0.00614451755533\n",
      "Loss for  48297 th iteration => 0.00614434379445\n",
      "Loss for  48298 th iteration => 0.00614418703327\n",
      "Loss for  48299 th iteration => 0.00614409310923\n",
      "Loss for  48300 th iteration => 0.00614392615952\n",
      "Loss for  48301 th iteration => 0.0061437969653\n",
      "Loss for  48302 th iteration => 0.006143665212\n",
      "Loss for  48303 th iteration => 0.00614350101758\n",
      "Loss for  48304 th iteration => 0.0061434041914\n",
      "Loss for  48305 th iteration => 0.00614322326895\n",
      "Loss for  48306 th iteration => 0.00614308514357\n",
      "Loss for  48307 th iteration => 0.00614295064276\n",
      "Loss for  48308 th iteration => 0.00614285214533\n",
      "Loss for  48309 th iteration => 0.00614268891375\n",
      "Loss for  48310 th iteration => 0.00614255739161\n",
      "Loss for  48311 th iteration => 0.0061424271208\n",
      "Loss for  48312 th iteration => 0.0061422638617\n",
      "Loss for  48313 th iteration => 0.00614220273357\n",
      "Loss for  48314 th iteration => 0.00614199835113\n",
      "Loss for  48315 th iteration => 0.00614187172281\n",
      "Loss for  48316 th iteration => 0.0061417776155\n",
      "Loss for  48317 th iteration => 0.00614159551745\n",
      "Loss for  48318 th iteration => 0.00614149384436\n",
      "Loss for  48319 th iteration => 0.00614133197659\n",
      "Loss for  48320 th iteration => 0.00614116568316\n",
      "Loss for  48321 th iteration => 0.00614102763639\n",
      "Loss for  48322 th iteration => 0.00614088960023\n",
      "Loss for  48323 th iteration => 0.00614075180274\n",
      "Loss for  48324 th iteration => 0.00614065716942\n",
      "Loss for  48325 th iteration => 0.00614049030337\n",
      "Loss for  48326 th iteration => 0.00614036253511\n",
      "Loss for  48327 th iteration => 0.00614022873978\n",
      "Loss for  48328 th iteration => 0.00614006808357\n",
      "Loss for  48329 th iteration => 0.00613996845744\n",
      "Loss for  48330 th iteration => 0.00613982600429\n",
      "Loss for  48331 th iteration => 0.00613966590518\n",
      "Loss for  48332 th iteration => 0.00613953459204\n",
      "Loss for  48333 th iteration => 0.00613940198301\n",
      "Loss for  48334 th iteration => 0.00613924687744\n",
      "Loss for  48335 th iteration => 0.0061391862901\n",
      "Loss for  48336 th iteration => 0.00613898578345\n",
      "Loss for  48337 th iteration => 0.00613885778937\n",
      "Loss for  48338 th iteration => 0.00613872522123\n",
      "Loss for  48339 th iteration => 0.006138562361\n",
      "Loss for  48340 th iteration => 0.00613846515767\n",
      "Loss for  48341 th iteration => 0.0061383236362\n",
      "Loss for  48342 th iteration => 0.00613815963314\n",
      "Loss for  48343 th iteration => 0.00613802922656\n",
      "Loss for  48344 th iteration => 0.00613789826152\n",
      "Loss for  48345 th iteration => 0.00613773524012\n",
      "Loss for  48346 th iteration => 0.00613759882502\n",
      "Loss for  48347 th iteration => 0.00613749277381\n",
      "Loss for  48348 th iteration => 0.00613733515394\n",
      "Loss for  48349 th iteration => 0.00613720154871\n",
      "Loss for  48350 th iteration => 0.00613707164808\n",
      "Loss for  48351 th iteration => 0.00613696021232\n",
      "Loss for  48352 th iteration => 0.00613678930392\n",
      "Loss for  48353 th iteration => 0.00613666467546\n",
      "Loss for  48354 th iteration => 0.00613652898885\n",
      "Loss for  48355 th iteration => 0.00613636933513\n",
      "Loss for  48356 th iteration => 0.00613626860068\n",
      "Loss for  48357 th iteration => 0.00613608981337\n",
      "Loss for  48358 th iteration => 0.00613595199922\n",
      "Loss for  48359 th iteration => 0.00613581632415\n",
      "Loss for  48360 th iteration => 0.00613572161157\n",
      "Loss for  48361 th iteration => 0.00613555522531\n",
      "Loss for  48362 th iteration => 0.00613542746838\n",
      "Loss for  48363 th iteration => 0.00613529406252\n",
      "Loss for  48364 th iteration => 0.00613513350706\n",
      "Loss for  48365 th iteration => 0.00613503561949\n",
      "Loss for  48366 th iteration => 0.00613489164177\n",
      "Loss for  48367 th iteration => 0.00613473239889\n",
      "Loss for  48368 th iteration => 0.00613464635163\n",
      "Loss for  48369 th iteration => 0.00613448449021\n",
      "Loss for  48370 th iteration => 0.00613433557961\n",
      "Loss for  48371 th iteration => 0.00613423634987\n",
      "Loss for  48372 th iteration => 0.00613403864777\n",
      "Loss for  48373 th iteration => 0.0061339412365\n",
      "Loss for  48374 th iteration => 0.00613376218491\n",
      "Loss for  48375 th iteration => 0.00613362447405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  48376 th iteration => 0.0061334892511\n",
      "Loss for  48377 th iteration => 0.00613339160243\n",
      "Loss for  48378 th iteration => 0.00613322834227\n",
      "Loss for  48379 th iteration => 0.00613309768603\n",
      "Loss for  48380 th iteration => 0.00613296736959\n",
      "Loss for  48381 th iteration => 0.00613280402651\n",
      "Loss for  48382 th iteration => 0.00613274323165\n",
      "Loss for  48383 th iteration => 0.0061325400185\n",
      "Loss for  48384 th iteration => 0.00613241310665\n",
      "Loss for  48385 th iteration => 0.00613231924905\n",
      "Loss for  48386 th iteration => 0.00613213772697\n",
      "Loss for  48387 th iteration => 0.00613203630983\n",
      "Loss for  48388 th iteration => 0.00613187555349\n",
      "Loss for  48389 th iteration => 0.00613171080802\n",
      "Loss for  48390 th iteration => 0.00613157317569\n",
      "Loss for  48391 th iteration => 0.00613143555398\n",
      "Loss for  48392 th iteration => 0.00613129794282\n",
      "Loss for  48393 th iteration => 0.00613116336388\n",
      "Loss for  48394 th iteration => 0.00613106236579\n",
      "Loss for  48395 th iteration => 0.00613090266646\n",
      "Loss for  48396 th iteration => 0.00613076863457\n",
      "Loss for  48397 th iteration => 0.00613064190511\n",
      "Loss for  48398 th iteration => 0.00613047720319\n",
      "Loss for  48399 th iteration => 0.00613037374751\n",
      "Loss for  48400 th iteration => 0.00613021415643\n",
      "Loss for  48401 th iteration => 0.00613008313936\n",
      "Loss for  48402 th iteration => 0.00612995106925\n",
      "Loss for  48403 th iteration => 0.00612979555154\n",
      "Loss for  48404 th iteration => 0.00612973483485\n",
      "Loss for  48405 th iteration => 0.00612953616253\n",
      "Loss for  48406 th iteration => 0.00612940784804\n",
      "Loss for  48407 th iteration => 0.0061292764362\n",
      "Loss for  48408 th iteration => 0.00612911322275\n",
      "Loss for  48409 th iteration => 0.00612901663714\n",
      "Loss for  48410 th iteration => 0.00612883567671\n",
      "Loss for  48411 th iteration => 0.00612869913666\n",
      "Loss for  48412 th iteration => 0.00612860750098\n",
      "Loss for  48413 th iteration => 0.00612843864499\n",
      "Loss for  48414 th iteration => 0.0061283140058\n",
      "Loss for  48415 th iteration => 0.00612817915059\n",
      "Loss for  48416 th iteration => 0.00612805287836\n",
      "Loss for  48417 th iteration => 0.00612793070312\n",
      "Loss for  48418 th iteration => 0.00612775764197\n",
      "Loss for  48419 th iteration => 0.00612762839684\n",
      "Loss for  48420 th iteration => 0.00612747289088\n",
      "Loss for  48421 th iteration => 0.00612737612041\n",
      "Loss for  48422 th iteration => 0.00612721344322\n",
      "Loss for  48423 th iteration => 0.00612708149561\n",
      "Loss for  48424 th iteration => 0.00612695392211\n",
      "Loss for  48425 th iteration => 0.00612678706634\n",
      "Loss for  48426 th iteration => 0.00612669432825\n",
      "Loss for  48427 th iteration => 0.00612651285619\n",
      "Loss for  48428 th iteration => 0.00612637715393\n",
      "Loss for  48429 th iteration => 0.00612628174902\n",
      "Loss for  48430 th iteration => 0.00612611686668\n",
      "Loss for  48431 th iteration => 0.00612598845086\n",
      "Loss for  48432 th iteration => 0.00612585651561\n",
      "Loss for  48433 th iteration => 0.00612569533441\n",
      "Loss for  48434 th iteration => 0.00612559845358\n",
      "Loss for  48435 th iteration => 0.00612545457122\n",
      "Loss for  48436 th iteration => 0.00612529602454\n",
      "Loss for  48437 th iteration => 0.00612520851627\n",
      "Loss for  48438 th iteration => 0.00612504890826\n",
      "Loss for  48439 th iteration => 0.00612489861005\n",
      "Loss for  48440 th iteration => 0.00612480156085\n",
      "Loss for  48441 th iteration => 0.00612460372161\n",
      "Loss for  48442 th iteration => 0.00612450772522\n",
      "Loss for  48443 th iteration => 0.00612432964171\n",
      "Loss for  48444 th iteration => 0.00612419234468\n",
      "Loss for  48445 th iteration => 0.0061240573248\n",
      "Loss for  48446 th iteration => 0.00612395848174\n",
      "Loss for  48447 th iteration => 0.0061237972247\n",
      "Loss for  48448 th iteration => 0.0061236654131\n",
      "Loss for  48449 th iteration => 0.00612353706088\n",
      "Loss for  48450 th iteration => 0.00612337296617\n",
      "Loss for  48451 th iteration => 0.00612323858149\n",
      "Loss for  48452 th iteration => 0.00612313157391\n",
      "Loss for  48453 th iteration => 0.00612297711921\n",
      "Loss for  48454 th iteration => 0.00612288238924\n",
      "Loss for  48455 th iteration => 0.00612273021292\n",
      "Loss for  48456 th iteration => 0.00612257401317\n",
      "Loss for  48457 th iteration => 0.00612244752564\n",
      "Loss for  48458 th iteration => 0.00612228317607\n",
      "Loss for  48459 th iteration => 0.00612214595152\n",
      "Loss for  48460 th iteration => 0.00612200873766\n",
      "Loss for  48461 th iteration => 0.00612187153442\n",
      "Loss for  48462 th iteration => 0.00612173434174\n",
      "Loss for  48463 th iteration => 0.00612159789972\n",
      "Loss for  48464 th iteration => 0.00612150724178\n",
      "Loss for  48465 th iteration => 0.00612133804033\n",
      "Loss for  48466 th iteration => 0.00612121433988\n",
      "Loss for  48467 th iteration => 0.00612107811706\n",
      "Loss for  48468 th iteration => 0.00612092162\n",
      "Loss for  48469 th iteration => 0.00612081983363\n",
      "Loss for  48470 th iteration => 0.0061206805501\n",
      "Loss for  48471 th iteration => 0.00612051852784\n",
      "Loss for  48472 th iteration => 0.00612039402318\n",
      "Loss for  48473 th iteration => 0.00612030058856\n",
      "Loss for  48474 th iteration => 0.00612011969223\n",
      "Loss for  48475 th iteration => 0.00612001870929\n",
      "Loss for  48476 th iteration => 0.0061198571028\n",
      "Loss for  48477 th iteration => 0.00611969165736\n",
      "Loss for  48478 th iteration => 0.00611959812336\n",
      "Loss for  48479 th iteration => 0.0061194173818\n",
      "Loss for  48480 th iteration => 0.00611928179614\n",
      "Loss for  48481 th iteration => 0.00611918731769\n",
      "Loss for  48482 th iteration => 0.00611902212253\n",
      "Loss for  48483 th iteration => 0.00611889464649\n",
      "Loss for  48484 th iteration => 0.00611876238514\n",
      "Loss for  48485 th iteration => 0.00611860215694\n",
      "Loss for  48486 th iteration => 0.00611850481347\n",
      "Loss for  48487 th iteration => 0.00611836187596\n",
      "Loss for  48488 th iteration => 0.0061182030327\n",
      "Loss for  48489 th iteration => 0.00611811685445\n",
      "Loss for  48490 th iteration => 0.00611795652155\n",
      "Loss for  48491 th iteration => 0.00611780758226\n",
      "Loss for  48492 th iteration => 0.0061177097796\n",
      "Loss for  48493 th iteration => 0.00611751300651\n",
      "Loss for  48494 th iteration => 0.00611741669493\n",
      "Loss for  48495 th iteration => 0.00611723911979\n",
      "Loss for  48496 th iteration => 0.00611710213323\n",
      "Loss for  48497 th iteration => 0.00611696743865\n",
      "Loss for  48498 th iteration => 0.0061168690667\n",
      "Loss for  48499 th iteration => 0.00611670794973\n",
      "Loss for  48500 th iteration => 0.00611657662763\n",
      "Loss for  48501 th iteration => 0.00611644839716\n",
      "Loss for  48502 th iteration => 0.00611628461526\n",
      "Loss for  48503 th iteration => 0.00611615058634\n",
      "Loss for  48504 th iteration => 0.00611604394451\n",
      "Loss for  48505 th iteration => 0.00611588977113\n",
      "Loss for  48506 th iteration => 0.00611579554419\n",
      "Loss for  48507 th iteration => 0.00611564346518\n",
      "Loss for  48508 th iteration => 0.00611548768908\n",
      "Loss for  48509 th iteration => 0.0061153613573\n",
      "Loss for  48510 th iteration => 0.00611519730329\n",
      "Loss for  48511 th iteration => 0.00611506038913\n",
      "Loss for  48512 th iteration => 0.00611492348565\n",
      "Loss for  48513 th iteration => 0.00611478659279\n",
      "Loss for  48514 th iteration => 0.00611464971049\n",
      "Loss for  48515 th iteration => 0.00611451379703\n",
      "Loss for  48516 th iteration => 0.00611442314981\n",
      "Loss for  48517 th iteration => 0.00611425454623\n",
      "Loss for  48518 th iteration => 0.0061141308801\n",
      "Loss for  48519 th iteration => 0.00611399523163\n",
      "Loss for  48520 th iteration => 0.00611383879228\n",
      "Loss for  48521 th iteration => 0.00611373776576\n",
      "Loss for  48522 th iteration => 0.00611359828747\n",
      "Loss for  48523 th iteration => 0.00611343688141\n",
      "Loss for  48524 th iteration => 0.00611331263511\n",
      "Loss for  48525 th iteration => 0.00611321944032\n",
      "Loss for  48526 th iteration => 0.00611303892107\n",
      "Loss for  48527 th iteration => 0.00611293818441\n",
      "Loss for  48528 th iteration => 0.00611277701996\n",
      "Loss for  48529 th iteration => 0.00611261158116\n",
      "Loss for  48530 th iteration => 0.00611251864718\n",
      "Loss for  48531 th iteration => 0.00611233800486\n",
      "Loss for  48532 th iteration => 0.00611220311796\n",
      "Loss for  48533 th iteration => 0.00611210827605\n",
      "Loss for  48534 th iteration => 0.00611194405082\n",
      "Loss for  48535 th iteration => 0.00611181623907\n",
      "Loss for  48536 th iteration => 0.00611168491999\n",
      "Loss for  48537 th iteration => 0.00611152438356\n",
      "Loss for  48538 th iteration => 0.00611142830128\n",
      "Loss for  48539 th iteration => 0.00611128470098\n",
      "Loss for  48540 th iteration => 0.00611112718944\n",
      "Loss for  48541 th iteration => 0.00611103999676\n",
      "Loss for  48542 th iteration => 0.00611088127002\n",
      "Loss for  48543 th iteration => 0.0061107313735\n",
      "Loss for  48544 th iteration => 0.00611063512021\n",
      "Loss for  48545 th iteration => 0.00611043767539\n",
      "Loss for  48546 th iteration => 0.00611034232387\n",
      "Loss for  48547 th iteration => 0.00611016468281\n",
      "Loss for  48548 th iteration => 0.00611002800639\n",
      "Loss for  48549 th iteration => 0.00610989416463\n",
      "Loss for  48550 th iteration => 0.00610979508904\n",
      "Loss for  48551 th iteration => 0.0061096352802\n",
      "Loss for  48552 th iteration => 0.00610950328596\n",
      "Loss for  48553 th iteration => 0.00610937633221\n",
      "Loss for  48554 th iteration => 0.00610921268279\n",
      "Loss for  48555 th iteration => 0.00610911098232\n",
      "Loss for  48556 th iteration => 0.0061089514575\n",
      "Loss for  48557 th iteration => 0.00610882387946\n",
      "Loss for  48558 th iteration => 0.00610873031668\n",
      "Loss for  48559 th iteration => 0.00610855055122\n",
      "Loss for  48560 th iteration => 0.00610844944061\n",
      "Loss for  48561 th iteration => 0.00610829225134\n",
      "Loss for  48562 th iteration => 0.00610812806987\n",
      "Loss for  48563 th iteration => 0.00610799146848\n",
      "Loss for  48564 th iteration => 0.00610785487772\n",
      "Loss for  48565 th iteration => 0.00610771829754\n",
      "Loss for  48566 th iteration => 0.00610758172786\n",
      "Loss for  48567 th iteration => 0.0061074467153\n",
      "Loss for  48568 th iteration => 0.00610735523784\n",
      "Loss for  48569 th iteration => 0.00610718805061\n",
      "Loss for  48570 th iteration => 0.00610706363718\n",
      "Loss for  48571 th iteration => 0.00610693084411\n",
      "Loss for  48572 th iteration => 0.00610680431761\n",
      "Loss for  48573 th iteration => 0.00610668419339\n",
      "Loss for  48574 th iteration => 0.00610651106107\n",
      "Loss for  48575 th iteration => 0.0061063833514\n",
      "Loss for  48576 th iteration => 0.00610627190615\n",
      "Loss for  48577 th iteration => 0.00610610381508\n",
      "Loss for  48578 th iteration => 0.00610597911709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  48579 th iteration => 0.0061058461318\n",
      "Loss for  48580 th iteration => 0.00610568652382\n",
      "Loss for  48581 th iteration => 0.00610558837587\n",
      "Loss for  48582 th iteration => 0.00610541113731\n",
      "Loss for  48583 th iteration => 0.00610527466795\n",
      "Loss for  48584 th iteration => 0.00610514111981\n",
      "Loss for  48585 th iteration => 0.00610504450721\n",
      "Loss for  48586 th iteration => 0.00610488265194\n",
      "Loss for  48587 th iteration => 0.00610475311177\n",
      "Loss for  48588 th iteration => 0.00610462412052\n",
      "Loss for  48589 th iteration => 0.00610446221364\n",
      "Loss for  48590 th iteration => 0.00610440272235\n",
      "Loss for  48591 th iteration => 0.00610420032479\n",
      "Loss for  48592 th iteration => 0.00610407511921\n",
      "Loss for  48593 th iteration => 0.00610398234097\n",
      "Loss for  48594 th iteration => 0.00610380220773\n",
      "Loss for  48595 th iteration => 0.00610370189263\n",
      "Loss for  48596 th iteration => 0.0061035420863\n",
      "Loss for  48597 th iteration => 0.00610337831994\n",
      "Loss for  48598 th iteration => 0.00610324192877\n",
      "Loss for  48599 th iteration => 0.00610310554819\n",
      "Loss for  48600 th iteration => 0.0061029697083\n",
      "Loss for  48601 th iteration => 0.00610287478108\n",
      "Loss for  48602 th iteration => 0.00610271145735\n",
      "Loss for  48603 th iteration => 0.00610258351809\n",
      "Loss for  48604 th iteration => 0.00610245314265\n",
      "Loss for  48605 th iteration => 0.00610229243718\n",
      "Loss for  48606 th iteration => 0.00610219621214\n",
      "Loss for  48607 th iteration => 0.00610205390316\n",
      "Loss for  48608 th iteration => 0.00610189676797\n",
      "Loss for  48609 th iteration => 0.00610176585309\n",
      "Loss for  48610 th iteration => 0.00610163625329\n",
      "Loss for  48611 th iteration => 0.00610152760004\n",
      "Loss for  48612 th iteration => 0.00610135715556\n",
      "Loss for  48613 th iteration => 0.00610123526108\n",
      "Loss for  48614 th iteration => 0.00610109986529\n",
      "Loss for  48615 th iteration => 0.00610094311735\n",
      "Loss for  48616 th iteration => 0.00610084250254\n",
      "Loss for  48617 th iteration => 0.00610066748536\n",
      "Loss for  48618 th iteration => 0.00610056729617\n",
      "Loss for  48619 th iteration => 0.00610040942986\n",
      "Loss for  48620 th iteration => 0.00610027623971\n",
      "Loss for  48621 th iteration => 0.00610015131063\n",
      "Loss for  48622 th iteration => 0.00609998854221\n",
      "Loss for  48623 th iteration => 0.00609985230602\n",
      "Loss for  48624 th iteration => 0.00609971653768\n",
      "Loss for  48625 th iteration => 0.00609961902602\n",
      "Loss for  48626 th iteration => 0.00609945612506\n",
      "Loss for  48627 th iteration => 0.00609933299792\n",
      "Loss for  48628 th iteration => 0.00609924067153\n",
      "Loss for  48629 th iteration => 0.00609906049733\n",
      "Loss for  48630 th iteration => 0.00609896063404\n",
      "Loss for  48631 th iteration => 0.00609879905847\n",
      "Loss for  48632 th iteration => 0.00609863664016\n",
      "Loss for  48633 th iteration => 0.0060985418941\n",
      "Loss for  48634 th iteration => 0.00609836348311\n",
      "Loss for  48635 th iteration => 0.00609822813571\n",
      "Loss for  48636 th iteration => 0.00609813521082\n",
      "Loss for  48637 th iteration => 0.00609797027515\n",
      "Loss for  48638 th iteration => 0.00609784440177\n",
      "Loss for  48639 th iteration => 0.00609771235101\n",
      "Loss for  48640 th iteration => 0.00609755377417\n",
      "Loss for  48641 th iteration => 0.00609745595952\n",
      "Loss for  48642 th iteration => 0.0060973153252\n",
      "Loss for  48643 th iteration => 0.00609715670314\n",
      "Loss for  48644 th iteration => 0.00609703085753\n",
      "Loss for  48645 th iteration => 0.0060969374403\n",
      "Loss for  48646 th iteration => 0.00609675856004\n",
      "Loss for  48647 th iteration => 0.00609665762092\n",
      "Loss for  48648 th iteration => 0.00609650038674\n",
      "Loss for  48649 th iteration => 0.00609633526243\n",
      "Loss for  48650 th iteration => 0.00609619918092\n",
      "Loss for  48651 th iteration => 0.00609606310998\n",
      "Loss for  48652 th iteration => 0.00609592930858\n",
      "Loss for  48653 th iteration => 0.00609583105809\n",
      "Loss for  48654 th iteration => 0.00609567164798\n",
      "Loss for  48655 th iteration => 0.00609554044547\n",
      "Loss for  48656 th iteration => 0.0060954139238\n",
      "Loss for  48657 th iteration => 0.006095251093\n",
      "Loss for  48658 th iteration => 0.00609511783775\n",
      "Loss for  48659 th iteration => 0.00609501168345\n",
      "Loss for  48660 th iteration => 0.00609485782427\n",
      "Loss for  48661 th iteration => 0.00609472424443\n",
      "Loss for  48662 th iteration => 0.00609459988512\n",
      "Loss for  48663 th iteration => 0.00609448597897\n",
      "Loss for  48664 th iteration => 0.00609431998927\n",
      "Loss for  48665 th iteration => 0.00609419425297\n",
      "Loss for  48666 th iteration => 0.00609406330777\n",
      "Loss for  48667 th iteration => 0.00609390272229\n",
      "Loss for  48668 th iteration => 0.00609380655382\n",
      "Loss for  48669 th iteration => 0.00609362793322\n",
      "Loss for  48670 th iteration => 0.00609349334013\n",
      "Loss for  48671 th iteration => 0.00609340182489\n",
      "Loss for  48672 th iteration => 0.00609323588928\n",
      "Loss for  48673 th iteration => 0.00609311142955\n",
      "Loss for  48674 th iteration => 0.00609297837488\n",
      "Loss for  48675 th iteration => 0.00609282121566\n",
      "Loss for  48676 th iteration => 0.00609272280144\n",
      "Loss for  48677 th iteration => 0.00609258292907\n",
      "Loss for  48678 th iteration => 0.00609242341316\n",
      "Loss for  48679 th iteration => 0.0060922994587\n",
      "Loss for  48680 th iteration => 0.00609220640706\n",
      "Loss for  48681 th iteration => 0.00609202757106\n",
      "Loss for  48682 th iteration => 0.00609192699876\n",
      "Loss for  48683 th iteration => 0.00609176829536\n",
      "Loss for  48684 th iteration => 0.00609160280026\n",
      "Loss for  48685 th iteration => 0.00609146692532\n",
      "Loss for  48686 th iteration => 0.00609133106094\n",
      "Loss for  48687 th iteration => 0.00609119815994\n",
      "Loss for  48688 th iteration => 0.00609110099712\n",
      "Loss for  48689 th iteration => 0.0060909409073\n",
      "Loss for  48690 th iteration => 0.00609081080015\n",
      "Loss for  48691 th iteration => 0.00609068359113\n",
      "Loss for  48692 th iteration => 0.00609052101396\n",
      "Loss for  48693 th iteration => 0.00609046260743\n",
      "Loss for  48694 th iteration => 0.0060902614751\n",
      "Loss for  48695 th iteration => 0.00609013472575\n",
      "Loss for  48696 th iteration => 0.00609000182322\n",
      "Loss for  48697 th iteration => 0.00608985045988\n",
      "Loss for  48698 th iteration => 0.00608979119108\n",
      "Loss for  48699 th iteration => 0.00608959231437\n",
      "Loss for  48700 th iteration => 0.00608946832423\n",
      "Loss for  48701 th iteration => 0.00608933600063\n",
      "Loss for  48702 th iteration => 0.00608917728343\n",
      "Loss for  48703 th iteration => 0.0060890804953\n",
      "Loss for  48704 th iteration => 0.00608894145545\n",
      "Loss for  48705 th iteration => 0.00608878031015\n",
      "Loss for  48706 th iteration => 0.00608865143635\n",
      "Loss for  48707 th iteration => 0.00608852318528\n",
      "Loss for  48708 th iteration => 0.00608836214388\n",
      "Loss for  48709 th iteration => 0.00608822789173\n",
      "Loss for  48710 th iteration => 0.00608812383861\n",
      "Loss for  48711 th iteration => 0.00608796847662\n",
      "Loss for  48712 th iteration => 0.00608783699462\n",
      "Loss for  48713 th iteration => 0.00608770967983\n",
      "Loss for  48714 th iteration => 0.00608759924512\n",
      "Loss for  48715 th iteration => 0.0060874316778\n",
      "Loss for  48716 th iteration => 0.00608730811506\n",
      "Loss for  48717 th iteration => 0.00608717559352\n",
      "Loss for  48718 th iteration => 0.00608701718042\n",
      "Loss for  48719 th iteration => 0.00608691943684\n",
      "Loss for  48720 th iteration => 0.00608674257117\n",
      "Loss for  48721 th iteration => 0.00608660708886\n",
      "Loss for  48722 th iteration => 0.00608651724317\n",
      "Loss for  48723 th iteration => 0.00608635023406\n",
      "Loss for  48724 th iteration => 0.00608622744539\n",
      "Loss for  48725 th iteration => 0.00608609331576\n",
      "Loss for  48726 th iteration => 0.00608593782915\n",
      "Loss for  48727 th iteration => 0.0060858372876\n",
      "Loss for  48728 th iteration => 0.00608570016852\n",
      "Loss for  48729 th iteration => 0.0060855396861\n",
      "Loss for  48730 th iteration => 0.00608541611969\n",
      "Loss for  48731 th iteration => 0.00608532337172\n",
      "Loss for  48732 th iteration => 0.00608514482756\n",
      "Loss for  48733 th iteration => 0.00608504456106\n",
      "Loss for  48734 th iteration => 0.00608488592937\n",
      "Loss for  48735 th iteration => 0.0060847219191\n",
      "Loss for  48736 th iteration => 0.00608458634291\n",
      "Loss for  48737 th iteration => 0.00608445077729\n",
      "Loss for  48738 th iteration => 0.00608431723849\n",
      "Loss for  48739 th iteration => 0.00608422120306\n",
      "Loss for  48740 th iteration => 0.00608406057908\n",
      "Loss for  48741 th iteration => 0.00608393160703\n",
      "Loss for  48742 th iteration => 0.00608380385618\n",
      "Loss for  48743 th iteration => 0.00608364219259\n",
      "Loss for  48744 th iteration => 0.00608354951862\n",
      "Loss for  48745 th iteration => 0.00608340550772\n",
      "Loss for  48746 th iteration => 0.00608325027874\n",
      "Loss for  48747 th iteration => 0.00608311911845\n",
      "Loss for  48748 th iteration => 0.00608299295584\n",
      "Loss for  48749 th iteration => 0.00608288202414\n",
      "Loss for  48750 th iteration => 0.00608271449659\n",
      "Loss for  48751 th iteration => 0.0060825913579\n",
      "Loss for  48752 th iteration => 0.00608245879221\n",
      "Loss for  48753 th iteration => 0.00608230088628\n",
      "Loss for  48754 th iteration => 0.00608220330703\n",
      "Loss for  48755 th iteration => 0.00608206547555\n",
      "Loss for  48756 th iteration => 0.00608190455116\n",
      "Loss for  48757 th iteration => 0.00608177602858\n",
      "Loss for  48758 th iteration => 0.00608164803321\n",
      "Loss for  48759 th iteration => 0.00608148785792\n",
      "Loss for  48760 th iteration => 0.00608135242582\n",
      "Loss for  48761 th iteration => 0.00608121700425\n",
      "Loss for  48762 th iteration => 0.00608108349494\n",
      "Loss for  48763 th iteration => 0.00608098467249\n",
      "Loss for  48764 th iteration => 0.00608082476977\n",
      "Loss for  48765 th iteration => 0.00608074571436\n",
      "Loss for  48766 th iteration => 0.00608058143225\n",
      "Loss for  48767 th iteration => 0.00608043977709\n",
      "Loss for  48768 th iteration => 0.00608033786564\n",
      "Loss for  48769 th iteration => 0.00608014594132\n",
      "Loss for  48770 th iteration => 0.00608004831934\n",
      "Loss for  48771 th iteration => 0.00607987243\n",
      "Loss for  48772 th iteration => 0.00607973707446\n",
      "Loss for  48773 th iteration => 0.00607960504062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  48774 th iteration => 0.00607950874928\n",
      "Loss for  48775 th iteration => 0.00607934875887\n",
      "Loss for  48776 th iteration => 0.00607921962058\n",
      "Loss for  48777 th iteration => 0.00607909241387\n",
      "Loss for  48778 th iteration => 0.00607893114802\n",
      "Loss for  48779 th iteration => 0.00607887305655\n",
      "Loss for  48780 th iteration => 0.00607867174286\n",
      "Loss for  48781 th iteration => 0.00607854797285\n",
      "Loss for  48782 th iteration => 0.00607845537344\n",
      "Loss for  48783 th iteration => 0.00607827727524\n",
      "Loss for  48784 th iteration => 0.00607817716262\n",
      "Loss for  48785 th iteration => 0.00607801939553\n",
      "Loss for  48786 th iteration => 0.00607785626013\n",
      "Loss for  48787 th iteration => 0.00607772098246\n",
      "Loss for  48788 th iteration => 0.00607758571536\n",
      "Loss for  48789 th iteration => 0.00607745200832\n",
      "Loss for  48790 th iteration => 0.00607735605719\n",
      "Loss for  48791 th iteration => 0.00607719593554\n",
      "Loss for  48792 th iteration => 0.00607706706882\n",
      "Loss for  48793 th iteration => 0.00607693979934\n",
      "Loss for  48794 th iteration => 0.00607677851861\n",
      "Loss for  48795 th iteration => 0.00607664541102\n",
      "Loss for  48796 th iteration => 0.00607654142491\n",
      "Loss for  48797 th iteration => 0.00607638698742\n",
      "Loss for  48798 th iteration => 0.00607625560867\n",
      "Loss for  48799 th iteration => 0.0060761301334\n",
      "Loss for  48800 th iteration => 0.00607601872103\n",
      "Loss for  48801 th iteration => 0.00607585250836\n",
      "Loss for  48802 th iteration => 0.00607572862551\n",
      "Loss for  48803 th iteration => 0.00607559740891\n",
      "Loss for  48804 th iteration => 0.00607543872506\n",
      "Loss for  48805 th iteration => 0.00607534223725\n",
      "Loss for  48806 th iteration => 0.00607516498523\n",
      "Loss for  48807 th iteration => 0.00607503106708\n",
      "Loss for  48808 th iteration => 0.00607494054652\n",
      "Loss for  48809 th iteration => 0.00607477519725\n",
      "Loss for  48810 th iteration => 0.00607465178275\n",
      "Loss for  48811 th iteration => 0.00607451926409\n",
      "Loss for  48812 th iteration => 0.00607436320018\n",
      "Loss for  48813 th iteration => 0.0060742650846\n",
      "Loss for  48814 th iteration => 0.00607412651635\n",
      "Loss for  48815 th iteration => 0.00607396747296\n",
      "Loss for  48816 th iteration => 0.00607384470756\n",
      "Loss for  48817 th iteration => 0.00607375191382\n",
      "Loss for  48818 th iteration => 0.00607357441788\n",
      "Loss for  48819 th iteration => 0.00607347411887\n",
      "Loss for  48820 th iteration => 0.00607331654102\n",
      "Loss for  48821 th iteration => 0.00607315186963\n",
      "Loss for  48822 th iteration => 0.00607301679819\n",
      "Loss for  48823 th iteration => 0.00607288173729\n",
      "Loss for  48824 th iteration => 0.00607274999401\n",
      "Loss for  48825 th iteration => 0.00607265276984\n",
      "Loss for  48826 th iteration => 0.00607249431639\n",
      "Loss for  48827 th iteration => 0.00607236421058\n",
      "Loss for  48828 th iteration => 0.00607223857547\n",
      "Loss for  48829 th iteration => 0.0060720762591\n",
      "Loss for  48830 th iteration => 0.00607201884061\n",
      "Loss for  48831 th iteration => 0.00607181873237\n",
      "Loss for  48832 th iteration => 0.00607169231181\n",
      "Loss for  48833 th iteration => 0.00607156066032\n",
      "Loss for  48834 th iteration => 0.00607141024133\n",
      "Loss for  48835 th iteration => 0.00607135079962\n",
      "Loss for  48836 th iteration => 0.00607115392797\n",
      "Loss for  48837 th iteration => 0.0060710294401\n",
      "Loss for  48838 th iteration => 0.00607089918375\n",
      "Loss for  48839 th iteration => 0.0060707400423\n",
      "Loss for  48840 th iteration => 0.00607064603552\n",
      "Loss for  48841 th iteration => 0.00607050551821\n",
      "Loss for  48842 th iteration => 0.00607034694802\n",
      "Loss for  48843 th iteration => 0.00607021714203\n",
      "Loss for  48844 th iteration => 0.00607009139307\n",
      "Loss for  48845 th iteration => 0.00606993042882\n",
      "Loss for  48846 th iteration => 0.006069797534\n",
      "Loss for  48847 th iteration => 0.00606969297505\n",
      "Loss for  48848 th iteration => 0.00606953969394\n",
      "Loss for  48849 th iteration => 0.00606940776822\n",
      "Loss for  48850 th iteration => 0.0060692834669\n",
      "Loss for  48851 th iteration => 0.0060691710191\n",
      "Loss for  48852 th iteration => 0.00606900652784\n",
      "Loss for  48853 th iteration => 0.0060688815382\n",
      "Loss for  48854 th iteration => 0.00606875200728\n",
      "Loss for  48855 th iteration => 0.00606859225215\n",
      "Loss for  48856 th iteration => 0.00606849741463\n",
      "Loss for  48857 th iteration => 0.00606832046021\n",
      "Loss for  48858 th iteration => 0.0060681870003\n",
      "Loss for  48859 th iteration => 0.0060680951602\n",
      "Loss for  48860 th iteration => 0.00606793170964\n",
      "Loss for  48861 th iteration => 0.00606780701036\n",
      "Loss for  48862 th iteration => 0.00606767635576\n",
      "Loss for  48863 th iteration => 0.00606751904148\n",
      "Loss for  48864 th iteration => 0.00606742279408\n",
      "Loss for  48865 th iteration => 0.00606728325071\n",
      "Loss for  48866 th iteration => 0.00606712575681\n",
      "Loss for  48867 th iteration => 0.00606704283017\n",
      "Loss for  48868 th iteration => 0.00606688354965\n",
      "Loss for  48869 th iteration => 0.00606673815329\n",
      "Loss for  48870 th iteration => 0.0060666411143\n",
      "Loss for  48871 th iteration => 0.00606644743649\n",
      "Loss for  48872 th iteration => 0.00606635292834\n",
      "Loss for  48873 th iteration => 0.00606617770671\n",
      "Loss for  48874 th iteration => 0.00606604294731\n",
      "Loss for  48875 th iteration => 0.00606591172227\n",
      "Loss for  48876 th iteration => 0.00606581304857\n",
      "Loss for  48877 th iteration => 0.00606565660021\n",
      "Loss for  48878 th iteration => 0.00606552514723\n",
      "Loss for  48879 th iteration => 0.00606540141512\n",
      "Loss for  48880 th iteration => 0.00606523933429\n",
      "Loss for  48881 th iteration => 0.00606513900821\n",
      "Loss for  48882 th iteration => 0.00606498188128\n",
      "Loss for  48883 th iteration => 0.0060648566651\n",
      "Loss for  48884 th iteration => 0.00606476348854\n",
      "Loss for  48885 th iteration => 0.00606458714733\n",
      "Loss for  48886 th iteration => 0.00606448646389\n",
      "Loss for  48887 th iteration => 0.00606433276149\n",
      "Loss for  48888 th iteration => 0.00606416976896\n",
      "Loss for  48889 th iteration => 0.00606403508403\n",
      "Loss for  48890 th iteration => 0.00606390040969\n",
      "Loss for  48891 th iteration => 0.00606376574588\n",
      "Loss for  48892 th iteration => 0.00606363109255\n",
      "Loss for  48893 th iteration => 0.00606349986015\n",
      "Loss for  48894 th iteration => 0.00606340613808\n",
      "Loss for  48895 th iteration => 0.00606324494329\n",
      "Loss for  48896 th iteration => 0.00606311845338\n",
      "Loss for  48897 th iteration => 0.00606299258032\n",
      "Loss for  48898 th iteration => 0.0060628616937\n",
      "Loss for  48899 th iteration => 0.00606274960346\n",
      "Loss for  48900 th iteration => 0.00606257442491\n",
      "Loss for  48901 th iteration => 0.00606245292246\n",
      "Loss for  48902 th iteration => 0.0060623378188\n",
      "Loss for  48903 th iteration => 0.00606217614659\n",
      "Loss for  48904 th iteration => 0.00606204895959\n",
      "Loss for  48905 th iteration => 0.00606192219813\n",
      "Loss for  48906 th iteration => 0.00606176029483\n",
      "Loss for  48907 th iteration => 0.00606166817774\n",
      "Loss for  48908 th iteration => 0.00606149103796\n",
      "Loss for  48909 th iteration => 0.00606135856773\n",
      "Loss for  48910 th iteration => 0.00606126424353\n",
      "Loss for  48911 th iteration => 0.00606110384989\n",
      "Loss for  48912 th iteration => 0.0060609767139\n",
      "Loss for  48913 th iteration => 0.00606084906898\n",
      "Loss for  48914 th iteration => 0.00606068936488\n",
      "Loss for  48915 th iteration => 0.00606059659669\n",
      "Loss for  48916 th iteration => 0.00606045451539\n",
      "Loss for  48917 th iteration => 0.00606030013191\n",
      "Loss for  48918 th iteration => 0.00606021260021\n",
      "Loss for  48919 th iteration => 0.00606005847397\n",
      "Loss for  48920 th iteration => 0.00605990857034\n",
      "Loss for  48921 th iteration => 0.00605981658839\n",
      "Loss for  48922 th iteration => 0.00605962127453\n",
      "Loss for  48923 th iteration => 0.00605948678808\n",
      "Loss for  48924 th iteration => 0.0060593523122\n",
      "Loss for  48925 th iteration => 0.00605921865584\n",
      "Loss for  48926 th iteration => 0.00605912361907\n",
      "Loss for  48927 th iteration => 0.00605896414053\n",
      "Loss for  48928 th iteration => 0.00605883623696\n",
      "Loss for  48929 th iteration => 0.00605870956201\n",
      "Loss for  48930 th iteration => 0.00605854985854\n",
      "Loss for  48931 th iteration => 0.00605841689754\n",
      "Loss for  48932 th iteration => 0.00605831384236\n",
      "Loss for  48933 th iteration => 0.00605816003457\n",
      "Loss for  48934 th iteration => 0.0060580296293\n",
      "Loss for  48935 th iteration => 0.00605790410083\n",
      "Loss for  48936 th iteration => 0.00605779392373\n",
      "Loss for  48937 th iteration => 0.00605762871147\n",
      "Loss for  48938 th iteration => 0.00605750544068\n",
      "Loss for  48939 th iteration => 0.00605737516382\n",
      "Loss for  48940 th iteration => 0.00605721715235\n",
      "Loss for  48941 th iteration => 0.0060571215442\n",
      "Loss for  48942 th iteration => 0.00605694579803\n",
      "Loss for  48943 th iteration => 0.00605681245963\n",
      "Loss for  48944 th iteration => 0.00605672166137\n",
      "Loss for  48945 th iteration => 0.00605655814101\n",
      "Loss for  48946 th iteration => 0.00605643451039\n",
      "Loss for  48947 th iteration => 0.00605630375929\n",
      "Loss for  48948 th iteration => 0.00605614754027\n",
      "Loss for  48949 th iteration => 0.00605605091441\n",
      "Loss for  48950 th iteration => 0.00605591259851\n",
      "Loss for  48951 th iteration => 0.00605575514586\n",
      "Loss for  48952 th iteration => 0.0060556322456\n",
      "Loss for  48953 th iteration => 0.00605553968681\n",
      "Loss for  48954 th iteration => 0.00605536352923\n",
      "Loss for  48955 th iteration => 0.00605526348293\n",
      "Loss for  48956 th iteration => 0.00605510817466\n",
      "Loss for  48957 th iteration => 0.00605494450767\n",
      "Loss for  48958 th iteration => 0.00605481022703\n",
      "Loss for  48959 th iteration => 0.00605467595692\n",
      "Loss for  48960 th iteration => 0.00605454523127\n",
      "Loss for  48961 th iteration => 0.00605444677137\n",
      "Loss for  48962 th iteration => 0.00605429109931\n",
      "Loss for  48963 th iteration => 0.00605415983036\n",
      "Loss for  48964 th iteration => 0.00605403690429\n",
      "Loss for  48965 th iteration => 0.0060538752842\n",
      "Loss for  48966 th iteration => 0.00605377523775\n",
      "Loss for  48967 th iteration => 0.00605361881832\n",
      "Loss for  48968 th iteration => 0.00605349140507\n",
      "Loss for  48969 th iteration => 0.00605336231257\n",
      "Loss for  48970 th iteration => 0.00605321144934\n",
      "Loss for  48971 th iteration => 0.00605315172446\n",
      "Loss for  48972 th iteration => 0.00605295824114\n",
      "Loss for  48973 th iteration => 0.00605283173273\n",
      "Loss for  48974 th iteration => 0.00605270505729\n",
      "Loss for  48975 th iteration => 0.00605254392025\n",
      "Loss for  48976 th iteration => 0.0060524518018\n",
      "Loss for  48977 th iteration => 0.00605227365459\n",
      "Loss for  48978 th iteration => 0.00605214308271\n",
      "Loss for  48979 th iteration => 0.00605204918874\n",
      "Loss for  48980 th iteration => 0.00605188913142\n",
      "Loss for  48981 th iteration => 0.00605176250877\n",
      "Loss for  48982 th iteration => 0.00605163780695\n",
      "Loss for  48983 th iteration => 0.00605150642683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  48984 th iteration => 0.00605139577822\n",
      "Loss for  48985 th iteration => 0.00605122060717\n",
      "Loss for  48986 th iteration => 0.0060511001317\n",
      "Loss for  48987 th iteration => 0.00605098475142\n",
      "Loss for  48988 th iteration => 0.00605082427408\n",
      "Loss for  48989 th iteration => 0.00605069690046\n",
      "Loss for  48990 th iteration => 0.00605057128792\n",
      "Loss for  48991 th iteration => 0.00605040924367\n",
      "Loss for  48992 th iteration => 0.00605031823\n",
      "Loss for  48993 th iteration => 0.00605014145419\n",
      "Loss for  48994 th iteration => 0.00605000990656\n",
      "Loss for  48995 th iteration => 0.00604991484059\n",
      "Loss for  48996 th iteration => 0.00604975615095\n",
      "Loss for  48997 th iteration => 0.00604962831894\n",
      "Loss for  48998 th iteration => 0.00604950233242\n",
      "Loss for  48999 th iteration => 0.00604934197761\n",
      "Loss for  49000 th iteration => 0.00604925113133\n",
      "Loss for  49001 th iteration => 0.00604910823116\n",
      "Loss for  49002 th iteration => 0.00604895570964\n",
      "Loss for  49003 th iteration => 0.00604886646549\n",
      "Loss for  49004 th iteration => 0.00604871498985\n",
      "Loss for  49005 th iteration => 0.00604856346837\n",
      "Loss for  49006 th iteration => 0.0060484740431\n",
      "Loss for  49007 th iteration => 0.00604827855522\n",
      "Loss for  49008 th iteration => 0.00604814456113\n",
      "Loss for  49009 th iteration => 0.00604801057761\n",
      "Loss for  49010 th iteration => 0.0060478780835\n",
      "Loss for  49011 th iteration => 0.00604778178241\n",
      "Loss for  49012 th iteration => 0.00604762452726\n",
      "Loss for  49013 th iteration => 0.00604749541122\n",
      "Loss for  49014 th iteration => 0.00604737090798\n",
      "Loss for  49015 th iteration => 0.00604721107007\n",
      "Loss for  49016 th iteration => 0.00604707910161\n",
      "Loss for  49017 th iteration => 0.00604697518439\n",
      "Loss for  49018 th iteration => 0.00604682320162\n",
      "Loss for  49019 th iteration => 0.00604669197789\n",
      "Loss for  49020 th iteration => 0.00604656894541\n",
      "Loss for  49021 th iteration => 0.00604645688469\n",
      "Loss for  49022 th iteration => 0.00604629404795\n",
      "Loss for  49023 th iteration => 0.00604616941631\n",
      "Loss for  49024 th iteration => 0.00604604145578\n",
      "Loss for  49025 th iteration => 0.00604588214223\n",
      "Loss for  49026 th iteration => 0.00604578879186\n",
      "Loss for  49027 th iteration => 0.00604561289435\n",
      "Loss for  49028 th iteration => 0.00604548095615\n",
      "Loss for  49029 th iteration => 0.00604538834291\n",
      "Loss for  49030 th iteration => 0.00604522759347\n",
      "Loss for  49031 th iteration => 0.0060451022055\n",
      "Loss for  49032 th iteration => 0.00604497416786\n",
      "Loss for  49033 th iteration => 0.00604481624855\n",
      "Loss for  49034 th iteration => 0.00604472297171\n",
      "Loss for  49035 th iteration => 0.00604458250912\n",
      "Loss for  49036 th iteration => 0.00604442802873\n",
      "Loss for  49037 th iteration => 0.00604434275494\n",
      "Loss for  49038 th iteration => 0.00604418770432\n",
      "Loss for  49039 th iteration => 0.00604404013647\n",
      "Loss for  49040 th iteration => 0.00604394715303\n",
      "Loss for  49041 th iteration => 0.00604375272579\n",
      "Loss for  49042 th iteration => 0.00604361892674\n",
      "Loss for  49043 th iteration => 0.00604348513826\n",
      "Loss for  49044 th iteration => 0.00604335243917\n",
      "Loss for  49045 th iteration => 0.00604325819101\n",
      "Loss for  49046 th iteration => 0.00604309927383\n",
      "Loss for  49047 th iteration => 0.00604297220663\n",
      "Loss for  49048 th iteration => 0.00604284604546\n",
      "Loss for  49049 th iteration => 0.0060426868023\n",
      "Loss for  49050 th iteration => 0.00604255479215\n",
      "Loss for  49051 th iteration => 0.0060424524102\n",
      "Loss for  49052 th iteration => 0.00604229928231\n",
      "Loss for  49053 th iteration => 0.00604216959189\n",
      "Loss for  49054 th iteration => 0.00604204513697\n",
      "Loss for  49055 th iteration => 0.00604193515742\n",
      "Loss for  49056 th iteration => 0.00604177086663\n",
      "Loss for  49057 th iteration => 0.00604164807731\n",
      "Loss for  49058 th iteration => 0.00604151866405\n",
      "Loss for  49059 th iteration => 0.00604136119157\n",
      "Loss for  49060 th iteration => 0.00604126638973\n",
      "Loss for  49061 th iteration => 0.00604109096589\n",
      "Loss for  49062 th iteration => 0.00604095900957\n",
      "Loss for  49063 th iteration => 0.00604086803433\n",
      "Loss for  49064 th iteration => 0.00604070603579\n",
      "Loss for  49065 th iteration => 0.00604058228623\n",
      "Loss for  49066 th iteration => 0.00604045299912\n",
      "Loss for  49067 th iteration => 0.00604029671862\n",
      "Loss for  49068 th iteration => 0.00604020210533\n",
      "Loss for  49069 th iteration => 0.00604006305831\n",
      "Loss for  49070 th iteration => 0.00603990765652\n",
      "Loss for  49071 th iteration => 0.00603982486637\n",
      "Loss for  49072 th iteration => 0.00603966771953\n",
      "Loss for  49073 th iteration => 0.00603952263641\n",
      "Loss for  49074 th iteration => 0.00603942755581\n",
      "Loss for  49075 th iteration => 0.00603923434782\n",
      "Loss for  49076 th iteration => 0.00603914142291\n",
      "Loss for  49077 th iteration => 0.00603896701874\n",
      "Loss for  49078 th iteration => 0.00603883428604\n",
      "Loss for  49079 th iteration => 0.00603874170734\n",
      "Loss for  49080 th iteration => 0.00603858148781\n",
      "Loss for  49081 th iteration => 0.00603845615382\n",
      "Loss for  49082 th iteration => 0.00603832862673\n",
      "Loss for  49083 th iteration => 0.00603817078068\n",
      "Loss for  49084 th iteration => 0.00603807725068\n",
      "Loss for  49085 th iteration => 0.00603793763485\n",
      "Loss for  49086 th iteration => 0.0060377832756\n",
      "Loss for  49087 th iteration => 0.00603765882309\n",
      "Loss for  49088 th iteration => 0.00603756628234\n",
      "Loss for  49089 th iteration => 0.00603739165608\n",
      "Loss for  49090 th iteration => 0.00603729164921\n",
      "Loss for  49091 th iteration => 0.00603714013262\n",
      "Loss for  49092 th iteration => 0.00603697736649\n",
      "Loss for  49093 th iteration => 0.00603684386446\n",
      "Loss for  49094 th iteration => 0.00603671042317\n",
      "Loss for  49095 th iteration => 0.00603661609154\n",
      "Loss for  49096 th iteration => 0.00603645782169\n",
      "Loss for  49097 th iteration => 0.00603633069033\n",
      "Loss for  49098 th iteration => 0.00603620515726\n",
      "Loss for  49099 th iteration => 0.00603604724727\n",
      "Loss for  49100 th iteration => 0.00603591469894\n",
      "Loss for  49101 th iteration => 0.00603581222086\n",
      "Loss for  49102 th iteration => 0.00603565975421\n",
      "Loss for  49103 th iteration => 0.00603552998422\n",
      "Loss for  49104 th iteration => 0.00603540516679\n",
      "Loss for  49105 th iteration => 0.0060352957244\n",
      "Loss for  49106 th iteration => 0.00603513229781\n",
      "Loss for  49107 th iteration => 0.00603500922996\n",
      "Loss for  49108 th iteration => 0.00603488065693\n",
      "Loss for  49109 th iteration => 0.00603472292977\n",
      "Loss for  49110 th iteration => 0.00603462894441\n",
      "Loss for  49111 th iteration => 0.00603445484372\n",
      "Loss for  49112 th iteration => 0.00603432234403\n",
      "Loss for  49113 th iteration => 0.00603423084226\n",
      "Loss for  49114 th iteration => 0.00603406993183\n",
      "Loss for  49115 th iteration => 0.00603394567976\n",
      "Loss for  49116 th iteration => 0.00603381745683\n",
      "Loss for  49117 th iteration => 0.00603366069763\n",
      "Loss for  49118 th iteration => 0.00603356646149\n",
      "Loss for  49119 th iteration => 0.0060334278175\n",
      "Loss for  49120 th iteration => 0.00603327284122\n",
      "Loss for  49121 th iteration => 0.00603314944931\n",
      "Loss for  49122 th iteration => 0.00603305719177\n",
      "Loss for  49123 th iteration => 0.00603288266931\n",
      "Loss for  49124 th iteration => 0.00603278294738\n",
      "Loss for  49125 th iteration => 0.00603263072107\n",
      "Loss for  49126 th iteration => 0.00603246810452\n",
      "Loss for  49127 th iteration => 0.00603233479717\n",
      "Loss for  49128 th iteration => 0.00603220172614\n",
      "Loss for  49129 th iteration => 0.00603210817364\n",
      "Loss for  49130 th iteration => 0.0060319495091\n",
      "Loss for  49131 th iteration => 0.00603182316526\n",
      "Loss for  49132 th iteration => 0.00603169722914\n",
      "Loss for  49133 th iteration => 0.00603153934664\n",
      "Loss for  49134 th iteration => 0.00603140709823\n",
      "Loss for  49135 th iteration => 0.00603130538354\n",
      "Loss for  49136 th iteration => 0.00603115253842\n",
      "Loss for  49137 th iteration => 0.00603102353951\n",
      "Loss for  49138 th iteration => 0.00603089853697\n",
      "Loss for  49139 th iteration => 0.00603078976288\n",
      "Loss for  49140 th iteration => 0.00603062595357\n",
      "Loss for  49141 th iteration => 0.00603050366263\n",
      "Loss for  49142 th iteration => 0.00603037469584\n",
      "Loss for  49143 th iteration => 0.00603021775656\n",
      "Loss for  49144 th iteration => 0.00603012336653\n",
      "Loss for  49145 th iteration => 0.00602994927305\n",
      "Loss for  49146 th iteration => 0.00602981727999\n",
      "Loss for  49147 th iteration => 0.00602972624881\n",
      "Loss for  49148 th iteration => 0.00602956525065\n",
      "Loss for  49149 th iteration => 0.00602944148086\n",
      "Loss for  49150 th iteration => 0.00602931315855\n",
      "Loss for  49151 th iteration => 0.00602915689322\n",
      "Loss for  49152 th iteration => 0.00602906276133\n",
      "Loss for  49153 th iteration => 0.00602892433389\n",
      "Loss for  49154 th iteration => 0.0060287693187\n",
      "Loss for  49155 th iteration => 0.0060286859434\n",
      "Loss for  49156 th iteration => 0.00602853030581\n",
      "Loss for  49157 th iteration => 0.006028384715\n",
      "Loss for  49158 th iteration => 0.00602829106681\n",
      "Loss for  49159 th iteration => 0.0060280985445\n",
      "Loss for  49160 th iteration => 0.00602796542146\n",
      "Loss for  49161 th iteration => 0.006027832309\n",
      "Loss for  49162 th iteration => 0.0060276999807\n",
      "Loss for  49163 th iteration => 0.00602760639413\n",
      "Loss for  49164 th iteration => 0.00602744814359\n",
      "Loss for  49165 th iteration => 0.00602732178473\n",
      "Loss for  49166 th iteration => 0.00602719624364\n",
      "Loss for  49167 th iteration => 0.00602703802392\n",
      "Loss for  49168 th iteration => 0.0060269064981\n",
      "Loss for  49169 th iteration => 0.00602680463323\n",
      "Loss for  49170 th iteration => 0.00602665231956\n",
      "Loss for  49171 th iteration => 0.00602652318685\n",
      "Loss for  49172 th iteration => 0.00602639937011\n",
      "Loss for  49173 th iteration => 0.00602628982855\n",
      "Loss for  49174 th iteration => 0.00602612665139\n",
      "Loss for  49175 th iteration => 0.00602600412875\n",
      "Loss for  49176 th iteration => 0.00602587577215\n",
      "Loss for  49177 th iteration => 0.00602571862295\n",
      "Loss for  49178 th iteration => 0.00602562482141\n",
      "Loss for  49179 th iteration => 0.00602545027825\n",
      "Loss for  49180 th iteration => 0.00602531912077\n",
      "Loss for  49181 th iteration => 0.00602522783853\n",
      "Loss for  49182 th iteration => 0.00602506747014\n",
      "Loss for  49183 th iteration => 0.0060249434706\n",
      "Loss for  49184 th iteration => 0.00602481575683\n",
      "Loss for  49185 th iteration => 0.00602465928281\n",
      "Loss for  49186 th iteration => 0.00602456634039\n",
      "Loss for  49187 th iteration => 0.0060244269734\n",
      "Loss for  49188 th iteration => 0.00602427342083\n",
      "Loss for  49189 th iteration => 0.0060241890991\n",
      "Loss for  49190 th iteration => 0.00602403477612\n",
      "Loss for  49191 th iteration => 0.00602388828097\n",
      "Loss for  49192 th iteration => 0.00602379590558\n",
      "Loss for  49193 th iteration => 0.0060236022596\n",
      "Loss for  49194 th iteration => 0.00602346933128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  49195 th iteration => 0.00602333641352\n",
      "Loss for  49196 th iteration => 0.0060232049994\n",
      "Loss for  49197 th iteration => 0.00602311098764\n",
      "Loss for  49198 th iteration => 0.00602295353991\n",
      "Loss for  49199 th iteration => 0.00602282677916\n",
      "Loss for  49200 th iteration => 0.00602270201765\n",
      "Loss for  49201 th iteration => 0.00602254329188\n",
      "Loss for  49202 th iteration => 0.00602241262471\n",
      "Loss for  49203 th iteration => 0.00602231029794\n",
      "Loss for  49204 th iteration => 0.0060221588255\n",
      "Loss for  49205 th iteration => 0.00602202925065\n",
      "Loss for  49206 th iteration => 0.0060219070957\n",
      "Loss for  49207 th iteration => 0.00602179626416\n",
      "Loss for  49208 th iteration => 0.00602163410547\n",
      "Loss for  49209 th iteration => 0.00602151096675\n",
      "Loss for  49210 th iteration => 0.00602138360237\n",
      "Loss for  49211 th iteration => 0.00602122586314\n",
      "Loss for  49212 th iteration => 0.00602113302787\n",
      "Loss for  49213 th iteration => 0.00602095787022\n",
      "Loss for  49214 th iteration => 0.00602082770242\n",
      "Loss for  49215 th iteration => 0.00602073581458\n",
      "Loss for  49216 th iteration => 0.00602057642833\n",
      "Loss for  49217 th iteration => 0.00602045184834\n",
      "Loss for  49218 th iteration => 0.00602032509164\n",
      "Loss for  49219 th iteration => 0.00602016806343\n",
      "Loss for  49220 th iteration => 0.00602011159026\n",
      "Loss for  49221 th iteration => 0.00601991230252\n",
      "Loss for  49222 th iteration => 0.00601979229281\n",
      "Loss for  49223 th iteration => 0.00601970163378\n",
      "Loss for  49224 th iteration => 0.00601952665534\n",
      "Loss for  49225 th iteration => 0.00601942852872\n",
      "Loss for  49226 th iteration => 0.00601927275491\n",
      "Loss for  49227 th iteration => 0.00601911255957\n",
      "Loss for  49228 th iteration => 0.00601897982571\n",
      "Loss for  49229 th iteration => 0.0060188471024\n",
      "Loss for  49230 th iteration => 0.00601871678046\n",
      "Loss for  49231 th iteration => 0.00601862193802\n",
      "Loss for  49232 th iteration => 0.00601846569665\n",
      "Loss for  49233 th iteration => 0.00601833813162\n",
      "Loss for  49234 th iteration => 0.00601821455017\n",
      "Loss for  49235 th iteration => 0.006018055142\n",
      "Loss for  49236 th iteration => 0.00601792535361\n",
      "Loss for  49237 th iteration => 0.00601782249588\n",
      "Loss for  49238 th iteration => 0.00601767193219\n",
      "Loss for  49239 th iteration => 0.00601754184834\n",
      "Loss for  49240 th iteration => 0.00601742153932\n",
      "Loss for  49241 th iteration => 0.00601730911643\n",
      "Loss for  49242 th iteration => 0.00601714825683\n",
      "Loss for  49243 th iteration => 0.00601702422249\n",
      "Loss for  49244 th iteration => 0.006016898128\n",
      "Loss for  49245 th iteration => 0.00601673952212\n",
      "Loss for  49246 th iteration => 0.00601664792787\n",
      "Loss for  49247 th iteration => 0.00601647204011\n",
      "Loss for  49248 th iteration => 0.00601634302771\n",
      "Loss for  49249 th iteration => 0.0060162501547\n",
      "Loss for  49250 th iteration => 0.00601609212833\n",
      "Loss for  49251 th iteration => 0.00601596659111\n",
      "Loss for  49252 th iteration => 0.00601584116643\n",
      "Loss for  49253 th iteration => 0.00601568394614\n",
      "Loss for  49254 th iteration => 0.00601562792625\n",
      "Loss for  49255 th iteration => 0.00601542874566\n",
      "Loss for  49256 th iteration => 0.00601530912002\n",
      "Loss for  49257 th iteration => 0.00601521821947\n",
      "Loss for  49258 th iteration => 0.00601504386756\n",
      "Loss for  49259 th iteration => 0.00601494550666\n",
      "Loss for  49260 th iteration => 0.00601479042805\n",
      "Loss for  49261 th iteration => 0.00601462942873\n",
      "Loss for  49262 th iteration => 0.00601449688907\n",
      "Loss for  49263 th iteration => 0.00601436435996\n",
      "Loss for  49264 th iteration => 0.00601423524873\n",
      "Loss for  49265 th iteration => 0.00601413929624\n",
      "Loss for  49266 th iteration => 0.00601398453872\n",
      "Loss for  49267 th iteration => 0.00601385589306\n",
      "Loss for  49268 th iteration => 0.00601373376612\n",
      "Loss for  49269 th iteration => 0.00601357413874\n",
      "Loss for  49270 th iteration => 0.00601347618215\n",
      "Loss for  49271 th iteration => 0.00601332110242\n",
      "Loss for  49272 th iteration => 0.00601319588018\n",
      "Loss for  49273 th iteration => 0.00601306832037\n",
      "Loss for  49274 th iteration => 0.00601296503089\n",
      "Loss for  49275 th iteration => 0.00601283055364\n",
      "Loss for  49276 th iteration => 0.00601268009041\n",
      "Loss for  49277 th iteration => 0.00601254839764\n",
      "Loss for  49278 th iteration => 0.00601239576409\n",
      "Loss for  49279 th iteration => 0.00601229856933\n",
      "Loss for  49280 th iteration => 0.00601212571142\n",
      "Loss for  49281 th iteration => 0.00601199427955\n",
      "Loss for  49282 th iteration => 0.0060119068881\n",
      "Loss for  49283 th iteration => 0.00601174375134\n",
      "Loss for  49284 th iteration => 0.00601162369935\n",
      "Loss for  49285 th iteration => 0.0060114938568\n",
      "Loss for  49286 th iteration => 0.00601137279489\n",
      "Loss for  49287 th iteration => 0.00601125521915\n",
      "Loss for  49288 th iteration => 0.00601108819248\n",
      "Loss for  49289 th iteration => 0.00601096349864\n",
      "Loss for  49290 th iteration => 0.00601085574997\n",
      "Loss for  49291 th iteration => 0.00601069310643\n",
      "Loss for  49292 th iteration => 0.00601057140248\n",
      "Loss for  49293 th iteration => 0.0060104435322\n",
      "Loss for  49294 th iteration => 0.00601028724875\n",
      "Loss for  49295 th iteration => 0.00601019388668\n",
      "Loss for  49296 th iteration => 0.00601002113637\n",
      "Loss for  49297 th iteration => 0.00600988990032\n",
      "Loss for  49298 th iteration => 0.00600979880649\n",
      "Loss for  49299 th iteration => 0.00600963955418\n",
      "Loss for  49300 th iteration => 0.00600951579126\n",
      "Loss for  49301 th iteration => 0.00600938914553\n",
      "Loss for  49302 th iteration => 0.00600923295594\n",
      "Loss for  49303 th iteration => 0.00600914042037\n",
      "Loss for  49304 th iteration => 0.0060090020946\n",
      "Loss for  49305 th iteration => 0.00600884896278\n",
      "Loss for  49306 th iteration => 0.00600876411671\n",
      "Loss for  49307 th iteration => 0.00600861159503\n",
      "Loss for  49308 th iteration => 0.0060084646801\n",
      "Loss for  49309 th iteration => 0.00600837400229\n",
      "Loss for  49310 th iteration => 0.00600818238791\n",
      "Loss for  49311 th iteration => 0.00600805012364\n",
      "Loss for  49312 th iteration => 0.00600791786994\n",
      "Loss for  49313 th iteration => 0.00600778675655\n",
      "Loss for  49314 th iteration => 0.00600769211335\n",
      "Loss for  49315 th iteration => 0.00600753659689\n",
      "Loss for  49316 th iteration => 0.00600740926237\n",
      "Loss for  49317 th iteration => 0.00600728637468\n",
      "Loss for  49318 th iteration => 0.00600712872623\n",
      "Loss for  49319 th iteration => 0.00600699841415\n",
      "Loss for  49320 th iteration => 0.00600689566813\n",
      "Loss for  49321 th iteration => 0.00600674591888\n",
      "Loss for  49322 th iteration => 0.00600661597376\n",
      "Loss for  49323 th iteration => 0.00600649526893\n",
      "Loss for  49324 th iteration => 0.00600638368924\n",
      "Loss for  49325 th iteration => 0.00600622392495\n",
      "Loss for  49326 th iteration => 0.00600609975471\n",
      "Loss for  49327 th iteration => 0.00600597471679\n",
      "Loss for  49328 th iteration => 0.00600581601355\n",
      "Loss for  49329 th iteration => 0.00600572543748\n",
      "Loss for  49330 th iteration => 0.00600555125008\n",
      "Loss for  49331 th iteration => 0.00600542176571\n",
      "Loss for  49332 th iteration => 0.00600532837728\n",
      "Loss for  49333 th iteration => 0.00600517178683\n",
      "Loss for  49334 th iteration => 0.00600504577302\n",
      "Loss for  49335 th iteration => 0.00600492174557\n",
      "Loss for  49336 th iteration => 0.0060047633483\n",
      "Loss for  49337 th iteration => 0.00600467472638\n",
      "Loss for  49338 th iteration => 0.00600453292511\n",
      "Loss for  49339 th iteration => 0.00600438381827\n",
      "Loss for  49340 th iteration => 0.00600429432811\n",
      "Loss for  49341 th iteration => 0.00600414679751\n",
      "Loss for  49342 th iteration => 0.00600399561503\n",
      "Loss for  49343 th iteration => 0.00600387435802\n",
      "Loss for  49344 th iteration => 0.00600371466642\n",
      "Loss for  49345 th iteration => 0.00600358259341\n",
      "Loss for  49346 th iteration => 0.00600345053098\n",
      "Loss for  49347 th iteration => 0.0060033213216\n",
      "Loss for  49348 th iteration => 0.00600322404846\n",
      "Loss for  49349 th iteration => 0.00600307154412\n",
      "Loss for  49350 th iteration => 0.00600294157757\n",
      "Loss for  49351 th iteration => 0.0060028217041\n",
      "Loss for  49352 th iteration => 0.00600266253606\n",
      "Loss for  49353 th iteration => 0.00600253051732\n",
      "Loss for  49354 th iteration => 0.00600239988051\n",
      "Loss for  49355 th iteration => 0.00600230485223\n",
      "Loss for  49356 th iteration => 0.00600214776628\n",
      "Loss for  49357 th iteration => 0.00600202928316\n",
      "Loss for  49358 th iteration => 0.00600193879638\n",
      "Loss for  49359 th iteration => 0.00600176516773\n",
      "Loss for  49360 th iteration => 0.00600166723307\n",
      "Loss for  49361 th iteration => 0.00600151246143\n",
      "Loss for  49362 th iteration => 0.00600135125587\n",
      "Loss for  49363 th iteration => 0.0060012635471\n",
      "Loss for  49364 th iteration => 0.00600108849878\n",
      "Loss for  49365 th iteration => 0.00600099937561\n",
      "Loss for  49366 th iteration => 0.00600083890099\n",
      "Loss for  49367 th iteration => 0.00600071712023\n",
      "Loss for  49368 th iteration => 0.00600058924078\n",
      "Loss for  49369 th iteration => 0.00600043504475\n",
      "Loss for  49370 th iteration => 0.00600034095801\n",
      "Loss for  49371 th iteration => 0.00600020461471\n",
      "Loss for  49372 th iteration => 0.00600005056027\n",
      "Loss for  49373 th iteration => 0.00599992923007\n",
      "Loss for  49374 th iteration => 0.00599983797615\n",
      "Loss for  49375 th iteration => 0.0059996652964\n",
      "Loss for  49376 th iteration => 0.0059995666051\n",
      "Loss for  49377 th iteration => 0.00599941568508\n",
      "Loss for  49378 th iteration => 0.00599925451382\n",
      "Loss for  49379 th iteration => 0.00599912263741\n",
      "Loss for  49380 th iteration => 0.00599899154939\n",
      "Loss for  49381 th iteration => 0.00599889855791\n",
      "Loss for  49382 th iteration => 0.00599874213653\n",
      "Loss for  49383 th iteration => 0.00599861646735\n",
      "Loss for  49384 th iteration => 0.0059984926612\n",
      "Loss for  49385 th iteration => 0.00599833575669\n",
      "Loss for  49386 th iteration => 0.00599820551193\n",
      "Loss for  49387 th iteration => 0.00599810423244\n",
      "Loss for  49388 th iteration => 0.00599795376413\n",
      "Loss for  49389 th iteration => 0.00599782529827\n",
      "Loss for  49390 th iteration => 0.00599770357223\n",
      "Loss for  49391 th iteration => 0.00599759383671\n",
      "Loss for  49392 th iteration => 0.0059974332958\n",
      "Loss for  49393 th iteration => 0.00599731066541\n",
      "Loss for  49394 th iteration => 0.00599718483191\n",
      "Loss for  49395 th iteration => 0.0059970276874\n",
      "Loss for  49396 th iteration => 0.00599693629697\n",
      "Loss for  49397 th iteration => 0.00599676280001\n",
      "Loss for  49398 th iteration => 0.00599663362553\n",
      "Loss for  49399 th iteration => 0.00599654125096\n",
      "Loss for  49400 th iteration => 0.00599638439027\n",
      "Loss for  49401 th iteration => 0.00599625941073\n",
      "Loss for  49402 th iteration => 0.00599613509275\n",
      "Loss for  49403 th iteration => 0.00599597774995\n",
      "Loss for  49404 th iteration => 0.0059958887371\n",
      "Loss for  49405 th iteration => 0.00599574797907\n",
      "Loss for  49406 th iteration => 0.00599559863755\n",
      "Loss for  49407 th iteration => 0.00599551069683\n",
      "Loss for  49408 th iteration => 0.00599536234888\n",
      "Loss for  49409 th iteration => 0.00599521246749\n",
      "Loss for  49410 th iteration => 0.0059951258361\n",
      "Loss for  49411 th iteration => 0.00599493200555\n",
      "Loss for  49412 th iteration => 0.00599480031256\n",
      "Loss for  49413 th iteration => 0.0059946686301\n",
      "Loss for  49414 th iteration => 0.00599453981006\n",
      "Loss for  49415 th iteration => 0.00599444337521\n",
      "Loss for  49416 th iteration => 0.0059942907581\n",
      "Loss for  49417 th iteration => 0.00599416170189\n",
      "Loss for  49418 th iteration => 0.00599404164381\n",
      "Loss for  49419 th iteration => 0.00599388314326\n",
      "Loss for  49420 th iteration => 0.00599378465167\n",
      "Loss for  49421 th iteration => 0.00599363177\n",
      "Loss for  49422 th iteration => 0.00599350607409\n",
      "Loss for  49423 th iteration => 0.00599338035734\n",
      "Loss for  49424 th iteration => 0.00599323204827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  49425 th iteration => 0.00599314037978\n",
      "Loss for  49426 th iteration => 0.0059929840144\n",
      "Loss for  49427 th iteration => 0.00599285762853\n",
      "Loss for  49428 th iteration => 0.00599273590904\n",
      "Loss for  49429 th iteration => 0.00599257507002\n",
      "Loss for  49430 th iteration => 0.00599248773281\n",
      "Loss for  49431 th iteration => 0.00599231323969\n",
      "Loss for  49432 th iteration => 0.00599222409381\n",
      "Loss for  49433 th iteration => 0.0059920643798\n",
      "Loss for  49434 th iteration => 0.00599194260876\n",
      "Loss for  49435 th iteration => 0.0059918154576\n",
      "Loss for  49436 th iteration => 0.00599166130339\n",
      "Loss for  49437 th iteration => 0.0059915682643\n",
      "Loss for  49438 th iteration => 0.00599143155988\n",
      "Loss for  49439 th iteration => 0.00599127854744\n",
      "Loss for  49440 th iteration => 0.00599119604374\n",
      "Loss for  49441 th iteration => 0.00599104264107\n",
      "Loss for  49442 th iteration => 0.00599089815975\n",
      "Loss for  49443 th iteration => 0.0059908065106\n",
      "Loss for  49444 th iteration => 0.00599061559694\n",
      "Loss for  49445 th iteration => 0.00599048408733\n",
      "Loss for  49446 th iteration => 0.00599035258827\n",
      "Loss for  49447 th iteration => 0.00599022259214\n",
      "Loss for  49448 th iteration => 0.00599012890885\n",
      "Loss for  49449 th iteration => 0.00598997391327\n",
      "Loss for  49450 th iteration => 0.00598984759389\n",
      "Loss for  49451 th iteration => 0.00598972517207\n",
      "Loss for  49452 th iteration => 0.00598956796447\n",
      "Loss for  49453 th iteration => 0.00598943881917\n",
      "Loss for  49454 th iteration => 0.00598933675091\n",
      "Loss for  49455 th iteration => 0.00598918780836\n",
      "Loss for  49456 th iteration => 0.00598905858905\n",
      "Loss for  49457 th iteration => 0.00598893925248\n",
      "Loss for  49458 th iteration => 0.00598882773993\n",
      "Loss for  49459 th iteration => 0.00598866901451\n",
      "Loss for  49460 th iteration => 0.00598854534719\n",
      "Loss for  49461 th iteration => 0.00598842128178\n",
      "Loss for  49462 th iteration => 0.00598826314738\n",
      "Loss for  49463 th iteration => 0.00598817347817\n",
      "Loss for  49464 th iteration => 0.00598799951606\n",
      "Loss for  49465 th iteration => 0.00598787160372\n",
      "Loss for  49466 th iteration => 0.00598777813996\n",
      "Loss for  49467 th iteration => 0.00598762310022\n",
      "Loss for  49468 th iteration => 0.00598749707715\n",
      "Loss for  49469 th iteration => 0.00598737453459\n",
      "Loss for  49470 th iteration => 0.00598721696936\n",
      "Loss for  49471 th iteration => 0.00598716190592\n",
      "Loss for  49472 th iteration => 0.0059869659802\n",
      "Loss for  49473 th iteration => 0.00598684597205\n",
      "Loss for  49474 th iteration => 0.00598675550322\n",
      "Loss for  49475 th iteration => 0.00598658315399\n",
      "Loss for  49476 th iteration => 0.00598648525413\n",
      "Loss for  49477 th iteration => 0.00598633377834\n",
      "Loss for  49478 th iteration => 0.00598617397289\n",
      "Loss for  49479 th iteration => 0.00598604265687\n",
      "Loss for  49480 th iteration => 0.00598591155854\n",
      "Loss for  49481 th iteration => 0.00598582044646\n",
      "Loss for  49482 th iteration => 0.00598566325162\n",
      "Loss for  49483 th iteration => 0.00598553948964\n",
      "Loss for  49484 th iteration => 0.00598541488237\n",
      "Loss for  49485 th iteration => 0.00598525913614\n",
      "Loss for  49486 th iteration => 0.00598512909257\n",
      "Loss for  49487 th iteration => 0.00598502921467\n",
      "Loss for  49488 th iteration => 0.00598487845223\n",
      "Loss for  49489 th iteration => 0.00598475141312\n",
      "Loss for  49490 th iteration => 0.00598462901809\n",
      "Loss for  49491 th iteration => 0.00598452104341\n",
      "Loss for  49492 th iteration => 0.00598436022706\n",
      "Loss for  49493 th iteration => 0.00598423901027\n",
      "Loss for  49494 th iteration => 0.00598411286514\n",
      "Loss for  49495 th iteration => 0.00598395717023\n",
      "Loss for  49496 th iteration => 0.00598386543234\n",
      "Loss for  49497 th iteration => 0.00598369289006\n",
      "Loss for  49498 th iteration => 0.00598356410124\n",
      "Loss for  49499 th iteration => 0.005983472716\n",
      "Loss for  49500 th iteration => 0.00598331596748\n",
      "Loss for  49501 th iteration => 0.00598319201443\n",
      "Loss for  49502 th iteration => 0.00598306777159\n",
      "Loss for  49503 th iteration => 0.00598291149212\n",
      "Loss for  49504 th iteration => 0.00598282247786\n",
      "Loss for  49505 th iteration => 0.00598268262238\n",
      "Loss for  49506 th iteration => 0.00598253371407\n",
      "Loss for  49507 th iteration => 0.00598244652385\n",
      "Loss for  49508 th iteration => 0.00598229850604\n",
      "Loss for  49509 th iteration => 0.0059821494533\n",
      "Loss for  49510 th iteration => 0.0059820630746\n",
      "Loss for  49511 th iteration => 0.00598186991313\n",
      "Loss for  49512 th iteration => 0.00598173878035\n",
      "Loss for  49513 th iteration => 0.0059816076581\n",
      "Loss for  49514 th iteration => 0.00598147954856\n",
      "Loss for  49515 th iteration => 0.00598138339863\n",
      "Loss for  49516 th iteration => 0.00598123159417\n",
      "Loss for  49517 th iteration => 0.0059811028681\n",
      "Loss for  49518 th iteration => 0.00598098357763\n",
      "Loss for  49519 th iteration => 0.00598082574436\n",
      "Loss for  49520 th iteration => 0.00598072738054\n",
      "Loss for  49521 th iteration => 0.00598057547176\n",
      "Loss for  49522 th iteration => 0.0059804499426\n",
      "Loss for  49523 th iteration => 0.00598032545581\n",
      "Loss for  49524 th iteration => 0.0059802200574\n",
      "Loss for  49525 th iteration => 0.00598005779969\n",
      "Loss for  49526 th iteration => 0.00597993838794\n",
      "Loss for  49527 th iteration => 0.00597981080522\n",
      "Loss for  49528 th iteration => 0.00597965691166\n",
      "Loss for  49529 th iteration => 0.00597956373989\n",
      "Loss for  49530 th iteration => 0.0059793923269\n",
      "Loss for  49531 th iteration => 0.00597926288751\n",
      "Loss for  49532 th iteration => 0.00597917308398\n",
      "Loss for  49533 th iteration => 0.00597901512046\n",
      "Loss for  49534 th iteration => 0.00597889274721\n",
      "Loss for  49535 th iteration => 0.00597876729131\n",
      "Loss for  49536 th iteration => 0.00597861258977\n",
      "Loss for  49537 th iteration => 0.00597852175398\n",
      "Loss for  49538 th iteration => 0.00597838389743\n",
      "Loss for  49539 th iteration => 0.00597823341509\n",
      "Loss for  49540 th iteration => 0.00597814863939\n",
      "Loss for  49541 th iteration => 0.00597799857279\n",
      "Loss for  49542 th iteration => 0.00597785193348\n",
      "Loss for  49543 th iteration => 0.0059777635072\n",
      "Loss for  49544 th iteration => 0.00597757191505\n",
      "Loss for  49545 th iteration => 0.00597744096546\n",
      "Loss for  49546 th iteration => 0.00597731002641\n",
      "Loss for  49547 th iteration => 0.00597718142467\n",
      "Loss for  49548 th iteration => 0.00597708651693\n",
      "Loss for  49549 th iteration => 0.00597693383527\n",
      "Loss for  49550 th iteration => 0.00597680635335\n",
      "Loss for  49551 th iteration => 0.00597668618377\n",
      "Loss for  49552 th iteration => 0.00597652875271\n",
      "Loss for  49553 th iteration => 0.00597640093676\n",
      "Loss for  49554 th iteration => 0.00597629777273\n",
      "Loss for  49555 th iteration => 0.00597615102001\n",
      "Loss for  49556 th iteration => 0.00597602075715\n",
      "Loss for  49557 th iteration => 0.00597590462905\n",
      "Loss for  49558 th iteration => 0.00597579062194\n",
      "Loss for  49559 th iteration => 0.00597563475052\n",
      "Loss for  49560 th iteration => 0.00597550938503\n",
      "Loss for  49561 th iteration => 0.00597538810306\n",
      "Loss for  49562 th iteration => 0.00597522834053\n",
      "Loss for  49563 th iteration => 0.00597514138498\n",
      "Loss for  49564 th iteration => 0.00597496785894\n",
      "Loss for  49565 th iteration => 0.00597487936218\n",
      "Loss for  49566 th iteration => 0.00597472045673\n",
      "Loss for  49567 th iteration => 0.00597459939152\n",
      "Loss for  49568 th iteration => 0.00597447299244\n",
      "Loss for  49569 th iteration => 0.00597431960022\n",
      "Loss for  49570 th iteration => 0.00597422731649\n",
      "Loss for  49571 th iteration => 0.00597409109951\n",
      "Loss for  49572 th iteration => 0.00597393941936\n",
      "Loss for  49573 th iteration => 0.0059738565115\n",
      "Loss for  49574 th iteration => 0.00597370493949\n",
      "Loss for  49575 th iteration => 0.00597356017328\n",
      "Loss for  49576 th iteration => 0.00597347023649\n",
      "Loss for  49577 th iteration => 0.00597327997792\n",
      "Loss for  49578 th iteration => 0.0059731492113\n",
      "Loss for  49579 th iteration => 0.00597301845523\n",
      "Loss for  49580 th iteration => 0.00597288946925\n",
      "Loss for  49581 th iteration => 0.00597279555304\n",
      "Loss for  49582 th iteration => 0.00597264224308\n",
      "Loss for  49583 th iteration => 0.00597251575753\n",
      "Loss for  49584 th iteration => 0.00597239495484\n",
      "Loss for  49585 th iteration => 0.00597223828001\n",
      "Loss for  49586 th iteration => 0.00597211026256\n",
      "Loss for  49587 th iteration => 0.00597200776619\n",
      "Loss for  49588 th iteration => 0.0059718607091\n",
      "Loss for  49589 th iteration => 0.00597173111882\n",
      "Loss for  49590 th iteration => 0.00597161420912\n",
      "Loss for  49591 th iteration => 0.0059715013477\n",
      "Loss for  49592 th iteration => 0.00597134508254\n",
      "Loss for  49593 th iteration => 0.0059712204804\n",
      "Loss for  49594 th iteration => 0.005971098797\n",
      "Loss for  49595 th iteration => 0.0059709398055\n",
      "Loss for  49596 th iteration => 0.00597085244088\n",
      "Loss for  49597 th iteration => 0.00597067915876\n",
      "Loss for  49598 th iteration => 0.00597059133057\n",
      "Loss for  49599 th iteration => 0.00597043211819\n",
      "Loss for  49600 th iteration => 0.00597031172985\n",
      "Loss for  49601 th iteration => 0.0059701850156\n",
      "Loss for  49602 th iteration => 0.00597003230849\n",
      "Loss for  49603 th iteration => 0.00596993943459\n",
      "Loss for  49604 th iteration => 0.00596980405611\n",
      "Loss for  49605 th iteration => 0.00596965199022\n",
      "Loss for  49606 th iteration => 0.00596956978857\n",
      "Loss for  49607 th iteration => 0.00596941786657\n",
      "Loss for  49608 th iteration => 0.00596927382559\n",
      "Loss for  49609 th iteration => 0.00596918351996\n",
      "Loss for  49610 th iteration => 0.00596899407255\n",
      "Loss for  49611 th iteration => 0.0059688634889\n",
      "Loss for  49612 th iteration => 0.0059687329158\n",
      "Loss for  49613 th iteration => 0.00596860381333\n",
      "Loss for  49614 th iteration => 0.00596851030336\n",
      "Loss for  49615 th iteration => 0.00596835694742\n",
      "Loss for  49616 th iteration => 0.0059682308794\n",
      "Loss for  49617 th iteration => 0.00596811001948\n",
      "Loss for  49618 th iteration => 0.00596795383759\n",
      "Loss for  49619 th iteration => 0.00596782583441\n",
      "Loss for  49620 th iteration => 0.00596772353348\n",
      "Loss for  49621 th iteration => 0.00596757664185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  49622 th iteration => 0.00596744725702\n",
      "Loss for  49623 th iteration => 0.00596733029864\n",
      "Loss for  49624 th iteration => 0.00596721778321\n",
      "Loss for  49625 th iteration => 0.00596706170886\n",
      "Loss for  49626 th iteration => 0.00596693728898\n",
      "Loss for  49627 th iteration => 0.00596681578222\n",
      "Loss for  49628 th iteration => 0.00596665698706\n",
      "Loss for  49629 th iteration => 0.00596656978507\n",
      "Loss for  49630 th iteration => 0.0059663967293\n",
      "Loss for  49631 th iteration => 0.00596630903625\n",
      "Loss for  49632 th iteration => 0.00596615004761\n",
      "Loss for  49633 th iteration => 0.00596602980857\n",
      "Loss for  49634 th iteration => 0.00596590330395\n",
      "Loss for  49635 th iteration => 0.00596575076017\n",
      "Loss for  49636 th iteration => 0.00596565801102\n",
      "Loss for  49637 th iteration => 0.00596552280439\n",
      "Loss for  49638 th iteration => 0.00596537102744\n",
      "Loss for  49639 th iteration => 0.00596528856445\n",
      "Loss for  49640 th iteration => 0.00596513725467\n",
      "Loss for  49641 th iteration => 0.00596499298287\n",
      "Loss for  49642 th iteration => 0.0059649032592\n",
      "Loss for  49643 th iteration => 0.00596471419077\n",
      "Loss for  49644 th iteration => 0.00596458379003\n",
      "Loss for  49645 th iteration => 0.00596445339984\n",
      "Loss for  49646 th iteration => 0.00596432440307\n",
      "Loss for  49647 th iteration => 0.00596423081004\n",
      "Loss for  49648 th iteration => 0.00596407789482\n",
      "Loss for  49649 th iteration => 0.00596395176037\n",
      "Loss for  49650 th iteration => 0.0059638313246\n",
      "Loss for  49651 th iteration => 0.00596367541703\n",
      "Loss for  49652 th iteration => 0.00596354760648\n",
      "Loss for  49653 th iteration => 0.00596344510755\n",
      "Loss for  49654 th iteration => 0.00596329877271\n",
      "Loss for  49655 th iteration => 0.00596316920413\n",
      "Loss for  49656 th iteration => 0.00596305280579\n",
      "Loss for  49657 th iteration => 0.00596293997163\n",
      "Loss for  49658 th iteration => 0.00596278457478\n",
      "Loss for  49659 th iteration => 0.00596265985322\n",
      "Loss for  49660 th iteration => 0.0059625390044\n",
      "Loss for  49661 th iteration => 0.00596238016791\n",
      "Loss for  49662 th iteration => 0.0059622498695\n",
      "Loss for  49663 th iteration => 0.00596212001684\n",
      "Loss for  49664 th iteration => 0.00596203170016\n",
      "Loss for  49665 th iteration => 0.00596187371085\n",
      "Loss for  49666 th iteration => 0.00596175280919\n",
      "Loss for  49667 th iteration => 0.00596162734282\n",
      "Loss for  49668 th iteration => 0.00596147409798\n",
      "Loss for  49669 th iteration => 0.00596138271851\n",
      "Loss for  49670 th iteration => 0.0059612464869\n",
      "Loss for  49671 th iteration => 0.00596109587478\n",
      "Loss for  49672 th iteration => 0.00596097077984\n",
      "Loss for  49673 th iteration => 0.00596084904268\n",
      "Loss for  49674 th iteration => 0.00596074278523\n",
      "Loss for  49675 th iteration => 0.00596061552331\n",
      "Loss for  49676 th iteration => 0.00596046339395\n",
      "Loss for  49677 th iteration => 0.00596033683592\n",
      "Loss for  49678 th iteration => 0.00596018369687\n",
      "Loss for  49679 th iteration => 0.00596009297093\n",
      "Loss for  49680 th iteration => 0.00595995708796\n",
      "Loss for  49681 th iteration => 0.00595980505361\n",
      "Loss for  49682 th iteration => 0.00595967841486\n",
      "Loss for  49683 th iteration => 0.00595955883876\n",
      "Loss for  49684 th iteration => 0.00595940301014\n",
      "Loss for  49685 th iteration => 0.00595927552872\n",
      "Loss for  49686 th iteration => 0.00595917252365\n",
      "Loss for  49687 th iteration => 0.00595902705182\n",
      "Loss for  49688 th iteration => 0.00595889699499\n",
      "Loss for  49689 th iteration => 0.00595878165604\n",
      "Loss for  49690 th iteration => 0.00595866793134\n",
      "Loss for  49691 th iteration => 0.00595851364737\n",
      "Loss for  49692 th iteration => 0.00595838819102\n",
      "Loss for  49693 th iteration => 0.00595826843089\n",
      "Loss for  49694 th iteration => 0.00595810958244\n",
      "Loss for  49695 th iteration => 0.00595797946675\n",
      "Loss for  49696 th iteration => 0.00595785000083\n",
      "Loss for  49697 th iteration => 0.00595776096907\n",
      "Loss for  49698 th iteration => 0.00595760404904\n",
      "Loss for  49699 th iteration => 0.00595748245564\n",
      "Loss for  49700 th iteration => 0.00595735803527\n",
      "Loss for  49701 th iteration => 0.00595720412181\n",
      "Loss for  49702 th iteration => 0.0059571139781\n",
      "Loss for  49703 th iteration => 0.00595697691149\n",
      "Loss for  49704 th iteration => 0.0059568273146\n",
      "Loss for  49705 th iteration => 0.00595670157987\n",
      "Loss for  49706 th iteration => 0.00595658113546\n",
      "Loss for  49707 th iteration => 0.00595647365075\n",
      "Loss for  49708 th iteration => 0.00595631406471\n",
      "Loss for  49709 th iteration => 0.0059561941083\n",
      "Loss for  49710 th iteration => 0.00595606903007\n",
      "Loss for  49711 th iteration => 0.00595591475765\n",
      "Loss for  49712 th iteration => 0.00595582596099\n",
      "Loss for  49713 th iteration => 0.00595568850011\n",
      "Loss for  49714 th iteration => 0.0059555380243\n",
      "Loss for  49715 th iteration => 0.00595541017508\n",
      "Loss for  49716 th iteration => 0.00595529217766\n",
      "Loss for  49717 th iteration => 0.00595513609153\n",
      "Loss for  49718 th iteration => 0.00595500610751\n",
      "Loss for  49719 th iteration => 0.00595487729368\n",
      "Loss for  49720 th iteration => 0.00595478295793\n",
      "Loss for  49721 th iteration => 0.00595462918204\n",
      "Loss for  49722 th iteration => 0.00595451144335\n",
      "Loss for  49723 th iteration => 0.00595442205136\n",
      "Loss for  49724 th iteration => 0.00595425137561\n",
      "Loss for  49725 th iteration => 0.0059541545799\n",
      "Loss for  49726 th iteration => 0.00595400403339\n",
      "Loss for  49727 th iteration => 0.00595384499448\n",
      "Loss for  49728 th iteration => 0.00595371506138\n",
      "Loss for  49729 th iteration => 0.00595358620728\n",
      "Loss for  49730 th iteration => 0.00595349596279\n",
      "Loss for  49731 th iteration => 0.0059533406074\n",
      "Loss for  49732 th iteration => 0.00595321782903\n",
      "Loss for  49733 th iteration => 0.00595309494563\n",
      "Loss for  49734 th iteration => 0.00595293987466\n",
      "Loss for  49735 th iteration => 0.00595285155775\n",
      "Loss for  49736 th iteration => 0.0059527131977\n",
      "Loss for  49737 th iteration => 0.00595256485269\n",
      "Loss for  49738 th iteration => 0.00595243824303\n",
      "Loss for  49739 th iteration => 0.00595231948818\n",
      "Loss for  49740 th iteration => 0.00595221038408\n",
      "Loss for  49741 th iteration => 0.00595205246885\n",
      "Loss for  49742 th iteration => 0.00595193122268\n",
      "Loss for  49743 th iteration => 0.00595180778479\n",
      "Loss for  49744 th iteration => 0.00595165225286\n",
      "Loss for  49745 th iteration => 0.00595156554776\n",
      "Loss for  49746 th iteration => 0.00595142633434\n",
      "Loss for  49747 th iteration => 0.0059512775384\n",
      "Loss for  49748 th iteration => 0.00595114838949\n",
      "Loss for  49749 th iteration => 0.00595103204294\n",
      "Loss for  49750 th iteration => 0.00595087568047\n",
      "Loss for  49751 th iteration => 0.00595074587888\n",
      "Loss for  49752 th iteration => 0.00595061761257\n",
      "Loss for  49753 th iteration => 0.00595052226614\n",
      "Loss for  49754 th iteration => 0.00595037010871\n",
      "Loss for  49755 th iteration => 0.00595029090673\n",
      "Loss for  49756 th iteration => 0.0059501375793\n",
      "Loss for  49757 th iteration => 0.00594999663477\n",
      "Loss for  49758 th iteration => 0.00594990482788\n",
      "Loss for  49759 th iteration => 0.00594971634152\n",
      "Loss for  49760 th iteration => 0.00594962711472\n",
      "Loss for  49761 th iteration => 0.00594945727026\n",
      "Loss for  49762 th iteration => 0.00594932909044\n",
      "Loss for  49763 th iteration => 0.00594923748969\n",
      "Loss for  49764 th iteration => 0.0059490838204\n",
      "Loss for  49765 th iteration => 0.00594895977772\n",
      "Loss for  49766 th iteration => 0.00594883848872\n",
      "Loss for  49767 th iteration => 0.0059486831275\n",
      "Loss for  49768 th iteration => 0.00594855616238\n",
      "Loss for  49769 th iteration => 0.00594845530024\n",
      "Loss for  49770 th iteration => 0.0059483103667\n",
      "Loss for  49771 th iteration => 0.00594821854515\n",
      "Loss for  49772 th iteration => 0.00594807799308\n",
      "Loss for  49773 th iteration => 0.00594792683125\n",
      "Loss for  49774 th iteration => 0.00594781064203\n",
      "Loss for  49775 th iteration => 0.0059476535866\n",
      "Loss for  49776 th iteration => 0.00594752391025\n",
      "Loss for  49777 th iteration => 0.00594739424447\n",
      "Loss for  49778 th iteration => 0.00594726458922\n",
      "Loss for  49779 th iteration => 0.00594713595632\n",
      "Loss for  49780 th iteration => 0.00594704872832\n",
      "Loss for  49781 th iteration => 0.00594689088376\n",
      "Loss for  49782 th iteration => 0.0059467711802\n",
      "Loss for  49783 th iteration => 0.00594664574951\n",
      "Loss for  49784 th iteration => 0.00594649381092\n",
      "Loss for  49785 th iteration => 0.00594640290307\n",
      "Loss for  49786 th iteration => 0.00594626709443\n",
      "Loss for  49787 th iteration => 0.00594611799819\n",
      "Loss for  49788 th iteration => 0.00594603457337\n",
      "Loss for  49789 th iteration => 0.00594588579774\n",
      "Loss for  49790 th iteration => 0.00594574070769\n",
      "Loss for  49791 th iteration => 0.0059456533757\n",
      "Loss for  49792 th iteration => 0.00594546330916\n",
      "Loss for  49793 th iteration => 0.00594533373067\n",
      "Loss for  49794 th iteration => 0.0059452041627\n",
      "Loss for  49795 th iteration => 0.00594507715245\n",
      "Loss for  49796 th iteration => 0.00594498311829\n",
      "Loss for  49797 th iteration => 0.00594483224817\n",
      "Loss for  49798 th iteration => 0.00594470575326\n",
      "Loss for  49799 th iteration => 0.00594458728222\n",
      "Loss for  49800 th iteration => 0.00594443169299\n",
      "Loss for  49801 th iteration => 0.00594433425933\n",
      "Loss for  49802 th iteration => 0.00594418447797\n",
      "Loss for  49803 th iteration => 0.0059440599795\n",
      "Loss for  49804 th iteration => 0.005943937835\n",
      "Loss for  49805 th iteration => 0.0059438327686\n",
      "Loss for  49806 th iteration => 0.00594367317424\n",
      "Loss for  49807 th iteration => 0.00594355427716\n",
      "Loss for  49808 th iteration => 0.00594342921858\n",
      "Loss for  49809 th iteration => 0.00594327597802\n",
      "Loss for  49810 th iteration => 0.00594318519265\n",
      "Loss for  49811 th iteration => 0.00594301518709\n",
      "Loss for  49812 th iteration => 0.00594288778128\n",
      "Loss for  49813 th iteration => 0.00594279798894\n",
      "Loss for  49814 th iteration => 0.00594264305343\n",
      "Loss for  49815 th iteration => 0.00594252082893\n",
      "Loss for  49816 th iteration => 0.005942398264\n",
      "Loss for  49817 th iteration => 0.00594224399229\n",
      "Loss for  49818 th iteration => 0.00594218907017\n",
      "Loss for  49819 th iteration => 0.00594199629076\n",
      "Loss for  49820 th iteration => 0.00594187795871\n",
      "Loss for  49821 th iteration => 0.00594178899997\n",
      "Loss for  49822 th iteration => 0.0059416189612\n",
      "Loss for  49823 th iteration => 0.00594152260839\n",
      "Loss for  49824 th iteration => 0.00594137355293\n",
      "Loss for  49825 th iteration => 0.00594121644025\n",
      "Loss for  49826 th iteration => 0.00594108704416\n",
      "Loss for  49827 th iteration => 0.00594095765857\n",
      "Loss for  49828 th iteration => 0.00594083190792\n",
      "Loss for  49829 th iteration => 0.00594073523473\n",
      "Loss for  49830 th iteration => 0.00594058734809\n",
      "Loss for  49831 th iteration => 0.00594045825716\n",
      "Loss for  49832 th iteration => 0.00594034272671\n",
      "Loss for  49833 th iteration => 0.0059401870908\n",
      "Loss for  49834 th iteration => 0.00594008765804\n",
      "Loss for  49835 th iteration => 0.00593994022383\n",
      "Loss for  49836 th iteration => 0.00593981376132\n",
      "Loss for  49837 th iteration => 0.00593969503674\n",
      "Loss for  49838 th iteration => 0.00593958643221\n",
      "Loss for  49839 th iteration => 0.00593942993045\n",
      "Loss for  49840 th iteration => 0.00593930832982\n",
      "Loss for  49841 th iteration => 0.00593918631773\n",
      "Loss for  49842 th iteration => 0.00593903041934\n",
      "Loss for  49843 th iteration => 0.00593894263487\n",
      "Loss for  49844 th iteration => 0.00593877175806\n",
      "Loss for  49845 th iteration => 0.00593864561135\n",
      "Loss for  49846 th iteration => 0.00593855316561\n",
      "Loss for  49847 th iteration => 0.00593840122755\n",
      "Loss for  49848 th iteration => 0.00593827639269\n",
      "Loss for  49849 th iteration => 0.00593815678229\n",
      "Loss for  49850 th iteration => 0.00593800130411\n",
      "Loss for  49851 th iteration => 0.00593790549051\n",
      "Loss for  49852 th iteration => 0.00593775461528\n",
      "Loss for  49853 th iteration => 0.00593763532886\n",
      "Loss for  49854 th iteration => 0.00593754568544\n",
      "Loss for  49855 th iteration => 0.00593737668684\n",
      "Loss for  49856 th iteration => 0.00593727964835\n",
      "Loss for  49857 th iteration => 0.00593713313595\n",
      "Loss for  49858 th iteration => 0.00593697492877\n",
      "Loss for  49859 th iteration => 0.0059368457115\n",
      "Loss for  49860 th iteration => 0.00593671650476\n",
      "Loss for  49861 th iteration => 0.00593658730847\n",
      "Loss for  49862 th iteration => 0.00593646081461\n",
      "Loss for  49863 th iteration => 0.0059363724818\n",
      "Loss for  49864 th iteration => 0.00593621661092\n",
      "Loss for  49865 th iteration => 0.00593609590464\n",
      "Loss for  49866 th iteration => 0.00593597484529\n",
      "Loss for  49867 th iteration => 0.00593584922705\n",
      "Loss for  49868 th iteration => 0.00593574240683\n",
      "Loss for  49869 th iteration => 0.00593557332251\n",
      "Loss for  49870 th iteration => 0.00593545842119\n",
      "Loss for  49871 th iteration => 0.00593534590019\n",
      "Loss for  49872 th iteration => 0.00593519273411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  49873 th iteration => 0.00593506818862\n",
      "Loss for  49874 th iteration => 0.00593494946228\n",
      "Loss for  49875 th iteration => 0.00593479186888\n",
      "Loss for  49876 th iteration => 0.00593466274641\n",
      "Loss for  49877 th iteration => 0.00593453447984\n",
      "Loss for  49878 th iteration => 0.00593444546279\n",
      "Loss for  49879 th iteration => 0.00593429047223\n",
      "Loss for  49880 th iteration => 0.00593416897824\n",
      "Loss for  49881 th iteration => 0.00593404640298\n",
      "Loss for  49882 th iteration => 0.00593389267277\n",
      "Loss for  49883 th iteration => 0.0059338044651\n",
      "Loss for  49884 th iteration => 0.00593366744477\n",
      "Loss for  49885 th iteration => 0.00593351964991\n",
      "Loss for  49886 th iteration => 0.00593339413541\n",
      "Loss for  49887 th iteration => 0.0059332758523\n",
      "Loss for  49888 th iteration => 0.00593316768419\n",
      "Loss for  49889 th iteration => 0.00593301060595\n",
      "Loss for  49890 th iteration => 0.00593289017801\n",
      "Loss for  49891 th iteration => 0.00593276750873\n",
      "Loss for  49892 th iteration => 0.00593261286299\n",
      "Loss for  49893 th iteration => 0.00593252709741\n",
      "Loss for  49894 th iteration => 0.00593238822268\n",
      "Loss for  49895 th iteration => 0.00593224078838\n",
      "Loss for  49896 th iteration => 0.0059321119328\n",
      "Loss for  49897 th iteration => 0.00593199688002\n",
      "Loss for  49898 th iteration => 0.00593184114732\n",
      "Loss for  49899 th iteration => 0.00593171215615\n",
      "Loss for  49900 th iteration => 0.00593158496404\n",
      "Loss for  49901 th iteration => 0.00593148974989\n",
      "Loss for  49902 th iteration => 0.00593133947922\n",
      "Loss for  49903 th iteration => 0.00593125942501\n",
      "Loss for  49904 th iteration => 0.00593110850016\n",
      "Loss for  49905 th iteration => 0.00593096684512\n",
      "Loss for  49906 th iteration => 0.00593087730019\n",
      "Loss for  49907 th iteration => 0.00593068887041\n",
      "Loss for  49908 th iteration => 0.00593055991971\n",
      "Loss for  49909 th iteration => 0.00593043097954\n",
      "Loss for  49910 th iteration => 0.00593030446972\n",
      "Loss for  49911 th iteration => 0.00593021170929\n",
      "Loss for  49912 th iteration => 0.00593006080003\n",
      "Loss for  49913 th iteration => 0.0059299356195\n",
      "Loss for  49914 th iteration => 0.00592981706885\n",
      "Loss for  49915 th iteration => 0.00592966211343\n",
      "Loss for  49916 th iteration => 0.00592956594655\n",
      "Loss for  49917 th iteration => 0.00592941613576\n",
      "Loss for  49918 th iteration => 0.00592929293944\n",
      "Loss for  49919 th iteration => 0.00592917071331\n",
      "Loss for  49920 th iteration => 0.00592906685831\n",
      "Loss for  49921 th iteration => 0.00592890747348\n",
      "Loss for  49922 th iteration => 0.00592878964684\n",
      "Loss for  49923 th iteration => 0.00592866474818\n",
      "Loss for  49924 th iteration => 0.00592851262736\n",
      "Loss for  49925 th iteration => 0.00592842195283\n",
      "Loss for  49926 th iteration => 0.00592825265225\n",
      "Loss for  49927 th iteration => 0.00592812614529\n",
      "Loss for  49928 th iteration => 0.00592803673179\n",
      "Loss for  49929 th iteration => 0.0059278826475\n",
      "Loss for  49930 th iteration => 0.0059277608519\n",
      "Loss for  49931 th iteration => 0.00592763908833\n",
      "Loss for  49932 th iteration => 0.00592748546058\n",
      "Loss for  49933 th iteration => 0.00592743120244\n",
      "Loss for  49934 th iteration => 0.00592723890802\n",
      "Loss for  49935 th iteration => 0.00592712150989\n",
      "Loss for  49936 th iteration => 0.00592703279523\n",
      "Loss for  49937 th iteration => 0.00592686376039\n",
      "Loss for  49938 th iteration => 0.00592676766567\n",
      "Loss for  49939 th iteration => 0.00592661944928\n",
      "Loss for  49940 th iteration => 0.00592646265793\n",
      "Loss for  49941 th iteration => 0.00592633388941\n",
      "Loss for  49942 th iteration => 0.00592620563104\n",
      "Loss for  49943 th iteration => 0.00592611548701\n",
      "Loss for  49944 th iteration => 0.00592596231255\n",
      "Loss for  49945 th iteration => 0.00592583973144\n",
      "Loss for  49946 th iteration => 0.00592571893255\n",
      "Loss for  49947 th iteration => 0.00592556562532\n",
      "Loss for  49948 th iteration => 0.00592543884026\n",
      "Loss for  49949 th iteration => 0.00592533915737\n",
      "Loss for  49950 th iteration => 0.00592519320296\n",
      "Loss for  49951 th iteration => 0.00592506654182\n",
      "Loss for  49952 th iteration => 0.00592495008465\n",
      "Loss for  49953 th iteration => 0.0059248401286\n",
      "Loss for  49954 th iteration => 0.00592468571139\n",
      "Loss for  49955 th iteration => 0.00592456331699\n",
      "Loss for  49956 th iteration => 0.0059244433194\n",
      "Loss for  49957 th iteration => 0.00592428669681\n",
      "Loss for  49958 th iteration => 0.00592420085753\n",
      "Loss for  49959 th iteration => 0.00592403029747\n",
      "Loss for  49960 th iteration => 0.00592394340926\n",
      "Loss for  49961 th iteration => 0.00592378715016\n",
      "Loss for  49962 th iteration => 0.00592366786391\n",
      "Loss for  49963 th iteration => 0.00592354394145\n",
      "Loss for  49964 th iteration => 0.00592339249697\n",
      "Loss for  49965 th iteration => 0.0059233025182\n",
      "Loss for  49966 th iteration => 0.0059231678577\n",
      "Loss for  49967 th iteration => 0.00592301978973\n",
      "Loss for  49968 th iteration => 0.00592293583779\n",
      "Loss for  49969 th iteration => 0.00592278947251\n",
      "Loss for  49970 th iteration => 0.00592264414655\n",
      "Loss for  49971 th iteration => 0.00592252526383\n",
      "Loss for  49972 th iteration => 0.00592237044407\n",
      "Loss for  49973 th iteration => 0.00592224184442\n",
      "Loss for  49974 th iteration => 0.00592211325531\n",
      "Loss for  49975 th iteration => 0.00592198734342\n",
      "Loss for  49976 th iteration => 0.00592189217031\n",
      "Loss for  49977 th iteration => 0.00592174437254\n",
      "Loss for  49978 th iteration => 0.00592161678501\n",
      "Loss for  49979 th iteration => 0.00592150134023\n",
      "Loss for  49980 th iteration => 0.00592134606453\n",
      "Loss for  49981 th iteration => 0.00592121751879\n",
      "Loss for  49982 th iteration => 0.00592109089868\n",
      "Loss for  49983 th iteration => 0.00592099652172\n",
      "Loss for  49984 th iteration => 0.00592084652842\n",
      "Loss for  49985 th iteration => 0.00592076768305\n",
      "Loss for  49986 th iteration => 0.00592061640929\n",
      "Loss for  49987 th iteration => 0.00592047602257\n",
      "Loss for  49988 th iteration => 0.00592038606979\n",
      "Loss for  49989 th iteration => 0.00592019808389\n",
      "Loss for  49990 th iteration => 0.00592011052847\n",
      "Loss for  49991 th iteration => 0.0059199414123\n",
      "Loss for  49992 th iteration => 0.00591981548413\n",
      "Loss for  49993 th iteration => 0.00591972367416\n",
      "Loss for  49994 th iteration => 0.00591957266851\n",
      "Loss for  49995 th iteration => 0.0059194485303\n",
      "Loss for  49996 th iteration => 0.00591932979167\n",
      "Loss for  49997 th iteration => 0.00591917535333\n",
      "Loss for  49998 th iteration => 0.00591908000419\n",
      "Loss for  49999 th iteration => 0.00591893023682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(numiter):\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = relu(Z1,play = \"forward\")\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2,play = \"forward\")\n",
    "   \n",
    "    L = (1.0/m) * -np.sum(np.multiply(y,np.log(A2)) + np.multiply(1-y,np.log(1-A2)))\n",
    "    \n",
    "    dZ2 = A2 - y\n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),relu(A1,play=\"backward\"))\n",
    "    dW2 = (1.0/m) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1.0/m) * np.sum(dZ2,axis = 1,keepdims = True)\n",
    "    dW1 = (1.0/m) * np.dot(dZ1,X.T)\n",
    "    db1 = (1.0/m) * np.sum(dZ1,axis = 1,keepdims = True)\n",
    "    W1 = W1 - learningrate * dW1\n",
    "    b1 = b1 - learningrate * db1\n",
    "    W2 = W2 - learningrate * dW2\n",
    "    b2 = b2 - learningrate* db2\n",
    "    print \"Loss for \",i,\"th iteration =>\",L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predictions\n",
    "\n",
    "\n",
    "def prediction(X,W1,b1,W2,b2):\n",
    "    Z1pred = np.dot(W1,X) + b1\n",
    "    A1pred = relu(Z1pred,play = \"forward\")\n",
    "    Z2pred = np.dot(W2,A1pred) + b2\n",
    "    A2pred = sigmoid(Z2pred,play = \"forward\")\n",
    "    prediction = []\n",
    "\n",
    "\n",
    "    for i in range(A2pred.shape[1]):\n",
    "        if (A2pred[0][i]>0.5):\n",
    "            prediction.append(1)\n",
    "        elif (A2pred[0][i] <= 0.5):\n",
    "            prediction.append(0)\n",
    "    N = len(prediction)\n",
    "    prediction = np.array(prediction)\n",
    "    prediction =prediction.reshape(1,N)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTrain = prediction(X,W1,b1,W2,b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truelabel = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Label and True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction label for traindata: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "True label for train data: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print \"prediction label for traindata:\",predictionTrain\n",
    "print \"True label for train data:\", Truelabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tc1 = [3,4]\n",
    "Tc2 = [14,16]\n",
    "TClass1 = np.matlib.repmat(Tc1, no,1) + np.random.randn(no,len(c1))\n",
    "TClass2 = np.matlib.repmat(Tc2, no,1) + np.random.randn(no,len(c2))\n",
    "TData = np.append(TClass1,TClass2,axis = 0)\n",
    "Testlabel  = np.append(np.zeros((no,1)),np.ones((no,1)),axis = 0)\n",
    "X2 = TData.T\n",
    "y2 = Testlabel.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/xeHGLhW+77dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a2d366950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(TClass1[:,0],TClass1[:,1],'ro')\n",
    "plt.plot(TClass2[:,0],TClass2[:,1],'bo')\n",
    "\n",
    "plt.ylabel('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction label for testndata: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "True label for test data: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "predictionTest = prediction(X2,W1,b1,W2,b2)\n",
    "print \"prediction label for testndata:\",predictionTest\n",
    "print \"True label for test data:\",y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(predictionTest)\n",
    "y_pred = y_pred.astype(int)\n",
    "y_true = np.array(y2)\n",
    "y_true = y_true.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true[0],y_pred[0])\n",
    "cm = cm.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
    "print \"Accuracy:\",Accuracy*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
